{
  "source_html": "html\\iclr-oral.html",
  "paper_count": 207,
  "conference": "iclr",
  "year": 2025,
  "status": "oral",
  "papers": [
    {
      "paper_id": "GMwRl2e9Y1@OpenReview",
      "index": 1,
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "authors": [
        "Christopher Fifty",
        "Ronald Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Re"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "vector",
        "codebook",
        "quantization",
        "encoder",
        "restructuring",
        "vaes",
        "rotation",
        "trick",
        "output",
        "layer"
      ],
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows _around_ the vector quantization layer rather than _through_ it in a straight-through approximation.This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder.Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GMwRl2e9Y1"
        ],
        "venue": [
          "/venue/GMwRl2e9Y1@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GMwRl2e9Y1"
        ],
        "detail": [
          "https://openreview.net/forum?id=GMwRl2e9Y1"
        ]
      },
      "scores": {
        "pdf": 380,
        "kimi": 461
      },
      "raw_excerpt": "Restructuring Vector Quantization with the Rotation Trick [PDF 380 ] [Copy] [Kimi 461 ] [REL] Authors : Christopher Fifty , Ronald Junkins , Dennis Duan , Aniketh Iyengar , Jerry Liu , Ehsan Amid , Sebastian Thrun , Christopher Re Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows _around_ the vector quantization layer rather than _through_ it in a straight-through approximation.This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder.Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "GRMfXcAAFh@OpenReview",
      "index": 2,
      "title": "Oscillatory State-Space Models",
      "authors": [
        "T. Konstantin Rusch",
        "Daniela Rus"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "linoss",
        "oscillatory",
        "long",
        "space",
        "state",
        "stable",
        "discretization",
        "range",
        "forecasting",
        "sequences"
      ],
      "summary": "We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with sequences of length 50k.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GRMfXcAAFh"
        ],
        "venue": [
          "/venue/GRMfXcAAFh@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GRMfXcAAFh"
        ],
        "detail": [
          "https://openreview.net/forum?id=GRMfXcAAFh"
        ]
      },
      "scores": {
        "pdf": 185,
        "kimi": 219
      },
      "raw_excerpt": "Oscillatory State-Space Models [PDF 185 ] [Copy] [Kimi 219 ] [REL] Authors : T. Konstantin Rusch , Daniela Rus We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with sequences of length 50k. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "cmfyMV45XO@OpenReview",
      "index": 3,
      "title": "Feedback Favors the Generalization of Neural ODEs",
      "authors": [
        "Jindou Jia",
        "Zihan Yang",
        "Meng Wang",
        "Kexin Guo",
        "Jianfei Yang",
        "Xiang Yu",
        "Lei Guo"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "feedback",
        "neural",
        "odes",
        "latent",
        "generalization",
        "favors",
        "neatly",
        "correct",
        "learned",
        "dynamics"
      ],
      "summary": "The well-known generalization problem hinders the application of artificial neural networks in continuous-time prediction tasks with varying latent dynamics. In sharp contrast, biological systems can neatly adapt to evolving environments benefiting from real-time feedback mechanisms. Inspired by the feedback philosophy, we present feedback neural networks, showing that a feedback loop can flexibly correct the learned latent dynamics of neural ordinary differential equations (neural ODEs), leading to a prominent generalization improvement. The feedback neural network is a novel two-DOF neural network, which possesses robust performance in unseen scenarios with no loss of accuracy performance on previous tasks. A linear feedback form is presented to correct the learned latent dynamics firstly, with a convergence guarantee. Then, domain randomization is utilized to learn a nonlinear neural feedback form. Finally, extensive tests including trajectory prediction of a real irregular object and model predictive control of a quadrotor with various uncertainties, are implemented, indicating significant improvements over state-of-the-art model-based and learning-based methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cmfyMV45XO"
        ],
        "venue": [
          "/venue/cmfyMV45XO@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cmfyMV45XO"
        ],
        "detail": [
          "https://openreview.net/forum?id=cmfyMV45XO"
        ]
      },
      "scores": {
        "pdf": 113,
        "kimi": 161
      },
      "raw_excerpt": "Feedback Favors the Generalization of Neural ODEs [PDF 113 ] [Copy] [Kimi 161 ] [REL] Authors : Jindou Jia , Zihan Yang , Meng Wang , Kexin Guo , Jianfei Yang , Xiang Yu , Lei Guo The well-known generalization problem hinders the application of artificial neural networks in continuous-time prediction tasks with varying latent dynamics. In sharp contrast, biological systems can neatly adapt to evolving environments benefiting from real-time feedback mechanisms. Inspired by the feedback philosophy, we present feedback neural networks, showing that a feedback loop can flexibly correct the learned latent dynamics of neural ordinary differential equations (neural ODEs), leading to a prominent generalization improvement. The feedback neural network is a novel two-DOF neural network, which possesses robust performance in unseen scenarios with no loss of accuracy performance on previous tasks. A linear feedback form is presented to correct the learned latent dynamics firstly, with a convergence guarantee. Then, domain randomization is utilized to learn a nonlinear neural feedback form. Finally, extensive tests including trajectory prediction of a real irregular object and model predictive control of a quadrotor with various uncertainties, are implemented, indicating significant improvements over state-of-the-art model-based and learning-based methods. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "k38Th3x4d9@OpenReview",
      "index": 4,
      "title": "Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery",
      "authors": [
        "Xiao Han",
        "Saima Absar",
        "Lu Zhang",
        "Shuhan Yuan"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "aerca",
        "root",
        "anomalies",
        "granger",
        "series",
        "exogenous",
        "causes",
        "causal",
        "multivariate",
        "cause"
      ],
      "summary": "Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies as interventions on the exogenous variables of time series, AERCA not only learns the Granger causality among time series but also explicitly models the distributions of exogenous variables under normal conditions. AERCA then identifies the root causes of anomalies by highlighting exogenous variables that significantly deviate from their normal states. Experiments on multiple synthetic and real-world datasets demonstrate that AERCA can accurately capture the causal relationships among time series and effectively identify the root causes of anomalies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k38Th3x4d9"
        ],
        "venue": [
          "/venue/k38Th3x4d9@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k38Th3x4d9"
        ],
        "detail": [
          "https://openreview.net/forum?id=k38Th3x4d9"
        ]
      },
      "scores": {
        "pdf": 79,
        "kimi": 111
      },
      "raw_excerpt": "Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery [PDF 79 ] [Copy] [Kimi 111 ] [REL] Authors : Xiao Han , Saima Absar , Lu Zhang , Shuhan Yuan Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies as interventions on the exogenous variables of time series, AERCA not only learns the Granger causality among time series but also explicitly models the distributions of exogenous variables under normal conditions. AERCA then identifies the root causes of anomalies by highlighting exogenous variables that significantly deviate from their normal states. Experiments on multiple synthetic and real-world datasets demonstrate that AERCA can accurately capture the causal relationships among time series and effectively identify the root causes of anomalies. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "rdv6yeMFpn@OpenReview",
      "index": 5,
      "title": "Homomorphism Expressivity of Spectral Invariant Graph Neural Networks",
      "authors": [
        "Jingchu Gai",
        "Yiheng Du",
        "Bohang Zhang",
        "Haggai Maron",
        "Liwei Wang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "gnns",
        "homomorphism",
        "spectral",
        "expressivity",
        "citet",
        "invariant",
        "graph",
        "invariants",
        "quantitative",
        "arvind2024hierarchy"
      ],
      "summary": "Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs --- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as \\emph{parallel trees}. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend \\citet{arvind2024hierarchy} and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by \\citet{zhang2024expressive}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rdv6yeMFpn"
        ],
        "venue": [
          "/venue/rdv6yeMFpn@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rdv6yeMFpn"
        ],
        "detail": [
          "https://openreview.net/forum?id=rdv6yeMFpn"
        ]
      },
      "scores": {
        "pdf": 75,
        "kimi": 97
      },
      "raw_excerpt": "Homomorphism Expressivity of Spectral Invariant Graph Neural Networks [PDF 75 ] [Copy] [Kimi 97 ] [REL] Authors : Jingchu Gai , Yiheng Du , Bohang Zhang , Haggai Maron , Liwei Wang Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs --- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as \\emph{parallel trees}. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend \\citet{arvind2024hierarchy} and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by \\citet{zhang2024expressive}. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "vf5aUZT0Fz@OpenReview",
      "index": 6,
      "title": "DEPT: Decoupled Embeddings for Pre-training Language Models",
      "authors": [
        "Alex Iacob",
        "Lorenzo Sani",
        "Meghdad Kurmanji",
        "William Shen",
        "Xinchi Qiu",
        "Dongqi Cai",
        "Yan Gao",
        "Nic Lane"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "dept",
        "training",
        "pre",
        "curse",
        "vocabulary",
        "billion",
        "embeddings",
        "decoupled",
        "syntactical",
        "semantical"
      ],
      "summary": "Past works have shown that lexical, syntactical, and semantical differences in heterogeneous data sources can cause challenges such as negative interference or the ``curse of multilinguality''. Because of this, training on such heterogeneous corpora requires extensive and costly efforts to balance data mixtures. We propose a novel pre-training framework to alleviate this curse. Our method, DEPT, decouples embeddings from the transformer body while simultaneously training the latter in multiple contexts without a shared global vocabulary. DEPT: (1) trains robustly and effectively under significant data heterogeneity, (2) reduces token embedding parameters by up to 80% and communication costs by 714x for billion-scale models, (3) enhances transformer body plasticity and generalization, improving average perplexity upward of 15.3-20% and improving performance for downstream fine-tuning in our experiments, and (4) permits training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated multilingual pre-training of a billion-scale model, reducing total parameters by 24% versus standard training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vf5aUZT0Fz"
        ],
        "venue": [
          "/venue/vf5aUZT0Fz@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vf5aUZT0Fz"
        ],
        "detail": [
          "https://openreview.net/forum?id=vf5aUZT0Fz"
        ]
      },
      "scores": {
        "pdf": 162,
        "kimi": 202
      },
      "raw_excerpt": "DEPT: Decoupled Embeddings for Pre-training Language Models [PDF 162 ] [Copy] [Kimi 202 ] [REL] Authors : Alex Iacob , Lorenzo Sani , Meghdad Kurmanji , William Shen , Xinchi Qiu , Dongqi Cai , Yan Gao , Nic Lane Past works have shown that lexical, syntactical, and semantical differences in heterogeneous data sources can cause challenges such as negative interference or the ``curse of multilinguality''. Because of this, training on such heterogeneous corpora requires extensive and costly efforts to balance data mixtures. We propose a novel pre-training framework to alleviate this curse. Our method, DEPT, decouples embeddings from the transformer body while simultaneously training the latter in multiple contexts without a shared global vocabulary. DEPT: (1) trains robustly and effectively under significant data heterogeneity, (2) reduces token embedding parameters by up to 80% and communication costs by 714x for billion-scale models, (3) enhances transformer body plasticity and generalization, improving average perplexity upward of 15.3-20% and improving performance for downstream fine-tuning in our experiments, and (4) permits training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated multilingual pre-training of a billion-scale model, reducing total parameters by 24% versus standard training. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "wM2sfVgMDH@OpenReview",
      "index": 7,
      "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
      "authors": [
        "Yinan Zheng",
        "Ruiming Liang",
        "Kexin ZHENG",
        "Jinliang Zheng",
        "Liyuan Mao",
        "Jianxiong Li",
        "Weihao Gu",
        "Rui Ai",
        "Shengbo Li",
        "Xianyuan Zhan",
        "Jingjing Liu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "planning",
        "driving",
        "planner",
        "behaviors",
        "diffusion",
        "autonomous",
        "guidance",
        "flexible",
        "nuplan",
        "modal"
      ],
      "summary": "Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wM2sfVgMDH"
        ],
        "venue": [
          "/venue/wM2sfVgMDH@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wM2sfVgMDH"
        ],
        "detail": [
          "https://openreview.net/forum?id=wM2sfVgMDH"
        ]
      },
      "scores": {
        "pdf": 101,
        "kimi": 101
      },
      "raw_excerpt": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance [PDF 101 ] [Copy] [Kimi 101 ] [REL] Authors : Yinan Zheng , Ruiming Liang , Kexin ZHENG , Jinliang Zheng , Liyuan Mao , Jianxiong Li , Weihao Gu , Rui Ai , Shengbo Li , Xianyuan Zhan , Jingjing Liu Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "CLE09ESvul@OpenReview",
      "index": 8,
      "title": "What should a neuron aim for? Designing local objective functions based on information theory",
      "authors": [
        "Andreas Schneider",
        "Valentin Neuhaus",
        "David Ehrlich",
        "Abdullah Makkeh",
        "Alexander S Ecker",
        "Viola Priesemann",
        "Michael Wibral"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "information",
        "local",
        "neurons",
        "neuron",
        "pid",
        "designing",
        "goals",
        "learning",
        "achieving",
        "redundantly"
      ],
      "summary": "In modern deep neural networks, the learning dynamics of the individual neurons is often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. We here show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e. feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CLE09ESvul"
        ],
        "venue": [
          "/venue/CLE09ESvul@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CLE09ESvul"
        ],
        "detail": [
          "https://openreview.net/forum?id=CLE09ESvul"
        ]
      },
      "scores": {
        "pdf": 77,
        "kimi": 119
      },
      "raw_excerpt": "What should a neuron aim for? Designing local objective functions based on information theory [PDF 77 ] [Copy] [Kimi 119 ] [REL] Authors : Andreas Schneider , Valentin Neuhaus , David Ehrlich , Abdullah Makkeh , Alexander S Ecker , Viola Priesemann , Michael Wibral In modern deep neural networks, the learning dynamics of the individual neurons is often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. We here show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e. feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "JDm7oIcx4Y@OpenReview",
      "index": 9,
      "title": "Accelerated training through iterative gradient propagation along the residual path",
      "authors": [
        "Erwan Fagnou",
        "Paul Caillon",
        "Blaise Delattre",
        "Alexandre Allauzen"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "backpropagation",
        "residual",
        "gradient",
        "highway",
        "iterative",
        "path",
        "sequentiality",
        "along",
        "architectures",
        "major"
      ],
      "summary": "Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.Such models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures.However, the computational cost of backpropagation remains a major burden, accounting for most of the training time.Taking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.Through an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JDm7oIcx4Y"
        ],
        "venue": [
          "/venue/JDm7oIcx4Y@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JDm7oIcx4Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=JDm7oIcx4Y"
        ]
      },
      "scores": {
        "pdf": 72,
        "kimi": 85
      },
      "raw_excerpt": "Accelerated training through iterative gradient propagation along the residual path [PDF 72 ] [Copy] [Kimi 85 ] [REL] Authors : Erwan Fagnou , Paul Caillon , Blaise Delattre , Alexandre Allauzen Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.Such models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures.However, the computational cost of backpropagation remains a major burden, accounting for most of the training time.Taking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.Through an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "VVixJ9QavY@OpenReview",
      "index": 10,
      "title": "Reasoning Elicitation in Language Models via Counterfactual Feedback",
      "authors": [
        "Alihan Hüyük",
        "Xinnuo Xu",
        "Jacqueline Maasch",
        "Aditya Nori",
        "Javier Hernandez"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "reasoning",
        "counterfactual",
        "language",
        "factual",
        "metrics",
        "elicitation",
        "fine",
        "models",
        "capabilities",
        "underdeveloped"
      ],
      "summary": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VVixJ9QavY"
        ],
        "venue": [
          "/venue/VVixJ9QavY@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VVixJ9QavY"
        ],
        "detail": [
          "https://openreview.net/forum?id=VVixJ9QavY"
        ]
      },
      "scores": {
        "pdf": 98,
        "kimi": 144
      },
      "raw_excerpt": "Reasoning Elicitation in Language Models via Counterfactual Feedback [PDF 98 ] [Copy] [Kimi 144 ] [REL] Authors : Alihan Hüyük , Xinnuo Xu , Jacqueline Maasch , Aditya Nori , Javier Hernandez Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Ha6RTeWMd0@OpenReview",
      "index": 11,
      "title": "SAM 2: Segment Anything in Images and Videos",
      "authors": [
        "Nikhila Ravi",
        "Valentin Gabeur",
        "Yuan-Ting Hu",
        "Ronghang Hu",
        "Chaitanya Ryali",
        "Tengyu Ma",
        "Haitham Khedr",
        "Roman Rädle",
        "Chloe Rolland",
        "Laura Gustafson",
        "Eric Mintun",
        "Junting Pan",
        "Kalyan Vasudev Alwala",
        "Nicolas Carion",
        "Chao-Yuan Wu",
        "Ross Girshick",
        "Piotr Dollar",
        "Christoph Feichtenhofer"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "sam",
        "anything",
        "segmentation",
        "segment",
        "video",
        "model",
        "videos",
        "images",
        "promptable",
        "data"
      ],
      "summary": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ha6RTeWMd0"
        ],
        "venue": [
          "/venue/Ha6RTeWMd0@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ha6RTeWMd0"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ha6RTeWMd0"
        ]
      },
      "scores": {
        "pdf": 91,
        "kimi": 74
      },
      "raw_excerpt": "SAM 2: Segment Anything in Images and Videos [PDF 91 ] [Copy] [Kimi 74 ] [REL] Authors : Nikhila Ravi , Valentin Gabeur , Yuan-Ting Hu , Ronghang Hu , Chaitanya Ryali , Tengyu Ma , Haitham Khedr , Roman Rädle , Chloe Rolland , Laura Gustafson , Eric Mintun , Junting Pan , Kalyan Vasudev Alwala , Nicolas Carion , Chao-Yuan Wu , Ross Girshick , Piotr Dollar , Christoph Feichtenhofer We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "xXTkbTBmqq@OpenReview",
      "index": 12,
      "title": "OLMoE: Open Mixture-of-Experts Language Models",
      "authors": [
        "Niklas Muennighoff",
        "Luca Soldaini",
        "Dirk Groeneveld",
        "Kyle Lo",
        "Jacob Morrison",
        "Sewon Min",
        "Weijia Shi",
        "Pete Walsh",
        "Oyvind Tafjord",
        "Nathan Lambert",
        "Yuling Gu",
        "Shane Arora",
        "Akshita Bhagia",
        "Dustin Schwenk",
        "David Wadden",
        "Alexander Wettig",
        "Binyuan Hui",
        "Tim Dettmers",
        "Douwe Kiela",
        "Ali Farhadi",
        "Noah Smith",
        "Pang Wei Koh",
        "Amanpreet Singh",
        "Hanna Hajishirzi"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "olmoe",
        "moe",
        "experts",
        "open",
        "deepseekmoe",
        "mixture",
        "16b",
        "language",
        "pretrain",
        "trillion"
      ],
      "summary": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xXTkbTBmqq"
        ],
        "venue": [
          "/venue/xXTkbTBmqq@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xXTkbTBmqq"
        ],
        "detail": [
          "https://openreview.net/forum?id=xXTkbTBmqq"
        ]
      },
      "scores": {
        "pdf": 77,
        "kimi": 95
      },
      "raw_excerpt": "OLMoE: Open Mixture-of-Experts Language Models [PDF 77 ] [Copy] [Kimi 95 ] [REL] Authors : Niklas Muennighoff , Luca Soldaini , Dirk Groeneveld , Kyle Lo , Jacob Morrison , Sewon Min , Weijia Shi , Pete Walsh , Oyvind Tafjord , Nathan Lambert , Yuling Gu , Shane Arora , Akshita Bhagia , Dustin Schwenk , David Wadden , Alexander Wettig , Binyuan Hui , Tim Dettmers , Douwe Kiela , Ali Farhadi , Noah Smith , Pang Wei Koh , Amanpreet Singh , Hanna Hajishirzi We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "8EfxjTCg2k@OpenReview",
      "index": 13,
      "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
      "authors": [
        "Chi-Heng Lin",
        "Shangqian Gao",
        "James Smith",
        "Abhishek Patel",
        "Shikhar Tuli",
        "Yilin Shen",
        "Hongxia Jin",
        "Yen-Chang Hsu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "modegpt",
        "compression",
        "decomposition",
        "frameworkthat",
        "a98",
        "performancewith",
        "modular",
        "ona",
        "tive",
        "meth"
      ],
      "summary": "Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources.While recent compression methods based on low-rank matrices show potentialsolutions, they often suffer from significant loss of accuracy or introduce substantialoverhead in parameters and inference time. In this paper, we introduce Modular De-composition (MoDeGPT), a new, efficient, and structured compression frameworkthat overcomes these limitations. MoDeGPT jointly decomposes pairs of consecu-tive subcomponents within Transformer blocks, reduces hidden dimensions throughoutput reconstruction on a larger structural scale than conventional low-rank meth-ods, and repurposes three classical matrix decomposition algorithms—Nyströmapproximation, CR decomposition, and SVD—to ensure bounded errors in ournovel decomposition approach. Our experiments show that MoDeGPT, withoutrelying on backward propagation, consistently matches or surpasses the performance of prior techniques that depend on gradient information, while achieving a98% reduction in compute costs when compressing a 13B-parameter model. OnLLaMA-2/3 and OPT models, MoDeGPT retains 90-95% of zero-shot performancewith compression rates of 25-30%. The compression process can be completed ona single GPU in a few hours, boosting inference throughput by up to 46%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8EfxjTCg2k"
        ],
        "venue": [
          "/venue/8EfxjTCg2k@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8EfxjTCg2k"
        ],
        "detail": [
          "https://openreview.net/forum?id=8EfxjTCg2k"
        ]
      },
      "scores": {
        "pdf": 64,
        "kimi": 84
      },
      "raw_excerpt": "MoDeGPT: Modular Decomposition for Large Language Model Compression [PDF 64 ] [Copy] [Kimi 84 ] [REL] Authors : Chi-Heng Lin , Shangqian Gao , James Smith , Abhishek Patel , Shikhar Tuli , Yilin Shen , Hongxia Jin , Yen-Chang Hsu Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources.While recent compression methods based on low-rank matrices show potentialsolutions, they often suffer from significant loss of accuracy or introduce substantialoverhead in parameters and inference time. In this paper, we introduce Modular De-composition (MoDeGPT), a new, efficient, and structured compression frameworkthat overcomes these limitations. MoDeGPT jointly decomposes pairs of consecu-tive subcomponents within Transformer blocks, reduces hidden dimensions throughoutput reconstruction on a larger structural scale than conventional low-rank meth-ods, and repurposes three classical matrix decomposition algorithms—Nyströmapproximation, CR decomposition, and SVD—to ensure bounded errors in ournovel decomposition approach. Our experiments show that MoDeGPT, withoutrelying on backward propagation, consistently matches or surpasses the performance of prior techniques that depend on gradient information, while achieving a98% reduction in compute costs when compressing a 13B-parameter model. OnLLaMA-2/3 and OPT models, MoDeGPT retains 90-95% of zero-shot performancewith compression rates of 25-30%. The compression process can be completed ona single GPU in a few hours, boosting inference throughput by up to 46%. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "UvTo3tVBk2@OpenReview",
      "index": 14,
      "title": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues",
      "authors": [
        "Riccardo Grazzi",
        "Julien Siems",
        "Jörg Franke",
        "Arber Zela",
        "Frank Hutter",
        "massimiliano pontil"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "lrnns",
        "deltanet",
        "tracking",
        "eigenvalues",
        "mamba",
        "parity",
        "rnns",
        "matrices",
        "state",
        "negative"
      ],
      "summary": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [ 0 , 1 ] [ 0 , 1 ] and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3 3 . Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [ − 1 , 1 ] [ − 1 , 1 ] . Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UvTo3tVBk2"
        ],
        "venue": [
          "/venue/UvTo3tVBk2@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UvTo3tVBk2"
        ],
        "detail": [
          "https://openreview.net/forum?id=UvTo3tVBk2"
        ]
      },
      "scores": {
        "pdf": 34,
        "kimi": 46
      },
      "raw_excerpt": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues [PDF 34 ] [Copy] [Kimi 46 ] [REL] Authors : Riccardo Grazzi , Julien Siems , Jörg Franke , Arber Zela , Frank Hutter , massimiliano pontil Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [ 0 , 1 ] [ 0 , 1 ] and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3 3 . Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [ − 1 , 1 ] [ − 1 , 1 ] . Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "kbjJ9ZOakb@OpenReview",
      "index": 15,
      "title": "Learning and aligning single-neuron invariance manifolds in visual cortex",
      "authors": [
        "Mohammad Bashiri",
        "Luca Baroni",
        "Ján Antolík",
        "Fabian Sinz"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "invariance",
        "neurons",
        "manifolds",
        "neuron",
        "sensory",
        "visual",
        "aligning",
        "selectivity",
        "manifold",
        "accurately"
      ],
      "summary": "Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. Building on recent advances in learning continuous invariance manifolds, we introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kbjJ9ZOakb"
        ],
        "venue": [
          "/venue/kbjJ9ZOakb@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kbjJ9ZOakb"
        ],
        "detail": [
          "https://openreview.net/forum?id=kbjJ9ZOakb"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 39
      },
      "raw_excerpt": "Learning and aligning single-neuron invariance manifolds in visual cortex [PDF 33 ] [Copy] [Kimi 39 ] [REL] Authors : Mohammad Bashiri , Luca Baroni , Ján Antolík , Fabian Sinz Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. Building on recent advances in learning continuous invariance manifolds, we introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "0ctvBgKFgc@OpenReview",
      "index": 16,
      "title": "ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids",
      "authors": [
        "Hannes Stärk",
        "Bowen Jing",
        "Tomas Geffner",
        "Jason Yim",
        "Tommi Jaakkola",
        "Arash Vahdat",
        "Karsten Kreis"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ellipsoids",
        "protein",
        "proteins",
        "protcomposer",
        "substructure",
        "layouts",
        "helix",
        "conditioning",
        "enables",
        "designability"
      ],
      "summary": "We develop ProtComposer to generate protein structures conditioned on spatial protein layouts that are specified via a set of 3D ellipsoids capturing substructure shapes and semantics. At inference time, we condition on ellipsoids that are hand-constructed, extracted from existing proteins, or from a statistical model, with each option unlocking new capabilities. Hand-specifying ellipsoids enables users to control the location, size, orientation, secondary structure, and approximate shape of protein substructures. Conditioning on ellipsoids of existing proteins enables redesigning their substructure's connectivity or editing substructure properties. By conditioning on novel and diverse ellipsoid layouts from a simple statistical model, we improve protein generation with expanded Pareto frontiers between designability, novelty, and diversity. Further, this enables sampling designable proteins with a helix-fraction that matches PDB proteins, unlike existing generative models that commonly oversample conceptually simple helix bundles.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0ctvBgKFgc"
        ],
        "venue": [
          "/venue/0ctvBgKFgc@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0ctvBgKFgc"
        ],
        "detail": [
          "https://openreview.net/forum?id=0ctvBgKFgc"
        ]
      },
      "scores": {
        "pdf": 31,
        "kimi": 32
      },
      "raw_excerpt": "ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids [PDF 31 ] [Copy] [Kimi 32 ] [REL] Authors : Hannes Stärk , Bowen Jing , Tomas Geffner , Jason Yim , Tommi Jaakkola , Arash Vahdat , Karsten Kreis We develop ProtComposer to generate protein structures conditioned on spatial protein layouts that are specified via a set of 3D ellipsoids capturing substructure shapes and semantics. At inference time, we condition on ellipsoids that are hand-constructed, extracted from existing proteins, or from a statistical model, with each option unlocking new capabilities. Hand-specifying ellipsoids enables users to control the location, size, orientation, secondary structure, and approximate shape of protein substructures. Conditioning on ellipsoids of existing proteins enables redesigning their substructure's connectivity or editing substructure properties. By conditioning on novel and diverse ellipsoid layouts from a simple statistical model, we improve protein generation with expanded Pareto frontiers between designability, novelty, and diversity. Further, this enables sampling designable proteins with a helix-fraction that matches PDB proteins, unlike existing generative models that commonly oversample conceptually simple helix bundles. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "TVQLu34bdw@OpenReview",
      "index": 17,
      "title": "Proteina: Scaling Flow-based Protein Structure Generative Models",
      "authors": [
        "Tomas Geffner",
        "Kieran Didi",
        "Zuobai Zhang",
        "Danny Reidenbach",
        "Zhonglin Cao",
        "Jason Yim",
        "Mario Geiger",
        "Christian Dallago",
        "Emine Kucukbenli",
        "Arash Vahdat",
        "Karsten Kreis"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "protein",
        "proteina",
        "novo",
        "backbone",
        "guidance",
        "backbones",
        "flow",
        "conditioning",
        "proteins",
        "autoguidance"
      ],
      "summary": "Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop Proteina, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to 5 × 5 × as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TVQLu34bdw"
        ],
        "venue": [
          "/venue/TVQLu34bdw@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TVQLu34bdw"
        ],
        "detail": [
          "https://openreview.net/forum?id=TVQLu34bdw"
        ]
      },
      "scores": {
        "pdf": 49,
        "kimi": 39
      },
      "raw_excerpt": "Proteina: Scaling Flow-based Protein Structure Generative Models [PDF 49 ] [Copy] [Kimi 39 ] [REL] Authors : Tomas Geffner , Kieran Didi , Zuobai Zhang , Danny Reidenbach , Zhonglin Cao , Jason Yim , Mario Geiger , Christian Dallago , Emine Kucukbenli , Arash Vahdat , Karsten Kreis Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop Proteina, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to 5 × 5 × as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "HnhNRrLPwm@OpenReview",
      "index": 18,
      "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
      "authors": [
        "Peng Xia",
        "Siwei Han",
        "Shi Qiu",
        "Yiyang Zhou",
        "Zhaoyang Wang",
        "Wenhao Zheng",
        "Zhaorun Chen",
        "Chenhang Cui",
        "Mingyu Ding",
        "Linjie Li",
        "Lijuan Wang",
        "Huaxiu Yao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "mmie",
        "interleaved",
        "lvlms",
        "multimodal",
        "evaluation",
        "comprehension",
        "benchmark",
        "vision",
        "advancements",
        "language"
      ],
      "summary": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HnhNRrLPwm"
        ],
        "venue": [
          "/venue/HnhNRrLPwm@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HnhNRrLPwm"
        ],
        "detail": [
          "https://openreview.net/forum?id=HnhNRrLPwm"
        ]
      },
      "scores": {
        "pdf": 79,
        "kimi": 65
      },
      "raw_excerpt": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models [PDF 79 ] [Copy] [Kimi 65 ] [REL] Authors : Peng Xia , Siwei Han , Shi Qiu , Yiyang Zhou , Zhaoyang Wang , Wenhao Zheng , Zhaorun Chen , Chenhang Cui , Mingyu Ding , Linjie Li , Lijuan Wang , Huaxiu Yao Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "1pXzC30ry5@OpenReview",
      "index": 19,
      "title": "RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything",
      "authors": [
        "Shilin Xu",
        "Haobo Yuan",
        "Qingyu Shi",
        "Lu Qi",
        "Jingbo Wang",
        "Yibo Yang",
        "Yining Li",
        "Kai Chen",
        "Yunhai Tong",
        "Bernard Ghanem",
        "Xiangtai Li",
        "Ming-Hsuan Yang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "rmp",
        "segmentation",
        "sam",
        "real",
        "purpose",
        "tasks",
        "time",
        "multi",
        "adapter",
        "comunity"
      ],
      "summary": "Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks. However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios. To explore real-time segmentation, recent advancements primarily focus on semantic segmentation within specific environments, such as autonomous driving. However, they often overlook the generalization ability of these models across diverse scenarios. Therefore, to fill this gap, this work explores a novel real-time segmentation setting called real-time multi-purpose segmentation. It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. Unlike previous methods, which use a specific design for each task, we aim to use only a single end-to-end model to accomplish all these tasks in real-time. To meet real-time requirements and balance multi-task learning, we present a novel dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM). It contains an efficient encoder and an efficient decoupled adapter to perform prompt-driven decoding. Moreover, we further explore different training strategies and one new adapter design to boost co-training performance further. We benchmark several strong baselines by extending existing works to support our multi-purpose segmentation. Extensive experiments demonstrate that RMP-SAM is effective and generalizes well on proposed benchmarks and other specific semantic tasks. Our implementation of RMP-SAM achieves the optimal balance between accuracy and speed for these tasks. Code and model will be available to the comunity.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1pXzC30ry5"
        ],
        "venue": [
          "/venue/1pXzC30ry5@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1pXzC30ry5"
        ],
        "detail": [
          "https://openreview.net/forum?id=1pXzC30ry5"
        ]
      },
      "scores": {
        "pdf": 36,
        "kimi": 32
      },
      "raw_excerpt": "RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything [PDF 36 ] [Copy] [Kimi 32 ] [REL] Authors : Shilin Xu , Haobo Yuan , Qingyu Shi , Lu Qi , Jingbo Wang , Yibo Yang , Yining Li , Kai Chen , Yunhai Tong , Bernard Ghanem , Xiangtai Li , Ming-Hsuan Yang Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks. However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios. To explore real-time segmentation, recent advancements primarily focus on semantic segmentation within specific environments, such as autonomous driving. However, they often overlook the generalization ability of these models across diverse scenarios. Therefore, to fill this gap, this work explores a novel real-time segmentation setting called real-time multi-purpose segmentation. It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. Unlike previous methods, which use a specific design for each task, we aim to use only a single end-to-end model to accomplish all these tasks in real-time. To meet real-time requirements and balance multi-task learning, we present a novel dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM). It contains an efficient encoder and an efficient decoupled adapter to perform prompt-driven decoding. Moreover, we further explore different training strategies and one new adapter design to boost co-training performance further. We benchmark several strong baselines by extending existing works to support our multi-purpose segmentation. Extensive experiments demonstrate that RMP-SAM is effective and generalizes well on proposed benchmarks and other specific semantic tasks. Our implementation of RMP-SAM achieves the optimal balance between accuracy and speed for these tasks. Code and model will be available to the comunity. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "DzGe40glxs@OpenReview",
      "index": 20,
      "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
      "authors": [
        "Thomas Bush",
        "Stephen Chung",
        "Usman Anwar",
        "Adrià Garriga-Alonso",
        "David Krueger"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "planning",
        "agent",
        "plans",
        "guez",
        "free",
        "reinforcement",
        "plan",
        "representations",
        "sokoban",
        "drc"
      ],
      "summary": "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by [Guez et al. (2019)](https://arxiv.org/abs/1901.03559), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in agent's representations) have causal effect on agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, enabling improved diagnosis, interpretation, and control of agent planning processes.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DzGe40glxs"
        ],
        "venue": [
          "/venue/DzGe40glxs@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DzGe40glxs"
        ],
        "detail": [
          "https://openreview.net/forum?id=DzGe40glxs"
        ]
      },
      "scores": {
        "pdf": 66,
        "kimi": 82
      },
      "raw_excerpt": "Interpreting Emergent Planning in Model-Free Reinforcement Learning [PDF 66 ] [Copy] [Kimi 82 ] [REL] Authors : Thomas Bush , Stephen Chung , Usman Anwar , Adrià Garriga-Alonso , David Krueger We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by [Guez et al. (2019)](https://arxiv.org/abs/1901.03559), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in agent's representations) have causal effect on agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, enabling improved diagnosis, interpretation, and control of agent planning processes. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "vo9t20wsmd@OpenReview",
      "index": 21,
      "title": "Faster Cascades via Speculative Decoding",
      "authors": [
        "Harikrishna Narasimhan",
        "Wittawat Jitkrittum",
        "Ankit Singh Rawat",
        "Seungyeon Kim",
        "Neha Gupta",
        "Aditya Krishna Menon",
        "Sanjiv Kumar"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "speculative",
        "cascades",
        "decoding",
        "deferral",
        "rule",
        "cascading",
        "offs",
        "approaches",
        "execution",
        "quality"
      ],
      "summary": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades offer better cost-quality trade-offs, often even outperforming the large model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vo9t20wsmd"
        ],
        "venue": [
          "/venue/vo9t20wsmd@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vo9t20wsmd"
        ],
        "detail": [
          "https://openreview.net/forum?id=vo9t20wsmd"
        ]
      },
      "scores": {
        "pdf": 51,
        "kimi": 74
      },
      "raw_excerpt": "Faster Cascades via Speculative Decoding [PDF 51 ] [Copy] [Kimi 74 ] [REL] Authors : Harikrishna Narasimhan , Wittawat Jitkrittum , Ankit Singh Rawat , Seungyeon Kim , Neha Gupta , Aditya Krishna Menon , Sanjiv Kumar Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades offer better cost-quality trade-offs, often even outperforming the large model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "6s5uXNWGIh@OpenReview",
      "index": 22,
      "title": "MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
      "authors": [
        "Jun Shern Chan",
        "Neil Chowdhury",
        "Oliver Jaffe",
        "James Aung",
        "Dane Sherburn",
        "Evan Mays",
        "Giulio Starace",
        "Kevin Liu",
        "Leon Maksin",
        "Tejal Patwardhan",
        "Aleksander Madry",
        "Lilian Weng"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "engineering",
        "kaggle",
        "agents",
        "mle",
        "bench",
        "machine",
        "competitions",
        "benchmark",
        "aide",
        "medal"
      ],
      "summary": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 71 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup — OpenAI's o1-preview with AIDE scaffolding — achieves at least the level of a Kaggle bronze medal in 17.3\\% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code to facilitate future research in understanding the ML engineering capabilities of AI agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6s5uXNWGIh"
        ],
        "venue": [
          "/venue/6s5uXNWGIh@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6s5uXNWGIh"
        ],
        "detail": [
          "https://openreview.net/forum?id=6s5uXNWGIh"
        ]
      },
      "scores": {
        "pdf": 34,
        "kimi": 39
      },
      "raw_excerpt": "MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering [PDF 34 ] [Copy] [Kimi 39 ] [REL] Authors : Jun Shern Chan , Neil Chowdhury , Oliver Jaffe , James Aung , Dane Sherburn , Evan Mays , Giulio Starace , Kevin Liu , Leon Maksin , Tejal Patwardhan , Aleksander Madry , Lilian Weng We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 71 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup — OpenAI's o1-preview with AIDE scaffolding — achieves at least the level of a Kaggle bronze medal in 17.3\\% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code to facilitate future research in understanding the ML engineering capabilities of AI agents. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "t8FG4cJuL3@OpenReview",
      "index": 23,
      "title": "Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games",
      "authors": [
        "Yanzheng Chen",
        "Jun Yu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "games",
        "iterate",
        "algorithms",
        "last",
        "varying",
        "player",
        "convergence",
        "gradient",
        "behaviours",
        "perturbed"
      ],
      "summary": "Last-iterate convergence behaviours of well-known algorithms are intensively investigated in various games, such as two-player bilinear zero-sum games.However, most known last-iterate convergence properties rely on strict settings where the underlying games must have time-invariant payoffs.Besides, the limited known attempts on the games with time-varying payoffs are in two-player bilinear time-varying zero-sum games and strictly monotone games. By contrast, in other time-varying games, the last-iterate behaviours of two classic algorithms, i.e., optimistic gradient (OG) and extra gradient (EG) algorithms, still lack research, especially the convergence rates in multi-player games.In this paper, we investigate the last-iterate behaviours of OG and EG algorithms for convergent perturbed games, which extend upon the usual model of time-invariant games and incorporate external factors, such as vanishing noises.Using the recently proposed notion of the tangent residual (or its modifications) as the potential function of games and the measure of proximity to the Nash equilibrium, we prove that the last-iterate convergence rates of EG and OG algorithms for perturbed games on bounded convex closed sets are O ( 1 / T − − √ ) O ( 1 / T ) if such games converge to monotone games at rates fast enough and that such a result holds true for certain unconstrained perturbed games. With this result, we address an open questionasking for the last-iterate convergence rate of the extra gradient and the optimistic gradient algorithms in constrained and time-varying settings. The above convergence rates are similar to known tight results on corresponding time-invariant games.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=t8FG4cJuL3"
        ],
        "venue": [
          "/venue/t8FG4cJuL3@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=t8FG4cJuL3"
        ],
        "detail": [
          "https://openreview.net/forum?id=t8FG4cJuL3"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 28
      },
      "raw_excerpt": "Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games [PDF 14 ] [Copy] [Kimi 28 ] [REL] Authors : Yanzheng Chen , Jun Yu Last-iterate convergence behaviours of well-known algorithms are intensively investigated in various games, such as two-player bilinear zero-sum games.However, most known last-iterate convergence properties rely on strict settings where the underlying games must have time-invariant payoffs.Besides, the limited known attempts on the games with time-varying payoffs are in two-player bilinear time-varying zero-sum games and strictly monotone games. By contrast, in other time-varying games, the last-iterate behaviours of two classic algorithms, i.e., optimistic gradient (OG) and extra gradient (EG) algorithms, still lack research, especially the convergence rates in multi-player games.In this paper, we investigate the last-iterate behaviours of OG and EG algorithms for convergent perturbed games, which extend upon the usual model of time-invariant games and incorporate external factors, such as vanishing noises.Using the recently proposed notion of the tangent residual (or its modifications) as the potential function of games and the measure of proximity to the Nash equilibrium, we prove that the last-iterate convergence rates of EG and OG algorithms for perturbed games on bounded convex closed sets are O ( 1 / T − − √ ) O ( 1 / T ) if such games converge to monotone games at rates fast enough and that such a result holds true for certain unconstrained perturbed games. With this result, we address an open questionasking for the last-iterate convergence rate of the extra gradient and the optimistic gradient algorithms in constrained and time-varying settings. The above convergence rates are similar to known tight results on corresponding time-invariant games. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "st77ShxP1K@OpenReview",
      "index": 24,
      "title": "Do as We Do, Not as You Think: the Conformity of Large Language Models",
      "authors": [
        "Zhiyuan Weng",
        "Guikun Chen",
        "Wenguan Wang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "conformity",
        "llms",
        "benchform",
        "collaborative",
        "think",
        "agent",
        "influencing",
        "systems",
        "language",
        "strategies"
      ],
      "summary": "Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BENCHFORM, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs’ behavior in collaborative scenarios. Several representative LLMs are evaluated on BENCHFORM, using metrics such as conformity rate and independence rate to quantify conformity’s impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs’ conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=st77ShxP1K"
        ],
        "venue": [
          "/venue/st77ShxP1K@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=st77ShxP1K"
        ],
        "detail": [
          "https://openreview.net/forum?id=st77ShxP1K"
        ]
      },
      "scores": {
        "pdf": 111,
        "kimi": 162
      },
      "raw_excerpt": "Do as We Do, Not as You Think: the Conformity of Large Language Models [PDF 111 ] [Copy] [Kimi 162 ] [REL] Authors : Zhiyuan Weng , Guikun Chen , Wenguan Wang Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BENCHFORM, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs’ behavior in collaborative scenarios. Several representative LLMs are evaluated on BENCHFORM, using metrics such as conformity rate and independence rate to quantify conformity’s impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs’ conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code will be publicly available. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "xoXn62FzD0@OpenReview",
      "index": 25,
      "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
      "authors": [
        "João Loula",
        "Benjamin LeBrun",
        "Li Du",
        "Ben Lipkin",
        "Clemente Pasti",
        "Gabriel Grand",
        "Tianyu Liu",
        "Yahya Emara",
        "Marjorie Freedman",
        "Jason Eisner",
        "Ryan Cotterell",
        "Vikash Mansinghka",
        "Alexander Lew",
        "Tim Vieira",
        "Timothy O'Donnell"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "syntactic",
        "smc",
        "lew",
        "sequential",
        "monte",
        "carlo",
        "llm",
        "constraints",
        "inference",
        "generation"
      ],
      "summary": "A wide range of LLM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints nontrivially alters the distribution over sequences, usually making exact sampling intractable. In this work, building on the Language Model Probabilistic Programming framework of Lew et al. (2023), we develop an approach to approximate inference for controlled LLM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computation in light of new information during the course of generation. We demonstrate that our approach improves downstream performance on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis. We compare to a number of alternative and ablated approaches, showing that our accuracy improvements are driven by better approximation to the full Bayesian posterior.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xoXn62FzD0"
        ],
        "venue": [
          "/venue/xoXn62FzD0@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xoXn62FzD0"
        ],
        "detail": [
          "https://openreview.net/forum?id=xoXn62FzD0"
        ]
      },
      "scores": {
        "pdf": 47,
        "kimi": 76
      },
      "raw_excerpt": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo [PDF 47 ] [Copy] [Kimi 76 ] [REL] Authors : João Loula , Benjamin LeBrun , Li Du , Ben Lipkin , Clemente Pasti , Gabriel Grand , Tianyu Liu , Yahya Emara , Marjorie Freedman , Jason Eisner , Ryan Cotterell , Vikash Mansinghka , Alexander Lew , Tim Vieira , Timothy O'Donnell A wide range of LLM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints nontrivially alters the distribution over sequences, usually making exact sampling intractable. In this work, building on the Language Model Probabilistic Programming framework of Lew et al. (2023), we develop an approach to approximate inference for controlled LLM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computation in light of new information during the course of generation. We demonstrate that our approach improves downstream performance on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis. We compare to a number of alternative and ablated approaches, showing that our accuracy improvements are driven by better approximation to the full Bayesian posterior. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "wg1PCg3CUP@OpenReview",
      "index": 26,
      "title": "Scaling Laws for Precision",
      "authors": [
        "Tanishq Kumar",
        "Zachary Ankner",
        "Benjamin Spector",
        "Blake Bordelon",
        "Niklas Muennighoff",
        "Mansheej Paul",
        "Cengiz Pehlevan",
        "Christopher Re",
        "Aditi Raghunathan"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "precision",
        "laws",
        "training",
        "scaling",
        "pretraining",
        "precisions",
        "inference",
        "post",
        "quantization",
        "degradation"
      ],
      "summary": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wg1PCg3CUP"
        ],
        "venue": [
          "/venue/wg1PCg3CUP@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wg1PCg3CUP"
        ],
        "detail": [
          "https://openreview.net/forum?id=wg1PCg3CUP"
        ]
      },
      "scores": {
        "pdf": 49,
        "kimi": 81
      },
      "raw_excerpt": "Scaling Laws for Precision [PDF 49 ] [Copy] [Kimi 81 ] [REL] Authors : Tanishq Kumar , Zachary Ankner , Benjamin Spector , Blake Bordelon , Niklas Muennighoff , Mansheej Paul , Cengiz Pehlevan , Christopher Re , Aditi Raghunathan Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "tyEyYT267x@OpenReview",
      "index": 27,
      "title": "Interpolating Autoregressive and Discrete Denoising Diffusion Language Models",
      "authors": [
        "Marianne Arriola",
        "Aaron Gokaslan",
        "Justin Chiu",
        "Jiaqi Han",
        "Zhihan Yang",
        "Zhixuan Qi",
        "Subham Sahoo",
        "Volodymyr Kuleshov"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "autoregressive",
        "sar",
        "diffusion",
        "language",
        "models",
        "denoising",
        "generation",
        "interpolating",
        "discrete",
        "variance"
      ],
      "summary": "Diffusion language models offer unique benefits over autoregressive (AR) models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of semi-autoregressive (SAR) diffusion models that interpolate between discrete denoising diffusion and autoregressive models. We propose a recipe for building effective SAR models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. SAR models overcome key limitations of diffusion language models, setting a new state-of-the-art performance on language modeling benchmarks and enabling generation of arbitrary-length sequences.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tyEyYT267x"
        ],
        "venue": [
          "/venue/tyEyYT267x@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tyEyYT267x"
        ],
        "detail": [
          "https://openreview.net/forum?id=tyEyYT267x"
        ]
      },
      "scores": {
        "pdf": 80,
        "kimi": 77
      },
      "raw_excerpt": "Interpolating Autoregressive and Discrete Denoising Diffusion Language Models [PDF 80 ] [Copy] [Kimi 77 ] [REL] Authors : Marianne Arriola , Aaron Gokaslan , Justin Chiu , Jiaqi Han , Zhihan Yang , Zhixuan Qi , Subham Sahoo , Volodymyr Kuleshov Diffusion language models offer unique benefits over autoregressive (AR) models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of semi-autoregressive (SAR) diffusion models that interpolate between discrete denoising diffusion and autoregressive models. We propose a recipe for building effective SAR models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. SAR models overcome key limitations of diffusion language models, setting a new state-of-the-art performance on language modeling benchmarks and enabling generation of arbitrary-length sequences. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "tcvMzR2NrP@OpenReview",
      "index": 28,
      "title": "Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective",
      "authors": [
        "Neta Shaul",
        "Itai Gat",
        "Marton Havasi",
        "Daniel Severo",
        "Anuroop Sriram",
        "Peter Holderrieth",
        "Brian Karrer",
        "Yaron Lipman",
        "Ricky T. Q. Chen"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "probability",
        "paths",
        "kinetic",
        "discrete",
        "construction",
        "path",
        "space",
        "domain",
        "generation",
        "colloquially"
      ],
      "summary": "The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tcvMzR2NrP"
        ],
        "venue": [
          "/venue/tcvMzR2NrP@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tcvMzR2NrP"
        ],
        "detail": [
          "https://openreview.net/forum?id=tcvMzR2NrP"
        ]
      },
      "scores": {
        "pdf": 44,
        "kimi": 54
      },
      "raw_excerpt": "Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective [PDF 44 ] [Copy] [Kimi 54 ] [REL] Authors : Neta Shaul , Itai Gat , Marton Havasi , Daniel Severo , Anuroop Sriram , Peter Holderrieth , Brian Karrer , Yaron Lipman , Ricky T. Q. Chen The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "tPNHOoZFl9@OpenReview",
      "index": 29,
      "title": "Learning Dynamics of LLM Finetuning",
      "authors": [
        "YI REN",
        "Danica Sutherland"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "finetuning",
        "dpo",
        "phrases",
        "llm",
        "responses",
        "preference",
        "learning",
        "dynamics",
        "tuning",
        "might"
      ],
      "summary": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique ``squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tPNHOoZFl9"
        ],
        "venue": [
          "/venue/tPNHOoZFl9@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tPNHOoZFl9"
        ],
        "detail": [
          "https://openreview.net/forum?id=tPNHOoZFl9"
        ]
      },
      "scores": {
        "pdf": 111,
        "kimi": 131
      },
      "raw_excerpt": "Learning Dynamics of LLM Finetuning [PDF 111 ] [Copy] [Kimi 131 ] [REL] Authors : YI REN , Danica Sutherland Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique ``squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "sbG8qhMjkZ@OpenReview",
      "index": 30,
      "title": "Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent",
      "authors": [
        "Krishna Balasubramanian",
        "Sayan Banerjee",
        "PROMIT GHOSAL"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ksd",
        "stein",
        "rates",
        "wasserstein",
        "particle",
        "convergence",
        "descent",
        "shi2024finite",
        "bilinear",
        "variational"
      ],
      "summary": "We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ( \\KSD \\KSD ) and Wasserstein-2 metrics. Our key insight is the observation that the time derivative of the relative entropy between the joint density of N N particle locations and the N N -fold product target measure, starting from a regular initial distribution, splits into a dominant negative part proportional to N N times the expected \\KSD 2 \\KSD 2 and a smaller positive part. This observation leads to \\KSD \\KSD rates of order 1 / N − − √ 1 / N , in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension d d . By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Matérn' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sbG8qhMjkZ"
        ],
        "venue": [
          "/venue/sbG8qhMjkZ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sbG8qhMjkZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=sbG8qhMjkZ"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 18
      },
      "raw_excerpt": "Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent [PDF 9 ] [Copy] [Kimi 18 ] [REL] Authors : Krishna Balasubramanian , Sayan Banerjee , PROMIT GHOSAL We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ( \\KSD \\KSD ) and Wasserstein-2 metrics. Our key insight is the observation that the time derivative of the relative entropy between the joint density of N N particle locations and the N N -fold product target measure, starting from a regular initial distribution, splits into a dominant negative part proportional to N N times the expected \\KSD 2 \\KSD 2 and a smaller positive part. This observation leads to \\KSD \\KSD rates of order 1 / N − − √ 1 / N , in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension d d . By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Matérn' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "pqOjj90Vwp@OpenReview",
      "index": 31,
      "title": "Towards a Complete Logical Framework for GNN Expressiveness",
      "authors": [
        "Tuo Xu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "gnn",
        "logical",
        "expressiveness",
        "gnns",
        "expressivity",
        "homomorphism",
        "graph",
        "subareas",
        "framework",
        "lehman"
      ],
      "summary": "Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the specific graph patterns captured by them. Given the connection between WL tests and first-order logic, some have explored the logical expressiveness of Message Passing Neural Networks. This paper aims to establish a comprehensive and systematic relationship between GNNs and logic. We propose a framework for identifying the equivalent logical formulas for arbitrary GNN architectures, which not only explains existing models, but also provides inspiration for future research. As case studies, we analyze multiple classes of prominent GNNs within this framework, unifying different subareas of the field. Additionally, we conduct a detailed examination of homomorphism expressivity from a logical perspective and present a general method for determining the homomorphism expressivity of arbitrary GNN models, as well as addressing several open problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pqOjj90Vwp"
        ],
        "venue": [
          "/venue/pqOjj90Vwp@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pqOjj90Vwp"
        ],
        "detail": [
          "https://openreview.net/forum?id=pqOjj90Vwp"
        ]
      },
      "scores": {
        "pdf": 39,
        "kimi": 27
      },
      "raw_excerpt": "Towards a Complete Logical Framework for GNN Expressiveness [PDF 39 ] [Copy] [Kimi 27 ] [REL] Author : Tuo Xu Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the specific graph patterns captured by them. Given the connection between WL tests and first-order logic, some have explored the logical expressiveness of Message Passing Neural Networks. This paper aims to establish a comprehensive and systematic relationship between GNNs and logic. We propose a framework for identifying the equivalent logical formulas for arbitrary GNN architectures, which not only explains existing models, but also provides inspiration for future research. As case studies, we analyze multiple classes of prominent GNNs within this framework, unifying different subareas of the field. Additionally, we conduct a detailed examination of homomorphism expressivity from a logical perspective and present a general method for determining the homomorphism expressivity of arbitrary GNN models, as well as addressing several open problems. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "or8mMhmyRV@OpenReview",
      "index": 32,
      "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
      "authors": [
        "Martin Klissarov",
        "Mikael Henaff",
        "Roberta Raileanu",
        "Shagun Sodhani",
        "Pascal Vincent",
        "Amy Zhang",
        "Pierre-Luc Bacon",
        "Doina Precup",
        "Marlos C. Machado",
        "Pierluca D'Oro"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "maestromotif",
        "skill",
        "skills",
        "language",
        "feedback",
        "llm",
        "nethack",
        "nle",
        "design",
        "natural"
      ],
      "summary": "Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=or8mMhmyRV"
        ],
        "venue": [
          "/venue/or8mMhmyRV@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=or8mMhmyRV"
        ],
        "detail": [
          "https://openreview.net/forum?id=or8mMhmyRV"
        ]
      },
      "scores": {
        "pdf": 31,
        "kimi": 47
      },
      "raw_excerpt": "MaestroMotif: Skill Design from Artificial Intelligence Feedback [PDF 31 ] [Copy] [Kimi 47 ] [REL] Authors : Martin Klissarov , Mikael Henaff , Roberta Raileanu , Shagun Sodhani , Pascal Vincent , Amy Zhang , Pierre-Luc Bacon , Doina Precup , Marlos C. Machado , Pierluca D'Oro Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "o9kqa5K3tB@OpenReview",
      "index": 33,
      "title": "On the Benefits of Memory for Modeling Time-Dependent PDEs",
      "authors": [
        "Ricardo Buitrago Ruiz",
        "Tanya Marwah",
        "Albert Gu",
        "Andrej Risteski"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "pdes",
        "memno",
        "memory",
        "dependent",
        "benefits",
        "markovian",
        "fnos",
        "modeling",
        "past",
        "zwanzig"
      ],
      "summary": "Data-driven techniques have emerged as a promising alternative to traditional numerical methods. For time-dependent PDEs, many approaches are Markovian---the evolution of the trained system only depends on the current state, and not the past states. In this work, we investigate the benefits of using memory for modeling time-dependent PDEs: that is, when past states are explicitly used to predict the future. Motivated by the Mori-Zwanzig theory of model reduction, we theoretically exhibit examples of simple (even linear) PDEs, in which a solution that uses memory is arbitrarily better than a Markovian solution. Additionally, we introduce Memory Neural Operator (MemNO), a neural operator architecture that combines recent state space models (specifically, S4) and Fourier Neural Operators (FNOs) to effectively model memory. We empirically demonstrate that when the PDEs are supplied in low resolution or contain observation noise at train and test time, MemNO significantly outperforms the baselines without memory---with up to 6 × 6 × reduction in test error. Furthermore, we show that this benefit is particularly pronounced when the PDE solutions have significant high-frequency Fourier modes (e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark dataset consisting of such PDEs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o9kqa5K3tB"
        ],
        "venue": [
          "/venue/o9kqa5K3tB@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o9kqa5K3tB"
        ],
        "detail": [
          "https://openreview.net/forum?id=o9kqa5K3tB"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 19
      },
      "raw_excerpt": "On the Benefits of Memory for Modeling Time-Dependent PDEs [PDF 13 ] [Copy] [Kimi 19 ] [REL] Authors : Ricardo Buitrago Ruiz , Tanya Marwah , Albert Gu , Andrej Risteski Data-driven techniques have emerged as a promising alternative to traditional numerical methods. For time-dependent PDEs, many approaches are Markovian---the evolution of the trained system only depends on the current state, and not the past states. In this work, we investigate the benefits of using memory for modeling time-dependent PDEs: that is, when past states are explicitly used to predict the future. Motivated by the Mori-Zwanzig theory of model reduction, we theoretically exhibit examples of simple (even linear) PDEs, in which a solution that uses memory is arbitrarily better than a Markovian solution. Additionally, we introduce Memory Neural Operator (MemNO), a neural operator architecture that combines recent state space models (specifically, S4) and Fourier Neural Operators (FNOs) to effectively model memory. We empirically demonstrate that when the PDEs are supplied in low resolution or contain observation noise at train and test time, MemNO significantly outperforms the baselines without memory---with up to 6 × 6 × reduction in test error. Furthermore, we show that this benefit is particularly pronounced when the PDE solutions have significant high-frequency Fourier modes (e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark dataset consisting of such PDEs. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "o5TsWTUSeF@OpenReview",
      "index": 34,
      "title": "ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding",
      "authors": [
        "Zhengzhuo Xu",
        "Bowen Qu",
        "Yiyan Qi",
        "SiNan Du",
        "Chengjin Xu",
        "Chun Yuan",
        "Jian Guo"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "chartmoe",
        "connector",
        "chart",
        "moe",
        "diversely",
        "mllms",
        "json",
        "expert",
        "alignment",
        "understanding"
      ],
      "summary": "Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, current MLLMs still struggle to provide faithful data and reliable analysis only based on charts. To address it, we propose ChartMoE, which employs the Mixture of Expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train several linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with nearly 1 million chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts diversely and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o5TsWTUSeF"
        ],
        "venue": [
          "/venue/o5TsWTUSeF@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o5TsWTUSeF"
        ],
        "detail": [
          "https://openreview.net/forum?id=o5TsWTUSeF"
        ]
      },
      "scores": {
        "pdf": 45,
        "kimi": 53
      },
      "raw_excerpt": "ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding [PDF 45 ] [Copy] [Kimi 53 ] [REL] Authors : Zhengzhuo Xu , Bowen Qu , Yiyan Qi , SiNan Du , Chengjin Xu , Chun Yuan , Jian Guo Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, current MLLMs still struggle to provide faithful data and reliable analysis only based on charts. To address it, we propose ChartMoE, which employs the Mixture of Expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train several linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with nearly 1 million chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts diversely and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "m2nmp8P5in@OpenReview",
      "index": 35,
      "title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
      "authors": [
        "Parshin Shojaee",
        "Kazem Meidani",
        "Shashank Gupta",
        "Amir Barati Farimani",
        "Chandan Reddy"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "llm",
        "scientific",
        "equations",
        "discovery",
        "equation",
        "symbolic",
        "domain",
        "knowledge",
        "programs",
        "priors"
      ],
      "summary": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=m2nmp8P5in"
        ],
        "venue": [
          "/venue/m2nmp8P5in@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=m2nmp8P5in"
        ],
        "detail": [
          "https://openreview.net/forum?id=m2nmp8P5in"
        ]
      },
      "scores": {
        "pdf": 26,
        "kimi": 41
      },
      "raw_excerpt": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models [PDF 26 ] [Copy] [Kimi 41 ] [REL] Authors : Parshin Shojaee , Kazem Meidani , Shashank Gupta , Amir Barati Farimani , Chandan Reddy Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "k3tbMMW8rH@OpenReview",
      "index": 36,
      "title": "Feedback Schrödinger Bridge Matching",
      "authors": [
        "Panagiotis Theodoropoulos",
        "Guan-Horng Liu",
        "Nikolaos Komianos",
        "Vincent Pacelli",
        "Evangelos Theodorou"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "matching",
        "fsbm",
        "scalability",
        "eot",
        "frameworks",
        "feedback",
        "pairings",
        "bridge",
        "schrödinger",
        "transport"
      ],
      "summary": "Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. Fully unsupervised methods make minimal assumptions but incur high computational costs, limiting their practicality. On the other hand, imposing full supervision of the matching process with optimal pairings improves scalability, however, it can be infeasible in most applications.To strike a balance between scalability and minimal supervision, we introduce Feedback Schrödinger Bridge Matching (FSBM), a novel semi-supervised matching framework that incorporates a small portion ( < 8 < 8 % of the entire dataset) of pre-aligned pairs as state feedback to guide the transport map of non-coupled samples, thereby significantly improving efficiency. This is achieved by formulating a static Entropic Optimal Transport (EOT) problem with an additional term capturing the semi-supervised guidance. The generalized EOT objective is then recast into a dynamic formulation to leverage the scalability of matching frameworks. Extensive experiments demonstrate that FSBM accelerates training and enhances generalization by leveraging coupled pairs' guidance, opening new avenues for training matching frameworks with partially aligned datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k3tbMMW8rH"
        ],
        "venue": [
          "/venue/k3tbMMW8rH@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k3tbMMW8rH"
        ],
        "detail": [
          "https://openreview.net/forum?id=k3tbMMW8rH"
        ]
      },
      "scores": {
        "pdf": 28,
        "kimi": 24
      },
      "raw_excerpt": "Feedback Schrödinger Bridge Matching [PDF 28 ] [Copy] [Kimi 24 ] [REL] Authors : Panagiotis Theodoropoulos , Guan-Horng Liu , Nikolaos Komianos , Vincent Pacelli , Evangelos Theodorou Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. Fully unsupervised methods make minimal assumptions but incur high computational costs, limiting their practicality. On the other hand, imposing full supervision of the matching process with optimal pairings improves scalability, however, it can be infeasible in most applications.To strike a balance between scalability and minimal supervision, we introduce Feedback Schrödinger Bridge Matching (FSBM), a novel semi-supervised matching framework that incorporates a small portion ( < 8 < 8 % of the entire dataset) of pre-aligned pairs as state feedback to guide the transport map of non-coupled samples, thereby significantly improving efficiency. This is achieved by formulating a static Entropic Optimal Transport (EOT) problem with an additional term capturing the semi-supervised guidance. The generalized EOT objective is then recast into a dynamic formulation to leverage the scalability of matching frameworks. Extensive experiments demonstrate that FSBM accelerates training and enhances generalization by leveraging coupled pairs' guidance, opening new avenues for training matching frameworks with partially aligned datasets. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "ja4rpheN2n@OpenReview",
      "index": 37,
      "title": "GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation",
      "authors": [
        "Ziwei Yang",
        "Zheng Chen",
        "XIN LIU",
        "Rikuto Kotoge",
        "Peng Chen",
        "Yasuko Matsubara",
        "Yasushi Sakurai",
        "Jimeng Sun"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "gesubnet",
        "subtype",
        "gene",
        "disease",
        "subtypes",
        "patient",
        "specific",
        "databases",
        "mismatch",
        "genes"
      ],
      "summary": "Retrieving gene functional networks from knowledge databases presents a challenge due to the mismatch between disease networks and subtype-specific variations. Current solutions, including statistical and deep learning methods, often fail to effectively integrate gene interaction knowledge from databases or explicitly learn subtype-specific interactions. To address this mismatch, we propose GeSubNet, which learns a unified representation capable of predicting gene interactions while distinguishing between different disease subtypes. Graphs generated by such representations can be considered subtype-specific networks. GeSubNet is a multi-step representation learning framework with three modules: First, a deep generative model learns distinct disease subtypes from patient gene expression profiles. Second, a graph neural network captures representations of prior gene networks from knowledge databases, ensuring accurate physical gene interactions. Finally, we integrate these two representations using an inference loss that leverages graph generation capabilities, conditioned on the patient separation loss, to refine subtype-specific information in the learned representation. GeSubNet consistently outperforms traditional methods, with average improvements of 30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged over four cancer datasets. Particularly, we conduct a biological simulation experiment to assess how the behavior of selected genes from over 11,000 candidates affects subtypes or patient distributions. The results show that the generated network has the potential to identify subtype-specific genes with an 83% likelihood of impacting patient distribution shifts. The GeSubNet resource is available: https://anonymous.4open.science/r/GeSubNet/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ja4rpheN2n"
        ],
        "venue": [
          "/venue/ja4rpheN2n@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ja4rpheN2n"
        ],
        "detail": [
          "https://openreview.net/forum?id=ja4rpheN2n"
        ]
      },
      "scores": {
        "pdf": 23,
        "kimi": 22
      },
      "raw_excerpt": "GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation [PDF 23 ] [Copy] [Kimi 22 ] [REL] Authors : Ziwei Yang , Zheng Chen , XIN LIU , Rikuto Kotoge , Peng Chen , Yasuko Matsubara , Yasushi Sakurai , Jimeng Sun Retrieving gene functional networks from knowledge databases presents a challenge due to the mismatch between disease networks and subtype-specific variations. Current solutions, including statistical and deep learning methods, often fail to effectively integrate gene interaction knowledge from databases or explicitly learn subtype-specific interactions. To address this mismatch, we propose GeSubNet, which learns a unified representation capable of predicting gene interactions while distinguishing between different disease subtypes. Graphs generated by such representations can be considered subtype-specific networks. GeSubNet is a multi-step representation learning framework with three modules: First, a deep generative model learns distinct disease subtypes from patient gene expression profiles. Second, a graph neural network captures representations of prior gene networks from knowledge databases, ensuring accurate physical gene interactions. Finally, we integrate these two representations using an inference loss that leverages graph generation capabilities, conditioned on the patient separation loss, to refine subtype-specific information in the learned representation. GeSubNet consistently outperforms traditional methods, with average improvements of 30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged over four cancer datasets. Particularly, we conduct a biological simulation experiment to assess how the behavior of selected genes from over 11,000 candidates affects subtypes or patient distributions. The results show that the generated network has the potential to identify subtype-specific genes with an 83% likelihood of impacting patient distribution shifts. The GeSubNet resource is available: https://anonymous.4open.science/r/GeSubNet/ Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "gc8QAQfXv6@OpenReview",
      "index": 38,
      "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
      "authors": [
        "Gangwei Jiang",
        "caigao jiang",
        "Zhaoyi Li",
        "Siqiao Xue",
        "JUN ZHOU",
        "Linqi Song",
        "Defu Lian",
        "Ying Wei"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "forgetting",
        "catastrophic",
        "continual",
        "function",
        "llms",
        "unlocking",
        "tasks",
        "training",
        "overwriting",
        "model"
      ],
      "summary": "Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior.Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions.Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gc8QAQfXv6"
        ],
        "venue": [
          "/venue/gc8QAQfXv6@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gc8QAQfXv6"
        ],
        "detail": [
          "https://openreview.net/forum?id=gc8QAQfXv6"
        ]
      },
      "scores": {
        "pdf": 31,
        "kimi": 50
      },
      "raw_excerpt": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning [PDF 31 ] [Copy] [Kimi 50 ] [REL] Authors : Gangwei Jiang , caigao jiang , Zhaoyi Li , Siqiao Xue , JUN ZHOU , Linqi Song , Defu Lian , Ying Wei Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior.Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions.Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "gHLWTzKiZV@OpenReview",
      "index": 39,
      "title": "Composing Unbalanced Flows for Flexible Docking and Relaxation",
      "authors": [
        "Gabriele Corso",
        "Vignesh Ram Somnath",
        "Noah Getz",
        "Regina Barzilay",
        "Tommi Jaakkola",
        "Andreas Krause"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "docking",
        "unbalanced",
        "flexibility",
        "poses",
        "energetically",
        "flexible",
        "relaxation",
        "composing",
        "protein",
        "favorable"
      ],
      "summary": "Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack the flexibility to define effective maps between such complex distributions. To address this limitation we propose Unbalanced Flow Matching, a generalization of Flow Matching (FM) that allows trading off sample efficiency with approximation accuracy and enables more accurate transport. Empirically, we apply Unbalanced FM on flexible docking and structure relaxation, demonstrating our ability to model protein flexibility and generate energetically favorable poses. On the PDBBind docking benchmark, our method FlexDock improves the docking performance while increasing the proportion of energetically favorable poses from 30% to 73%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gHLWTzKiZV"
        ],
        "venue": [
          "/venue/gHLWTzKiZV@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gHLWTzKiZV"
        ],
        "detail": [
          "https://openreview.net/forum?id=gHLWTzKiZV"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 19
      },
      "raw_excerpt": "Composing Unbalanced Flows for Flexible Docking and Relaxation [PDF 18 ] [Copy] [Kimi 19 ] [REL] Authors : Gabriele Corso , Vignesh Ram Somnath , Noah Getz , Regina Barzilay , Tommi Jaakkola , Andreas Krause Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack the flexibility to define effective maps between such complex distributions. To address this limitation we propose Unbalanced Flow Matching, a generalization of Flow Matching (FM) that allows trading off sample efficiency with approximation accuracy and enables more accurate transport. Empirically, we apply Unbalanced FM on flexible docking and structure relaxation, demonstrating our ability to model protein flexibility and generate energetically favorable poses. On the PDBBind docking benchmark, our method FlexDock improves the docking performance while increasing the proportion of energetically favorable poses from 30% to 73%. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "f4gF6AIHRy@OpenReview",
      "index": 40,
      "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection",
      "authors": [
        "Ziqing Fan",
        "Siyuan Du",
        "Shengchao Hu",
        "Pingjie Wang",
        "Li Shen",
        "Ya Zhang",
        "Dacheng Tao",
        "Yanfeng Wang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "disf",
        "file",
        "training",
        "selection",
        "collapse",
        "submodular",
        "combatting",
        "590m",
        "pre",
        "files"
      ],
      "summary": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of γ γ -weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=f4gF6AIHRy"
        ],
        "venue": [
          "/venue/f4gF6AIHRy@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=f4gF6AIHRy"
        ],
        "detail": [
          "https://openreview.net/forum?id=f4gF6AIHRy"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 28
      },
      "raw_excerpt": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection [PDF 19 ] [Copy] [Kimi 28 ] [REL] Authors : Ziqing Fan , Siyuan Du , Shengchao Hu , Pingjie Wang , Li Shen , Ya Zhang , Dacheng Tao , Yanfeng Wang Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of γ γ -weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "eHehzSDUFp@OpenReview",
      "index": 41,
      "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
      "authors": [
        "Jiyeon Kim",
        "Hyunji Lee",
        "Hyowon Cho",
        "Joel Jang",
        "Hyeonbin Hwang",
        "Seungpil Won",
        "Youbin Ahn",
        "Dohaeng Lee",
        "Minjoon Seo"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "knowledge",
        "acquisition",
        "entropy",
        "pretraining",
        "sources",
        "memory",
        "retention",
        "model",
        "decline",
        "hinders"
      ],
      "summary": "In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eHehzSDUFp"
        ],
        "venue": [
          "/venue/eHehzSDUFp@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eHehzSDUFp"
        ],
        "detail": [
          "https://openreview.net/forum?id=eHehzSDUFp"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 39
      },
      "raw_excerpt": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition [PDF 35 ] [Copy] [Kimi 39 ] [REL] Authors : Jiyeon Kim , Hyunji Lee , Hyowon Cho , Joel Jang , Hyeonbin Hwang , Seungpil Won , Youbin Ahn , Dohaeng Lee , Minjoon Seo In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "dhAL5fy8wS@OpenReview",
      "index": 42,
      "title": "Data Selection via Optimal Control for Language Models",
      "authors": [
        "Yuxian Gu",
        "Li Dong",
        "Hongning Wang",
        "Yaru Hao",
        "Qingxiu Dong",
        "Furu Wei",
        "Minlie Huang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "pds",
        "pmp",
        "selection",
        "data",
        "commmoncrawl",
        "lms",
        "400b",
        "optimal",
        "corpora",
        "10t"
      ],
      "summary": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics.Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes.Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws.PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. We will open-source our code, models, and data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dhAL5fy8wS"
        ],
        "venue": [
          "/venue/dhAL5fy8wS@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dhAL5fy8wS"
        ],
        "detail": [
          "https://openreview.net/forum?id=dhAL5fy8wS"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 53
      },
      "raw_excerpt": "Data Selection via Optimal Control for Language Models [PDF 35 ] [Copy] [Kimi 53 ] [REL] Authors : Yuxian Gu , Li Dong , Hongning Wang , Yaru Hao , Qingxiu Dong , Furu Wei , Minlie Huang This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics.Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes.Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws.PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. We will open-source our code, models, and data. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "cNmu0hZ4CL@OpenReview",
      "index": 43,
      "title": "Comparing noisy neural population dynamics using optimal transport distances",
      "authors": [
        "Amin Nejatbakhsh",
        "Victor Geadah",
        "Alex Williams",
        "David Lipshutz"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "neural",
        "responses",
        "noisy",
        "systems",
        "comparing",
        "computational",
        "transport",
        "biological",
        "artificial",
        "capabilities"
      ],
      "summary": "Biological and artificial neural systems form high-dimensional neural representations that underpin their computational capabilities. Methods for quantifying geometric similarity in neural representations have become a popular tool for identifying computational principles that are potentially shared across neural systems. These methods generally assume that neural responses are deterministic and static. However, responses of biological systems, and some artificial systems, are noisy and dynamically unfold over time. Furthermore, these characteristics can have substantial influence on a system’s computational capabilities. Here, we demonstrate that existing metrics can fail to capture key differences between neural systems with noisy dynamic responses. We then propose a metric for comparing the geometry of noisy neural trajectories, which can be derived as an optimal transport distance between Gaussian processes. We use the metric to compare models of neural responses in different regions of the motor system and to compare the dynamics of latent diffusion models for text-to-image synthesis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cNmu0hZ4CL"
        ],
        "venue": [
          "/venue/cNmu0hZ4CL@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cNmu0hZ4CL"
        ],
        "detail": [
          "https://openreview.net/forum?id=cNmu0hZ4CL"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 11
      },
      "raw_excerpt": "Comparing noisy neural population dynamics using optimal transport distances [PDF 18 ] [Copy] [Kimi 11 ] [REL] Authors : Amin Nejatbakhsh , Victor Geadah , Alex Williams , David Lipshutz Biological and artificial neural systems form high-dimensional neural representations that underpin their computational capabilities. Methods for quantifying geometric similarity in neural representations have become a popular tool for identifying computational principles that are potentially shared across neural systems. These methods generally assume that neural responses are deterministic and static. However, responses of biological systems, and some artificial systems, are noisy and dynamically unfold over time. Furthermore, these characteristics can have substantial influence on a system’s computational capabilities. Here, we demonstrate that existing metrics can fail to capture key differences between neural systems with noisy dynamic responses. We then propose a metric for comparing the geometry of noisy neural trajectories, which can be derived as an optimal transport distance between Gaussian processes. We use the metric to compare models of neural responses in different regions of the motor system and to compare the dynamics of latent diffusion models for text-to-image synthesis. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "cH65nS5sOz@OpenReview",
      "index": 44,
      "title": "Subgraph Federated Learning for Local Generalization",
      "authors": [
        "Sungwon Kim",
        "Yoonho Lee",
        "Yunhak Oh",
        "Namkyeong Lee",
        "Sukwon Yun",
        "Junseok Lee",
        "Sein Kim",
        "Carl Yang",
        "Chanyoung Park"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "fedlog",
        "overfitting",
        "local",
        "unseen",
        "federated",
        "generalization",
        "client",
        "data",
        "subgraph",
        "mutable"
      ],
      "summary": "Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution. Since they focus solely on performing well on each client's local data, they are prone to overfitting to their local distributions (i.e., local overfitting), which hinders their ability to generalize to unseen data with diverse label distributions. In contrast, our proposed method, FedLoG, effectively tackles this issue by mitigating local overfitting. Our model generates global synthetic data by condensing the reliable information from each class representation and its structural information across clients. Using these synthetic data as a training set, we alleviate the local overfitting problem by adaptively generalizing the absent knowledge within each local dataset. This enhances the generalization capabilities of local models, enabling them to handle unseen data effectively. Our model outperforms baselines in our proposed experimental settings, which are designed to measure generalization power to unseen data in practical scenarios. Our code is available at https://anonymous.4open.science/r/FedLoG-89EE",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cH65nS5sOz"
        ],
        "venue": [
          "/venue/cH65nS5sOz@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cH65nS5sOz"
        ],
        "detail": [
          "https://openreview.net/forum?id=cH65nS5sOz"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 12
      },
      "raw_excerpt": "Subgraph Federated Learning for Local Generalization [PDF 20 ] [Copy] [Kimi 12 ] [REL] Authors : Sungwon Kim , Yoonho Lee , Yunhak Oh , Namkyeong Lee , Sukwon Yun , Junseok Lee , Sein Kim , Carl Yang , Chanyoung Park Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution. Since they focus solely on performing well on each client's local data, they are prone to overfitting to their local distributions (i.e., local overfitting), which hinders their ability to generalize to unseen data with diverse label distributions. In contrast, our proposed method, FedLoG, effectively tackles this issue by mitigating local overfitting. Our model generates global synthetic data by condensing the reliable information from each class representation and its structural information across clients. Using these synthetic data as a training set, we alleviate the local overfitting problem by adaptively generalizing the absent knowledge within each local dataset. This enhances the generalization capabilities of local models, enabling them to handle unseen data effectively. Our model outperforms baselines in our proposed experimental settings, which are designed to measure generalization power to unseen data in practical scenarios. Our code is available at https://anonymous.4open.science/r/FedLoG-89EE Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "bnINPG5A32@OpenReview",
      "index": 45,
      "title": "RB-Modulation: Training-Free Personalization using Stochastic Optimal Control",
      "authors": [
        "Litu Rout",
        "Yujia Chen",
        "Nataniel Ruiz",
        "Abhishek Kumar",
        "Constantine Caramanis",
        "Sanjay Shakkottai",
        "Wen-Sheng Chu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "style",
        "content",
        "modulation",
        "reference",
        "personalization",
        "training",
        "free",
        "controlnets",
        "difficulties",
        "stochastic"
      ],
      "summary": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models.Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image.With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of *content* and *style* in a training-free manner. Additionally, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bnINPG5A32"
        ],
        "venue": [
          "/venue/bnINPG5A32@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bnINPG5A32"
        ],
        "detail": [
          "https://openreview.net/forum?id=bnINPG5A32"
        ]
      },
      "scores": {
        "pdf": 27,
        "kimi": 18
      },
      "raw_excerpt": "RB-Modulation: Training-Free Personalization using Stochastic Optimal Control [PDF 27 ] [Copy] [Kimi 18 ] [REL] Authors : Litu Rout , Yujia Chen , Nataniel Ruiz , Abhishek Kumar , Constantine Caramanis , Sanjay Shakkottai , Wen-Sheng Chu We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models.Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image.With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of *content* and *style* in a training-free manner. Additionally, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "bVTM2QKYuA@OpenReview",
      "index": 46,
      "title": "The Representation Geometry of Features and Hierarchy in Large Language Models",
      "authors": [
        "Kiho Park",
        "Yo Joong Choe",
        "Yibo Jiang",
        "Victor Veitch"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "representation",
        "concepts",
        "formalization",
        "language",
        "contrasts",
        "animal",
        "natural",
        "geometry",
        "hypothesis",
        "directions"
      ],
      "summary": "The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as _directions_ in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as _vectors_. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bVTM2QKYuA"
        ],
        "venue": [
          "/venue/bVTM2QKYuA@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bVTM2QKYuA"
        ],
        "detail": [
          "https://openreview.net/forum?id=bVTM2QKYuA"
        ]
      },
      "scores": {
        "pdf": 41,
        "kimi": 44
      },
      "raw_excerpt": "The Representation Geometry of Features and Hierarchy in Large Language Models [PDF 41 ] [Copy] [Kimi 44 ] [REL] Authors : Kiho Park , Yo Joong Choe , Yibo Jiang , Victor Veitch The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as _directions_ in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as _vectors_. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "rFpZnn11gj@OpenReview",
      "index": 47,
      "title": "PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration",
      "authors": [
        "Yuxuan Sun",
        "Yunlong Zhang",
        "Yixuan Si",
        "Chenglu Zhu",
        "Kai Zhang",
        "Zhongyi Shui",
        "Jingxiong Li",
        "Xuan Gong",
        "XINHENG LYU",
        "Tao Lin",
        "Lin Yang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "pathgen",
        "pathology",
        "image",
        "clip",
        "wsi",
        "pairs",
        "quality",
        "vlms",
        "slide",
        "captions"
      ],
      "summary": "Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model to generate captions for these images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathGen-1-6M/PathGen-1.6M.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rFpZnn11gj"
        ],
        "venue": [
          "/venue/rFpZnn11gj@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rFpZnn11gj"
        ],
        "detail": [
          "https://openreview.net/forum?id=rFpZnn11gj"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 46
      },
      "raw_excerpt": "PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration [PDF 33 ] [Copy] [Kimi 46 ] [REL] Authors : Yuxuan Sun , Yunlong Zhang , Yixuan Si , Chenglu Zhu , Kai Zhang , Zhongyi Shui , Jingxiong Li , Xuan Gong , XINHENG LYU , Tao Lin , Lin Yang Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model to generate captions for these images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathGen-1-6M/PathGen-1.6M. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "aWXnKanInf@OpenReview",
      "index": 48,
      "title": "TopoLM: brain-like spatio-functional organization in a topographic language model",
      "authors": [
        "Neil Rathi",
        "Johannes Mehrer",
        "Badr AlKhamissi",
        "Taha Binhuraib",
        "Nicholas Blauch",
        "Martin Schrimpf"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "topolm",
        "organization",
        "language",
        "brain",
        "functional",
        "spatio",
        "clusters",
        "topographic",
        "human",
        "system"
      ],
      "summary": "Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aWXnKanInf"
        ],
        "venue": [
          "/venue/aWXnKanInf@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aWXnKanInf"
        ],
        "detail": [
          "https://openreview.net/forum?id=aWXnKanInf"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 23
      },
      "raw_excerpt": "TopoLM: brain-like spatio-functional organization in a topographic language model [PDF 17 ] [Copy] [Kimi 23 ] [REL] Authors : Neil Rathi , Johannes Mehrer , Badr AlKhamissi , Taha Binhuraib , Nicholas Blauch , Martin Schrimpf Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "VpWki1v2P8@OpenReview",
      "index": 49,
      "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
      "authors": [
        "Jui-Nan Yen",
        "Si Si",
        "Zhao Meng",
        "Felix Yu",
        "Venkata Sai Surya Subramanyam Duvvuri",
        "Inderjit Dhillon",
        "Cho-Jui Hsieh",
        "Sanjiv Kumar"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "lora",
        "rite",
        "gemma",
        "llm",
        "transformation",
        "optimizers",
        "equilibration",
        "hellaswag",
        "openbookqa",
        "optimization"
      ],
      "summary": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VpWki1v2P8"
        ],
        "venue": [
          "/venue/VpWki1v2P8@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VpWki1v2P8"
        ],
        "detail": [
          "https://openreview.net/forum?id=VpWki1v2P8"
        ]
      },
      "scores": {
        "pdf": 41,
        "kimi": 26
      },
      "raw_excerpt": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization [PDF 41 ] [Copy] [Kimi 26 ] [REL] Authors : Jui-Nan Yen , Si Si , Zhao Meng , Felix Yu , Venkata Sai Surya Subramanyam Duvvuri , Inderjit Dhillon , Cho-Jui Hsieh , Sanjiv Kumar Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA). Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "YrycTjllL0@OpenReview",
      "index": 50,
      "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
      "authors": [
        "Terry Yue Zhuo",
        "Minh Chien Vu",
        "Jenny Chim",
        "Han Hu",
        "Wenhao Yu",
        "Ratnadira Widyasari",
        "Imam Nur Bani Yusuf",
        "Haolan Zhan",
        "Junda He",
        "Indraneil Paul",
        "Simon Brunner",
        "Chen GONG",
        "James Hoang",
        "Armel Zebaze",
        "Xiaoheng Hong",
        "Wen-Ding Li",
        "Jean Kaddour",
        "Ming Xu",
        "Zhihan Zhang",
        "Prateek Yadav",
        "Naman Jain",
        "Alex Gu",
        "Zhoujun Cheng",
        "Jiawei Liu",
        "Qian Liu",
        "Zijian Wang",
        "David Lo",
        "Binyuan Hui",
        "Niklas Muennighoff",
        "Daniel Fried",
        "Xiaoning Du",
        "Harm de Vries",
        "Leandro Von Werra"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "bigcodebench",
        "llms",
        "calls",
        "instructions",
        "tasks",
        "function",
        "tools",
        "diverse",
        "task",
        "solve"
      ],
      "summary": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YrycTjllL0"
        ],
        "venue": [
          "/venue/YrycTjllL0@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YrycTjllL0"
        ],
        "detail": [
          "https://openreview.net/forum?id=YrycTjllL0"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 22
      },
      "raw_excerpt": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions [PDF 18 ] [Copy] [Kimi 22 ] [REL] Authors : Terry Yue Zhuo , Minh Chien Vu , Jenny Chim , Han Hu , Wenhao Yu , Ratnadira Widyasari , Imam Nur Bani Yusuf , Haolan Zhan , Junda He , Indraneil Paul , Simon Brunner , Chen GONG , James Hoang , Armel Zebaze , Xiaoheng Hong , Wen-Ding Li , Jean Kaddour , Ming Xu , Zhihan Zhang , Prateek Yadav , Naman Jain , Alex Gu , Zhoujun Cheng , Jiawei Liu , Qian Liu , Zijian Wang , David Lo , Binyuan Hui , Niklas Muennighoff , Daniel Fried , Xiaoning Du , Harm de Vries , Leandro Von Werra Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "YUYJsHOf3c@OpenReview",
      "index": 51,
      "title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement",
      "authors": [
        "XIANGYU PENG",
        "Congying Xia",
        "Xinyi Yang",
        "Caiming Xiong",
        "Chien-Sheng Wu",
        "Chen Xing"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "regenesis",
        "reasoning",
        "self",
        "paths",
        "ood",
        "improvement",
        "generalists",
        "task",
        "post",
        "synthesizing"
      ],
      "summary": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose **Reasoning Generalist via Self-Improvement (ReGenesis)**, a method to *self-synthesize reasoning paths as post-training data by progressing from abstract to concrete*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YUYJsHOf3c"
        ],
        "venue": [
          "/venue/YUYJsHOf3c@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YUYJsHOf3c"
        ],
        "detail": [
          "https://openreview.net/forum?id=YUYJsHOf3c"
        ]
      },
      "scores": {
        "pdf": 55,
        "kimi": 70
      },
      "raw_excerpt": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement [PDF 55 ] [Copy] [Kimi 70 ] [REL] Authors : XIANGYU PENG , Congying Xia , Xinyi Yang , Caiming Xiong , Chien-Sheng Wu , Chen Xing Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose **Reasoning Generalist via Self-Improvement (ReGenesis)**, a method to *self-synthesize reasoning paths as post-training data by progressing from abstract to concrete*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "YLIsIzC74j@OpenReview",
      "index": 52,
      "title": "LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement",
      "authors": [
        "Zijie Geng",
        "Jie Wang",
        "Ziyan Liu",
        "Siyuan Xu",
        "Zhentao Tang",
        "Shixiong Kai",
        "Mingxuan Yuan",
        "Jianye HAO",
        "Feng Wu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "metrics",
        "placement",
        "lamplace",
        "stage",
        "macro",
        "chip",
        "cross",
        "predictor",
        "surrogate",
        "timing"
      ],
      "summary": "Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design.However, existing methods primarily focus on *online* optimization of *intermediate surrogate metrics* that are available at the current placement stage, rather than directly targeting the *cross-stage metrics*---such as the timing performance---that measure the final chip quality.This is mainly because of the high computational costs associated with performing post-placement stages for evaluating such metrics, making the *online* optimization impractical.Consequently, these optimizations struggle to align with actual performance improvements and can even lead to severe manufacturing issues.To bridge this gap, we propose **LaMPlace**, which **L**earns **a** **M**ask for optimizing cross-stage metrics in macro placement.Specifically, LaMPlace trains a predictor on *offline* data to estimate these *cross-stage metrics* and then leverages the predictor to quickly generate a mask, i.e., a pixel-level feature map that quantifies the impact of placing a macro in each chip grid location on the design metrics.This mask essentially acts as a fast evaluator, enabling placement decisions based on *cross-stage metrics* rather than *intermediate surrogate metrics*.Experiments on commonly used benchmarks demonstrate that LaMPlace significantly improves the chip quality across several key design metrics, achieving an average improvement of 9.6\\%, notably 43.0\\% and 30.4\\% in terms of WNS and TNS, respectively, which are two crucial cross-stage metrics that reflect the final chip quality in terms of the timing performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YLIsIzC74j"
        ],
        "venue": [
          "/venue/YLIsIzC74j@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YLIsIzC74j"
        ],
        "detail": [
          "https://openreview.net/forum?id=YLIsIzC74j"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 15
      },
      "raw_excerpt": "LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement [PDF 14 ] [Copy] [Kimi 15 ] [REL] Authors : Zijie Geng , Jie Wang , Ziyan Liu , Siyuan Xu , Zhentao Tang , Shixiong Kai , Mingxuan Yuan , Jianye HAO , Feng Wu Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design.However, existing methods primarily focus on *online* optimization of *intermediate surrogate metrics* that are available at the current placement stage, rather than directly targeting the *cross-stage metrics*---such as the timing performance---that measure the final chip quality.This is mainly because of the high computational costs associated with performing post-placement stages for evaluating such metrics, making the *online* optimization impractical.Consequently, these optimizations struggle to align with actual performance improvements and can even lead to severe manufacturing issues.To bridge this gap, we propose **LaMPlace**, which **L**earns **a** **M**ask for optimizing cross-stage metrics in macro placement.Specifically, LaMPlace trains a predictor on *offline* data to estimate these *cross-stage metrics* and then leverages the predictor to quickly generate a mask, i.e., a pixel-level feature map that quantifies the impact of placing a macro in each chip grid location on the design metrics.This mask essentially acts as a fast evaluator, enabling placement decisions based on *cross-stage metrics* rather than *intermediate surrogate metrics*.Experiments on commonly used benchmarks demonstrate that LaMPlace significantly improves the chip quality across several key design metrics, achieving an average improvement of 9.6\\%, notably 43.0\\% and 30.4\\% in terms of WNS and TNS, respectively, which are two crucial cross-stage metrics that reflect the final chip quality in terms of the timing performance. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Y6aHdDNQYD@OpenReview",
      "index": 53,
      "title": "MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection",
      "authors": [
        "Zhuoxiao Chen",
        "Junjie Meng",
        "Mahsa Baktashmotlagh",
        "Yonggang Zhang",
        "Zi Huang",
        "Yadan Luo"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "shifts",
        "synergy",
        "checkpoints",
        "test",
        "mos",
        "corruptions",
        "adaptation",
        "lidar",
        "cross",
        "corruption"
      ],
      "summary": "LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging cross-corruption cross-corruption scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: https://github.com/zhuoxiao-chen/MOS.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Y6aHdDNQYD"
        ],
        "venue": [
          "/venue/Y6aHdDNQYD@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Y6aHdDNQYD"
        ],
        "detail": [
          "https://openreview.net/forum?id=Y6aHdDNQYD"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 12
      },
      "raw_excerpt": "MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection [PDF 13 ] [Copy] [Kimi 12 ] [REL] Authors : Zhuoxiao Chen , Junjie Meng , Mahsa Baktashmotlagh , Yonggang Zhang , Zi Huang , Yadan Luo LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging cross-corruption cross-corruption scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: https://github.com/zhuoxiao-chen/MOS. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "xDrFWUmCne@OpenReview",
      "index": 54,
      "title": "Learning to Discretize Denoising Diffusion ODEs",
      "authors": [
        "Vinh Tong",
        "Trung-Dung Hoang",
        "Anji Liu",
        "Guy Van den Broeck",
        "Mathias Niepert"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ld3",
        "nfe",
        "dpms",
        "sampling",
        "diffusion",
        "unconditional",
        "afhqv2",
        "discretize",
        "generation",
        "pre"
      ],
      "summary": "Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xDrFWUmCne"
        ],
        "venue": [
          "/venue/xDrFWUmCne@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xDrFWUmCne"
        ],
        "detail": [
          "https://openreview.net/forum?id=xDrFWUmCne"
        ]
      },
      "scores": {
        "pdf": 37,
        "kimi": 29
      },
      "raw_excerpt": "Learning to Discretize Denoising Diffusion ODEs [PDF 37 ] [Copy] [Kimi 29 ] [REL] Authors : Vinh Tong , Trung-Dung Hoang , Anji Liu , Guy Van den Broeck , Mathias Niepert Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Xo0Q1N7CGk@OpenReview",
      "index": 55,
      "title": "An Investigation of Conformal Isometry Hypothesis for Grid Cells",
      "authors": [
        "Dehong Xu",
        "Ruiqi Gao",
        "Wenhao Zhang",
        "Xue-Xin Wei",
        "Yingnian Wu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "conformal",
        "hypothesis",
        "isometry",
        "grid",
        "hexagon",
        "neural",
        "space",
        "cells",
        "agent",
        "patterns"
      ],
      "summary": "This paper investigates the conformal isometry hypothesis as a potential explanation for hexagonal periodic patterns in grid cell response maps. The hypothesis posits that grid cell activity forms a high-dimensional vector in neural space, encoding the agent’s position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis suggests that this neural manifold is a conformally isometric embedding of physical space, where local displacements in neural space are proportional to those in physical space. In this paper, we conduct numerical experiments to show that this hypothesis leads to the hexagon periodic patterns of grid cells, agnostic to the choice of transformation models. Furthermore, we present a theoretical understanding that hexagon patterns emerge by minimizing our loss function because hexagon flat torus exhibits minimal deviation from local conformal isometry. In addition, we propose a conformal modulation of the agent's input velocity, enabling the recurrent neural network of grid cells to satisfy the conformal isometry hypothesis automatically.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Xo0Q1N7CGk"
        ],
        "venue": [
          "/venue/Xo0Q1N7CGk@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Xo0Q1N7CGk"
        ],
        "detail": [
          "https://openreview.net/forum?id=Xo0Q1N7CGk"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 16
      },
      "raw_excerpt": "An Investigation of Conformal Isometry Hypothesis for Grid Cells [PDF 14 ] [Copy] [Kimi 16 ] [REL] Authors : Dehong Xu , Ruiqi Gao , Wenhao Zhang , Xue-Xin Wei , Yingnian Wu This paper investigates the conformal isometry hypothesis as a potential explanation for hexagonal periodic patterns in grid cell response maps. The hypothesis posits that grid cell activity forms a high-dimensional vector in neural space, encoding the agent’s position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis suggests that this neural manifold is a conformally isometric embedding of physical space, where local displacements in neural space are proportional to those in physical space. In this paper, we conduct numerical experiments to show that this hypothesis leads to the hexagon periodic patterns of grid cells, agnostic to the choice of transformation models. Furthermore, we present a theoretical understanding that hexagon patterns emerge by minimizing our loss function because hexagon flat torus exhibits minimal deviation from local conformal isometry. In addition, we propose a conformal modulation of the agent's input velocity, enabling the recurrent neural network of grid cells to satisfy the conformal isometry hypothesis automatically. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "XmProj9cPs@OpenReview",
      "index": 56,
      "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows",
      "authors": [
        "Fangyu Lei",
        "Jixuan Chen",
        "Yuxiao Ye",
        "Ruisheng Cao",
        "Dongchan Shin",
        "Hongjin SU",
        "Zhaoqing Suo",
        "Hongcheng Gao",
        "Wenjing Hu",
        "Pengcheng Yin",
        "Victor Zhong",
        "Caiming Xiong",
        "Ruoxi Sun",
        "Qian Liu",
        "Sida Wang",
        "Tao Yu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "sql",
        "spider",
        "enterprise",
        "world",
        "text",
        "database",
        "real",
        "workflows",
        "workflow",
        "often"
      ],
      "summary": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics.We introduce Spider 2.0, an evaluation framework comprising 595 595 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake.We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 100 lines, which goes far beyond traditional text-to-SQL challenges.Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 15.1\\% of the tasks, compared with 91.2\\% on Spider 1.0 and 73.0\\% on BIRD.Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation --- especially in prior text-to-SQL benchmarks --- they require significant improvement in order to achieve adequate performance for real-world enterprise usage.Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XmProj9cPs"
        ],
        "venue": [
          "/venue/XmProj9cPs@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XmProj9cPs"
        ],
        "detail": [
          "https://openreview.net/forum?id=XmProj9cPs"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 24
      },
      "raw_excerpt": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows [PDF 16 ] [Copy] [Kimi 24 ] [REL] Authors : Fangyu Lei , Jixuan Chen , Yuxiao Ye , Ruisheng Cao , Dongchan Shin , Hongjin SU , Zhaoqing Suo , Hongcheng Gao , Wenjing Hu , Pengcheng Yin , Victor Zhong , Caiming Xiong , Ruoxi Sun , Qian Liu , Sida Wang , Tao Yu Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics.We introduce Spider 2.0, an evaluation framework comprising 595 595 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake.We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 100 lines, which goes far beyond traditional text-to-SQL challenges.Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 15.1\\% of the tasks, compared with 91.2\\% on Spider 1.0 and 73.0\\% on BIRD.Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation --- especially in prior text-to-SQL benchmarks --- they require significant improvement in order to achieve adequate performance for real-world enterprise usage.Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "XFYUwIyTxQ@OpenReview",
      "index": 57,
      "title": "EmbodiedSAM: Online Segment Any 3D Thing in Real Time",
      "authors": [
        "Xiuwei Xu",
        "Huangxing Chen",
        "Linqing Zhao",
        "Ziwei Wang",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "vfm",
        "masks",
        "perception",
        "embodiedsam",
        "online",
        "embodied",
        "frames",
        "sam",
        "segment",
        "instance"
      ],
      "summary": "Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the field of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either offline or too slow that cannot be applied in practical embodied tasks. In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting. This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so efficient object matching between frames is required. To address these challenges, we first propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively refined by a dual-level query decoder. In this way, the 2D masks are transferred to fine-grained shapes on 3D point clouds. Benefit from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efficient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves state-of-the-art performance among online 3D perception models, even outperforming offline VFM-assisted 3D instance segmentation methods by a large margin. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in data-efficient setting. Code and demo will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XFYUwIyTxQ"
        ],
        "venue": [
          "/venue/XFYUwIyTxQ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XFYUwIyTxQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=XFYUwIyTxQ"
        ]
      },
      "scores": {
        "pdf": 22,
        "kimi": 16
      },
      "raw_excerpt": "EmbodiedSAM: Online Segment Any 3D Thing in Real Time [PDF 22 ] [Copy] [Kimi 16 ] [REL] Authors : Xiuwei Xu , Huangxing Chen , Linqing Zhao , Ziwei Wang , Jie Zhou , Jiwen Lu Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the field of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either offline or too slow that cannot be applied in practical embodied tasks. In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting. This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so efficient object matching between frames is required. To address these challenges, we first propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively refined by a dual-level query decoder. In this way, the 2D masks are transferred to fine-grained shapes on 3D point clouds. Benefit from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efficient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves state-of-the-art performance among online 3D perception models, even outperforming offline VFM-assisted 3D instance segmentation methods by a large margin. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in data-efficient setting. Code and demo will be released. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "X1OfiRYCLn@OpenReview",
      "index": 58,
      "title": "Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping",
      "authors": [
        "Yue Yang",
        "Shuibo Zhang",
        "Wenqi Shao",
        "Kaipeng Zhang",
        "Yi Bin",
        "Yu Wang",
        "Ping Luo"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "vlb",
        "lvlms",
        "bootstrapping",
        "multimodal",
        "evaluation",
        "contamination",
        "benchmarks",
        "language",
        "vision",
        "seedbench"
      ],
      "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=X1OfiRYCLn"
        ],
        "venue": [
          "/venue/X1OfiRYCLn@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=X1OfiRYCLn"
        ],
        "detail": [
          "https://openreview.net/forum?id=X1OfiRYCLn"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 26
      },
      "raw_excerpt": "Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping [PDF 17 ] [Copy] [Kimi 26 ] [REL] Authors : Yue Yang , Shuibo Zhang , Wenqi Shao , Kaipeng Zhang , Yi Bin , Yu Wang , Ping Luo Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "odjMSBSWRt@OpenReview",
      "index": 59,
      "title": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
      "authors": [
        "Esben Kran",
        "Hieu Minh Nguyen",
        "Akash Kundu",
        "Sami Jawhar",
        "Jinsuk Park",
        "Mateusz Jurewicz"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "darkbench",
        "manipulative",
        "llms",
        "dark",
        "patterns",
        "companies",
        "sneaking",
        "sycophancy",
        "anthropomorphism",
        "untruthful"
      ],
      "summary": "We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=odjMSBSWRt"
        ],
        "venue": [
          "/venue/odjMSBSWRt@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=odjMSBSWRt"
        ],
        "detail": [
          "https://openreview.net/forum?id=odjMSBSWRt"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 20
      },
      "raw_excerpt": "DarkBench: Benchmarking Dark Patterns in Large Language Models [PDF 15 ] [Copy] [Kimi 20 ] [REL] Authors : Esben Kran , Hieu Minh Nguyen , Akash Kundu , Sami Jawhar , Jinsuk Park , Mateusz Jurewicz We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "WCRQFlji2q@OpenReview",
      "index": 60,
      "title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
      "authors": [
        "Javier Ferrando",
        "Oscar Obeso",
        "Senthooran Rajamanoharan",
        "Neel Nanda"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "entity",
        "autoencoders",
        "hallucinations",
        "directions",
        "refuse",
        "hallucinate",
        "chat",
        "know",
        "sparse",
        "model"
      ],
      "summary": "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WCRQFlji2q"
        ],
        "venue": [
          "/venue/WCRQFlji2q@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WCRQFlji2q"
        ],
        "detail": [
          "https://openreview.net/forum?id=WCRQFlji2q"
        ]
      },
      "scores": {
        "pdf": 34,
        "kimi": 44
      },
      "raw_excerpt": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models [PDF 34 ] [Copy] [Kimi 44 ] [REL] Authors : Javier Ferrando , Oscar Obeso , Senthooran Rajamanoharan , Neel Nanda Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "r5IXBlTCGc@OpenReview",
      "index": 61,
      "title": "Consistency Checks for Language Model Forecasters",
      "authors": [
        "Daniel Paleka",
        "Abhimanyu Pallavi Sudhir",
        "Alejandro Alvarez",
        "Vineeth Bhat",
        "Adam Shen",
        "Evan Wang",
        "Florian Tramer"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "forecasters",
        "consistency",
        "forecasting",
        "forecaster",
        "checks",
        "predictions",
        "questions",
        "arbitrageur",
        "truth",
        "llm"
      ],
      "summary": "Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters *instantaneously*? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on *arbitrage*: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60\\% probability of winning the 2024 US presidential election, an arbitrageur could trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate strongly with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=r5IXBlTCGc"
        ],
        "venue": [
          "/venue/r5IXBlTCGc@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=r5IXBlTCGc"
        ],
        "detail": [
          "https://openreview.net/forum?id=r5IXBlTCGc"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 28
      },
      "raw_excerpt": "Consistency Checks for Language Model Forecasters [PDF 12 ] [Copy] [Kimi 28 ] [REL] Authors : Daniel Paleka , Abhimanyu Pallavi Sudhir , Alejandro Alvarez , Vineeth Bhat , Adam Shen , Evan Wang , Florian Tramer Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters *instantaneously*? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on *arbitrage*: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60\\% probability of winning the 2024 US presidential election, an arbitrageur could trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate strongly with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "UHPnqSTBPO@OpenReview",
      "index": 62,
      "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
      "authors": [
        "Jaehun Jung",
        "Faeze Brahman",
        "Yejin Choi"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "evaluation",
        "judges",
        "agreement",
        "human",
        "escalate",
        "selective",
        "llm",
        "provable",
        "guarantees",
        "judge"
      ],
      "summary": "We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this *selective evaluation* framework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce *Simulated Annotators*, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose *Cascaded Selective Evaluation*, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, *guarantees* over 80% human agreement with almost 80% test coverage.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UHPnqSTBPO"
        ],
        "venue": [
          "/venue/UHPnqSTBPO@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UHPnqSTBPO"
        ],
        "detail": [
          "https://openreview.net/forum?id=UHPnqSTBPO"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 22
      },
      "raw_excerpt": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement [PDF 10 ] [Copy] [Kimi 22 ] [REL] Authors : Jaehun Jung , Faeze Brahman , Yejin Choi We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this *selective evaluation* framework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce *Simulated Annotators*, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose *Cascaded Selective Evaluation*, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, *guarantees* over 80% human agreement with almost 80% test coverage. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "tTPHgb0EtV@OpenReview",
      "index": 63,
      "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Fatih Ilhan",
        "Selim Tekin",
        "Ling Liu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "harmful",
        "booster",
        "tuning",
        "perturbation",
        "fine",
        "alignment",
        "regularizer",
        "attenuating",
        "qi2023fine",
        "loss"
      ],
      "summary": "Harmful fine-tuning attack \\citep{qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that \\textit{harmful perturbation} over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at \\url{https://anonymous.4open.science/r/Booster-EF18}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tTPHgb0EtV"
        ],
        "venue": [
          "/venue/tTPHgb0EtV@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tTPHgb0EtV"
        ],
        "detail": [
          "https://openreview.net/forum?id=tTPHgb0EtV"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 16
      },
      "raw_excerpt": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation [PDF 15 ] [Copy] [Kimi 16 ] [REL] Authors : Tiansheng Huang , Sihao Hu , Fatih Ilhan , Selim Tekin , Ling Liu Harmful fine-tuning attack \\citep{qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that \\textit{harmful perturbation} over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at \\url{https://anonymous.4open.science/r/Booster-EF18}. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "SBCMNc3Mq3@OpenReview",
      "index": 64,
      "title": "ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials",
      "authors": [
        "Pin Chen",
        "Zexin Xu",
        "Qing Mo",
        "Hongjin Zhong",
        "Fengyang Xu",
        "Yutong Lu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ecd",
        "electronic",
        "precision",
        "charge",
        "ernzerhof",
        "hse",
        "dft",
        "pbe",
        "density",
        "crystalline"
      ],
      "summary": "Supervised machine learning techniques are increasingly being adopted to speed up electronic structure predictions, serving as alternatives to first-principles methods like Density Functional Theory (DFT). Although current DFT datasets mainly emphasize chemical properties and atomic forces, the precise prediction of electronic charge density is essential for accurately determining a system's total energy and ground state properties. In this study, we introduce a novel electronic charge density dataset named ECD, which encompasses 140,646 stable crystal geometries with medium-precision Perdew–Burke–Ernzerhof (PBE) functional data. Within this dataset, a subset of 7,147 geometries includes high-precision electronic charge density data calculated using the Heyd–Scuseria–Ernzerhof (HSE) functional in DFT. By designing various benchmark tasks for crystalline materials and emphasizing training with large-scale PBE data while fine-tuning with a smaller subset of high-precision HSE data, we demonstrate the efficacy of current machine learning models in predicting electronic charge densities.The ECD dataset and baseline models are open-sourced to support community efforts in developing new methodologies and accelerating materials design and applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SBCMNc3Mq3"
        ],
        "venue": [
          "/venue/SBCMNc3Mq3@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SBCMNc3Mq3"
        ],
        "detail": [
          "https://openreview.net/forum?id=SBCMNc3Mq3"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 13
      },
      "raw_excerpt": "ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials [PDF 11 ] [Copy] [Kimi 13 ] [REL] Authors : Pin Chen , Zexin Xu , Qing Mo , Hongjin Zhong , Fengyang Xu , Yutong Lu Supervised machine learning techniques are increasingly being adopted to speed up electronic structure predictions, serving as alternatives to first-principles methods like Density Functional Theory (DFT). Although current DFT datasets mainly emphasize chemical properties and atomic forces, the precise prediction of electronic charge density is essential for accurately determining a system's total energy and ground state properties. In this study, we introduce a novel electronic charge density dataset named ECD, which encompasses 140,646 stable crystal geometries with medium-precision Perdew–Burke–Ernzerhof (PBE) functional data. Within this dataset, a subset of 7,147 geometries includes high-precision electronic charge density data calculated using the Heyd–Scuseria–Ernzerhof (HSE) functional in DFT. By designing various benchmark tasks for crystalline materials and emphasizing training with large-scale PBE data while fine-tuning with a smaller subset of high-precision HSE data, we demonstrate the efficacy of current machine learning models in predicting electronic charge densities.The ECD dataset and baseline models are open-sourced to support community efforts in developing new methodologies and accelerating materials design and applications. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "RuP17cJtZo@OpenReview",
      "index": 65,
      "title": "Generator Matching: Generative modeling with arbitrary Markov processes",
      "authors": [
        "Peter Holderrieth",
        "Marton Havasi",
        "Jason Yim",
        "Neta Shaul",
        "Itai Gat",
        "Tommi Jaakkola",
        "Brian Karrer",
        "Ricky T. Q. Chen",
        "Yaron Lipman"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "generator",
        "matching",
        "generative",
        "markov",
        "processes",
        "modeling",
        "generators",
        "jump",
        "arbitrary",
        "vein"
      ],
      "summary": "We introduce generator matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that generator matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it provides the foundation to expand the design space to new and unexplored Markov processes such as jump processes. Finally, generator matching enables the construction of superpositions of Markov generative processes and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on protein and image structure generation, showing that superposition with a jump process improves image generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RuP17cJtZo"
        ],
        "venue": [
          "/venue/RuP17cJtZo@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RuP17cJtZo"
        ],
        "detail": [
          "https://openreview.net/forum?id=RuP17cJtZo"
        ]
      },
      "scores": {
        "pdf": 25,
        "kimi": 30
      },
      "raw_excerpt": "Generator Matching: Generative modeling with arbitrary Markov processes [PDF 25 ] [Copy] [Kimi 30 ] [REL] Authors : Peter Holderrieth , Marton Havasi , Jason Yim , Neta Shaul , Itai Gat , Tommi Jaakkola , Brian Karrer , Ricky T. Q. Chen , Yaron Lipman We introduce generator matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that generator matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it provides the foundation to expand the design space to new and unexplored Markov processes such as jump processes. Finally, generator matching enables the construction of superpositions of Markov generative processes and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on protein and image structure generation, showing that superposition with a jump process improves image generation. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "RWJX5F5I9g@OpenReview",
      "index": 66,
      "title": "Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration",
      "authors": [
        "Chen Jiang",
        "Jiahui An",
        "Yating Liu",
        "Ni Ji"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "bbn",
        "exploration",
        "brain",
        "bandit",
        "mab",
        "neural",
        "network",
        "uncertain",
        "controls",
        "tasks"
      ],
      "summary": "How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel conditions. To understand how the brain’s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this type of network (which is named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield networks. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with an uncertainty bias in a way that resembles human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical basis for efficient exploration in biological neural networks and proposes a general, brain-inspired algorithmic architecture for efficient exploration in RL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RWJX5F5I9g"
        ],
        "venue": [
          "/venue/RWJX5F5I9g@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RWJX5F5I9g"
        ],
        "detail": [
          "https://openreview.net/forum?id=RWJX5F5I9g"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 18
      },
      "raw_excerpt": "Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration [PDF 17 ] [Copy] [Kimi 18 ] [REL] Authors : Chen Jiang , Jiahui An , Yating Liu , Ni Ji How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel conditions. To understand how the brain’s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this type of network (which is named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield networks. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with an uncertainty bias in a way that resembles human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical basis for efficient exploration in biological neural networks and proposes a general, brain-inspired algorithmic architecture for efficient exploration in RL. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "vzItLaEoDa@OpenReview",
      "index": 67,
      "title": "Open-World Reinforcement Learning over Long Short-Term Imagination",
      "authors": [
        "Jiajian Li",
        "Qi Wang",
        "Yunbo Wang",
        "Xin Jin",
        "Yang Li",
        "Wenjun Zeng",
        "Xiaokang Yang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "imagination",
        "world",
        "short",
        "term",
        "long",
        "open",
        "minedojo",
        "reinforcement",
        "jumpy",
        "agents"
      ],
      "summary": "Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be “short-sighted”, as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a long short-term world model long short-term world model . To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vzItLaEoDa"
        ],
        "venue": [
          "/venue/vzItLaEoDa@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vzItLaEoDa"
        ],
        "detail": [
          "https://openreview.net/forum?id=vzItLaEoDa"
        ]
      },
      "scores": {
        "pdf": 28,
        "kimi": 36
      },
      "raw_excerpt": "Open-World Reinforcement Learning over Long Short-Term Imagination [PDF 28 ] [Copy] [Kimi 36 ] [REL] Authors : Jiajian Li , Qi Wang , Yunbo Wang , Xin Jin , Yang Li , Wenjun Zeng , Xiaokang Yang Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be “short-sighted”, as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a long short-term world model long short-term world model . To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "QWunLKbBGF@OpenReview",
      "index": 68,
      "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs",
      "authors": [
        "Siyan Zhao",
        "Mingyi Hong",
        "Yang Liu",
        "Devamanyu Hazarika",
        "Kaixiang Lin"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "prefeval",
        "preference",
        "llms",
        "preferences",
        "following",
        "conversations",
        "user",
        "personalized",
        "conversational",
        "prompting"
      ],
      "summary": "Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced andproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10\\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. We also find that multiple stated preferences within a conversation improve adherence and models are not affected by conflicting preferences. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QWunLKbBGF"
        ],
        "venue": [
          "/venue/QWunLKbBGF@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QWunLKbBGF"
        ],
        "detail": [
          "https://openreview.net/forum?id=QWunLKbBGF"
        ]
      },
      "scores": {
        "pdf": 25,
        "kimi": 51
      },
      "raw_excerpt": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs [PDF 25 ] [Copy] [Kimi 51 ] [REL] Authors : Siyan Zhao , Mingyi Hong , Yang Liu , Devamanyu Hazarika , Kaixiang Lin Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced andproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10\\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. We also find that multiple stated preferences within a conversation improve adherence and models are not affected by conflicting preferences. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "QEHrmQPBdd@OpenReview",
      "index": 69,
      "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
      "authors": [
        "Yantao Liu",
        "Zijun Yao",
        "Rui Min",
        "Yixin Cao",
        "Lei Hou",
        "Juanzi Li"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "reward",
        "bench",
        "style",
        "models",
        "subtlety",
        "language",
        "responses",
        "evaluate",
        "subtle",
        "content"
      ],
      "summary": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.These findings highlight the significant room for improvement in current reward models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QEHrmQPBdd"
        ],
        "venue": [
          "/venue/QEHrmQPBdd@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QEHrmQPBdd"
        ],
        "detail": [
          "https://openreview.net/forum?id=QEHrmQPBdd"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 12
      },
      "raw_excerpt": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style [PDF 12 ] [Copy] [Kimi 12 ] [REL] Authors : Yantao Liu , Zijun Yao , Rui Min , Yixin Cao , Lei Hou , Juanzi Li Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.These findings highlight the significant room for improvement in current reward models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "syThiTmWWm@OpenReview",
      "index": 70,
      "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
      "authors": [
        "Xiaosen Zheng",
        "Tianyu Pang",
        "Chao Du",
        "Qian Liu",
        "Jing Jiang",
        "Min Lin"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "win",
        "cheating",
        "alpacaeval",
        "promotional",
        "benchmarks",
        "automatic",
        "rates",
        "arena",
        "bench",
        "llm"
      ],
      "summary": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a **\"null model\"** that always outputs a **constant** response (*irrelevant to input instructions*) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5 86.5 LC win rate on AlpacaEval 2.0; an 83.0 83.0 score on Arena-Hard-Auto; and a 9.55 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are **transferable** because we assume that the instructions of these benchmarks (e.g., 805 805 samples of AlpacaEval 2.0) are *private* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=syThiTmWWm"
        ],
        "venue": [
          "/venue/syThiTmWWm@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=syThiTmWWm"
        ],
        "detail": [
          "https://openreview.net/forum?id=syThiTmWWm"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 14
      },
      "raw_excerpt": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates [PDF 11 ] [Copy] [Kimi 14 ] [REL] Authors : Xiaosen Zheng , Tianyu Pang , Chao Du , Qian Liu , Jing Jiang , Min Lin Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a **\"null model\"** that always outputs a **constant** response (*irrelevant to input instructions*) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5 86.5 LC win rate on AlpacaEval 2.0; an 83.0 83.0 score on Arena-Hard-Auto; and a 9.55 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are **transferable** because we assume that the instructions of these benchmarks (e.g., 805 805 samples of AlpacaEval 2.0) are *private* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "PSiijdQjNU@OpenReview",
      "index": 71,
      "title": "Steering Protein Family Design through Profile Bayesian Flow",
      "authors": [
        "Jingjing Gong",
        "Yu Pei",
        "Siyu Long",
        "Yuxuan Song",
        "Zhe Zhang",
        "Wenhao Huang",
        "Ziyao Cao",
        "Shuyi Zhang",
        "Hao Zhou",
        "Wei-Ying Ma"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "protein",
        "profilebfn",
        "profile",
        "family",
        "proteins",
        "msa",
        "bayesian",
        "flow",
        "design",
        "steering"
      ],
      "summary": "Protein family design emerges as a promising alternative by combining the advantages of de novo protein design and mutation-based directed evolution.In this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for specifically generative modeling of protein families. ProfileBFN extends the discrete Bayesian Flow Network from an MSA profile perspective, which can be trained on single protein sequences by regarding it as a degenerate profile, thereby achieving efficient protein family design by avoiding large-scale MSA data construction and training. Empirical results show that ProfileBFN has a profound understanding of proteins. When generating diverse and novel family proteins, it can accurately capture the structural characteristics of the family. The enzyme produced by this method is more likely than the previous approach to have the corresponding function, offering better odds of generating diverse proteins with the desired functionality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PSiijdQjNU"
        ],
        "venue": [
          "/venue/PSiijdQjNU@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PSiijdQjNU"
        ],
        "detail": [
          "https://openreview.net/forum?id=PSiijdQjNU"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 11
      },
      "raw_excerpt": "Steering Protein Family Design through Profile Bayesian Flow [PDF 15 ] [Copy] [Kimi 11 ] [REL] Authors : Jingjing Gong , Yu Pei , Siyu Long , Yuxuan Song , Zhe Zhang , Wenhao Huang , Ziyao Cao , Shuyi Zhang , Hao Zhou , Wei-Ying Ma Protein family design emerges as a promising alternative by combining the advantages of de novo protein design and mutation-based directed evolution.In this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for specifically generative modeling of protein families. ProfileBFN extends the discrete Bayesian Flow Network from an MSA profile perspective, which can be trained on single protein sequences by regarding it as a degenerate profile, thereby achieving efficient protein family design by avoiding large-scale MSA data construction and training. Empirical results show that ProfileBFN has a profound understanding of proteins. When generating diverse and novel family proteins, it can accurately capture the structural characteristics of the family. The enzyme produced by this method is more likely than the previous approach to have the corresponding function, offering better odds of generating diverse proteins with the desired functionality. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "PBjCTeDL6o@OpenReview",
      "index": 72,
      "title": "Unlearning-based Neural Interpretations",
      "authors": [
        "Ching Lam Choi",
        "Alexandre Duplessis",
        "Serge Belongie"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "unlearning",
        "interpretations",
        "baselines",
        "static",
        "manipulable",
        "smooths",
        "departing",
        "debiased",
        "erasing",
        "blurring"
      ],
      "summary": "Gradient-based interpretations often require an anchor point of comparison to avoid saturation in computing feature importance. We show that current baselines defined using static functions —constant mapping, averaging or blurring — inject harmful colour, texture or frequency assumptions that deviate from model behaviour. This leads to accumulation of irregular gradients, resulting in attribution maps that are biased, fragile and manipulable. Departing from the static approach, we propose UNI UNI to compute an (un)learnable, debiased and adaptive baseline by perturbing the input towards an unlearning direction unlearning direction of steepest ascent. Our method discovers reliable baselines and succeeds in erasing salient features, which in turn locally smooths the high-curvature decision boundaries. Our analyses point to unlearning as a promising avenue for generating faithful, efficient and robust interpretations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PBjCTeDL6o"
        ],
        "venue": [
          "/venue/PBjCTeDL6o@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PBjCTeDL6o"
        ],
        "detail": [
          "https://openreview.net/forum?id=PBjCTeDL6o"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 26
      },
      "raw_excerpt": "Unlearning-based Neural Interpretations [PDF 19 ] [Copy] [Kimi 26 ] [REL] Authors : Ching Lam Choi , Alexandre Duplessis , Serge Belongie Gradient-based interpretations often require an anchor point of comparison to avoid saturation in computing feature importance. We show that current baselines defined using static functions —constant mapping, averaging or blurring — inject harmful colour, texture or frequency assumptions that deviate from model behaviour. This leads to accumulation of irregular gradients, resulting in attribution maps that are biased, fragile and manipulable. Departing from the static approach, we propose UNI UNI to compute an (un)learnable, debiased and adaptive baseline by perturbing the input towards an unlearning direction unlearning direction of steepest ascent. Our method discovers reliable baselines and succeeds in erasing salient features, which in turn locally smooths the high-curvature decision boundaries. Our analyses point to unlearning as a promising avenue for generating faithful, efficient and robust interpretations. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "P4o9akekdf@OpenReview",
      "index": 73,
      "title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images",
      "authors": [
        "Botao Ye",
        "Sifei Liu",
        "Songyou Peng",
        "Haofei Xu",
        "Xueting Li",
        "Ming-Hsuan Yang",
        "Marc Pollefeys"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "pose",
        "unposed",
        "gaussians",
        "gaussian",
        "splats",
        "view",
        "input",
        "estimation",
        "reconstruction",
        "primitives"
      ],
      "summary": "We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from unposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. The source code and trained models will be made available to the public.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=P4o9akekdf"
        ],
        "venue": [
          "/venue/P4o9akekdf@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=P4o9akekdf"
        ],
        "detail": [
          "https://openreview.net/forum?id=P4o9akekdf"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 11
      },
      "raw_excerpt": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images [PDF 15 ] [Copy] [Kimi 11 ] [REL] Authors : Botao Ye , Sifei Liu , Songyou Peng , Haofei Xu , Xueting Li , Ming-Hsuan Yang , Marc Pollefeys We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from unposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. The source code and trained models will be made available to the public. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "ZCOwwRAaEl@OpenReview",
      "index": 74,
      "title": "Latent Bayesian Optimization via Autoregressive Normalizing Flows",
      "authors": [
        "Seunghun Lee",
        "Jinyoung Park",
        "Jaewon Chu",
        "Minseo Yoon",
        "Hyunwoo Kim"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "optimization",
        "lbo",
        "normalizing",
        "latent",
        "bayesian",
        "token",
        "spaces",
        "autoregressive",
        "discrepancy",
        "input"
      ],
      "summary": "Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions.Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the complexity of high-dimensional and structured data spaces.However, existing LBO approaches often suffer from the value discrepancy problem, which arises from the reconstruction gap between latent and input spaces.This value discrepancy problem propagates errors throughout the optimization process, which induces suboptimal optimization outcomes.To address this issue, we propose a Normalizing Flow-based Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative model to establish accurate and one-to-one mappings between latent and input spaces.To deal with sequence-based inputs, we introduce SeqFlow, an autoregressive sequence-specialized normalizing flow model designed to maintain one-to-one mappings between the input and latent spaces. Moreover, we develop a token-level adaptive candidate sampling strategy that dynamically adjusts the exploration probability of each token based on the token-level importance in the optimization process.Through extensive experiments, our NF-BO method demonstrates superior performance in molecule generation tasks, significantly outperforming traditional optimization methods and existing LBO approaches.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZCOwwRAaEl"
        ],
        "venue": [
          "/venue/ZCOwwRAaEl@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZCOwwRAaEl"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZCOwwRAaEl"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 28
      },
      "raw_excerpt": "Latent Bayesian Optimization via Autoregressive Normalizing Flows [PDF 21 ] [Copy] [Kimi 28 ] [REL] Authors : Seunghun Lee , Jinyoung Park , Jaewon Chu , Minseo Yoon , Hyunwoo Kim Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions.Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the complexity of high-dimensional and structured data spaces.However, existing LBO approaches often suffer from the value discrepancy problem, which arises from the reconstruction gap between latent and input spaces.This value discrepancy problem propagates errors throughout the optimization process, which induces suboptimal optimization outcomes.To address this issue, we propose a Normalizing Flow-based Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative model to establish accurate and one-to-one mappings between latent and input spaces.To deal with sequence-based inputs, we introduce SeqFlow, an autoregressive sequence-specialized normalizing flow model designed to maintain one-to-one mappings between the input and latent spaces. Moreover, we develop a token-level adaptive candidate sampling strategy that dynamically adjusts the exploration probability of each token based on the token-level importance in the optimization process.Through extensive experiments, our NF-BO method demonstrates superior performance in molecule generation tasks, significantly outperforming traditional optimization methods and existing LBO approaches. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Ozo7qJ5vZi@OpenReview",
      "index": 75,
      "title": "KAN: Kolmogorov–Arnold Networks",
      "authors": [
        "Ziming Liu",
        "Yixuan Wang",
        "Sachin Vaidya",
        "Fabian Ruehle",
        "James Halverson",
        "Marin Soljacic",
        "Thomas Hou",
        "Max Tegmark"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "kans",
        "mlps",
        "arnold",
        "kolmogorov",
        "interpretability",
        "accuracy",
        "kan",
        "alternatives",
        "activation",
        "weights"
      ],
      "summary": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons''), KANs have learnable activation functions on edges (\"weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful ``collaborators'' helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs. Despite the slow training of KANs, their improved accuracy and interpretability show the potential to improve today's deep learning models which rely heavily on MLPs. More research is necessary to make KANs' training more efficient.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Ozo7qJ5vZi"
        ],
        "venue": [
          "/venue/Ozo7qJ5vZi@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Ozo7qJ5vZi"
        ],
        "detail": [
          "https://openreview.net/forum?id=Ozo7qJ5vZi"
        ]
      },
      "scores": {
        "pdf": 41,
        "kimi": 27
      },
      "raw_excerpt": "KAN: Kolmogorov–Arnold Networks [PDF 41 ] [Copy] [Kimi 27 ] [REL] Authors : Ziming Liu , Yixuan Wang , Sachin Vaidya , Fabian Ruehle , James Halverson , Marin Soljacic , Thomas Hou , Max Tegmark Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons''), KANs have learnable activation functions on edges (\"weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful ``collaborators'' helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs. Despite the slow training of KANs, their improved accuracy and interpretability show the potential to improve today's deep learning models which rely heavily on MLPs. More research is necessary to make KANs' training more efficient. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "OvoCm1gGhN@OpenReview",
      "index": 76,
      "title": "Differential Transformer",
      "authors": [
        "Tianzhu Ye",
        "Li Dong",
        "Yuqing Xia",
        "Yutao Sun",
        "Yi Zhu",
        "Gao Huang",
        "Furu Wei"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "transformer",
        "diff",
        "attention",
        "context",
        "hallucination",
        "overallocate",
        "irrelevant",
        "differential",
        "canceling",
        "distracted"
      ],
      "summary": "Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture for large language models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OvoCm1gGhN"
        ],
        "venue": [
          "/venue/OvoCm1gGhN@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OvoCm1gGhN"
        ],
        "detail": [
          "https://openreview.net/forum?id=OvoCm1gGhN"
        ]
      },
      "scores": {
        "pdf": 69,
        "kimi": 56
      },
      "raw_excerpt": "Differential Transformer [PDF 69 ] [Copy] [Kimi 56 ] [REL] Authors : Tianzhu Ye , Li Dong , Yuqing Xia , Yutao Sun , Yi Zhu , Gao Huang , Furu Wei Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture for large language models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "OlzB6LnXcS@OpenReview",
      "index": 77,
      "title": "One Step Diffusion via Shortcut Models",
      "authors": [
        "Kevin Frans",
        "Danijar Hafner",
        "Sergey Levine",
        "Pieter Abbeel"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "shortcut",
        "models",
        "step",
        "sampling",
        "budgets",
        "network",
        "training",
        "reflow",
        "diffusion",
        "multiple"
      ],
      "summary": "Diffusion models and flow matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce Shortcut Models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OlzB6LnXcS"
        ],
        "venue": [
          "/venue/OlzB6LnXcS@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OlzB6LnXcS"
        ],
        "detail": [
          "https://openreview.net/forum?id=OlzB6LnXcS"
        ]
      },
      "scores": {
        "pdf": 41,
        "kimi": 37
      },
      "raw_excerpt": "One Step Diffusion via Shortcut Models [PDF 41 ] [Copy] [Kimi 37 ] [REL] Authors : Kevin Frans , Danijar Hafner , Sergey Levine , Pieter Abbeel Diffusion models and flow matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce Shortcut Models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "OfjIlbelrT@OpenReview",
      "index": 78,
      "title": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference",
      "authors": [
        "Xunhao Lai",
        "Jianqiao Lu",
        "Yao Luo",
        "Yiyuan Ma",
        "Xun Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "attention",
        "sparse",
        "flexprefill",
        "patterns",
        "inference",
        "query",
        "sequence",
        "predefined",
        "aware",
        "filling"
      ],
      "summary": "Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold.FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OfjIlbelrT"
        ],
        "venue": [
          "/venue/OfjIlbelrT@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OfjIlbelrT"
        ],
        "detail": [
          "https://openreview.net/forum?id=OfjIlbelrT"
        ]
      },
      "scores": {
        "pdf": 27,
        "kimi": 35
      },
      "raw_excerpt": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference [PDF 27 ] [Copy] [Kimi 35 ] [REL] Authors : Xunhao Lai , Jianqiao Lu , Yao Luo , Yiyuan Ma , Xun Zhou Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold.FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "u1cQYxRI1H@OpenReview",
      "index": 79,
      "title": "Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport",
      "authors": [
        "Lvmin Zhang",
        "Anyi Rao",
        "Maneesh Agrawala"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "illumination",
        "editing",
        "wild",
        "harmonization",
        "diffusion",
        "light",
        "albedos",
        "imposing",
        "training",
        "consistent"
      ],
      "summary": "Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (> 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=u1cQYxRI1H"
        ],
        "venue": [
          "/venue/u1cQYxRI1H@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=u1cQYxRI1H"
        ],
        "detail": [
          "https://openreview.net/forum?id=u1cQYxRI1H"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 19
      },
      "raw_excerpt": "Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport [PDF 11 ] [Copy] [Kimi 19 ] [REL] Authors : Lvmin Zhang , Anyi Rao , Maneesh Agrawala Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (> 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "NN6QHwgRrQ@OpenReview",
      "index": 80,
      "title": "MAP: Multi-Human-Value Alignment Palette",
      "authors": [
        "Xinran Wang",
        "qi le",
        "Ammar Ahmed",
        "Enmao Diao",
        "Yi Zhou",
        "Nathalie Baracaldo",
        "Jie Ding",
        "Ali Anwar"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "alignment",
        "human",
        "values",
        "value",
        "palette",
        "map",
        "align",
        "across",
        "offs",
        "multi"
      ],
      "summary": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NN6QHwgRrQ"
        ],
        "venue": [
          "/venue/NN6QHwgRrQ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NN6QHwgRrQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=NN6QHwgRrQ"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 18
      },
      "raw_excerpt": "MAP: Multi-Human-Value Alignment Palette [PDF 17 ] [Copy] [Kimi 18 ] [REL] Authors : Xinran Wang , qi le , Ammar Ahmed , Enmao Diao , Yi Zhou , Nathalie Baracaldo , Jie Ding , Ali Anwar Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "SnDmPkOJ0T@OpenReview",
      "index": 81,
      "title": "REEF: Representation Encoding Fingerprints for Large Language Models",
      "authors": [
        "Jie Zhang",
        "Dongrui Liu",
        "Chen Qian",
        "Linfeng Zhang",
        "Yong Liu",
        "Yu Qiao",
        "Jing Shao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "reef",
        "suspect",
        "victim",
        "llms",
        "intellectual",
        "owners",
        "model",
        "parties",
        "fingerprints",
        "language"
      ],
      "summary": "Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SnDmPkOJ0T"
        ],
        "venue": [
          "/venue/SnDmPkOJ0T@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SnDmPkOJ0T"
        ],
        "detail": [
          "https://openreview.net/forum?id=SnDmPkOJ0T"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 22
      },
      "raw_excerpt": "REEF: Representation Encoding Fingerprints for Large Language Models [PDF 15 ] [Copy] [Kimi 22 ] [REL] Authors : Jie Zhang , Dongrui Liu , Chen Qian , Linfeng Zhang , Yong Liu , Yu Qiao , Jing Shao Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Mfnh1Sqdwf@OpenReview",
      "index": 82,
      "title": "Learning to Discover Regulatory Elements for Gene Expression Prediction",
      "authors": [
        "Xingyu Su",
        "Haiyang Yu",
        "Degui Zhi",
        "Shuiwang Ji"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "gene",
        "regulatory",
        "expression",
        "epigenomic",
        "elements",
        "dna",
        "seq2exp",
        "discover",
        "causal",
        "prediction"
      ],
      "summary": "We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Mfnh1Sqdwf"
        ],
        "venue": [
          "/venue/Mfnh1Sqdwf@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Mfnh1Sqdwf"
        ],
        "detail": [
          "https://openreview.net/forum?id=Mfnh1Sqdwf"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 7
      },
      "raw_excerpt": "Learning to Discover Regulatory Elements for Gene Expression Prediction [PDF 8 ] [Copy] [Kimi 7 ] [REL] Authors : Xingyu Su , Haiyang Yu , Degui Zhi , Shuiwang Ji We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "KSLkFYHlYg@OpenReview",
      "index": 83,
      "title": "ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design",
      "authors": [
        "Keir Adams",
        "Kento Abeywardane",
        "Jenna Fromer",
        "Connor Coley"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "bioisosteric",
        "shepherd",
        "pharmacophores",
        "drug",
        "ligand",
        "bioactive",
        "design",
        "chemical",
        "scoring",
        "electrostatics"
      ],
      "summary": "Engineering molecules to exhibit precise 3D intermolecular interactions with their environment forms the basis of chemical design. In ligand-based drug design, bioisosteric analogues of known bioactive hits are often identified by virtually screening chemical libraries with shape, electrostatic, and pharmacophore similarity scoring functions. We instead hypothesize that a generative model which learns the joint distribution over 3D molecular structures and their interaction profiles may facilitate 3D interaction-aware chemical design. We specifically design ShEPhERD, an SE(3)-equivariant diffusion model which jointly diffuses/denoises 3D molecular graphs and representations of their shapes, electrostatic potential surfaces, and (directional) pharmacophores to/from Gaussian noise. Inspired by traditional ligand discovery, we compose 3D similarity scoring functions to assess ShEPhERD’s ability to conditionally generate novel molecules with desired interaction profiles. We demonstrate ShEPhERD’s potential for impact via exemplary drug design tasks including natural product ligand hopping, protein-blind bioactive hit diversification, and bioisosteric fragment merging.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KSLkFYHlYg"
        ],
        "venue": [
          "/venue/KSLkFYHlYg@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KSLkFYHlYg"
        ],
        "detail": [
          "https://openreview.net/forum?id=KSLkFYHlYg"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 8
      },
      "raw_excerpt": "ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design [PDF 11 ] [Copy] [Kimi 8 ] [REL] Authors : Keir Adams , Kento Abeywardane , Jenna Fromer , Connor Coley Engineering molecules to exhibit precise 3D intermolecular interactions with their environment forms the basis of chemical design. In ligand-based drug design, bioisosteric analogues of known bioactive hits are often identified by virtually screening chemical libraries with shape, electrostatic, and pharmacophore similarity scoring functions. We instead hypothesize that a generative model which learns the joint distribution over 3D molecular structures and their interaction profiles may facilitate 3D interaction-aware chemical design. We specifically design ShEPhERD, an SE(3)-equivariant diffusion model which jointly diffuses/denoises 3D molecular graphs and representations of their shapes, electrostatic potential surfaces, and (directional) pharmacophores to/from Gaussian noise. Inspired by traditional ligand discovery, we compose 3D similarity scoring functions to assess ShEPhERD’s ability to conditionally generate novel molecules with desired interaction profiles. We demonstrate ShEPhERD’s potential for impact via exemplary drug design tasks including natural product ligand hopping, protein-blind bioactive hit diversification, and bioisosteric fragment merging. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "JWtrk7mprJ@OpenReview",
      "index": 84,
      "title": "Residual Deep Gaussian Processes on Manifolds",
      "authors": [
        "Kacper Wyrwal",
        "Andreas Krause",
        "Viacheslav (Slava) Borovitskiy"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "manifold",
        "manifolds",
        "gaussian",
        "optimisation",
        "residual",
        "shallow",
        "unneeded",
        "stylised",
        "processes",
        "models"
      ],
      "summary": "We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks.With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields.We target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon.For example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes.Our models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded.We further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process.Finally, we show our models to have potential for speeding up inference for non-manifold data, when, and if, it can be mapped to a proxy manifold well enough.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JWtrk7mprJ"
        ],
        "venue": [
          "/venue/JWtrk7mprJ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JWtrk7mprJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=JWtrk7mprJ"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 20
      },
      "raw_excerpt": "Residual Deep Gaussian Processes on Manifolds [PDF 21 ] [Copy] [Kimi 20 ] [REL] Authors : Kacper Wyrwal , Andreas Krause , Viacheslav (Slava) Borovitskiy We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks.With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields.We target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon.For example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes.Our models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded.We further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process.Finally, we show our models to have potential for speeding up inference for non-manifold data, when, and if, it can be mapped to a proxy manifold well enough. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Iyrtb9EJBp@OpenReview",
      "index": 85,
      "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse",
      "authors": [
        "Maojia Song",
        "Shang Hong Sim",
        "Rishabh Bhardwaj",
        "Hai Leong Chieu",
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "trust",
        "rag",
        "align",
        "llms",
        "qampari",
        "refuse",
        "asqa",
        "eli5",
        "trustworthiness",
        "llama"
      ],
      "summary": "LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce TRUST-SCORE, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effec- tively adapt LLMs to the RAG task as measured by TRUST-SCORE. Consequently, we propose TRUST-ALIGN, a method to align LLMs for improved TRUST-SCORE performance. 26 out of 27 models aligned using TRUST-ALIGN substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, TRUST-ALIGN outperforms FRONT on ASQA (↑12.56), QAMPARI (↑36.04), and ELI5 (↑17.69). TRUST-ALIGN also significantly enhances models’ ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of TRUST-ALIGN across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Iyrtb9EJBp"
        ],
        "venue": [
          "/venue/Iyrtb9EJBp@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Iyrtb9EJBp"
        ],
        "detail": [
          "https://openreview.net/forum?id=Iyrtb9EJBp"
        ]
      },
      "scores": {
        "pdf": 22,
        "kimi": 34
      },
      "raw_excerpt": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse [PDF 22 ] [Copy] [Kimi 34 ] [REL] Authors : Maojia Song , Shang Hong Sim , Rishabh Bhardwaj , Hai Leong Chieu , Navonil Majumder , Soujanya Poria LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce TRUST-SCORE, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effec- tively adapt LLMs to the RAG task as measured by TRUST-SCORE. Consequently, we propose TRUST-ALIGN, a method to align LLMs for improved TRUST-SCORE performance. 26 out of 27 models aligned using TRUST-ALIGN substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, TRUST-ALIGN outperforms FRONT on ASQA (↑12.56), QAMPARI (↑36.04), and ELI5 (↑17.69). TRUST-ALIGN also significantly enhances models’ ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of TRUST-ALIGN across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "HD6bWcj87Y@OpenReview",
      "index": 86,
      "title": "Data Shapley in One Training Run",
      "authors": [
        "Jiachen (Tianhao) Wang",
        "Prateek Mittal",
        "Dawn Song",
        "Ruoxi Jia"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "shapley",
        "run",
        "data",
        "pretraining",
        "training",
        "contribution",
        "retraining",
        "foundation",
        "model",
        "attributing"
      ],
      "summary": "Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HD6bWcj87Y"
        ],
        "venue": [
          "/venue/HD6bWcj87Y@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HD6bWcj87Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=HD6bWcj87Y"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 16
      },
      "raw_excerpt": "Data Shapley in One Training Run [PDF 14 ] [Copy] [Kimi 16 ] [REL] Authors : Jiachen (Tianhao) Wang , Prateek Mittal , Dawn Song , Ruoxi Jia Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Fur0DtynPX@OpenReview",
      "index": 87,
      "title": "GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling",
      "authors": [
        "Honghui Wang",
        "Shiji Song",
        "Gao Huang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "gridmix",
        "modulation",
        "spatial",
        "modeling",
        "marble",
        "pde",
        "fields",
        "neural",
        "global",
        "domain"
      ],
      "summary": "Significant advancements have been achieved in PDE modeling using neural fields. Despite their effectiveness, existing methods rely on global modulation, limiting their ability to reconstruct local details. While spatial modulation with vanilla grid-based representations offers a promising alternative, it struggles with inadequate global information modeling and over-fitting to the training spatial domain. To address these challenges, we propose GridMix, a novel approach that models spatial modulation as a mixture of grid-based representations. GridMix effectively explores global structures while preserving locality for fine-grained modulation. Furthermore, we introduce spatial domain augmentation to enhance the robustness of the modulated neural fields against spatial domain variations. With all these innovations,our comprehensive approach culminates in MARBLE, a framework that significantly advancing the capabilities of neural fields in PDE modeling. The effectiveness of MARBLE is extensivelyvalidated on diverse benchmarks encompassing dynamics modeling and geometric prediction.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Fur0DtynPX"
        ],
        "venue": [
          "/venue/Fur0DtynPX@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Fur0DtynPX"
        ],
        "detail": [
          "https://openreview.net/forum?id=Fur0DtynPX"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 10
      },
      "raw_excerpt": "GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling [PDF 12 ] [Copy] [Kimi 10 ] [REL] Authors : Honghui Wang , Shiji Song , Gao Huang Significant advancements have been achieved in PDE modeling using neural fields. Despite their effectiveness, existing methods rely on global modulation, limiting their ability to reconstruct local details. While spatial modulation with vanilla grid-based representations offers a promising alternative, it struggles with inadequate global information modeling and over-fitting to the training spatial domain. To address these challenges, we propose GridMix, a novel approach that models spatial modulation as a mixture of grid-based representations. GridMix effectively explores global structures while preserving locality for fine-grained modulation. Furthermore, we introduce spatial domain augmentation to enhance the robustness of the modulated neural fields against spatial domain variations. With all these innovations,our comprehensive approach culminates in MARBLE, a framework that significantly advancing the capabilities of neural fields in PDE modeling. The effectiveness of MARBLE is extensivelyvalidated on diverse benchmarks encompassing dynamics modeling and geometric prediction. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "FSjIrOm1vz@OpenReview",
      "index": 88,
      "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
      "authors": [
        "Zhenrui Yue",
        "Honglei Zhuang",
        "Aijun Bai",
        "Kai Hui",
        "Rolf Jagerman",
        "Hansi Zeng",
        "Zhen Qin",
        "Dong Wang",
        "Xuanhui Wang",
        "Michael Bendersky"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "rag",
        "inference",
        "scaling",
        "computation",
        "context",
        "knowledge",
        "allocated",
        "llms",
        "performance",
        "strategies"
      ],
      "summary": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FSjIrOm1vz"
        ],
        "venue": [
          "/venue/FSjIrOm1vz@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FSjIrOm1vz"
        ],
        "detail": [
          "https://openreview.net/forum?id=FSjIrOm1vz"
        ]
      },
      "scores": {
        "pdf": 23,
        "kimi": 44
      },
      "raw_excerpt": "Inference Scaling for Long-Context Retrieval Augmented Generation [PDF 23 ] [Copy] [Kimi 44 ] [REL] Authors : Zhenrui Yue , Honglei Zhuang , Aijun Bai , Kai Hui , Rolf Jagerman , Hansi Zeng , Zhen Qin , Dong Wang , Xuanhui Wang , Michael Bendersky The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "FIj9IEPCKr@OpenReview",
      "index": 89,
      "title": "Proxy Denoising for Source-Free Domain Adaptation",
      "authors": [
        "Song Tang",
        "Wenxin Su",
        "Yan Gan",
        "Mao Ye",
        "Jianwei Dr. Zhang",
        "Xiatian Zhu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "vil",
        "proxy",
        "sfda",
        "prode",
        "adaptation",
        "source",
        "domain",
        "denoising",
        "supervision",
        "capitalize"
      ],
      "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, we introduce a novel Proxy Denoising (__ProDe__) approach. The key idea is to leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Concretely, we design a proxy denoising mechanism to correct ViL's predictions. This is grounded on a proxy confidence theory that models the dynamic effect of proxy's divergence against the domain-invariant space during adaptation. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set, generalized SFDA, multi-target, multi-source, and test-time settings. Our code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FIj9IEPCKr"
        ],
        "venue": [
          "/venue/FIj9IEPCKr@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FIj9IEPCKr"
        ],
        "detail": [
          "https://openreview.net/forum?id=FIj9IEPCKr"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 18
      },
      "raw_excerpt": "Proxy Denoising for Source-Free Domain Adaptation [PDF 13 ] [Copy] [Kimi 18 ] [REL] Authors : Song Tang , Wenxin Su , Yan Gan , Mao Ye , Jianwei Dr. Zhang , Xiatian Zhu Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, we introduce a novel Proxy Denoising (__ProDe__) approach. The key idea is to leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Concretely, we design a proxy denoising mechanism to correct ViL's predictions. This is grounded on a proxy confidence theory that models the dynamic effect of proxy's divergence against the domain-invariant space during adaptation. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set, generalized SFDA, multi-target, multi-source, and test-time settings. Our code will be released. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "FBkpCyujtS@OpenReview",
      "index": 90,
      "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs",
      "authors": [
        "Minh Nguyen",
        "Andrew Baker",
        "Clement Neo",
        "Allen Roush",
        "Andreas Kirsch",
        "Ravid Shwartz-Ziv"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "sampling",
        "min",
        "creative",
        "llm",
        "outputs",
        "diversity",
        "gpqa",
        "text",
        "token",
        "quality"
      ],
      "summary": "Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. However, popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures, leading to incoherent or repetitive outputs. To address this challenge, we propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by scaling according to the top token's probability. We conduct extensive experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing, demonstrating that min-p sampling improves both the quality and diversity of generated text, particularly at high temperatures. Moreover, human evaluations reveal a clear preference for min-p sampling in terms of both text quality and diversity. Min-p sampling has been adopted by multiple open-source LLM implementations, highlighting its practical utility and potential impact.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FBkpCyujtS"
        ],
        "venue": [
          "/venue/FBkpCyujtS@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FBkpCyujtS"
        ],
        "detail": [
          "https://openreview.net/forum?id=FBkpCyujtS"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 23
      },
      "raw_excerpt": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs [PDF 8 ] [Copy] [Kimi 23 ] [REL] Authors : Minh Nguyen , Andrew Baker , Clement Neo , Allen Roush , Andreas Kirsch , Ravid Shwartz-Ziv Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. However, popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures, leading to incoherent or repetitive outputs. To address this challenge, we propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by scaling according to the top token's probability. We conduct extensive experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing, demonstrating that min-p sampling improves both the quality and diversity of generated text, particularly at high temperatures. Moreover, human evaluations reveal a clear preference for min-p sampling in terms of both text quality and diversity. Min-p sampling has been adopted by multiple open-source LLM implementations, highlighting its practical utility and potential impact. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "EytBpUGB1Z@OpenReview",
      "index": 91,
      "title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
      "authors": [
        "Wenhao Wu",
        "Yizhong Wang",
        "Guangxuan Xiao",
        "Hao Peng",
        "Yao Fu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "heads",
        "retrieval",
        "context",
        "information",
        "hallucination",
        "mechanistically",
        "retrieving",
        "activated",
        "long",
        "factuality"
      ],
      "summary": "Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EytBpUGB1Z"
        ],
        "venue": [
          "/venue/EytBpUGB1Z@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EytBpUGB1Z"
        ],
        "detail": [
          "https://openreview.net/forum?id=EytBpUGB1Z"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 19
      },
      "raw_excerpt": "Retrieval Head Mechanistically Explains Long-Context Factuality [PDF 17 ] [Copy] [Kimi 19 ] [REL] Authors : Wenhao Wu , Yizhong Wang , Guangxuan Xiao , Hao Peng , Yao Fu Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "EO8xpnW7aX@OpenReview",
      "index": 92,
      "title": "Learning to Permute with Discrete Diffusion",
      "authors": [
        "Yongxing Zhang",
        "Donglin Yang",
        "Renjie Liao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "diffusion",
        "permute",
        "symmetricdiffusers",
        "discrete",
        "distribution",
        "reverse",
        "riffle",
        "learning",
        "plackett",
        "luce"
      ],
      "summary": "The group of permutations S n S n , also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over S n S n poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce *SymmetricDiffusers*, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over S n S n by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded \"denoising schedule\" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EO8xpnW7aX"
        ],
        "venue": [
          "/venue/EO8xpnW7aX@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EO8xpnW7aX"
        ],
        "detail": [
          "https://openreview.net/forum?id=EO8xpnW7aX"
        ]
      },
      "scores": {
        "pdf": 28,
        "kimi": 20
      },
      "raw_excerpt": "Learning to Permute with Discrete Diffusion [PDF 28 ] [Copy] [Kimi 20 ] [REL] Authors : Yongxing Zhang , Donglin Yang , Renjie Liao The group of permutations S n S n , also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over S n S n poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce *SymmetricDiffusers*, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over S n S n by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded \"denoising schedule\" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "E4Fk3YuG56@OpenReview",
      "index": 93,
      "title": "Cut Your Losses in Large-Vocabulary Language Models",
      "authors": [
        "Erik Wijmans",
        "Brody Huval",
        "Alexander Hertzberg",
        "Vladlen Koltun",
        "Philipp Krähenbühl"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "cce",
        "memory",
        "vocabulary",
        "cross",
        "entropy",
        "logits",
        "consumption",
        "computation",
        "logit",
        "cut"
      ],
      "summary": "As language models grow ever larger, so do their vocabularies.This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation.Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined.We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly.We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB.To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient.Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=E4Fk3YuG56"
        ],
        "venue": [
          "/venue/E4Fk3YuG56@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=E4Fk3YuG56"
        ],
        "detail": [
          "https://openreview.net/forum?id=E4Fk3YuG56"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 25
      },
      "raw_excerpt": "Cut Your Losses in Large-Vocabulary Language Models [PDF 12 ] [Copy] [Kimi 25 ] [REL] Authors : Erik Wijmans , Brody Huval , Alexander Hertzberg , Vladlen Koltun , Philipp Krähenbühl As language models grow ever larger, so do their vocabularies.This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation.Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined.We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly.We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB.To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient.Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "CxXGvKRDnL@OpenReview",
      "index": 94,
      "title": "Progressive Compression with Universally Quantized Diffusion Models",
      "authors": [
        "Yibo Yang",
        "Justus Will",
        "Stephan Mandt"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "diffusion",
        "compression",
        "elbo",
        "progressive",
        "models",
        "universally",
        "likelihood",
        "quantized",
        "codecs",
        "modeling"
      ],
      "summary": "Diffusion probabilistic models have achieved mainstream success in many generative modeling tasks, from image generation to inverse problem solving. A distinct feature of these models is that they correspond to deep hierarchical latent variable models optimizing a variational evidence lower bound (ELBO) on the data likelihood.Drawing on a basic connection between likelihood modeling and compression, we explore the potential of diffusion models for progressive coding, resulting in a sequence of bits that can be incrementally transmitted and decoded with progressively improving reconstruction quality.Unlike prior work based on Gaussian diffusion or conditional diffusion models, we propose a new form of diffusion model with uniform noise in the forward process, whose negative ELBO corresponds to the end-to-end compression cost using universal quantization.We obtain promising first results on image compression, achieving competitive rate-distortion-realism results on a wide range of bit-rates with a single model, bringing neural codecs a step closer to practical deployment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CxXGvKRDnL"
        ],
        "venue": [
          "/venue/CxXGvKRDnL@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CxXGvKRDnL"
        ],
        "detail": [
          "https://openreview.net/forum?id=CxXGvKRDnL"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 22
      },
      "raw_excerpt": "Progressive Compression with Universally Quantized Diffusion Models [PDF 18 ] [Copy] [Kimi 22 ] [REL] Authors : Yibo Yang , Justus Will , Stephan Mandt Diffusion probabilistic models have achieved mainstream success in many generative modeling tasks, from image generation to inverse problem solving. A distinct feature of these models is that they correspond to deep hierarchical latent variable models optimizing a variational evidence lower bound (ELBO) on the data likelihood.Drawing on a basic connection between likelihood modeling and compression, we explore the potential of diffusion models for progressive coding, resulting in a sequence of bits that can be incrementally transmitted and decoded with progressively improving reconstruction quality.Unlike prior work based on Gaussian diffusion or conditional diffusion models, we propose a new form of diffusion model with uniform noise in the forward process, whose negative ELBO corresponds to the end-to-end compression cost using universal quantization.We obtain promising first results on image compression, achieving competitive rate-distortion-realism results on a wide range of bit-rates with a single model, bringing neural codecs a step closer to practical deployment. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Cjz9Xhm7sI@OpenReview",
      "index": 95,
      "title": "High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation",
      "authors": [
        "Ziye Wang",
        "Yiran Qin",
        "Lin Zeng",
        "Ruimao Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "radar",
        "nowcasting",
        "gaussian",
        "stc",
        "gaumamba",
        "weather",
        "dynamic",
        "prediction",
        "forecasting",
        "spatiotemporal"
      ],
      "summary": "Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored. To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting. Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks. With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals. The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over 16 × 16 × higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Cjz9Xhm7sI"
        ],
        "venue": [
          "/venue/Cjz9Xhm7sI@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Cjz9Xhm7sI"
        ],
        "detail": [
          "https://openreview.net/forum?id=Cjz9Xhm7sI"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 13
      },
      "raw_excerpt": "High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation [PDF 10 ] [Copy] [Kimi 13 ] [REL] Authors : Ziye Wang , Yiran Qin , Lin Zeng , Ruimao Zhang Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored. To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting. Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks. With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals. The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over 16 × 16 × higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Bo62NeU6VF@OpenReview",
      "index": 96,
      "title": "Backtracking Improves Generation Safety",
      "authors": [
        "Yiming Zhang",
        "Jianfeng Chi",
        "Hailey Nguyen",
        "Kartikeya Upasani",
        "Daniel Bikel",
        "Jason E Weston",
        "Eric Michael Smith"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "backtracking",
        "safety",
        "unsafe",
        "helpfulness",
        "generation",
        "language",
        "happily",
        "harmlessness",
        "backtrack",
        "models"
      ],
      "summary": "Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic.In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text.This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety.Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to \"undo\" and recover from their own unsafe generation through the introduction of a special [RESET] token.Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness.We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\\% → → 1.5\\%) in our evaluations without regression in helpfulness.Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Bo62NeU6VF"
        ],
        "venue": [
          "/venue/Bo62NeU6VF@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Bo62NeU6VF"
        ],
        "detail": [
          "https://openreview.net/forum?id=Bo62NeU6VF"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 17
      },
      "raw_excerpt": "Backtracking Improves Generation Safety [PDF 11 ] [Copy] [Kimi 17 ] [REL] Authors : Yiming Zhang , Jianfeng Chi , Hailey Nguyen , Kartikeya Upasani , Daniel Bikel , Jason E Weston , Eric Michael Smith Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic.In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text.This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety.Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to \"undo\" and recover from their own unsafe generation through the introduction of a special [RESET] token.Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness.We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\\% → → 1.5\\%) in our evaluations without regression in helpfulness.Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "AP0ndQloqR@OpenReview",
      "index": 97,
      "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces",
      "authors": [
        "Saket Tiwari",
        "Omer Gottesman",
        "George D Konidaris"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "action",
        "dimensionality",
        "spaces",
        "attainable",
        "manifold",
        "continuous",
        "state",
        "reinforcement",
        "theoretical",
        "induce"
      ],
      "summary": "Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induce a set of attainable states in RL. We show that training dynamics of a two layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AP0ndQloqR"
        ],
        "venue": [
          "/venue/AP0ndQloqR@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AP0ndQloqR"
        ],
        "detail": [
          "https://openreview.net/forum?id=AP0ndQloqR"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 21
      },
      "raw_excerpt": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces [PDF 12 ] [Copy] [Kimi 21 ] [REL] Authors : Saket Tiwari , Omer Gottesman , George D Konidaris Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induce a set of attainable states in RL. We show that training dynamics of a two layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "9pW2J49flQ@OpenReview",
      "index": 98,
      "title": "DeepLTL: Learning to Efficiently Satisfy Complex LTL Instructions",
      "authors": [
        "Mathias Jackermeier",
        "Alessandro Abate"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ltl",
        "deepltl",
        "specifications",
        "satisfy",
        "efficiently",
        "policies",
        "instructions",
        "horizon",
        "learning",
        "büchi"
      ],
      "summary": "Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to the finite-horizon fragment of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of Büchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9pW2J49flQ"
        ],
        "venue": [
          "/venue/9pW2J49flQ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9pW2J49flQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=9pW2J49flQ"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 12
      },
      "raw_excerpt": "DeepLTL: Learning to Efficiently Satisfy Complex LTL Instructions [PDF 7 ] [Copy] [Kimi 12 ] [REL] Authors : Mathias Jackermeier , Alessandro Abate Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to the finite-horizon fragment of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of Büchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "9VGTk2NYjF@OpenReview",
      "index": 99,
      "title": "The Complexity of Two-Team Polymatrix Games with Independent Adversaries",
      "authors": [
        "Alexandros Hollender",
        "Gilbert Maystre",
        "Sai Ganesh Nagarajan"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "polymatrix",
        "games",
        "team",
        "teams",
        "multiplayer",
        "adversaries",
        "nash",
        "setting",
        "equilibria",
        "players"
      ],
      "summary": "Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the computation of Nash equilibria in such games where each pair of players plays either a zero-sum or a coordination game. We are particularly interested in the setting where players can be grouped into a small number of teams of identical interest. While the three-team version of the problem is known to be PPAD-complete, the complexity for two teams has remained open. Our main contribution is to prove that the two-team version remains hard, namely it is CLS-hard. Furthermore, we show that this lower bound is tight for the setting where one of the teams consists of multiple independent adversaries. On the way to obtaining our main result, we prove hardness of finding any stationary point in the simplest type of non-convex-concave min-max constrained optimization problem, namely for a class of bilinear polynomial objective functions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9VGTk2NYjF"
        ],
        "venue": [
          "/venue/9VGTk2NYjF@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9VGTk2NYjF"
        ],
        "detail": [
          "https://openreview.net/forum?id=9VGTk2NYjF"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 10
      },
      "raw_excerpt": "The Complexity of Two-Team Polymatrix Games with Independent Adversaries [PDF 3 ] [Copy] [Kimi 10 ] [REL] Authors : Alexandros Hollender , Gilbert Maystre , Sai Ganesh Nagarajan Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the computation of Nash equilibria in such games where each pair of players plays either a zero-sum or a coordination game. We are particularly interested in the setting where players can be grouped into a small number of teams of identical interest. While the three-team version of the problem is known to be PPAD-complete, the complexity for two teams has remained open. Our main contribution is to prove that the two-team version remains hard, namely it is CLS-hard. Furthermore, we show that this lower bound is tight for the setting where one of the teams consists of multiple independent adversaries. On the way to obtaining our main result, we prove hardness of finding any stationary point in the simplest type of non-convex-concave min-max constrained optimization problem, namely for a class of bilinear polynomial objective functions. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "8enWnd6Gp3@OpenReview",
      "index": 100,
      "title": "TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes",
      "authors": [
        "Minghao Guo",
        "Bohan Wang",
        "Kaiming He",
        "Wojciech Matusik"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "tetsphere",
        "splatting",
        "volumetric",
        "meshes",
        "tetrahedral",
        "quality",
        "manifoldness",
        "lagrangian",
        "mesh",
        "underused"
      ],
      "summary": "We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres, with geometric regularizations and constraints that effectively resolve common mesh issues such as irregular triangles, non-manifoldness, and floating artifacts. Experimental results on multi-view and single-view reconstruction highlight TetSphere splatting's superior mesh quality while maintaining competitive reconstruction accuracy compared to state-of-the-art methods. Additionally, TetSphere splatting demonstrates versatility by seamlessly integrating into generative modeling tasks, such as image-to-3D and text-to-3D generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8enWnd6Gp3"
        ],
        "venue": [
          "/venue/8enWnd6Gp3@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8enWnd6Gp3"
        ],
        "detail": [
          "https://openreview.net/forum?id=8enWnd6Gp3"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 7
      },
      "raw_excerpt": "TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes [PDF 10 ] [Copy] [Kimi 7 ] [REL] Authors : Minghao Guo , Bohan Wang , Kaiming He , Wojciech Matusik We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres, with geometric regularizations and constraints that effectively resolve common mesh issues such as irregular triangles, non-manifoldness, and floating artifacts. Experimental results on multi-view and single-view reconstruction highlight TetSphere splatting's superior mesh quality while maintaining competitive reconstruction accuracy compared to state-of-the-art methods. Additionally, TetSphere splatting demonstrates versatility by seamlessly integrating into generative modeling tasks, such as image-to-3D and text-to-3D generation. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "84pDoCD4lH@OpenReview",
      "index": 101,
      "title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities",
      "authors": [
        "Zheyuan Zhang",
        "Fengyuan Hu",
        "Jayjun Lee",
        "Freda Shi",
        "Parisa Kordjamshidi",
        "Joyce Chai",
        "Ziqiao Ma"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "vlms",
        "ambiguities",
        "language",
        "spatial",
        "conventions",
        "vision",
        "reference",
        "comfort",
        "reasoning",
        "ambiguous"
      ],
      "summary": "Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=84pDoCD4lH"
        ],
        "venue": [
          "/venue/84pDoCD4lH@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=84pDoCD4lH"
        ],
        "detail": [
          "https://openreview.net/forum?id=84pDoCD4lH"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 22
      },
      "raw_excerpt": "Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities [PDF 13 ] [Copy] [Kimi 22 ] [REL] Authors : Zheyuan Zhang , Fengyuan Hu , Jayjun Lee , Freda Shi , Parisa Kordjamshidi , Joyce Chai , Ziqiao Ma Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "6EUtjXAvmj@OpenReview",
      "index": 102,
      "title": "Variational Diffusion Posterior Sampling with Midpoint Guidance",
      "authors": [
        "Badr MOUFAD",
        "Yazid Janati el idrissi",
        "Lisa Bedin",
        "Alain Oliviero Durmus",
        "randal douc",
        "Eric Moulines",
        "Jimmy Olsson"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "guidance",
        "diffusion",
        "posterior",
        "intractable",
        "sampling",
        "midpoint",
        "priors",
        "term",
        "inverse",
        "score"
      ],
      "summary": "Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches formulate the problem as that of sampling from a surrogate diffusion model targeting the posterior and decompose its scores into two terms: the prior score and an intractable guidance term. While the former is replaced by the pre-trained score of the considered diffusion model, the guidance term has to be estimated. In this paper, we propose a novel approach that utilises a decomposition of the transitions which, in contrast to previous methods, allows a trade-off between the complexity of the intractable guidance term and that of the prior transitions. We validate the proposed approach through extensive experiments on linear and nonlinear inverse problems, including challenging cases with latent diffusion models as priors, and demonstrate its effectiveness in reconstructing electrocardiogram (ECG) from partial measurements for accurate cardiac diagnosis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6EUtjXAvmj"
        ],
        "venue": [
          "/venue/6EUtjXAvmj@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6EUtjXAvmj"
        ],
        "detail": [
          "https://openreview.net/forum?id=6EUtjXAvmj"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 16
      },
      "raw_excerpt": "Variational Diffusion Posterior Sampling with Midpoint Guidance [PDF 16 ] [Copy] [Kimi 16 ] [REL] Authors : Badr MOUFAD , Yazid Janati el idrissi , Lisa Bedin , Alain Oliviero Durmus , randal douc , Eric Moulines , Jimmy Olsson Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches formulate the problem as that of sampling from a surrogate diffusion model targeting the posterior and decompose its scores into two terms: the prior score and an intractable guidance term. While the former is replaced by the pre-trained score of the considered diffusion model, the guidance term has to be estimated. In this paper, we propose a novel approach that utilises a decomposition of the transitions which, in contrast to previous methods, allows a trade-off between the complexity of the intractable guidance term and that of the prior transitions. We validate the proposed approach through extensive experiments on linear and nonlinear inverse problems, including challenging cases with latent diffusion models as priors, and demonstrate its effectiveness in reconstructing electrocardiogram (ECG) from partial measurements for accurate cardiac diagnosis. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "5UKrnKuspb@OpenReview",
      "index": 103,
      "title": "NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields",
      "authors": [
        "Hanqiao Ye",
        "Yuzhou Liu",
        "Yangdong Liu",
        "Shuhan Shen"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "neuralplane",
        "plane",
        "primitives",
        "reconstruction",
        "planar",
        "neural",
        "observations",
        "semantics",
        "coplanarity",
        "scannetv2"
      ],
      "summary": "3D maps assembled from planar primitives are compact and expressive in representing man-made environments, making them suitable for a spectrum of applications. In this paper, we present **NeuralPlane**, a novel approach that explores **neural** fields for multi-view 3D **plane** reconstruction. Our method is centered upon the core idea of distilling geometric and semantic cues from inconsistent 2D plane observations into a unified 3D neural representation, which unlocks the full leverage of plane attributes. This idea is accomplished by NeuralPlane via several key designs, including: 1) a monocular module that generates geometrically smooth and semantically meaningful segments as 2D plane observations, 2) a plane-guided training procedure that implicitly learns accurate plane locations from multi-view plane observations, and 3) a self-supervised feature field termed *Neural Coplanarity Field* that enables the modeling of scene semantics alongside the geometry. Without relying on plane annotations, our method achieves high-fidelity reconstruction comprising planar primitives that are not only crisp but also well-aligned with the semantic content. Comprehensive experiments on ScanNetv2 and ScanNet++ demonstrate the superiority of our results in both geometry and semantics.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5UKrnKuspb"
        ],
        "venue": [
          "/venue/5UKrnKuspb@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5UKrnKuspb"
        ],
        "detail": [
          "https://openreview.net/forum?id=5UKrnKuspb"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 8
      },
      "raw_excerpt": "NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields [PDF 10 ] [Copy] [Kimi 8 ] [REL] Authors : Hanqiao Ye , Yuzhou Liu , Yangdong Liu , Shuhan Shen 3D maps assembled from planar primitives are compact and expressive in representing man-made environments, making them suitable for a spectrum of applications. In this paper, we present **NeuralPlane**, a novel approach that explores **neural** fields for multi-view 3D **plane** reconstruction. Our method is centered upon the core idea of distilling geometric and semantic cues from inconsistent 2D plane observations into a unified 3D neural representation, which unlocks the full leverage of plane attributes. This idea is accomplished by NeuralPlane via several key designs, including: 1) a monocular module that generates geometrically smooth and semantically meaningful segments as 2D plane observations, 2) a plane-guided training procedure that implicitly learns accurate plane locations from multi-view plane observations, and 3) a self-supervised feature field termed *Neural Coplanarity Field* that enables the modeling of scene semantics alongside the geometry. Without relying on plane annotations, our method achieves high-fidelity reconstruction comprising planar primitives that are not only crisp but also well-aligned with the semantic content. Comprehensive experiments on ScanNetv2 and ScanNet++ demonstrate the superiority of our results in both geometry and semantics. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "5U1rlpX68A@OpenReview",
      "index": 104,
      "title": "S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning",
      "authors": [
        "Yichen Wu",
        "Hongming Piao",
        "Long-Kai Huang",
        "Renzhen Wang",
        "Wanhua Li",
        "Hanspeter Pfister",
        "Deyu Meng",
        "Kede Ma",
        "Ying Wei"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "lora",
        "gating",
        "scalable",
        "incremental",
        "low",
        "rank",
        "adaptation",
        "foundation",
        "learning",
        "trained"
      ],
      "summary": "Continual Learning (CL) with foundation models has recently emerged as a promising approach to harnessing the power of pre-trained models for sequential tasks. Existing prompt-based methods generally use a gating mechanism to select relevant prompts aligned with the test query for further processing. However, the success of these methods largely depends on the precision of the gating mechanism, which becomes less scalable with additional computational overhead as tasks increases. To overcome these issues, we propose a Scalable Low-Rank Adaptation (S-LoRA) method for CL (in particular class incremental learning), which incrementally decouples the learning of the direction and magnitude of LoRA parameters. S-LoRA supports efficient inference by employing the last-stage trained model for direct testing without a gating process. Our theoretical and empirical analysis demonstrates that S-LoRA tends to follow a low-loss trajectory that converges to an overlapped low-loss region, resulting in an excellent stability-plasticity trade-off in CL. Furthermore, based on our findings, we develop variants of S-LoRA with further improved scalability. Extensive experiments across multiple CL benchmarks and various foundation models consistently validate the effectiveness of S-LoRA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5U1rlpX68A"
        ],
        "venue": [
          "/venue/5U1rlpX68A@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5U1rlpX68A"
        ],
        "detail": [
          "https://openreview.net/forum?id=5U1rlpX68A"
        ]
      },
      "scores": {
        "pdf": 32,
        "kimi": 29
      },
      "raw_excerpt": "S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning [PDF 32 ] [Copy] [Kimi 29 ] [REL] Authors : Yichen Wu , Hongming Piao , Long-Kai Huang , Renzhen Wang , Wanhua Li , Hanspeter Pfister , Deyu Meng , Kede Ma , Ying Wei Continual Learning (CL) with foundation models has recently emerged as a promising approach to harnessing the power of pre-trained models for sequential tasks. Existing prompt-based methods generally use a gating mechanism to select relevant prompts aligned with the test query for further processing. However, the success of these methods largely depends on the precision of the gating mechanism, which becomes less scalable with additional computational overhead as tasks increases. To overcome these issues, we propose a Scalable Low-Rank Adaptation (S-LoRA) method for CL (in particular class incremental learning), which incrementally decouples the learning of the direction and magnitude of LoRA parameters. S-LoRA supports efficient inference by employing the last-stage trained model for direct testing without a gating process. Our theoretical and empirical analysis demonstrates that S-LoRA tends to follow a low-loss trajectory that converges to an overlapped low-loss region, resulting in an excellent stability-plasticity trade-off in CL. Furthermore, based on our findings, we develop variants of S-LoRA with further improved scalability. Extensive experiments across multiple CL benchmarks and various foundation models consistently validate the effectiveness of S-LoRA. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "51WraMid8K@OpenReview",
      "index": 105,
      "title": "A Probabilistic Perspective on Unlearning and Alignment for Large Language Models",
      "authors": [
        "Yan Scholten",
        "Stephan Günnemann",
        "Leo Schwinn"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "unlearning",
        "evaluations",
        "probabilistic",
        "llms",
        "deterministic",
        "alignment",
        "estimates",
        "output",
        "language",
        "comprehensive"
      ],
      "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework in LLMs. Namely, we derive novel metrics with high-probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Through a case study focused on unlearning, we reveal that deterministic evaluations falsely indicate successful unlearning, whereas our probabilistic evaluations demonstrate that most if not all of the supposedly unlearned information remains accessible in these models. Additionally, we propose a novel unlearning loss based on entropy optimization and adaptive temperature scaling, which significantly improves unlearning in probabilistic settings on recent benchmarks. Our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=51WraMid8K"
        ],
        "venue": [
          "/venue/51WraMid8K@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=51WraMid8K"
        ],
        "detail": [
          "https://openreview.net/forum?id=51WraMid8K"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 22
      },
      "raw_excerpt": "A Probabilistic Perspective on Unlearning and Alignment for Large Language Models [PDF 9 ] [Copy] [Kimi 22 ] [REL] Authors : Yan Scholten , Stephan Günnemann , Leo Schwinn Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework in LLMs. Namely, we derive novel metrics with high-probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Through a case study focused on unlearning, we reveal that deterministic evaluations falsely indicate successful unlearning, whereas our probabilistic evaluations demonstrate that most if not all of the supposedly unlearned information remains accessible in these models. Additionally, we propose a novel unlearning loss based on entropy optimization and adaptive temperature scaling, which significantly improves unlearning in probabilistic settings on recent benchmarks. Our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "4OaO3GjP7k@OpenReview",
      "index": 106,
      "title": "Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning",
      "authors": [
        "HyunKyu Lee",
        "Sung Whan Yoon"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "reward",
        "landscapes",
        "flatter",
        "robustness",
        "flat",
        "policy",
        "actions",
        "parameter",
        "reinforcement",
        "space"
      ],
      "summary": "Investigating flat minima on loss surfaces in parameter space is well-documented in the supervised learning context, highlighting its advantages for model generalization. However, limited attention has been paid to the reinforcement learning (RL) context, where the impact of flatter reward landscapes in policy parameter space remains largely unexplored. Beyond merely extrapolating from supervised learning, which suggests a link between flat reward landscapes and enhanced generalization, we aim to formally connect the flatness of the reward surface to the robustness of RL models. In policy models where a deep neural network determines actions, flatter reward landscapes in response to parameter perturbations lead to consistent rewards even when actions are perturbed. Moreover, robustness to actions further contributes to robustness against other variations, such as changes in state transition probabilities and reward functions. We extensively simulate various RL environments, confirming the consistent benefits of flatter reward landscapes in enhancing the robustness of RL under diverse conditions, including action selection, transition dynamics, and reward functions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4OaO3GjP7k"
        ],
        "venue": [
          "/venue/4OaO3GjP7k@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4OaO3GjP7k"
        ],
        "detail": [
          "https://openreview.net/forum?id=4OaO3GjP7k"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 24
      },
      "raw_excerpt": "Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning [PDF 16 ] [Copy] [Kimi 24 ] [REL] Authors : HyunKyu Lee , Sung Whan Yoon Investigating flat minima on loss surfaces in parameter space is well-documented in the supervised learning context, highlighting its advantages for model generalization. However, limited attention has been paid to the reinforcement learning (RL) context, where the impact of flatter reward landscapes in policy parameter space remains largely unexplored. Beyond merely extrapolating from supervised learning, which suggests a link between flat reward landscapes and enhanced generalization, we aim to formally connect the flatness of the reward surface to the robustness of RL models. In policy models where a deep neural network determines actions, flatter reward landscapes in response to parameter perturbations lead to consistent rewards even when actions are perturbed. Moreover, robustness to actions further contributes to robustness against other variations, such as changes in state transition probabilities and reward functions. We extensively simulate various RL environments, confirming the consistent benefits of flatter reward landscapes in enhancing the robustness of RL under diverse conditions, including action selection, transition dynamics, and reward functions. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "4FWAwZtd2n@OpenReview",
      "index": 107,
      "title": "Scaling Test-Time Compute Optimally Can be More Effective than Scaling LLM Parameters",
      "authors": [
        "Charlie Snell",
        "Jaehoon Lee",
        "Kelvin Xu",
        "Aviral Kumar"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "test",
        "compute",
        "scaling",
        "time",
        "prompt",
        "llm",
        "inference",
        "llms",
        "improve",
        "computation"
      ],
      "summary": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a ``compute-optimal'' scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4FWAwZtd2n"
        ],
        "venue": [
          "/venue/4FWAwZtd2n@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4FWAwZtd2n"
        ],
        "detail": [
          "https://openreview.net/forum?id=4FWAwZtd2n"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 28
      },
      "raw_excerpt": "Scaling Test-Time Compute Optimally Can be More Effective than Scaling LLM Parameters [PDF 10 ] [Copy] [Kimi 28 ] [REL] Authors : Charlie Snell , Jaehoon Lee , Kelvin Xu , Aviral Kumar Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a ``compute-optimal'' scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "3i13Gev2hV@OpenReview",
      "index": 108,
      "title": "Compositional Entailment Learning for Hyperbolic Vision-Language Models",
      "authors": [
        "Avik Pal",
        "Max van Spengler",
        "Guido D&#x27;Amely di Melendugno",
        "Alessandro Flaborea",
        "Fabio Galasso",
        "Pascal Mettes"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "hyperbolic",
        "entailment",
        "vision",
        "textual",
        "language",
        "compositional",
        "image",
        "hierarchical",
        "boxes",
        "text"
      ],
      "summary": "Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3i13Gev2hV"
        ],
        "venue": [
          "/venue/3i13Gev2hV@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3i13Gev2hV"
        ],
        "detail": [
          "https://openreview.net/forum?id=3i13Gev2hV"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 17
      },
      "raw_excerpt": "Compositional Entailment Learning for Hyperbolic Vision-Language Models [PDF 21 ] [Copy] [Kimi 17 ] [REL] Authors : Avik Pal , Max van Spengler , Guido D&#x27;Amely di Melendugno , Alessandro Flaborea , Fabio Galasso , Pascal Mettes Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "3IFRygQKGL@OpenReview",
      "index": 109,
      "title": "OptionZero: Planning with Learned Options",
      "authors": [
        "Po-Wei Huang",
        "Pei-Chiun Peng",
        "Hung Guei",
        "Ti-Rong Wu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "optionzero",
        "options",
        "muzero",
        "planning",
        "learned",
        "learns",
        "superhuman",
        "games",
        "131",
        "human"
      ],
      "summary": "Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data. Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named OptionZero. OptionZero incorporates an option network into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dynamics network in MuZero to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3IFRygQKGL"
        ],
        "venue": [
          "/venue/3IFRygQKGL@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3IFRygQKGL"
        ],
        "detail": [
          "https://openreview.net/forum?id=3IFRygQKGL"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 9
      },
      "raw_excerpt": "OptionZero: Planning with Learned Options [PDF 13 ] [Copy] [Kimi 9 ] [REL] Authors : Po-Wei Huang , Pei-Chiun Peng , Hung Guei , Ti-Rong Wu Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data. Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named OptionZero. OptionZero incorporates an option network into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dynamics network in MuZero to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "2efNHgYRvM@OpenReview",
      "index": 110,
      "title": "On the Identification of Temporal Causal Representation with Instantaneous Dependence",
      "authors": [
        "Zijian Li",
        "Yifan Shen",
        "Kaitao Zheng",
        "Ruichu Cai",
        "Xiangchen Song",
        "Mingming Gong",
        "Guangyi Chen",
        "Kun Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "causal",
        "latent",
        "instantaneous",
        "textbf",
        "identifiability",
        "entification",
        "instantane",
        "sparse",
        "temporally",
        "atent"
      ],
      "summary": "Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \\textbf{ID}entification framework for instantane\\textbf{O}us \\textbf{L}atent dynamics (\\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2efNHgYRvM"
        ],
        "venue": [
          "/venue/2efNHgYRvM@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2efNHgYRvM"
        ],
        "detail": [
          "https://openreview.net/forum?id=2efNHgYRvM"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 13
      },
      "raw_excerpt": "On the Identification of Temporal Causal Representation with Instantaneous Dependence [PDF 7 ] [Copy] [Kimi 13 ] [REL] Authors : Zijian Li , Yifan Shen , Kaitao Zheng , Ruichu Cai , Xiangchen Song , Mingming Gong , Guangyi Chen , Kun Zhang Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \\textbf{ID}entification framework for instantane\\textbf{O}us \\textbf{L}atent dynamics (\\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "25kAzqzTrz@OpenReview",
      "index": 111,
      "title": "Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning",
      "authors": [
        "Jingyang Li",
        "Jiachun Pan",
        "Vincent Tan",
        "Kim-chuan Toh",
        "Pan Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "fixmatch",
        "ssl",
        "dnns",
        "theoretical",
        "supervised",
        "sohn",
        "freematch",
        "softmatch",
        "semantic",
        "flexmatch"
      ],
      "summary": "Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL algorithms generalize better than SL on DNNs. In this work, we present the first theoretical justification for the enhanced test accuracy observed in FixMatch-like SSL applied to DNNs by taking convolutional neural networks (CNNs) on classification tasks as an example. Our theoretical analysis reveals that the semantic feature learning processes in FixMatch and SL are rather different. In particular, FixMatch learns all the discriminative features of each semantic class, while SL only randomly captures a subset of features due to the well-known lottery ticket hypothesis. Furthermore, we show that our analysis framework can be applied to other FixMatch-like SSL methods, e.g., FlexMatch, FreeMatch, Dash, and SoftMatch. Inspired by our theoretical analysis, we develop an improved variant of FixMatch, termed Semantic-Aware FixMatch (SA-FixMatch). Experimental results corroborate our theoretical findings and the enhanced generalization capability of SA-FixMatch.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=25kAzqzTrz"
        ],
        "venue": [
          "/venue/25kAzqzTrz@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=25kAzqzTrz"
        ],
        "detail": [
          "https://openreview.net/forum?id=25kAzqzTrz"
        ]
      },
      "scores": {
        "pdf": 23,
        "kimi": 17
      },
      "raw_excerpt": "Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning [PDF 23 ] [Copy] [Kimi 17 ] [REL] Authors : Jingyang Li , Jiachun Pan , Vincent Tan , Kim-chuan Toh , Pan Zhou Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL algorithms generalize better than SL on DNNs. In this work, we present the first theoretical justification for the enhanced test accuracy observed in FixMatch-like SSL applied to DNNs by taking convolutional neural networks (CNNs) on classification tasks as an example. Our theoretical analysis reveals that the semantic feature learning processes in FixMatch and SL are rather different. In particular, FixMatch learns all the discriminative features of each semantic class, while SL only randomly captures a subset of features due to the well-known lottery ticket hypothesis. Furthermore, we show that our analysis framework can be applied to other FixMatch-like SSL methods, e.g., FlexMatch, FreeMatch, Dash, and SoftMatch. Inspired by our theoretical analysis, we develop an improved variant of FixMatch, termed Semantic-Aware FixMatch (SA-FixMatch). Experimental results corroborate our theoretical findings and the enhanced generalization capability of SA-FixMatch. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "1CLzLXSFNn@OpenReview",
      "index": 112,
      "title": "TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis",
      "authors": [
        "Shiyu Wang",
        "Jiawei LI",
        "Xiaoming Shi",
        "Zhou Ye",
        "Baichuan Mo",
        "Wenze Lin",
        "Shengtong Ju",
        "Zhixuan Chu",
        "Ming Jin"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "series",
        "timemixer",
        "time",
        "tspm",
        "mrti",
        "patterns",
        "tid",
        "mrm",
        "across",
        "mcm"
      ],
      "summary": "Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce \\method, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1CLzLXSFNn"
        ],
        "venue": [
          "/venue/1CLzLXSFNn@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1CLzLXSFNn"
        ],
        "detail": [
          "https://openreview.net/forum?id=1CLzLXSFNn"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 14
      },
      "raw_excerpt": "TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis [PDF 20 ] [Copy] [Kimi 14 ] [REL] Authors : Shiyu Wang , Jiawei LI , Xiaoming Shi , Zhou Ye , Baichuan Mo , Wenze Lin , Shengtong Ju , Zhixuan Chu , Ming Jin Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce \\method, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "z5uVAKwmjf@OpenReview",
      "index": 113,
      "title": "AFlow: Automating Agentic Workflow Generation",
      "authors": [
        "Jiayi Zhang",
        "Jinyu Xiang",
        "Zhaoyang Yu",
        "Fengwei Teng",
        "XiongHui Chen",
        "Jiaqi Chen",
        "Mingchen Zhuge",
        "Xin Cheng",
        "Sirui Hong",
        "Jinlin Wang",
        "Bingnan Zheng",
        "Bang Liu",
        "Yuyu Luo",
        "Chenglin Wu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "aflow",
        "workflows",
        "agentic",
        "workflow",
        "generation",
        "automating",
        "code",
        "automated",
        "tree",
        "dollars"
      ],
      "summary": "Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce \\textbf{AFlow}, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7\\% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55\\% of its inference cost in dollars. The code will be made available as open-source upon publication.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=z5uVAKwmjf"
        ],
        "venue": [
          "/venue/z5uVAKwmjf@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=z5uVAKwmjf"
        ],
        "detail": [
          "https://openreview.net/forum?id=z5uVAKwmjf"
        ]
      },
      "scores": {
        "pdf": 23,
        "kimi": 30
      },
      "raw_excerpt": "AFlow: Automating Agentic Workflow Generation [PDF 23 ] [Copy] [Kimi 30 ] [REL] Authors : Jiayi Zhang , Jinyu Xiang , Zhaoyang Yu , Fengwei Teng , XiongHui Chen , Jiaqi Chen , Mingchen Zhuge , Xin Cheng , Sirui Hong , Jinlin Wang , Bingnan Zheng , Bang Liu , Yuyu Luo , Chenglin Wu Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce \\textbf{AFlow}, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7\\% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55\\% of its inference cost in dollars. The code will be made available as open-source upon publication. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "tc90LV0yRL@OpenReview",
      "index": 114,
      "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
      "authors": [
        "Andy K Zhang",
        "Neil Perry",
        "Riya Dulepet",
        "Joey Ji",
        "Celeste Menders",
        "Justin Lin",
        "Eliot Jones",
        "Gashon Hussein",
        "Samantha Liu",
        "Donovan Jasper",
        "Pura Peetathawatchai",
        "Ari Glenn",
        "Vikram Sivashankar",
        "Daniel Zamoshchin",
        "Leo Glikbarg",
        "Derek Askaryar",
        "Haoxiang Yang",
        "Aolin Zhang",
        "Rishi Alluri",
        "Nathan Tran",
        "Rinnara Sangpisit",
        "Kenny Oseleononmen",
        "Dan Boneh",
        "Daniel Ho",
        "Percy Liang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "claude",
        "cybersecurity",
        "cybench",
        "sonnet",
        "agents",
        "capabilities",
        "ctf",
        "tasks",
        "agent",
        "opus"
      ],
      "summary": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structured bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. Anonymized code and data are available at https://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4 and https://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tc90LV0yRL"
        ],
        "venue": [
          "/venue/tc90LV0yRL@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tc90LV0yRL"
        ],
        "detail": [
          "https://openreview.net/forum?id=tc90LV0yRL"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 16
      },
      "raw_excerpt": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models [PDF 10 ] [Copy] [Kimi 16 ] [REL] Authors : Andy K Zhang , Neil Perry , Riya Dulepet , Joey Ji , Celeste Menders , Justin Lin , Eliot Jones , Gashon Hussein , Samantha Liu , Donovan Jasper , Pura Peetathawatchai , Ari Glenn , Vikram Sivashankar , Daniel Zamoshchin , Leo Glikbarg , Derek Askaryar , Haoxiang Yang , Aolin Zhang , Rishi Alluri , Nathan Tran , Rinnara Sangpisit , Kenny Oseleononmen , Dan Boneh , Daniel Ho , Percy Liang Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structured bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. Anonymized code and data are available at https://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4 and https://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "eIJfOIMN9z@OpenReview",
      "index": 115,
      "title": "Language Representations Can be What Recommenders Need: Findings and Potentials",
      "authors": [
        "Leheng Sheng",
        "An Zhang",
        "Yi Zhang",
        "Yuxin Chen",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "language",
        "recommendation",
        "representations",
        "lms",
        "item",
        "recommenders",
        "advanced",
        "representation",
        "findings",
        "homomorphism"
      ],
      "summary": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space.Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.This outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Motivated by the finding of homomorphism, we explore the possibility of designing advanced collaborative filtering (CF) models purely based on language representations without ID-based embeddings.To be specific, we incorporate several crucial components (i.e., a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function) to build a simple yet effective model, with the language representations of item textual metadata (i.e., title) as the input.Empirical results show that such a simple model can outperform leading ID-based CF models on multiple datasets, which sheds light on using language representations for better recommendation.Moreover, we systematically analyze this simple model and find several key features for using advanced language representations:a good initialization for item representations, superior zero-shot recommendation abilities in new datasets, and being aware of user intention.Our findings highlight the connection between language modeling and behavior modeling, which can inspire both natural language processing and recommender system communities.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eIJfOIMN9z"
        ],
        "venue": [
          "/venue/eIJfOIMN9z@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eIJfOIMN9z"
        ],
        "detail": [
          "https://openreview.net/forum?id=eIJfOIMN9z"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 18
      },
      "raw_excerpt": "Language Representations Can be What Recommenders Need: Findings and Potentials [PDF 10 ] [Copy] [Kimi 18 ] [REL] Authors : Leheng Sheng , An Zhang , Yi Zhang , Yuxin Chen , Xiang Wang , Tat-Seng Chua Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space.Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.This outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Motivated by the finding of homomorphism, we explore the possibility of designing advanced collaborative filtering (CF) models purely based on language representations without ID-based embeddings.To be specific, we incorporate several crucial components (i.e., a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function) to build a simple yet effective model, with the language representations of item textual metadata (i.e., title) as the input.Empirical results show that such a simple model can outperform leading ID-based CF models on multiple datasets, which sheds light on using language representations for better recommendation.Moreover, we systematically analyze this simple model and find several key features for using advanced language representations:a good initialization for item representations, superior zero-shot recommendation abilities in new datasets, and being aware of user intention.Our findings highlight the connection between language modeling and behavior modeling, which can inspire both natural language processing and recommender system communities. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "ZuazHmXTns@OpenReview",
      "index": 116,
      "title": "Problem-Parameter-Free Federated Learning",
      "authors": [
        "Wenjing Yan",
        "Kai Zhang",
        "Xiaolu Wang",
        "Xuanyu Cao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "padamfed",
        "epsilon",
        "federated",
        "mathcal",
        "stepsize",
        "heterogeneity",
        "communication",
        "problem",
        "advantages",
        "learning"
      ],
      "summary": "Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned hyperparameters, particularly the learning rate or stepsize. This manual tuning process is challenging in federated settings due to data heterogeneity and limited accessibility of local datasets. Consequently, the reliance on problem-specific parameters hinders the widespread adoption of FL and potentially compromises its performance in dynamic or diverse environments. To address this issue, we introduce PAdaMFed, a novel algorithm for nonconvex FL that carefully combines adaptive stepsize and momentum techniques. PAdaMFed offers two key advantages: 1) it operates autonomously without relying on problem-specific parameters; and 2) it manages data heterogeneity and partial participation without requiring heterogeneity bounds. Despite these benefits, PAdaMFed provides several strong theoretical guarantees: 1) It achieves state-of-the-art convergence rates with a sample complexity of O ( ϵ − 4 ) O ( ϵ − 4 ) and communication complexity of O ( ϵ − 3 ) O ( ϵ − 3 ) to obtain an accuracy of | | ∇ f ( θ ) | | ≤ ϵ | | ∇ f ( θ ) | | ≤ ϵ , even using constant learning rates; 2) these complexities can be improved to the best-known O ( ϵ − 3 ) O ( ϵ − 3 ) for sampling and O ( ϵ − 2 ) O ( ϵ − 2 ) for communication when incorporating variance reduction; 3) it exhibits linear speedup with respect to the number of local update steps and participating clients at each global round. These attributes make PAdaMFed highly scalable and adaptable for various real-world FL applications. Extensive empirical evidence on both image classification and sentiment analysis tasks validates the efficacy of our approaches.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZuazHmXTns"
        ],
        "venue": [
          "/venue/ZuazHmXTns@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZuazHmXTns"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZuazHmXTns"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 8
      },
      "raw_excerpt": "Problem-Parameter-Free Federated Learning [PDF 10 ] [Copy] [Kimi 8 ] [REL] Authors : Wenjing Yan , Kai Zhang , Xiaolu Wang , Xuanyu Cao Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned hyperparameters, particularly the learning rate or stepsize. This manual tuning process is challenging in federated settings due to data heterogeneity and limited accessibility of local datasets. Consequently, the reliance on problem-specific parameters hinders the widespread adoption of FL and potentially compromises its performance in dynamic or diverse environments. To address this issue, we introduce PAdaMFed, a novel algorithm for nonconvex FL that carefully combines adaptive stepsize and momentum techniques. PAdaMFed offers two key advantages: 1) it operates autonomously without relying on problem-specific parameters; and 2) it manages data heterogeneity and partial participation without requiring heterogeneity bounds. Despite these benefits, PAdaMFed provides several strong theoretical guarantees: 1) It achieves state-of-the-art convergence rates with a sample complexity of O ( ϵ − 4 ) O ( ϵ − 4 ) and communication complexity of O ( ϵ − 3 ) O ( ϵ − 3 ) to obtain an accuracy of | | ∇ f ( θ ) | | ≤ ϵ | | ∇ f ( θ ) | | ≤ ϵ , even using constant learning rates; 2) these complexities can be improved to the best-known O ( ϵ − 3 ) O ( ϵ − 3 ) for sampling and O ( ϵ − 2 ) O ( ϵ − 2 ) for communication when incorporating variance reduction; 3) it exhibits linear speedup with respect to the number of local update steps and participating clients at each global round. These attributes make PAdaMFed highly scalable and adaptable for various real-world FL applications. Extensive empirical evidence on both image classification and sentiment analysis tasks validates the efficacy of our approaches. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "QKBu1BOAwd@OpenReview",
      "index": 117,
      "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions",
      "authors": [
        "Changle Qu",
        "Sunhao Dai",
        "Xiaochi Wei",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Jun Xu",
        "Ji-Rong Wen"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "documentation",
        "llms",
        "tool",
        "tools",
        "draft",
        "mastery",
        "external",
        "comprehension",
        "exploration",
        "utilization"
      ],
      "summary": "Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trials emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. Extensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities. Our code is available at https://anonymous.4open.science/r/DRAFT-10B3.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QKBu1BOAwd"
        ],
        "venue": [
          "/venue/QKBu1BOAwd@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QKBu1BOAwd"
        ],
        "detail": [
          "https://openreview.net/forum?id=QKBu1BOAwd"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 13
      },
      "raw_excerpt": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions [PDF 11 ] [Copy] [Kimi 13 ] [REL] Authors : Changle Qu , Sunhao Dai , Xiaochi Wei , Hengyi Cai , Shuaiqiang Wang , Dawei Yin , Jun Xu , Ji-Rong Wen Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trials emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. Extensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities. Our code is available at https://anonymous.4open.science/r/DRAFT-10B3. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "I4e82CIDxv@OpenReview",
      "index": 118,
      "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
      "authors": [
        "Samuel Marks",
        "Can Rager",
        "Eric Michaud",
        "Yonatan Belinkov",
        "David Bau",
        "Aaron Mueller"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "circuits",
        "sparse",
        "discovering",
        "feature",
        "interpretable",
        "polysemantic",
        "units",
        "editing",
        "language",
        "behaviors"
      ],
      "summary": "We introduce methods for discovering and applying **sparse feature circuits**. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms in neural networks. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=I4e82CIDxv"
        ],
        "venue": [
          "/venue/I4e82CIDxv@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=I4e82CIDxv"
        ],
        "detail": [
          "https://openreview.net/forum?id=I4e82CIDxv"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 16
      },
      "raw_excerpt": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models [PDF 11 ] [Copy] [Kimi 16 ] [REL] Authors : Samuel Marks , Can Rager , Eric Michaud , Yonatan Belinkov , David Bau , Aaron Mueller We introduce methods for discovering and applying **sparse feature circuits**. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms in neural networks. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "GGlpykXDCa@OpenReview",
      "index": 119,
      "title": "MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions",
      "authors": [
        "Jian Wu",
        "Linyi Yang",
        "Dongyuan Li",
        "Yuliang Ji",
        "Manabu Okumura",
        "Yue Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "mmqa",
        "table",
        "llms",
        "multi",
        "hop",
        "foreign",
        "capabilities",
        "tabular",
        "understanding",
        "wikitablequestions"
      ],
      "summary": "While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we present a \\textbf{M}ulti-table and Multi-hop Question Answering (MMQA) dataset to assess LLMs' understanding and reasoning capabilities in handling multi-table tasks. The MMQA dataset demands that models perform multiple inferences by drawing evidence from various tables, which are designed to be connected with each other and require models to identify and utilize relationships such as foreign and primary keys. Then, we introduce a comprehensive evaluation framework that tailors to assess LLMs' capabilities in several aspects including Multi-Table Retrieval, Text-to-SQL Generation, Multi-Table QA, Primary Key Selection, and Foreign Key Selection. Finally, we propose a novel multi-table retrieval method that achieves state-of-the-art (SOTA) performance on the MMQA dataset compared to several strong baselines. Our experiment results reveal that, compared with human performance, both open-source and commercial LLMs leave significant performance room for improvements in multi-table understanding and reasoning tasks. We believe that the MMQA benchmark will enhance and facilitate LLMs' multi-table capabilities in real-world scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GGlpykXDCa"
        ],
        "venue": [
          "/venue/GGlpykXDCa@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GGlpykXDCa"
        ],
        "detail": [
          "https://openreview.net/forum?id=GGlpykXDCa"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 14
      },
      "raw_excerpt": "MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions [PDF 12 ] [Copy] [Kimi 14 ] [REL] Authors : Jian Wu , Linyi Yang , Dongyuan Li , Yuliang Ji , Manabu Okumura , Yue Zhang While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we present a \\textbf{M}ulti-table and Multi-hop Question Answering (MMQA) dataset to assess LLMs' understanding and reasoning capabilities in handling multi-table tasks. The MMQA dataset demands that models perform multiple inferences by drawing evidence from various tables, which are designed to be connected with each other and require models to identify and utilize relationships such as foreign and primary keys. Then, we introduce a comprehensive evaluation framework that tailors to assess LLMs' capabilities in several aspects including Multi-Table Retrieval, Text-to-SQL Generation, Multi-Table QA, Primary Key Selection, and Foreign Key Selection. Finally, we propose a novel multi-table retrieval method that achieves state-of-the-art (SOTA) performance on the MMQA dataset compared to several strong baselines. Our experiment results reveal that, compared with human performance, both open-source and commercial LLMs leave significant performance room for improvements in multi-table understanding and reasoning tasks. We believe that the MMQA benchmark will enhance and facilitate LLMs' multi-table capabilities in real-world scenarios. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "CRmiX0v16e@OpenReview",
      "index": 120,
      "title": "Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation",
      "authors": [
        "Mohamed el amine Boudjoghra",
        "Angela Dai",
        "Jean Lahoud",
        "Hisham Cholakkal",
        "Rao Anwer",
        "Salman Khan",
        "Fahad Khan"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "yolo",
        "open",
        "instance",
        "vocabulary",
        "scannet200",
        "segmentation",
        "masks",
        "object",
        "mvpdist",
        "view"
      ],
      "summary": "Recent works on open-vocabulary 3D instance segmentation show strong promise but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on aggregated clip features from multi-view, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP. Consequently, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a novel open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that efficiently leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. We demonstrate that our proposed Multi-View Prompt Distribution (MVPDist) method makes use of multi-view information to account for misclassification from the object detector to predict a reliable label for 3D instance masks. Furthermore, since projections of 3D object instances are already contained within the 2D bounding boxes, we show that our proposed low granularity label maps, which require only a 2D object detector to construct, are sufficient and very fast to predict prompt IDs for 3D instance masks when used with our proposed MVPDist. We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to ∼ ∼ 16 × × speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7% while operating at 22 seconds per scene. Our code will be publically available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CRmiX0v16e"
        ],
        "venue": [
          "/venue/CRmiX0v16e@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CRmiX0v16e"
        ],
        "detail": [
          "https://openreview.net/forum?id=CRmiX0v16e"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 5
      },
      "raw_excerpt": "Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation [PDF 7 ] [Copy] [Kimi 5 ] [REL] Authors : Mohamed el amine Boudjoghra , Angela Dai , Jean Lahoud , Hisham Cholakkal , Rao Anwer , Salman Khan , Fahad Khan Recent works on open-vocabulary 3D instance segmentation show strong promise but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on aggregated clip features from multi-view, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP. Consequently, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a novel open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that efficiently leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. We demonstrate that our proposed Multi-View Prompt Distribution (MVPDist) method makes use of multi-view information to account for misclassification from the object detector to predict a reliable label for 3D instance masks. Furthermore, since projections of 3D object instances are already contained within the 2D bounding boxes, we show that our proposed low granularity label maps, which require only a 2D object detector to construct, are sufficient and very fast to predict prompt IDs for 3D instance masks when used with our proposed MVPDist. We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to ∼ ∼ 16 × × speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7% while operating at 22 seconds per scene. Our code will be publically available. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "5Jc7r5aqHJ@OpenReview",
      "index": 121,
      "title": "Energy-based Backdoor Defense Against Federated Graph Learning",
      "authors": [
        "Guancheng Wan",
        "Zitong Shi",
        "Wenke Huang",
        "Guibin Zhang",
        "Dacheng Tao",
        "Mang Ye"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "federated",
        "backdoor",
        "graph",
        "energy",
        "defense",
        "benign",
        "client",
        "malicious",
        "clients",
        "making"
      ],
      "summary": "Federated Graph Learning is rapidly evolving as a privacy-preserving collaborative approach. However, backdoor attacks are increasingly undermining federated systems by injecting carefully designed triggers that lead to the model making incorrect predictions. Trigger structures and injection locations in Federated Graph Learning are more diverse, making traditional federated defense methods less effective. In our work, we propose an effective Federated Graph Backdoor Defense using Topological Graph Energy (FedTGE). At the local client level, it injects distribution knowledge into the local model, assigning low energy to benign samples and high energy to the constructed malicious substitutes, and selects benign clients through clustering. At the global server level, the energy elements uploaded by each client are treated as new nodes to construct a global energy graph for energy propagation, making the selected clients' energy elements more similar and further adjusting the aggregation weights. Our method can handle high data heterogeneity, does not require a validation dataset, and is effective under both small and large malicious proportions. Extensive results on various settings of federated graph scenarios under backdoor attacks validate the effectiveness of this approach.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5Jc7r5aqHJ"
        ],
        "venue": [
          "/venue/5Jc7r5aqHJ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5Jc7r5aqHJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=5Jc7r5aqHJ"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 9
      },
      "raw_excerpt": "Energy-based Backdoor Defense Against Federated Graph Learning [PDF 9 ] [Copy] [Kimi 9 ] [REL] Authors : Guancheng Wan , Zitong Shi , Wenke Huang , Guibin Zhang , Dacheng Tao , Mang Ye Federated Graph Learning is rapidly evolving as a privacy-preserving collaborative approach. However, backdoor attacks are increasingly undermining federated systems by injecting carefully designed triggers that lead to the model making incorrect predictions. Trigger structures and injection locations in Federated Graph Learning are more diverse, making traditional federated defense methods less effective. In our work, we propose an effective Federated Graph Backdoor Defense using Topological Graph Energy (FedTGE). At the local client level, it injects distribution knowledge into the local model, assigning low energy to benign samples and high energy to the constructed malicious substitutes, and selects benign clients through clustering. At the global server level, the energy elements uploaded by each client are treated as new nodes to construct a global energy graph for energy propagation, making the selected clients' energy elements more similar and further adjusting the aggregation weights. Our method can handle high data heterogeneity, does not require a validation dataset, and is effective under both small and large malicious proportions. Extensive results on various settings of federated graph scenarios under backdoor attacks validate the effectiveness of this approach. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "ilOEOIqolQ@OpenReview",
      "index": 122,
      "title": "AI as Humanity’s Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text",
      "authors": [
        "Ximing Lu",
        "Melanie Sclar",
        "Skyler Hallinan",
        "Niloofar Mireshghallah",
        "Jiacheng Liu",
        "Seungju Han",
        "Allyson Ettinger",
        "Liwei Jiang",
        "Khyathi Chandu",
        "Nouha Dziri",
        "Yejin Choi"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "creativity",
        "index",
        "text",
        "web",
        "salieri",
        "llms",
        "verbatim",
        "human",
        "snippets",
        "linguistic"
      ],
      "summary": "Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpasshuman creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we explore variations in the CREATIVITY INDEX among different human authors and discuss the potential factors contributing to these differences. Finally, we showcase a novel application of CREATIVITY INDEX for zero-shot machine text detection, where it proves to be surprisingly effective—outperforming the strong zero-shot system DetectGPT by a substantial margin of 30.2%, and even surpassing a leading supervised system, GhostBuster, in five out of six domains.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ilOEOIqolQ"
        ],
        "venue": [
          "/venue/ilOEOIqolQ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ilOEOIqolQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=ilOEOIqolQ"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 7
      },
      "raw_excerpt": "AI as Humanity’s Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text [PDF 6 ] [Copy] [Kimi 7 ] [REL] Authors : Ximing Lu , Melanie Sclar , Skyler Hallinan , Niloofar Mireshghallah , Jiacheng Liu , Seungju Han , Allyson Ettinger , Liwei Jiang , Khyathi Chandu , Nouha Dziri , Yejin Choi Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpasshuman creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we explore variations in the CREATIVITY INDEX among different human authors and discuss the potential factors contributing to these differences. Finally, we showcase a novel application of CREATIVITY INDEX for zero-shot machine text detection, where it proves to be surprisingly effective—outperforming the strong zero-shot system DetectGPT by a substantial margin of 30.2%, and even surpassing a leading supervised system, GhostBuster, in five out of six domains. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "kRoWeLTpL4@OpenReview",
      "index": 123,
      "title": "Copyright-Protected Language Generation via Adaptive Model Fusion",
      "authors": [
        "Javier Abad",
        "Konstantin Donhauser",
        "Francesco Pinto",
        "Fanny Yang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "copyright",
        "copyrighted",
        "fuse",
        "protective",
        "protected",
        "material",
        "reproduction",
        "fusion",
        "regurgitation",
        "language"
      ],
      "summary": "The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown promise in addressing the complexities of copyright regulation. However, they often incur prohibitive computational costs or suffer from performance trade-offs. To overcome these limitations, we introduce Copyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines models trained on disjoint sets of copyrighted material during inference. In particular, CP-Fuse adaptively aggregates the model outputs to minimize the reproduction of copyrighted content, adhering to a crucial balancing property to prevent the regurgitation of memorized data. Through extensive experiments, we show that CP-Fuse significantly reduces the reproduction of protected material without compromising the quality of text and code generation. Moreover, its post-hoc nature allows seamless integration with other protective measures, further enhancing copyright safeguards. Lastly, we show that CP-Fuse is robust against common techniques for extracting training data",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kRoWeLTpL4"
        ],
        "venue": [
          "/venue/kRoWeLTpL4@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kRoWeLTpL4"
        ],
        "detail": [
          "https://openreview.net/forum?id=kRoWeLTpL4"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 6
      },
      "raw_excerpt": "Copyright-Protected Language Generation via Adaptive Model Fusion [PDF 3 ] [Copy] [Kimi 6 ] [REL] Authors : Javier Abad , Konstantin Donhauser , Francesco Pinto , Fanny Yang The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown promise in addressing the complexities of copyright regulation. However, they often incur prohibitive computational costs or suffer from performance trade-offs. To overcome these limitations, we introduce Copyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines models trained on disjoint sets of copyrighted material during inference. In particular, CP-Fuse adaptively aggregates the model outputs to minimize the reproduction of copyrighted content, adhering to a crucial balancing property to prevent the regurgitation of memorized data. Through extensive experiments, we show that CP-Fuse significantly reduces the reproduction of protected material without compromising the quality of text and code generation. Moreover, its post-hoc nature allows seamless integration with other protective measures, further enhancing copyright safeguards. Lastly, we show that CP-Fuse is robust against common techniques for extracting training data Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "meRCKuUpmc@OpenReview",
      "index": 124,
      "title": "Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation",
      "authors": [
        "Yang Tian",
        "Sizhe Yang",
        "Jia Zeng",
        "Ping Wang",
        "Dahua Lin",
        "Hao Dong",
        "Jiangmiao Pang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "robotic",
        "pidm",
        "end",
        "seer",
        "inverse",
        "scalable",
        "action",
        "world",
        "manipulation",
        "vision"
      ],
      "summary": "Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on \"action,\" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes \"vision,\" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=meRCKuUpmc"
        ],
        "venue": [
          "/venue/meRCKuUpmc@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=meRCKuUpmc"
        ],
        "detail": [
          "https://openreview.net/forum?id=meRCKuUpmc"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 9
      },
      "raw_excerpt": "Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation [PDF 9 ] [Copy] [Kimi 9 ] [REL] Authors : Yang Tian , Sizhe Yang , Jia Zeng , Ping Wang , Dahua Lin , Hao Dong , Jiangmiao Pang Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on \"action,\" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes \"vision,\" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "mtJSMcF3ek@OpenReview",
      "index": 125,
      "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models",
      "authors": [
        "Yuda Song",
        "Hanlin Zhang",
        "Udaya Ghai",
        "Carson Eisenach",
        "Sham Kakade",
        "Dean Foster"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "improvement",
        "self",
        "llm",
        "verification",
        "gap",
        "reweights",
        "mind",
        "language",
        "examining",
        "training"
      ],
      "summary": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the *generation-verification gap*. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. We believe our results have several empirical implications, and our study leaves many exciting future directions for understanding the potential and limits of LLM self-improvement.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mtJSMcF3ek"
        ],
        "venue": [
          "/venue/mtJSMcF3ek@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mtJSMcF3ek"
        ],
        "detail": [
          "https://openreview.net/forum?id=mtJSMcF3ek"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 35
      },
      "raw_excerpt": "Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models [PDF 21 ] [Copy] [Kimi 35 ] [REL] Authors : Yuda Song , Hanlin Zhang , Udaya Ghai , Carson Eisenach , Sham Kakade , Dean Foster Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the *generation-verification gap*. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. We believe our results have several empirical implications, and our study leaves many exciting future directions for understanding the potential and limits of LLM self-improvement. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "EjJGND0m1x@OpenReview",
      "index": 126,
      "title": "MIND over Body: Adaptive Thinking using Dynamic Computation",
      "authors": [
        "Mrinal Mathur",
        "Barak Pearlmutter",
        "Sergey Plis"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "computation",
        "complexity",
        "parameters",
        "input",
        "mind",
        "thinking",
        "dynamic",
        "efficiently",
        "inefficiently",
        "introspection"
      ],
      "summary": "While the human brain efficiently handles various computations with a limited number of neurons, traditional deep learning networks require a significant increase in parameters to improve performance. Yet, these parameters are used inefficiently as the networks employ the same amount of computation for inputs of the same size, regardless of the input's complexity. We address this inefficiency by introducing self-introspection capabilities to the network, enabling it to adjust the number of used parameters based on the internal representation of the task and adapt the computation time based on the task complexity. This enables the network to adaptively reuse parameters across tasks, dynamically adjusting the computational effort to match the complexity of the input. We demonstrate the effectiveness of this method on language modeling and computer vision tasks. Notably, our model surpasses much larger ResNet-50 and EfficientNet on ImageNet, achieving 96.62\\% accuracy, and achieves a 95.8\\% F1 score on the SQuAD dataset, all with just a three-layer network. These results showcase the potential for dynamic and reflective computation, contributing to the creation of intelligent systems that efficiently manage resources based on input data complexity.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EjJGND0m1x"
        ],
        "venue": [
          "/venue/EjJGND0m1x@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EjJGND0m1x"
        ],
        "detail": [
          "https://openreview.net/forum?id=EjJGND0m1x"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 20
      },
      "raw_excerpt": "MIND over Body: Adaptive Thinking using Dynamic Computation [PDF 12 ] [Copy] [Kimi 20 ] [REL] Authors : Mrinal Mathur , Barak Pearlmutter , Sergey Plis While the human brain efficiently handles various computations with a limited number of neurons, traditional deep learning networks require a significant increase in parameters to improve performance. Yet, these parameters are used inefficiently as the networks employ the same amount of computation for inputs of the same size, regardless of the input's complexity. We address this inefficiency by introducing self-introspection capabilities to the network, enabling it to adjust the number of used parameters based on the internal representation of the task and adapt the computation time based on the task complexity. This enables the network to adaptively reuse parameters across tasks, dynamically adjusting the computational effort to match the complexity of the input. We demonstrate the effectiveness of this method on language modeling and computer vision tasks. Notably, our model surpasses much larger ResNet-50 and EfficientNet on ImageNet, achieving 96.62\\% accuracy, and achieves a 95.8\\% F1 score on the SQuAD dataset, all with just a three-layer network. These results showcase the potential for dynamic and reflective computation, contributing to the creation of intelligent systems that efficiently manage resources based on input data complexity. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "kGvXIlIVLM@OpenReview",
      "index": 127,
      "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
      "authors": [
        "Huayu Chen",
        "Hang Su",
        "Peize Sun",
        "Jun Zhu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "guidance",
        "cca",
        "visual",
        "cfg",
        "alignment",
        "generation",
        "sampling",
        "contrastive",
        "unifying",
        "pretraining"
      ],
      "summary": "Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (1% of pretraining epochs) on the pretraining dataset. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kGvXIlIVLM"
        ],
        "venue": [
          "/venue/kGvXIlIVLM@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kGvXIlIVLM"
        ],
        "detail": [
          "https://openreview.net/forum?id=kGvXIlIVLM"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 13
      },
      "raw_excerpt": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment [PDF 13 ] [Copy] [Kimi 13 ] [REL] Authors : Huayu Chen , Hang Su , Peize Sun , Jun Zhu Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (1% of pretraining epochs) on the pretraining dataset. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "nwDRD4AMoN@OpenReview",
      "index": 128,
      "title": "Artificial Kuramoto Oscillatory Neurons",
      "authors": [
        "Takeru Miyato",
        "Sindy Löwe",
        "Andreas Geiger",
        "Max Welling"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "kuramoto",
        "neurons",
        "oscillatory",
        "neuroscience",
        "artificial",
        "representations",
        "akorn",
        "importance",
        "hypothesized",
        "dynamical"
      ],
      "summary": "It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (*AKOrN*) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nwDRD4AMoN"
        ],
        "venue": [
          "/venue/nwDRD4AMoN@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nwDRD4AMoN"
        ],
        "detail": [
          "https://openreview.net/forum?id=nwDRD4AMoN"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 10
      },
      "raw_excerpt": "Artificial Kuramoto Oscillatory Neurons [PDF 9 ] [Copy] [Kimi 10 ] [REL] Authors : Takeru Miyato , Sindy Löwe , Andreas Geiger , Max Welling It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (*AKOrN*) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "uKZdlihDDn@OpenReview",
      "index": 129,
      "title": "Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks",
      "authors": [
        "Mario Lino",
        "Tobias Pfaff",
        "Nils Thuerey"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "distributions",
        "fluid",
        "diffusion",
        "graph",
        "complex",
        "simulations",
        "statistics",
        "latent",
        "physical",
        "unsteady"
      ],
      "summary": "Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding of our work is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uKZdlihDDn"
        ],
        "venue": [
          "/venue/uKZdlihDDn@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uKZdlihDDn"
        ],
        "detail": [
          "https://openreview.net/forum?id=uKZdlihDDn"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 9
      },
      "raw_excerpt": "Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks [PDF 8 ] [Copy] [Kimi 9 ] [REL] Authors : Mario Lino , Tobias Pfaff , Nils Thuerey Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding of our work is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "uHLgDEgiS5@OpenReview",
      "index": 130,
      "title": "Capturing the Temporal Dependence of Training Data Influence",
      "authors": [
        "Jiachen (Tianhao) Wang",
        "Dawn Song",
        "James Y Zou",
        "Prateek Mittal",
        "Ruoxi Jia"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "loo",
        "data",
        "influence",
        "training",
        "trajectory",
        "embedding",
        "specific",
        "ordering",
        "stages",
        "emph"
      ],
      "summary": "Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula—are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \\emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \\emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uHLgDEgiS5"
        ],
        "venue": [
          "/venue/uHLgDEgiS5@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uHLgDEgiS5"
        ],
        "detail": [
          "https://openreview.net/forum?id=uHLgDEgiS5"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 9
      },
      "raw_excerpt": "Capturing the Temporal Dependence of Training Data Influence [PDF 9 ] [Copy] [Kimi 9 ] [REL] Authors : Jiachen (Tianhao) Wang , Dawn Song , James Y Zou , Prateek Mittal , Ruoxi Jia Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula—are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \\emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \\emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "uAFHCZRmXk@OpenReview",
      "index": 131,
      "title": "Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models",
      "authors": [
        "Simon Schrodi",
        "David Hoffmann",
        "Max Argus",
        "Volker Fischer",
        "Thomas Brox"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "gap",
        "modality",
        "bias",
        "object",
        "vlms",
        "contrastive",
        "imbalance",
        "vision",
        "tasks",
        "attributes"
      ],
      "summary": "Contrastive vision-language models (VLMs), like CLIP, have gained popularity for their versatile applicability to various downstream tasks. Despite their successes in some tasks, like zero-shot object recognition, they perform surprisingly poor on other tasks, like attribute recognition. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and to a bias towards objects over other factors, such as attributes. In this analysis paper, we investigate both phenomena thoroughly. We evaluated off-the-shelf VLMs and find that while the gap's influence on performance is typically overshadowed by other factors, we find indications that closing the gap indeed leads to improvements. Moreover, we find that, contrary to intuition, only few embedding dimensions drive the gap and that the embedding spaces are differently organized. To allow for a clean study of object bias, we introduce a definition and a corresponding measure of it. Equipped with this tool, we find that object bias does not lead to worse performance on other concepts, such as attributes per se. However, why do both phenomena, modality gap and object bias, emerge in the first place? To answer this fundamental question and uncover some of the inner workings of contrastive VLMs, we conducted experiments that allowed us to control the amount of shared information between the modalities. These experiments revealed that the driving factor behind both the modality gap and the object bias, is an information imbalance between images and captions, and unveiled an intriguing connection between the modality gap and entropy of the logits.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uAFHCZRmXk"
        ],
        "venue": [
          "/venue/uAFHCZRmXk@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uAFHCZRmXk"
        ],
        "detail": [
          "https://openreview.net/forum?id=uAFHCZRmXk"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 20
      },
      "raw_excerpt": "Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models [PDF 19 ] [Copy] [Kimi 20 ] [REL] Authors : Simon Schrodi , David Hoffmann , Max Argus , Volker Fischer , Thomas Brox Contrastive vision-language models (VLMs), like CLIP, have gained popularity for their versatile applicability to various downstream tasks. Despite their successes in some tasks, like zero-shot object recognition, they perform surprisingly poor on other tasks, like attribute recognition. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and to a bias towards objects over other factors, such as attributes. In this analysis paper, we investigate both phenomena thoroughly. We evaluated off-the-shelf VLMs and find that while the gap's influence on performance is typically overshadowed by other factors, we find indications that closing the gap indeed leads to improvements. Moreover, we find that, contrary to intuition, only few embedding dimensions drive the gap and that the embedding spaces are differently organized. To allow for a clean study of object bias, we introduce a definition and a corresponding measure of it. Equipped with this tool, we find that object bias does not lead to worse performance on other concepts, such as attributes per se. However, why do both phenomena, modality gap and object bias, emerge in the first place? To answer this fundamental question and uncover some of the inner workings of contrastive VLMs, we conducted experiments that allowed us to control the amount of shared information between the modalities. These experiments revealed that the driving factor behind both the modality gap and the object bias, is an information imbalance between images and captions, and unveiled an intriguing connection between the modality gap and entropy of the logits. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "rwqShzb9li@OpenReview",
      "index": 132,
      "title": "Linear Representations of Political Perspective Emerge in Large Language Models",
      "authors": [
        "Junsol Kim",
        "James Evans",
        "Aaron Schein"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ideological",
        "perspective",
        "slant",
        "subjective",
        "heads",
        "liberal",
        "politicians",
        "perspectives",
        "activation",
        "llm"
      ],
      "summary": "Large language models (LLMs) have demonstrated the ability to simulate responses aligned with human subjective perspectives, such as liberal or conservative ideologies in American politics. Our study reveals that LLMs achieve this by learning a ``geometry of perspective'' that linearly represents subjective perspectives in the activation space, where similar simulated perspectives are represented closer to each other. Specifically, we probe the hidden layers of open, transformer-based LLMs (\\texttt{Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b}) when prompted to generate texts under the ideological perspective of distinct politicians. We find a set of attention heads that represent U.S. ideological slant, which is primarily located in the middle layers known to encode high-level concepts and tasks. The activation of these attention heads, when prompted about U.S.~politicians and media outlets, linearly correlates with existing measures of their ideological slant. We use this activation to detect the ideological slant implicitly adopted by an LLM as it is generating each token. We further show that by intervening on these attention heads, we can tune LLM output to any position along the linear dimension from a liberal to conservative ideological perspective. Our research shows that political ideology serves as a fundamental dimension of LLM representations, and present an interpretability method to identify, monitor, and control the subjective perspective used to generate text. Code: https://osf.io/us9yx/?view_only=cf0fdcdb123e4d6bb7d10a64be5c1a09",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rwqShzb9li"
        ],
        "venue": [
          "/venue/rwqShzb9li@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rwqShzb9li"
        ],
        "detail": [
          "https://openreview.net/forum?id=rwqShzb9li"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 21
      },
      "raw_excerpt": "Linear Representations of Political Perspective Emerge in Large Language Models [PDF 14 ] [Copy] [Kimi 21 ] [REL] Authors : Junsol Kim , James Evans , Aaron Schein Large language models (LLMs) have demonstrated the ability to simulate responses aligned with human subjective perspectives, such as liberal or conservative ideologies in American politics. Our study reveals that LLMs achieve this by learning a ``geometry of perspective'' that linearly represents subjective perspectives in the activation space, where similar simulated perspectives are represented closer to each other. Specifically, we probe the hidden layers of open, transformer-based LLMs (\\texttt{Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b}) when prompted to generate texts under the ideological perspective of distinct politicians. We find a set of attention heads that represent U.S. ideological slant, which is primarily located in the middle layers known to encode high-level concepts and tasks. The activation of these attention heads, when prompted about U.S.~politicians and media outlets, linearly correlates with existing measures of their ideological slant. We use this activation to detect the ideological slant implicitly adopted by an LLM as it is generating each token. We further show that by intervening on these attention heads, we can tune LLM output to any position along the linear dimension from a liberal to conservative ideological perspective. Our research shows that political ideology serves as a fundamental dimension of LLM representations, and present an interpretability method to identify, monitor, and control the subjective perspective used to generate text. Code: https://osf.io/us9yx/?view_only=cf0fdcdb123e4d6bb7d10a64be5c1a09 Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "rfdblE10qm@OpenReview",
      "index": 133,
      "title": "Rethinking Reward Modeling in Preference-based Large Language Model Alignment",
      "authors": [
        "Hao Sun",
        "Yunyi Shen",
        "Jean-Francois Ton"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "reward",
        "modeling",
        "preference",
        "model",
        "alignment",
        "rethinking",
        "language",
        "bradley",
        "terry",
        "response"
      ],
      "summary": "The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear *why* this model --- originally developed for multi-player stochastic game matching --- can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. In this paper, we first establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use.Despite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization, this is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. We highlight the critical concept of *order consistency* in reward modeling and demonstrate that the BT model possesses this property.Moreover, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. To offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using 6 6 base LLMs, 2 2 datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rfdblE10qm"
        ],
        "venue": [
          "/venue/rfdblE10qm@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rfdblE10qm"
        ],
        "detail": [
          "https://openreview.net/forum?id=rfdblE10qm"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 28
      },
      "raw_excerpt": "Rethinking Reward Modeling in Preference-based Large Language Model Alignment [PDF 8 ] [Copy] [Kimi 28 ] [REL] Authors : Hao Sun , Yunyi Shen , Jean-Francois Ton The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear *why* this model --- originally developed for multi-player stochastic game matching --- can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. In this paper, we first establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use.Despite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization, this is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. We highlight the critical concept of *order consistency* in reward modeling and demonstrate that the BT model possesses this property.Moreover, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. To offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using 6 6 base LLMs, 2 2 datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "pISLZG7ktL@OpenReview",
      "index": 134,
      "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation",
      "authors": [
        "Fanqi Lin",
        "Yingdong Hu",
        "Pingyue Sheng",
        "Chuan Wen",
        "Jiacheng You",
        "Yang Gao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "demonstrations",
        "environments",
        "scaling",
        "objects",
        "data",
        "imitation",
        "robotic",
        "manipulation",
        "laws",
        "generalization"
      ],
      "summary": "Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy’s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\\% success rates in novel environments with unseen objects.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pISLZG7ktL"
        ],
        "venue": [
          "/venue/pISLZG7ktL@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pISLZG7ktL"
        ],
        "detail": [
          "https://openreview.net/forum?id=pISLZG7ktL"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 17
      },
      "raw_excerpt": "Data Scaling Laws in Imitation Learning for Robotic Manipulation [PDF 8 ] [Copy] [Kimi 17 ] [REL] Authors : Fanqi Lin , Yingdong Hu , Pingyue Sheng , Chuan Wen , Jiacheng You , Yang Gao Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy’s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\\% success rates in novel environments with unseen objects. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "mtSSFiqW6y@OpenReview",
      "index": 135,
      "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
      "authors": [
        "Gregor Bachmann",
        "Sotiris Anagnostidis",
        "Albert Pumarola",
        "Markos Georgopoulos",
        "Artsiom Sanakoyeu",
        "Yuming Du",
        "Edgar Schoenfeld",
        "Ali Thabet",
        "Jonas Kohler"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "judge",
        "draft",
        "405b",
        "speculative",
        "tokens",
        "decoding",
        "target",
        "llama",
        "speedup",
        "alignment"
      ],
      "summary": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of 9 × 9 × over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to 141 141 tokens/s for 8B/70B-Judge and 129 129 tokens/s for 8B/405B on 2 2 and 8 8 H100s respectively.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mtSSFiqW6y"
        ],
        "venue": [
          "/venue/mtSSFiqW6y@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mtSSFiqW6y"
        ],
        "detail": [
          "https://openreview.net/forum?id=mtSSFiqW6y"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 17
      },
      "raw_excerpt": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment [PDF 8 ] [Copy] [Kimi 17 ] [REL] Authors : Gregor Bachmann , Sotiris Anagnostidis , Albert Pumarola , Markos Georgopoulos , Artsiom Sanakoyeu , Yuming Du , Edgar Schoenfeld , Ali Thabet , Jonas Kohler The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of 9 × 9 × over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to 141 141 tokens/s for 8B/70B-Judge and 129 129 tokens/s for 8B/405B on 2 2 and 8 8 H100s respectively. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "mMPMHWOdOy@OpenReview",
      "index": 136,
      "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
      "authors": [
        "Haipeng Luo",
        "Qingfeng Sun",
        "Can Xu",
        "Pu Zhao",
        "Jian-Guang Lou",
        "Chongyang Tao",
        "Xiubo Geng",
        "Qingwei Lin",
        "Shifeng Chen",
        "Yansong Tang",
        "Dongmei Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "wizardmath",
        "evol",
        "math",
        "mathematical",
        "reasoning",
        "instruct",
        "mistral",
        "llms",
        "rleif",
        "language"
      ],
      "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mMPMHWOdOy"
        ],
        "venue": [
          "/venue/mMPMHWOdOy@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mMPMHWOdOy"
        ],
        "detail": [
          "https://openreview.net/forum?id=mMPMHWOdOy"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 27
      },
      "raw_excerpt": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct [PDF 21 ] [Copy] [Kimi 27 ] [REL] Authors : Haipeng Luo , Qingfeng Sun , Can Xu , Pu Zhao , Jian-Guang Lou , Chongyang Tao , Xiubo Geng , Qingwei Lin , Shifeng Chen , Yansong Tang , Dongmei Zhang Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "jOmk0uS1hl@OpenReview",
      "index": 137,
      "title": "Training on the Test Task Confounds Evaluation and Emergence",
      "authors": [
        "Ricardo Dominguez-Olmedo",
        "Florian Eddie Dorner",
        "Moritz Hardt"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "task",
        "test",
        "confounds",
        "training",
        "evaluation",
        "emergent",
        "language",
        "wrongful",
        "malpractice",
        "evaluations"
      ],
      "summary": "We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of techniques to include task-relevant data in the pretraining stage of a language model. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data before evaluation. Lastly, we show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models with broad implications for benchmarking and the study of emergent capabilities.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jOmk0uS1hl"
        ],
        "venue": [
          "/venue/jOmk0uS1hl@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jOmk0uS1hl"
        ],
        "detail": [
          "https://openreview.net/forum?id=jOmk0uS1hl"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Training on the Test Task Confounds Evaluation and Emergence [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Ricardo Dominguez-Olmedo , Florian Eddie Dorner , Moritz Hardt We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of techniques to include task-relevant data in the pretraining stage of a language model. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data before evaluation. Lastly, we show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models with broad implications for benchmarking and the study of emergent capabilities. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "hyfe5q5TD0@OpenReview",
      "index": 138,
      "title": "Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics",
      "authors": [
        "Runzhe Wu",
        "Ayush Sekhari",
        "Wen Sun",
        "Akshay Krishnamurthy"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "bellman",
        "computationally",
        "linear",
        "setting",
        "efficient",
        "deterministic",
        "statistically",
        "random",
        "completeness",
        "lqr"
      ],
      "summary": "We study computationally and statistically efficient Reinforcement Learning algorithms for the *linear Bellman Complete* setting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hyfe5q5TD0"
        ],
        "venue": [
          "/venue/hyfe5q5TD0@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hyfe5q5TD0"
        ],
        "detail": [
          "https://openreview.net/forum?id=hyfe5q5TD0"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 15
      },
      "raw_excerpt": "Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics [PDF 4 ] [Copy] [Kimi 15 ] [REL] Authors : Runzhe Wu , Ayush Sekhari , Wen Sun , Akshay Krishnamurthy We study computationally and statistically efficient Reinforcement Learning algorithms for the *linear Bellman Complete* setting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "hrqNOxpItr@OpenReview",
      "index": 139,
      "title": "Cross-Entropy Is All You Need To Invert the Data Generating Process",
      "authors": [
        "Patrik Reizinger",
        "Alice Bizeul",
        "Attila Juhos",
        "Julia E Vogt",
        "Randall Balestriero",
        "Wieland Brendel",
        "David Klindt"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "supervised",
        "linear",
        "factors",
        "variation",
        "latent",
        "dislib",
        "disentanglement",
        "generating",
        "invert",
        "representations"
      ],
      "summary": "Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation under a certain DGP. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation.Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hrqNOxpItr"
        ],
        "venue": [
          "/venue/hrqNOxpItr@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hrqNOxpItr"
        ],
        "detail": [
          "https://openreview.net/forum?id=hrqNOxpItr"
        ]
      },
      "scores": {
        "pdf": 22,
        "kimi": 35
      },
      "raw_excerpt": "Cross-Entropy Is All You Need To Invert the Data Generating Process [PDF 22 ] [Copy] [Kimi 35 ] [REL] Authors : Patrik Reizinger , Alice Bizeul , Attila Juhos , Julia E Vogt , Randall Balestriero , Wieland Brendel , David Klindt Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation under a certain DGP. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation.Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised learning. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "h0Ak8A5yqw@OpenReview",
      "index": 140,
      "title": "On the Role of Attention Heads in Large Language Model Safety",
      "authors": [
        "zhenhong zhou",
        "Haiyang Yu",
        "Xinghua Zhang",
        "Rongwu Xu",
        "Fei Huang",
        "Kun Wang",
        "Yang Liu",
        "Junfeng Fang",
        "Yongbin Li"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "safety",
        "heads",
        "head",
        "attention",
        "mechanisms",
        "ships",
        "language",
        "harmful",
        "attribution",
        "model"
      ],
      "summary": "Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose an novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Base on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to **16 × ↑ × ↑ ** more harmful queries, while only modifying **0.006\\%** ↓ ↓ of the parameters, in contrast to the ∼ ∼ **5\\%** modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms in large models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=h0Ak8A5yqw"
        ],
        "venue": [
          "/venue/h0Ak8A5yqw@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=h0Ak8A5yqw"
        ],
        "detail": [
          "https://openreview.net/forum?id=h0Ak8A5yqw"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 15
      },
      "raw_excerpt": "On the Role of Attention Heads in Large Language Model Safety [PDF 14 ] [Copy] [Kimi 15 ] [REL] Authors : zhenhong zhou , Haiyang Yu , Xinghua Zhang , Rongwu Xu , Fei Huang , Kun Wang , Yang Liu , Junfeng Fang , Yongbin Li Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose an novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Base on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to **16 × ↑ × ↑ ** more harmful queries, while only modifying **0.006\\%** ↓ ↓ of the parameters, in contrast to the ∼ ∼ **5\\%** modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms in large models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "fV0t65OBUu@OpenReview",
      "index": 141,
      "title": "Improving Probabilistic Diffusion Models With Optimal Covariance Matching",
      "authors": [
        "Zijing Ou",
        "Mingtian Zhang",
        "Andi Zhang",
        "Tim Xiao",
        "Yingzhen Li",
        "David Barber"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "covariance",
        "diffusion",
        "matching",
        "covariances",
        "probabilistic",
        "ocm",
        "optimal",
        "involves",
        "learned",
        "regressing"
      ],
      "summary": "The probabilistic diffusion model has become highly effective across various domains. Typically, sampling from a diffusion model involves using a denoising distribution characterized by a Gaussian with a learned mean and either fixed or learned covariances. In this paper, we leverage the recently proposed covariance moment matching technique and introduce a novel method for learning the diagonal covariances. Unlike traditional data-driven covariance approximation approaches, our method involves directly regressing the optimal analytic covariance using a new, unbiased objective named Optimal Covariance Matching (OCM). This approach can significantly reduce the approximation error in covariance prediction. We demonstrate how our method can substantially enhance the sampling efficiency, recall rate and likelihood of both diffusion models and latent diffusion models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fV0t65OBUu"
        ],
        "venue": [
          "/venue/fV0t65OBUu@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fV0t65OBUu"
        ],
        "detail": [
          "https://openreview.net/forum?id=fV0t65OBUu"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 11
      },
      "raw_excerpt": "Improving Probabilistic Diffusion Models With Optimal Covariance Matching [PDF 13 ] [Copy] [Kimi 11 ] [REL] Authors : Zijing Ou , Mingtian Zhang , Andi Zhang , Tim Xiao , Yingzhen Li , David Barber The probabilistic diffusion model has become highly effective across various domains. Typically, sampling from a diffusion model involves using a denoising distribution characterized by a Gaussian with a learned mean and either fixed or learned covariances. In this paper, we leverage the recently proposed covariance moment matching technique and introduce a novel method for learning the diagonal covariances. Unlike traditional data-driven covariance approximation approaches, our method involves directly regressing the optimal analytic covariance using a new, unbiased objective named Optimal Covariance Matching (OCM). This approach can significantly reduce the approximation error in covariance prediction. We demonstrate how our method can substantially enhance the sampling efficiency, recall rate and likelihood of both diffusion models and latent diffusion models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "fMTPkDEhLQ@OpenReview",
      "index": 142,
      "title": "Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity",
      "authors": [
        "Cedar Site Bai",
        "Brian Bullins"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "frac",
        "left",
        "right",
        "sigma",
        "hölder",
        "oracle",
        "bounds",
        "uniformly",
        "epsilon",
        "order"
      ],
      "summary": "In this paper, we provide tight lower bounds for the oracle complexity of minimizing high-order Hölder smooth and uniformly convex functions. Specifically, for a function whose p t h p t h -order derivatives are Hölder continuous with degree ν ν and parameter H H , and that is uniformly convex with degree q q and parameter σ σ , we focus on two asymmetric cases: (1) q > p + ν q > p + ν , and (2) q < p + ν q < p + ν . Given up to p t h p t h -order oracle access, we establish worst-case oracle complexities of Ω ( ( H σ ) 2 3 ( p + ν ) − 2 ( σ ϵ ) 2 ( q − p − ν ) q ( 3 ( p + ν ) − 2 ) ) Ω ( ( H σ ) 2 3 ( p + ν ) − 2 ( σ ϵ ) 2 ( q − p − ν ) q ( 3 ( p + ν ) − 2 ) ) in the first case with an ℓ ∞ ℓ ∞ -ball-truncated-Gaussian smoothed hard function and Ω ( ( H σ ) 2 3 ( p + ν ) − 2 + log log ( ( σ p + ν H q ) 1 p + ν − q 1 ϵ ) ) Ω ( ( H σ ) 2 3 ( p + ν ) − 2 + log ⁡ log ⁡ ( ( σ p + ν H q ) 1 p + ν − q 1 ϵ ) ) in the second case, for reaching an ϵ ϵ -approximate solution in terms of the optimality gap. Our analysis generalizes previous lower bounds for functions under first- and second-order smoothness as well as those for uniformly convex functions, and furthermore our results match the corresponding upper bounds in this general setting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fMTPkDEhLQ"
        ],
        "venue": [
          "/venue/fMTPkDEhLQ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fMTPkDEhLQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=fMTPkDEhLQ"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Cedar Site Bai , Brian Bullins In this paper, we provide tight lower bounds for the oracle complexity of minimizing high-order Hölder smooth and uniformly convex functions. Specifically, for a function whose p t h p t h -order derivatives are Hölder continuous with degree ν ν and parameter H H , and that is uniformly convex with degree q q and parameter σ σ , we focus on two asymmetric cases: (1) q > p + ν q > p + ν , and (2) q < p + ν q < p + ν . Given up to p t h p t h -order oracle access, we establish worst-case oracle complexities of Ω ( ( H σ ) 2 3 ( p + ν ) − 2 ( σ ϵ ) 2 ( q − p − ν ) q ( 3 ( p + ν ) − 2 ) ) Ω ( ( H σ ) 2 3 ( p + ν ) − 2 ( σ ϵ ) 2 ( q − p − ν ) q ( 3 ( p + ν ) − 2 ) ) in the first case with an ℓ ∞ ℓ ∞ -ball-truncated-Gaussian smoothed hard function and Ω ( ( H σ ) 2 3 ( p + ν ) − 2 + log log ( ( σ p + ν H q ) 1 p + ν − q 1 ϵ ) ) Ω ( ( H σ ) 2 3 ( p + ν ) − 2 + log ⁡ log ⁡ ( ( σ p + ν H q ) 1 p + ν − q 1 ϵ ) ) in the second case, for reaching an ϵ ϵ -approximate solution in terms of the optimality gap. Our analysis generalizes previous lower bounds for functions under first- and second-order smoothness as well as those for uniformly convex functions, and furthermore our results match the corresponding upper bounds in this general setting. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "fAAaT826Vv@OpenReview",
      "index": 143,
      "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
      "authors": [
        "Yu Feng",
        "Ben Zhou",
        "Weidong Lin",
        "Dan Roth"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "bird",
        "llm",
        "trustworthy",
        "probabilities",
        "abductions",
        "estimations",
        "bayesian",
        "accurate",
        "inference",
        "language"
      ],
      "summary": "Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision making and planning tasks. Current large language models (LLM) are insufficient for such accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates can further contribute to better and more trustworthy decision-making.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fAAaT826Vv"
        ],
        "venue": [
          "/venue/fAAaT826Vv@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fAAaT826Vv"
        ],
        "detail": [
          "https://openreview.net/forum?id=fAAaT826Vv"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 17
      },
      "raw_excerpt": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models [PDF 12 ] [Copy] [Kimi 17 ] [REL] Authors : Yu Feng , Ben Zhou , Weidong Lin , Dan Roth Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision making and planning tasks. Current large language models (LLM) are insufficient for such accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates can further contribute to better and more trustworthy decision-making. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Wr3UuEx72f@OpenReview",
      "index": 144,
      "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
      "authors": [
        "Hanyu Wang",
        "Saksham Suri",
        "Yixuan Ren",
        "Hao Chen",
        "Abhinav Shrivastava"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "larp",
        "tokenization",
        "autoregressive",
        "video",
        "discrete",
        "tokens",
        "tokenizing",
        "generation",
        "videos",
        "prior"
      ],
      "summary": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARPs strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs). Code and checkpoints will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Wr3UuEx72f"
        ],
        "venue": [
          "/venue/Wr3UuEx72f@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Wr3UuEx72f"
        ],
        "detail": [
          "https://openreview.net/forum?id=Wr3UuEx72f"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 15
      },
      "raw_excerpt": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior [PDF 12 ] [Copy] [Kimi 15 ] [REL] Authors : Hanyu Wang , Saksham Suri , Yixuan Ren , Hao Chen , Abhinav Shrivastava We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARPs strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs). Code and checkpoints will be released. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "WJaUkwci9o@OpenReview",
      "index": 145,
      "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
      "authors": [
        "Audrey Huang",
        "Adam Block",
        "Dylan Foster",
        "Dhruv Rohatgi",
        "Cyril Zhang",
        "Max Simchowitz",
        "Jordan Ash",
        "Akshay Krishnamurthy"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "improvement",
        "self",
        "sharpening",
        "sft",
        "rlhf",
        "language",
        "amortizing",
        "capabilities",
        "generating",
        "sequences"
      ],
      "summary": "Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as “sharpening.” Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ‘sharpen’ the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WJaUkwci9o"
        ],
        "venue": [
          "/venue/WJaUkwci9o@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WJaUkwci9o"
        ],
        "detail": [
          "https://openreview.net/forum?id=WJaUkwci9o"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 27
      },
      "raw_excerpt": "Self-Improvement in Language Models: The Sharpening Mechanism [PDF 15 ] [Copy] [Kimi 27 ] [REL] Authors : Audrey Huang , Adam Block , Dylan Foster , Dhruv Rohatgi , Cyril Zhang , Max Simchowitz , Jordan Ash , Akshay Krishnamurthy Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as “sharpening.” Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ‘sharpen’ the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "V4K9h1qNxE@OpenReview",
      "index": 146,
      "title": "Attention as a Hypernetwork",
      "authors": [
        "Simon Schug",
        "Seijin Kobayashi",
        "Yassir Akram",
        "Joao Sacramento",
        "Razvan Pascanu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "hypernetwork",
        "compositional",
        "compositions",
        "latent",
        "generalization",
        "attention",
        "transformers",
        "encountered",
        "head",
        "unseen"
      ],
      "summary": "Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training, but whose compositions have not.What mechanisms underlie this ability for compositional generalization?By reformulating multi-head attention as a hypernetwork, we reveal that a composable, low-dimensional latent code specifies key-query specific operations.We find empirically that this latent code is predictive of the subtasks the network performs on unseen task compositions, revealing that latent codes acquired during training are reused to solve unseen problem instances.To further examine the hypothesis that the intrinsic hypernetwork of multi-head attention supports compositional generalization, we ablate whether making the hypernetwork generated linear value network nonlinear strengthens compositionality.We find that this modification improves compositional generalization on abstract reasoning tasks.In particular, we introduce a symbolic version of the Raven's Progressive Matrices human intelligence test, which gives us precise control over the problem compositions encountered during training and evaluation.We demonstrate on this task how scaling model size and data enables compositional generalization in transformers and gives rise to a functionally structured latent space.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=V4K9h1qNxE"
        ],
        "venue": [
          "/venue/V4K9h1qNxE@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=V4K9h1qNxE"
        ],
        "detail": [
          "https://openreview.net/forum?id=V4K9h1qNxE"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 29
      },
      "raw_excerpt": "Attention as a Hypernetwork [PDF 21 ] [Copy] [Kimi 29 ] [REL] Authors : Simon Schug , Seijin Kobayashi , Yassir Akram , Joao Sacramento , Razvan Pascanu Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training, but whose compositions have not.What mechanisms underlie this ability for compositional generalization?By reformulating multi-head attention as a hypernetwork, we reveal that a composable, low-dimensional latent code specifies key-query specific operations.We find empirically that this latent code is predictive of the subtasks the network performs on unseen task compositions, revealing that latent codes acquired during training are reused to solve unseen problem instances.To further examine the hypothesis that the intrinsic hypernetwork of multi-head attention supports compositional generalization, we ablate whether making the hypernetwork generated linear value network nonlinear strengthens compositionality.We find that this modification improves compositional generalization on abstract reasoning tasks.In particular, we introduce a symbolic version of the Raven's Progressive Matrices human intelligence test, which gives us precise control over the problem compositions encountered during training and evaluation.We demonstrate on this task how scaling model size and data enables compositional generalization in transformers and gives rise to a functionally structured latent space. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "UV5p3JZMjC@OpenReview",
      "index": 147,
      "title": "Learning Randomized Algorithms with Transformers",
      "authors": [
        "Johannes von Oswald",
        "Seijin Kobayashi",
        "Yassir Akram",
        "Angelika Steger"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "randomization",
        "randomized",
        "algorithms",
        "transformers",
        "instilled",
        "remarkable",
        "adversarial",
        "deterministic",
        "strategies",
        "transformer"
      ],
      "summary": "Randomization is a powerful tool that endows algorithms with remarkable properties. For instance, randomized algorithms excel in adversarial settings, often surpassing the worst-case performance of deterministic algorithms with large margins. Furthermore, their success probability can be amplified by simple strategies such as repetition and majority voting. In this paper, we enhance deep neural networks, in particular transformer models, with randomization. We demonstrate for the first time that randomized algorithms can be instilled in transformers through learning, in a purely data- and objective-driven manner. First, we analyze known adversarial objectives for which randomized algorithms offer a distinct advantage over deterministic ones. We then show that common optimization techniques, such as gradient descent or evolutionary strategies, can effectively learn transformer parameters that make use of the randomness provided to the model. To illustrate the broad applicability of randomization in empowering neural networks, we study three conceptual tasks: associative recall, graph coloring, and agents that explore grid worlds. In addition to demonstrating increased robustness against oblivious adversaries through learned randomization, our experiments reveal remarkable performance improvements due to the inherently random nature of the neural networks' computation and predictions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UV5p3JZMjC"
        ],
        "venue": [
          "/venue/UV5p3JZMjC@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UV5p3JZMjC"
        ],
        "detail": [
          "https://openreview.net/forum?id=UV5p3JZMjC"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 18
      },
      "raw_excerpt": "Learning Randomized Algorithms with Transformers [PDF 9 ] [Copy] [Kimi 18 ] [REL] Authors : Johannes von Oswald , Seijin Kobayashi , Yassir Akram , Angelika Steger Randomization is a powerful tool that endows algorithms with remarkable properties. For instance, randomized algorithms excel in adversarial settings, often surpassing the worst-case performance of deterministic algorithms with large margins. Furthermore, their success probability can be amplified by simple strategies such as repetition and majority voting. In this paper, we enhance deep neural networks, in particular transformer models, with randomization. We demonstrate for the first time that randomized algorithms can be instilled in transformers through learning, in a purely data- and objective-driven manner. First, we analyze known adversarial objectives for which randomized algorithms offer a distinct advantage over deterministic ones. We then show that common optimization techniques, such as gradient descent or evolutionary strategies, can effectively learn transformer parameters that make use of the randomness provided to the model. To illustrate the broad applicability of randomization in empowering neural networks, we study three conceptual tasks: associative recall, graph coloring, and agents that explore grid worlds. In addition to demonstrating increased robustness against oblivious adversaries through learned randomization, our experiments reveal remarkable performance improvements due to the inherently random nature of the neural networks' computation and predictions. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "TwJrTz9cRS@OpenReview",
      "index": 148,
      "title": "HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models",
      "authors": [
        "Qiushi Huang",
        "Tom Ko",
        "Zhan ZHUANG",
        "Lilian Tang",
        "Yu Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "hira",
        "rank",
        "hadamard",
        "adaptation",
        "lora",
        "language",
        "peft",
        "tasks",
        "parameter",
        "efficient"
      ],
      "summary": "We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new tasks. HiRA addresses this by using a Hadamard product to retain high-rank update parameters, improving the model capacity. Empirically, HiRA outperforms LoRA and its variants on several tasks, with extensive ablation studies validating its effectiveness. Our code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TwJrTz9cRS"
        ],
        "venue": [
          "/venue/TwJrTz9cRS@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TwJrTz9cRS"
        ],
        "detail": [
          "https://openreview.net/forum?id=TwJrTz9cRS"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 18
      },
      "raw_excerpt": "HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models [PDF 16 ] [Copy] [Kimi 18 ] [REL] Authors : Qiushi Huang , Tom Ko , Zhan ZHUANG , Lilian Tang , Yu Zhang We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new tasks. HiRA addresses this by using a Hadamard product to retain high-rank update parameters, improving the model capacity. Empirically, HiRA outperforms LoRA and its variants on several tasks, with extensive ablation studies validating its effectiveness. Our code will be released. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "SctfBCLmWo@OpenReview",
      "index": 149,
      "title": "A Decade's Battle on Dataset Bias: Are We There Yet?",
      "authors": [
        "Zhuang Liu",
        "Kaiming He"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "dataset",
        "decade",
        "torralba",
        "battle",
        "bias",
        "yfcc",
        "datacomp",
        "efros",
        "hopefully",
        "rethink"
      ],
      "summary": "We revisit the ``dataset classification'' experiment suggested by Torralba & Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be explained by memorization. We hope our discovery will inspire the community to rethink issues involving dataset bias.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SctfBCLmWo"
        ],
        "venue": [
          "/venue/SctfBCLmWo@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SctfBCLmWo"
        ],
        "detail": [
          "https://openreview.net/forum?id=SctfBCLmWo"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 18
      },
      "raw_excerpt": "A Decade's Battle on Dataset Bias: Are We There Yet? [PDF 11 ] [Copy] [Kimi 18 ] [REL] Authors : Zhuang Liu , Kaiming He We revisit the ``dataset classification'' experiment suggested by Torralba & Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be explained by memorization. We hope our discovery will inspire the community to rethink issues involving dataset bias. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "SPS6HzVzyt@OpenReview",
      "index": 150,
      "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance",
      "authors": [
        "Sachin Goyal",
        "Christina Baek",
        "J Kolter",
        "Aditi Raghunathan"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "context",
        "instruction",
        "finetuning",
        "reliance",
        "parametric",
        "inversion",
        "follow",
        "phenomenon",
        "instructions",
        "tulu"
      ],
      "summary": "Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, despite an initial expected increase despite an initial expected increase . We call this phenomenon as the context-parametric inversion context-parametric inversion . This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions. We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia. We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. We tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SPS6HzVzyt"
        ],
        "venue": [
          "/venue/SPS6HzVzyt@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SPS6HzVzyt"
        ],
        "detail": [
          "https://openreview.net/forum?id=SPS6HzVzyt"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 11
      },
      "raw_excerpt": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance [PDF 8 ] [Copy] [Kimi 11 ] [REL] Authors : Sachin Goyal , Christina Baek , J Kolter , Aditi Raghunathan Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, despite an initial expected increase despite an initial expected increase . We call this phenomenon as the context-parametric inversion context-parametric inversion . This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions. We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia. We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. We tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "SI2hI0frk6@OpenReview",
      "index": 151,
      "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
      "authors": [
        "Chunting Zhou",
        "Lili Yu",
        "Arun Babu",
        "Kushal Tirumala",
        "Michihiro Yasunaga",
        "Leonid Shamis",
        "Jacob Kahn",
        "Xuezhe Ma",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "transfusion",
        "modal",
        "token",
        "images",
        "recipe",
        "language",
        "modality",
        "tokens",
        "reaping",
        "multi"
      ],
      "summary": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SI2hI0frk6"
        ],
        "venue": [
          "/venue/SI2hI0frk6@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SI2hI0frk6"
        ],
        "detail": [
          "https://openreview.net/forum?id=SI2hI0frk6"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 17
      },
      "raw_excerpt": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model [PDF 14 ] [Copy] [Kimi 17 ] [REL] Authors : Chunting Zhou , Lili Yu , Arun Babu , Kushal Tirumala , Michihiro Yasunaga , Leonid Shamis , Jacob Kahn , Xuezhe Ma , Luke Zettlemoyer , Omer Levy We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "QQBPWtvtcn@OpenReview",
      "index": 152,
      "title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
      "authors": [
        "Haian Jin",
        "Hanwen Jiang",
        "Hao Tan",
        "Kai Zhang",
        "Sai Bi",
        "Tianyuan Zhang",
        "Fujun Luan",
        "Noah Snavely",
        "Zexiang Xu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "lvsm",
        "view",
        "synthesis",
        "decoder",
        "novel",
        "inductive",
        "tokens",
        "encoder",
        "latent",
        "scene"
      ],
      "summary": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details: https://lvsm-web.github.io/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QQBPWtvtcn"
        ],
        "venue": [
          "/venue/QQBPWtvtcn@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QQBPWtvtcn"
        ],
        "detail": [
          "https://openreview.net/forum?id=QQBPWtvtcn"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 7
      },
      "raw_excerpt": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias [PDF 3 ] [Copy] [Kimi 7 ] [REL] Authors : Haian Jin , Hanwen Jiang , Hao Tan , Kai Zhang , Sai Bi , Tianyuan Zhang , Fujun Luan , Noah Snavely , Zexiang Xu We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details: https://lvsm-web.github.io/ Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "QFO1asgas2@OpenReview",
      "index": 153,
      "title": "Advantage Alignment Algorithms",
      "authors": [
        "Juan Duque",
        "Milad Aghajohari",
        "Timotheus Cooijmans",
        "Razvan Ciuca",
        "Tianyu Zhang",
        "Gauthier Gidel",
        "Aaron Courville"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "opponent",
        "shaping",
        "advantage",
        "alignment",
        "agents",
        "algorithms",
        "equilibria",
        "beneficial",
        "games",
        "dilemmas"
      ],
      "summary": "Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QFO1asgas2"
        ],
        "venue": [
          "/venue/QFO1asgas2@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QFO1asgas2"
        ],
        "detail": [
          "https://openreview.net/forum?id=QFO1asgas2"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 22
      },
      "raw_excerpt": "Advantage Alignment Algorithms [PDF 7 ] [Copy] [Kimi 22 ] [REL] Authors : Juan Duque , Milad Aghajohari , Timotheus Cooijmans , Razvan Ciuca , Tianyu Zhang , Gauthier Gidel , Aaron Courville Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "NxyfSW6mLK@OpenReview",
      "index": 154,
      "title": "REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments",
      "authors": [
        "Kaustubh Sridhar",
        "Souradeep Dutta",
        "Dinesh Jayaraman",
        "Insup Lee"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "regent",
        "generalist",
        "retrieval",
        "environments",
        "agent",
        "agents",
        "context",
        "pre",
        "adapt",
        "unseen"
      ],
      "summary": "Do generalist agents only require large models pre-trained on massive amounts of data to rapidly adapt to new environments? We propose a novel approach to pre-train relatively small models and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NxyfSW6mLK"
        ],
        "venue": [
          "/venue/NxyfSW6mLK@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NxyfSW6mLK"
        ],
        "detail": [
          "https://openreview.net/forum?id=NxyfSW6mLK"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 21
      },
      "raw_excerpt": "REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments [PDF 13 ] [Copy] [Kimi 21 ] [REL] Authors : Kaustubh Sridhar , Souradeep Dutta , Dinesh Jayaraman , Insup Lee Do generalist agents only require large models pre-trained on massive amounts of data to rapidly adapt to new environments? We propose a novel approach to pre-train relatively small models and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "NO6Tv6QcDs@OpenReview",
      "index": 155,
      "title": "Limits to scalable evaluation at the frontier: LLM as judge won’t beat twice the data",
      "authors": [
        "Florian Eddie Dorner",
        "Vivian Y. Nastl",
        "Moritz Hardt"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "judge",
        "debiasing",
        "evaluation",
        "frontier",
        "llm",
        "labels",
        "preferencing",
        "scalable",
        "beat",
        "costly"
      ],
      "summary": "High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide cheap model evaluations. Unfortunately, this method of using models as judges introduces biases, such as self-preferencing, that can distort model comparisons. An emerging family of debiasing tools promises to fix these issues by using a few high quality labels to debias a large number of model judgments. In this paper, we study how far such debiasing methods, in principle, can go. Our main result shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half. Our result speaks to the severe limitations of the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to assess newly released models that are possibly better than the judge. Through an empirical evaluation, we demonstrate that the sample size savings achievable in practice are even more modest than what our theoretical limit suggests. Along the way, our work provides new observations about debiasing methods for model evaluation, and points out promising avenues for future work.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NO6Tv6QcDs"
        ],
        "venue": [
          "/venue/NO6Tv6QcDs@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NO6Tv6QcDs"
        ],
        "detail": [
          "https://openreview.net/forum?id=NO6Tv6QcDs"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 15
      },
      "raw_excerpt": "Limits to scalable evaluation at the frontier: LLM as judge won’t beat twice the data [PDF 6 ] [Copy] [Kimi 15 ] [REL] Authors : Florian Eddie Dorner , Vivian Y. Nastl , Moritz Hardt High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide cheap model evaluations. Unfortunately, this method of using models as judges introduces biases, such as self-preferencing, that can distort model comparisons. An emerging family of debiasing tools promises to fix these issues by using a few high quality labels to debias a large number of model judgments. In this paper, we study how far such debiasing methods, in principle, can go. Our main result shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half. Our result speaks to the severe limitations of the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to assess newly released models that are possibly better than the judge. Through an empirical evaluation, we demonstrate that the sample size savings achievable in practice are even more modest than what our theoretical limit suggests. Along the way, our work provides new observations about debiasing methods for model evaluation, and points out promising avenues for future work. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "N8Oj1XhtYZ@OpenReview",
      "index": 156,
      "title": "SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers",
      "authors": [
        "Enze Xie",
        "Junsong Chen",
        "Junyu Chen",
        "Han Cai",
        "Haotian Tang",
        "Yujun Lin",
        "Zhekai Zhang",
        "Muyang Li",
        "Ligeng Zhu",
        "Yao Lu",
        "Song Han"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "sana",
        "text",
        "times",
        "4096",
        "dit",
        "resolution",
        "laptop",
        "1024",
        "image",
        "images"
      ],
      "summary": "We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 × × 4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8 × × , we trained an AE that can compress images 32 × × , effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024 × × 1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=N8Oj1XhtYZ"
        ],
        "venue": [
          "/venue/N8Oj1XhtYZ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=N8Oj1XhtYZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=N8Oj1XhtYZ"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 11
      },
      "raw_excerpt": "SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers [PDF 13 ] [Copy] [Kimi 11 ] [REL] Authors : Enze Xie , Junsong Chen , Junyu Chen , Han Cai , Haotian Tang , Yujun Lin , Zhekai Zhang , Muyang Li , Ligeng Zhu , Yao Lu , Song Han We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 × × 4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8 × × , we trained an AE that can compress images 32 × × , effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024 × × 1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "KIgaAqEFHW@OpenReview",
      "index": 157,
      "title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
      "authors": [
        "Jiewen Hu",
        "Thomas Zhu",
        "Sean Welleck"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "minictx",
        "texttt",
        "proving",
        "theorem",
        "context",
        "contexts",
        "projects",
        "minif2f",
        "ntp",
        "formal"
      ],
      "summary": "Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce miniCTX miniCTX , which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. miniCTX miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for miniCTX miniCTX , we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as miniF2F miniF2F . Alongside miniCTX miniCTX , we offer ntp-toolkit ntp-toolkit for automatically extracting and annotating theorem proving data, making it easy to add new projects into miniCTX miniCTX to ensure that contexts are not seen during training. miniCTX miniCTX offers a challenging and realistic evaluation of neural theorem provers.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KIgaAqEFHW"
        ],
        "venue": [
          "/venue/KIgaAqEFHW@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KIgaAqEFHW"
        ],
        "detail": [
          "https://openreview.net/forum?id=KIgaAqEFHW"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 11
      },
      "raw_excerpt": "miniCTX: Neural Theorem Proving with (Long-)Contexts [PDF 2 ] [Copy] [Kimi 11 ] [REL] Authors : Jiewen Hu , Thomas Zhu , Sean Welleck Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce miniCTX miniCTX , which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. miniCTX miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for miniCTX miniCTX , we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as miniF2F miniF2F . Alongside miniCTX miniCTX , we offer ntp-toolkit ntp-toolkit for automatically extracting and annotating theorem proving data, making it easy to add new projects into miniCTX miniCTX to ensure that contexts are not seen during training. miniCTX miniCTX offers a challenging and realistic evaluation of neural theorem provers. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "HvSytvg3Jh@OpenReview",
      "index": 158,
      "title": "AlphaEdit: Null-Space Constrained Model Editing for Language Models",
      "authors": [
        "Junfeng Fang",
        "Houcheng Jiang",
        "Kun Wang",
        "Yunshan Ma",
        "Jie Shi",
        "Xiang Wang",
        "Xiangnan He",
        "Tat-Seng Chua"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "alphaedit",
        "editing",
        "llms",
        "knowledge",
        "preserved",
        "locating",
        "null",
        "perturbation",
        "language",
        "projection"
      ],
      "summary": "Large language models (LLMs) often exhibit hallucinations, producing incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios.To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7% with a single line of additional code for projection solely.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HvSytvg3Jh"
        ],
        "venue": [
          "/venue/HvSytvg3Jh@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HvSytvg3Jh"
        ],
        "detail": [
          "https://openreview.net/forum?id=HvSytvg3Jh"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 17
      },
      "raw_excerpt": "AlphaEdit: Null-Space Constrained Model Editing for Language Models [PDF 18 ] [Copy] [Kimi 17 ] [REL] Authors : Junfeng Fang , Houcheng Jiang , Kun Wang , Yunshan Ma , Jie Shi , Xiang Wang , Xiangnan He , Tat-Seng Chua Large language models (LLMs) often exhibit hallucinations, producing incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios.To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7% with a single line of additional code for projection solely. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "HsHxSN23rM@OpenReview",
      "index": 159,
      "title": "STAR: Synthesis of Tailored Architectures",
      "authors": [
        "Armin Thomas",
        "Rom Parnichkun",
        "Alexander Amini",
        "Stefano Massaroli",
        "Michael Poli"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "architectures",
        "star",
        "genomes",
        "tailored",
        "frontier",
        "synthesis",
        "transformers",
        "quality",
        "recombined",
        "optimize"
      ],
      "summary": "Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive, with a variety of automated or manual approaches that fall short, due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection patterns, improving over highly-optimized Transformers and striped hybrid models on the frontier of quality, parameter size, and inference cache for autoregressive language modeling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HsHxSN23rM"
        ],
        "venue": [
          "/venue/HsHxSN23rM@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HsHxSN23rM"
        ],
        "detail": [
          "https://openreview.net/forum?id=HsHxSN23rM"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 6
      },
      "raw_excerpt": "STAR: Synthesis of Tailored Architectures [PDF 4 ] [Copy] [Kimi 6 ] [REL] Authors : Armin Thomas , Rom Parnichkun , Alexander Amini , Stefano Massaroli , Michael Poli Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive, with a variety of automated or manual approaches that fall short, due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection patterns, improving over highly-optimized Transformers and striped hybrid models on the frontier of quality, parameter size, and inference cache for autoregressive language modeling. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "FpiCLJrSW8@OpenReview",
      "index": 160,
      "title": "More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness",
      "authors": [
        "Aaron J. Li",
        "Satyapriya Krishna",
        "Hima Lakkaraju"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "trustworthiness",
        "rlhf",
        "preference",
        "attribution",
        "alignment",
        "aligned",
        "human",
        "preferences",
        "verticals",
        "llms"
      ],
      "summary": "The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely used to align LLMs with labeled human preferences, but its assumed effect on model trustworthiness hasn't been rigorously evaluated. To bridge this knowledge gap, this study investigates how models aligned with general-purpose preference data perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. Our results demonstrate that RLHF on human preferences doesn't automatically guarantee trustworthiness, and reverse effects are often observed. Furthermore, we propose to adapt efficient influence function based data attribution methods to the RLHF setting to better understand the influence of fine-tuning data on individual trustworthiness benchmarks, and show its feasibility by providing our estimated attribution scores. Together, our results underscore the need for more nuanced approaches for model alignment from both the data and framework perspectives, and we hope this research will guide the community towards developing language models that are increasingly capable without sacrificing trustworthiness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FpiCLJrSW8"
        ],
        "venue": [
          "/venue/FpiCLJrSW8@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FpiCLJrSW8"
        ],
        "detail": [
          "https://openreview.net/forum?id=FpiCLJrSW8"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 19
      },
      "raw_excerpt": "More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness [PDF 15 ] [Copy] [Kimi 19 ] [REL] Authors : Aaron J. Li , Satyapriya Krishna , Hima Lakkaraju The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely used to align LLMs with labeled human preferences, but its assumed effect on model trustworthiness hasn't been rigorously evaluated. To bridge this knowledge gap, this study investigates how models aligned with general-purpose preference data perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. Our results demonstrate that RLHF on human preferences doesn't automatically guarantee trustworthiness, and reverse effects are often observed. Furthermore, we propose to adapt efficient influence function based data attribution methods to the RLHF setting to better understand the influence of fine-tuning data on individual trustworthiness benchmarks, and show its feasibility by providing our estimated attribution scores. Together, our results underscore the need for more nuanced approaches for model alignment from both the data and framework perspectives, and we hope this research will guide the community towards developing language models that are increasingly capable without sacrificing trustworthiness. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "FVuqJt3c4L@OpenReview",
      "index": 161,
      "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
      "authors": [
        "Geeling Chau",
        "Christopher Wang",
        "Sabera Talukder",
        "Vighnesh Subramaniam",
        "Saraswati Soedarmadji",
        "Yisong Yue",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "popt",
        "decoding",
        "pretrained",
        "population",
        "neural",
        "transformer",
        "data",
        "subjects",
        "representations",
        "sparse"
      ],
      "summary": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address two key challenges in scaling models with neural time-series data: sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained representations and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how they can be used to extract neuroscience insights from massive amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FVuqJt3c4L"
        ],
        "venue": [
          "/venue/FVuqJt3c4L@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FVuqJt3c4L"
        ],
        "detail": [
          "https://openreview.net/forum?id=FVuqJt3c4L"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 9
      },
      "raw_excerpt": "Population Transformer: Learning Population-level Representations of Neural Activity [PDF 3 ] [Copy] [Kimi 9 ] [REL] Authors : Geeling Chau , Christopher Wang , Sabera Talukder , Vighnesh Subramaniam , Saraswati Soedarmadji , Yisong Yue , Boris Katz , Andrei Barbu We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address two key challenges in scaling models with neural time-series data: sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained representations and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how they can be used to extract neuroscience insights from massive amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "EzjsoomYEb@OpenReview",
      "index": 162,
      "title": "Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity",
      "authors": [
        "Yam Eitan",
        "Yoav Gelberg",
        "Guy Bar-Shalom",
        "Fabrizio Frasca",
        "Michael Bronstein",
        "Haggai Maron"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "homp",
        "topological",
        "smcn",
        "expressivity",
        "tdl",
        "mcn",
        "blindspots",
        "expressive",
        "message",
        "passing"
      ],
      "summary": "Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EzjsoomYEb"
        ],
        "venue": [
          "/venue/EzjsoomYEb@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EzjsoomYEb"
        ],
        "detail": [
          "https://openreview.net/forum?id=EzjsoomYEb"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 13
      },
      "raw_excerpt": "Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity [PDF 8 ] [Copy] [Kimi 13 ] [REL] Authors : Yam Eitan , Yoav Gelberg , Guy Bar-Shalom , Fabrizio Frasca , Michael Bronstein , Haggai Maron Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "EUSkm2sVJ6@OpenReview",
      "index": 163,
      "title": "How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning",
      "authors": [
        "Yao Tong",
        "Jiayuan Ye",
        "Sajjad Zarifzadeh",
        "Reza Shokri"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "usage",
        "dataset",
        "none",
        "data",
        "textit",
        "ourmethod",
        "much",
        "inference",
        "machine",
        "mistakenly"
      ],
      "summary": "How much of a given dataset was used to train a machine learning model? This is a critical question for data owners assessing the risk of unauthorized data usage and protecting their right (United States Code, 1976). However, previous work mistakenly treats this as a binary problem—inferring whether \\textit{all or none} or \\textit{any or none} of the data was used—which is fragile when faced with real, non-binary data usage risks. To address this, we propose a fine-grained analysis called Dataset Usage Cardinality Inference (\\ourmethod{}), which estimates the exact proportion of data used. Our algorithm, leveraging debiased membership guesses, matches the performance of the optimal MLE approach (with a maximum error <0.1) but with significantly lower (e.g., 300 × 300 × less) computational cost.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EUSkm2sVJ6"
        ],
        "venue": [
          "/venue/EUSkm2sVJ6@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EUSkm2sVJ6"
        ],
        "detail": [
          "https://openreview.net/forum?id=EUSkm2sVJ6"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 17
      },
      "raw_excerpt": "How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning [PDF 8 ] [Copy] [Kimi 17 ] [REL] Authors : Yao Tong , Jiayuan Ye , Sajjad Zarifzadeh , Reza Shokri How much of a given dataset was used to train a machine learning model? This is a critical question for data owners assessing the risk of unauthorized data usage and protecting their right (United States Code, 1976). However, previous work mistakenly treats this as a binary problem—inferring whether \\textit{all or none} or \\textit{any or none} of the data was used—which is fragile when faced with real, non-binary data usage risks. To address this, we propose a fine-grained analysis called Dataset Usage Cardinality Inference (\\ourmethod{}), which estimates the exact proportion of data used. Our algorithm, leveraging debiased membership guesses, matches the performance of the optimal MLE approach (with a maximum error <0.1) but with significantly lower (e.g., 300 × 300 × less) computational cost. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "zCxGCdzreM@OpenReview",
      "index": 164,
      "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks",
      "authors": [
        "Michael Matthews",
        "Michael Beukman",
        "Chris Lu",
        "Jakob Foerster"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "kinetix",
        "agent",
        "environments",
        "jax2d",
        "open",
        "physics",
        "tasks",
        "ended",
        "training",
        "agents"
      ],
      "summary": "While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.Our trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*. This includes solving some environments that standard RL training completely fails at.We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.We open-source Jax2D, Kinetix, and our final model weights.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zCxGCdzreM"
        ],
        "venue": [
          "/venue/zCxGCdzreM@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zCxGCdzreM"
        ],
        "detail": [
          "https://openreview.net/forum?id=zCxGCdzreM"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 13
      },
      "raw_excerpt": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks [PDF 6 ] [Copy] [Kimi 13 ] [REL] Authors : Michael Matthews , Michael Beukman , Chris Lu , Jakob Foerster While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.Our trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*. This includes solving some environments that standard RL training completely fails at.We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.We open-source Jax2D, Kinetix, and our final model weights. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "DJSZGGZYVi@OpenReview",
      "index": 165,
      "title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think",
      "authors": [
        "Sihyun Yu",
        "Sangkyung Kwak",
        "Huiwon Jang",
        "Jongheon Jeong",
        "Jonathan Huang",
        "Jinwoo Shin",
        "Saining Xie"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "diffusion",
        "representations",
        "generation",
        "sit",
        "guidance",
        "training",
        "quality",
        "transformers",
        "easier",
        "repa"
      ],
      "summary": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5 × × , matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DJSZGGZYVi"
        ],
        "venue": [
          "/venue/DJSZGGZYVi@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DJSZGGZYVi"
        ],
        "detail": [
          "https://openreview.net/forum?id=DJSZGGZYVi"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 13
      },
      "raw_excerpt": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think [PDF 14 ] [Copy] [Kimi 13 ] [REL] Authors : Sihyun Yu , Sangkyung Kwak , Huiwon Jang , Jongheon Jeong , Jonathan Huang , Jinwoo Shin , Saining Xie Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5 × × , matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "CjwERcAU7w@OpenReview",
      "index": 166,
      "title": "Training Language Models to Self-Correct via Reinforcement Learning",
      "authors": [
        "Aviral Kumar",
        "Vincent Zhuang",
        "Rishabh Agarwal",
        "Yi Su",
        "John Co-Reyes",
        "Avi Singh",
        "Kate Baumli",
        "Shariq Iqbal",
        "Colton Bishop",
        "Rebecca Roelofs",
        "Lei Zhang",
        "Katrina McKinney",
        "Disha Shrivastava",
        "Cosmin Paduraru",
        "George Tucker",
        "Doina Precup",
        "Feryal Behbahani",
        "Aleksandra Faust"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "correction",
        "self",
        "score",
        "training",
        "sft",
        "reinforcement",
        "generated",
        "traces",
        "responses",
        "reward"
      ],
      "summary": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CjwERcAU7w"
        ],
        "venue": [
          "/venue/CjwERcAU7w@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CjwERcAU7w"
        ],
        "detail": [
          "https://openreview.net/forum?id=CjwERcAU7w"
        ]
      },
      "scores": {
        "pdf": 31,
        "kimi": 32
      },
      "raw_excerpt": "Training Language Models to Self-Correct via Reinforcement Learning [PDF 31 ] [Copy] [Kimi 32 ] [REL] Authors : Aviral Kumar , Vincent Zhuang , Rishabh Agarwal , Yi Su , John Co-Reyes , Avi Singh , Kate Baumli , Shariq Iqbal , Colton Bishop , Rebecca Roelofs , Lei Zhang , Katrina McKinney , Disha Shrivastava , Cosmin Paduraru , George Tucker , Doina Precup , Feryal Behbahani , Aleksandra Faust Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "BPgK5XW1Nb@OpenReview",
      "index": 167,
      "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment",
      "authors": [
        "Dongyoung Kim",
        "Jaehyung Kim",
        "Kimin Lee",
        "Jinwoo Shin"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "preference",
        "alignment",
        "llm",
        "judgment",
        "annotated",
        "human",
        "spread",
        "annotation",
        "llms",
        "boosts"
      ],
      "summary": "Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data.Our key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data.To be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. Compared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective.In addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.Our experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs.For example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BPgK5XW1Nb"
        ],
        "venue": [
          "/venue/BPgK5XW1Nb@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BPgK5XW1Nb"
        ],
        "detail": [
          "https://openreview.net/forum?id=BPgK5XW1Nb"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 17
      },
      "raw_excerpt": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment [PDF 13 ] [Copy] [Kimi 17 ] [REL] Authors : Dongyoung Kim , Jaehyung Kim , Kimin Lee , Jinwoo Shin Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data.Our key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data.To be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. Compared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective.In addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.Our experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs.For example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "AoraWUmpLU@OpenReview",
      "index": 168,
      "title": "Exploring the Impact of Activation Functions in Training Neural ODEs",
      "authors": [
        "Tianxiang Gao",
        "Siyuan Sun",
        "Hailiang Liu",
        "Hongyang Gao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "odes",
        "activation",
        "training",
        "neural",
        "functions",
        "nonlinearity",
        "impact",
        "properties",
        "ntk",
        "overparameterized"
      ],
      "summary": "Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and convergence analysis. In this paper, we address these challenges by investigating the impact of activation functions. We demonstrate that the properties of activation functions—specifically smoothness and nonlinearity—are critical to the training dynamics. Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs, while sufficient nonlinearity is essential for maintaining the spectral properties of the Neural Tangent Kernel (NTK) during training. Together, these properties enable us to establish the global convergence of Neural ODEs under gradient descent in overparameterized regimes. Our theoretical findings are validated by numerical experiments, which not only support our analysis but also provide practical guidelines for scaling Neural ODEs, potentially leading to faster training and improved performance in real-world applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AoraWUmpLU"
        ],
        "venue": [
          "/venue/AoraWUmpLU@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AoraWUmpLU"
        ],
        "detail": [
          "https://openreview.net/forum?id=AoraWUmpLU"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 8
      },
      "raw_excerpt": "Exploring the Impact of Activation Functions in Training Neural ODEs [PDF 8 ] [Copy] [Kimi 8 ] [REL] Authors : Tianxiang Gao , Siyuan Sun , Hailiang Liu , Hongyang Gao Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and convergence analysis. In this paper, we address these challenges by investigating the impact of activation functions. We demonstrate that the properties of activation functions—specifically smoothness and nonlinearity—are critical to the training dynamics. Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs, while sufficient nonlinearity is essential for maintaining the spectral properties of the Neural Tangent Kernel (NTK) during training. Together, these properties enable us to establish the global convergence of Neural ODEs under gradient descent in overparameterized regimes. Our theoretical findings are validated by numerical experiments, which not only support our analysis but also provide practical guidelines for scaling Neural ODEs, potentially leading to faster training and improved performance in real-world applications. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "8zJRon6k5v@OpenReview",
      "index": 169,
      "title": "Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series",
      "authors": [
        "Byoungwoo Park",
        "Hyungi Lee",
        "Juho Lee"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "acssm",
        "amortized",
        "irregular",
        "doob",
        "elbo",
        "continuous",
        "inference",
        "latent",
        "control",
        "series"
      ],
      "summary": "Many real-world datasets, such as healthcare, climate, and economics, are often collected as irregular time series, which poses challenges for accurate modeling. In this paper, we propose the Amortized Control of continuous State Space Model (ACSSM) for continuous dynamical modeling of time series for irregular and discrete observations. We first present a multi-marginal Doob's h h -transform to construct a continuous dynamical system conditioned on these irregular observations. Following this, we introduce a variational inference algorithm with a tight evidence lower bound (ELBO), leveraging stochastic optimal control (SOC) theory to approximate the intractable Doob's h h -transform and simulate the conditioned dynamics. To improve efficiency and scalability during both training and inference, ACSSM employs amortized inference to decouple representation learning from the latent dynamics. Additionally, it incorporates a simulation-free latent dynamics framework and a transformer-based data assimilation scheme, facilitating parallel inference of the latent states and ELBO computation. Through empirical evaluations across a variety of real-world datasets, ACSSM demonstrates superior performance in tasks such as classification, regression, interpolation, and extrapolation, while maintaining computational efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8zJRon6k5v"
        ],
        "venue": [
          "/venue/8zJRon6k5v@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8zJRon6k5v"
        ],
        "detail": [
          "https://openreview.net/forum?id=8zJRon6k5v"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 7
      },
      "raw_excerpt": "Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series [PDF 3 ] [Copy] [Kimi 7 ] [REL] Authors : Byoungwoo Park , Hyungi Lee , Juho Lee Many real-world datasets, such as healthcare, climate, and economics, are often collected as irregular time series, which poses challenges for accurate modeling. In this paper, we propose the Amortized Control of continuous State Space Model (ACSSM) for continuous dynamical modeling of time series for irregular and discrete observations. We first present a multi-marginal Doob's h h -transform to construct a continuous dynamical system conditioned on these irregular observations. Following this, we introduce a variational inference algorithm with a tight evidence lower bound (ELBO), leveraging stochastic optimal control (SOC) theory to approximate the intractable Doob's h h -transform and simulate the conditioned dynamics. To improve efficiency and scalability during both training and inference, ACSSM employs amortized inference to decouple representation learning from the latent dynamics. Additionally, it incorporates a simulation-free latent dynamics framework and a transformer-based data assimilation scheme, facilitating parallel inference of the latent states and ELBO computation. Through empirical evaluations across a variety of real-world datasets, ACSSM demonstrates superior performance in tasks such as classification, regression, interpolation, and extrapolation, while maintaining computational efficiency. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "6Mxhg9PtDE@OpenReview",
      "index": 170,
      "title": "Safety Alignment Should be Made More Than Just a Few Tokens Deep",
      "authors": [
        "Xiangyu Qi",
        "Ashwinee Panda",
        "Kaifeng Lyu",
        "Xiao Ma",
        "Subhrajit Roy",
        "Ahmad Beirami",
        "Prateek Mittal",
        "Peter Henderson"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "alignment",
        "safety",
        "tokens",
        "attacks",
        "vulnerabilities",
        "tuning",
        "shallow",
        "fine",
        "issue",
        "unifiedly"
      ],
      "summary": "The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6Mxhg9PtDE"
        ],
        "venue": [
          "/venue/6Mxhg9PtDE@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6Mxhg9PtDE"
        ],
        "detail": [
          "https://openreview.net/forum?id=6Mxhg9PtDE"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 17
      },
      "raw_excerpt": "Safety Alignment Should be Made More Than Just a Few Tokens Deep [PDF 13 ] [Copy] [Kimi 17 ] [REL] Authors : Xiangyu Qi , Ashwinee Panda , Kaifeng Lyu , Xiao Ma , Subhrajit Roy , Ahmad Beirami , Prateek Mittal , Peter Henderson The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "5IkDAfabuo@OpenReview",
      "index": 171,
      "title": "Prioritized Generative Replay",
      "authors": [
        "Renhao Wang",
        "Kevin Frans",
        "Pieter Abbeel",
        "Sergey Levine",
        "Alexei Efros"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "replay",
        "prioritized",
        "generative",
        "online",
        "experience",
        "overfitting",
        "generations",
        "useful",
        "guidance",
        "relevance"
      ],
      "summary": "Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of ``relevance functions'' that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5IkDAfabuo"
        ],
        "venue": [
          "/venue/5IkDAfabuo@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5IkDAfabuo"
        ],
        "detail": [
          "https://openreview.net/forum?id=5IkDAfabuo"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 20
      },
      "raw_excerpt": "Prioritized Generative Replay [PDF 11 ] [Copy] [Kimi 20 ] [REL] Authors : Renhao Wang , Kevin Frans , Pieter Abbeel , Sergey Levine , Alexei Efros Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of ``relevance functions'' that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "1aF2D2CPHi@OpenReview",
      "index": 172,
      "title": "Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation",
      "authors": [
        "Yongxian Wei",
        "Zixuan Hu",
        "Li Shen",
        "Zhenyi Wang",
        "Chun Yuan",
        "Dacheng Tao"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "clip",
        "dfkd",
        "customization",
        "distillation",
        "knowledge",
        "vocabulary",
        "diversification",
        "images",
        "synthetic",
        "inversing"
      ],
      "summary": "Vision-language models such as CLIP have demonstrated strong zero-shot performance, but their considerable size and inefficient inference limit customizable deployment for users. While knowledge distillation is a solution, it still requires the original data, which is not always available due to copyrights and privacy concerns. For many users seeking open-vocabulary customization, Data-Free Knowledge Distillation (DFKD) emerges as a promising direction. Upon rethinking DFKD, we find that existing methods fail on CLIP due to their heavy reliance on BatchNorm layers, which are unexpectedly unusable in CLIP. Based on our findings, we adopt image-text matching to achieve DFKD for CLIP, enabling customization based on arbitrary class texts. This involves (i) inversing a surrogate dataset from CLIP based on text prompts; and (ii) distilling a student model from CLIP using the surrogate dataset. Specifically, we introduce style dictionary diversification to enhance the diversity of synthetic images. To prevent uncontrollable semantics introduced by diversification, we propose a class consistency maintaining strategy to ensure the consistency of synthetic images. Based on synthetic images with various styles, we further propose meta knowledge distillation to train the student model with good generalization ability. Moreover, we introduce a simple yet effective method to enable customization based on few example images. Comprehensive experiments showcase the superiority of our approach across twelve customized tasks, achieving a 9.33\\% improvement compared to existing DFKD methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1aF2D2CPHi"
        ],
        "venue": [
          "/venue/1aF2D2CPHi@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1aF2D2CPHi"
        ],
        "detail": [
          "https://openreview.net/forum?id=1aF2D2CPHi"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 19
      },
      "raw_excerpt": "Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation [PDF 19 ] [Copy] [Kimi 19 ] [REL] Authors : Yongxian Wei , Zixuan Hu , Li Shen , Zhenyi Wang , Chun Yuan , Dacheng Tao Vision-language models such as CLIP have demonstrated strong zero-shot performance, but their considerable size and inefficient inference limit customizable deployment for users. While knowledge distillation is a solution, it still requires the original data, which is not always available due to copyrights and privacy concerns. For many users seeking open-vocabulary customization, Data-Free Knowledge Distillation (DFKD) emerges as a promising direction. Upon rethinking DFKD, we find that existing methods fail on CLIP due to their heavy reliance on BatchNorm layers, which are unexpectedly unusable in CLIP. Based on our findings, we adopt image-text matching to achieve DFKD for CLIP, enabling customization based on arbitrary class texts. This involves (i) inversing a surrogate dataset from CLIP based on text prompts; and (ii) distilling a student model from CLIP using the surrogate dataset. Specifically, we introduce style dictionary diversification to enhance the diversity of synthetic images. To prevent uncontrollable semantics introduced by diversification, we propose a class consistency maintaining strategy to ensure the consistency of synthetic images. Based on synthetic images with various styles, we further propose meta knowledge distillation to train the student model with good generalization ability. Moreover, we introduce a simple yet effective method to enable customization based on few example images. Comprehensive experiments showcase the superiority of our approach across twelve customized tasks, achieving a 9.33\\% improvement compared to existing DFKD methods. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "1HCN4pjTb4@OpenReview",
      "index": 173,
      "title": "Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse",
      "authors": [
        "Arthur Jacot",
        "Peter Súkeník",
        "Zihan Wang",
        "Marco Mondelli"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "collapse",
        "balancedness",
        "neural",
        "ell",
        "weight",
        "dnns",
        "conditioning",
        "training",
        "features",
        "unconstrained"
      ],
      "summary": "Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on the unconstrained features model. Here, the features of the penultimate layer are free variables, which makes the model data-agnostic and puts into question its ability to capture DNN training. Our work addresses the issue, moving away from unconstrained features and studying DNNs that end with at least two linear layers. We first prove generic guarantees on neural collapse that assume (i) low training error and balancedness of linear layers (for within-class variability collapse), and (ii) bounded conditioning of the features before the linear part (for orthogonality of class-means, and their alignment with weight matrices). The balancedness refers to the fact that W ⊤ ℓ + 1 W ℓ + 1 ≈ W ℓ W ⊤ ℓ W ℓ + 1 ⊤ W ℓ + 1 ≈ W ℓ W ℓ ⊤ for any pair ofconsecutive weight matrices of the linear part, and the bounded conditioning requires a well-behaved ratio between largest and smallest non-zero singular values of the features. We then show that such assumptions hold for gradient descent training with weight decay: (i) for networks with a wide first layer, we prove low training error and balancedness, and (ii) for solutions that are either nearly optimal or stable under large learning rates, we additionally prove the bounded conditioning. Taken together, our results are the first to show neural collapse in the end-to-end training of DNNs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1HCN4pjTb4"
        ],
        "venue": [
          "/venue/1HCN4pjTb4@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1HCN4pjTb4"
        ],
        "detail": [
          "https://openreview.net/forum?id=1HCN4pjTb4"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 7
      },
      "raw_excerpt": "Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse [PDF 7 ] [Copy] [Kimi 7 ] [REL] Authors : Arthur Jacot , Peter Súkeník , Zihan Wang , Marco Mondelli Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on the unconstrained features model. Here, the features of the penultimate layer are free variables, which makes the model data-agnostic and puts into question its ability to capture DNN training. Our work addresses the issue, moving away from unconstrained features and studying DNNs that end with at least two linear layers. We first prove generic guarantees on neural collapse that assume (i) low training error and balancedness of linear layers (for within-class variability collapse), and (ii) bounded conditioning of the features before the linear part (for orthogonality of class-means, and their alignment with weight matrices). The balancedness refers to the fact that W ⊤ ℓ + 1 W ℓ + 1 ≈ W ℓ W ⊤ ℓ W ℓ + 1 ⊤ W ℓ + 1 ≈ W ℓ W ℓ ⊤ for any pair ofconsecutive weight matrices of the linear part, and the bounded conditioning requires a well-behaved ratio between largest and smallest non-zero singular values of the features. We then show that such assumptions hold for gradient descent training with weight decay: (i) for networks with a wide first layer, we prove low training error and balancedness, and (ii) for solutions that are either nearly optimal or stable under large learning rates, we additionally prove the bounded conditioning. Taken together, our results are the first to show neural collapse in the end-to-end training of DNNs. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "zl0HLZOJC9@OpenReview",
      "index": 174,
      "title": "Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution",
      "authors": [
        "Cuong Nguyen",
        "Thanh-Toan Do",
        "Gustavo Carneiro"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "l2d",
        "experts",
        "defer",
        "human",
        "workload",
        "probabilistic",
        "expert",
        "annotations",
        "cooperation",
        "missing"
      ],
      "summary": "Recent progress in machine learning research is gradually shifting its focus towards *human-AI cooperation* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is *learning to defer* (L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a time-consuming and expensive annotation process that can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier, which is critical to optimise resource allocation. We, therefore, propose a new probabilistic modelling approach inspired by the mixture-of-experts, where the Expectation - Maximisation algorithm is leverage to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets shows that our proposed probabilistic approach performs competitively, or surpasses previously proposed methods assessed on the same benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zl0HLZOJC9"
        ],
        "venue": [
          "/venue/zl0HLZOJC9@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zl0HLZOJC9"
        ],
        "detail": [
          "https://openreview.net/forum?id=zl0HLZOJC9"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Cuong Nguyen , Thanh-Toan Do , Gustavo Carneiro Recent progress in machine learning research is gradually shifting its focus towards *human-AI cooperation* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is *learning to defer* (L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a time-consuming and expensive annotation process that can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier, which is critical to optimise resource allocation. We, therefore, propose a new probabilistic modelling approach inspired by the mixture-of-experts, where the Expectation - Maximisation algorithm is leverage to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets shows that our proposed probabilistic approach performs competitively, or surpasses previously proposed methods assessed on the same benchmarks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "zBbZ2vdLzH@OpenReview",
      "index": 175,
      "title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance",
      "authors": [
        "Jonas Linkerhägner",
        "Cheng Shi",
        "Ivan Dokmanić"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "jdr",
        "graph",
        "rewiring",
        "node",
        "denoising",
        "gnns",
        "joint",
        "rewire",
        "feature",
        "improves"
      ],
      "summary": "In graph learning the graph and the node features both contain noisy information about the node labels. In this paper we propose joint denoising and rewiring (JDR)—an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification graph neural nets (GNNs). JDR improves the alignment between the leading eigenspaces of graph and feature matrices. To approximately solve the associated non-convex optimization problem we propose a heuristic that efficiently handles real-world graph datasets with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and verify the effectiveness of our approach through extensive experiments on synthetic and real-world graph datasets. The results show that JDR consistently outperforms existing rewiring methods on node classification using GNNs as downstream models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zBbZ2vdLzH"
        ],
        "venue": [
          "/venue/zBbZ2vdLzH@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zBbZ2vdLzH"
        ],
        "detail": [
          "https://openreview.net/forum?id=zBbZ2vdLzH"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 9
      },
      "raw_excerpt": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance [PDF 9 ] [Copy] [Kimi 9 ] [REL] Authors : Jonas Linkerhägner , Cheng Shi , Ivan Dokmanić In graph learning the graph and the node features both contain noisy information about the node labels. In this paper we propose joint denoising and rewiring (JDR)—an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification graph neural nets (GNNs). JDR improves the alignment between the leading eigenspaces of graph and feature matrices. To approximately solve the associated non-convex optimization problem we propose a heuristic that efficiently handles real-world graph datasets with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and verify the effectiveness of our approach through extensive experiments on synthetic and real-world graph datasets. The results show that JDR consistently outperforms existing rewiring methods on node classification using GNNs as downstream models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "wPMRwmytZe@OpenReview",
      "index": 176,
      "title": "Progressive distillation induces an implicit curriculum",
      "authors": [
        "Abhishek Panigrahi",
        "Bingbin Liu",
        "Sadhika Malladi",
        "Andrej Risteski",
        "Surbhi Goel"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "curriculum",
        "distillation",
        "progressive",
        "student",
        "teacher",
        "implicit",
        "checkpoints",
        "intermediate",
        "parity",
        "empirical"
      ],
      "summary": "Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several “intermediate” teachers. One empirically validated variant of this principle is progressive distillation, where the student learns from successive intermediate checkpoints of the teacher. Using sparse parity as a sandbox, we identify an implicit curriculum as one mechanism through which progressive distillation accelerates the student’s learning. This curriculum is available only through the intermediate checkpoints but not the final converged one, and imparts both empirical acceleration and a provable sample complexity benefit to the student. We then extend our investigation to Transformers trained on probabilistic context-free grammars (PCFGs) and real-world pre-training datasets (Wikipedia and Books). Through probing the teacher model, we identify an analogous implicit curriculum where the model progressively learns features that capture longer context. Our theoretical and empirical findings on sparse parity, complemented by empirical observations on more complex tasks, highlight the benefit of progressive distillation via implicit curriculum across setups.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wPMRwmytZe"
        ],
        "venue": [
          "/venue/wPMRwmytZe@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wPMRwmytZe"
        ],
        "detail": [
          "https://openreview.net/forum?id=wPMRwmytZe"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 16
      },
      "raw_excerpt": "Progressive distillation induces an implicit curriculum [PDF 6 ] [Copy] [Kimi 16 ] [REL] Authors : Abhishek Panigrahi , Bingbin Liu , Sadhika Malladi , Andrej Risteski , Surbhi Goel Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several “intermediate” teachers. One empirically validated variant of this principle is progressive distillation, where the student learns from successive intermediate checkpoints of the teacher. Using sparse parity as a sandbox, we identify an implicit curriculum as one mechanism through which progressive distillation accelerates the student’s learning. This curriculum is available only through the intermediate checkpoints but not the final converged one, and imparts both empirical acceleration and a provable sample complexity benefit to the student. We then extend our investigation to Transformers trained on probabilistic context-free grammars (PCFGs) and real-world pre-training datasets (Wikipedia and Books). Through probing the teacher model, we identify an analogous implicit curriculum where the model progressively learns features that capture longer context. Our theoretical and empirical findings on sparse parity, complemented by empirical observations on more complex tasks, highlight the benefit of progressive distillation via implicit curriculum across setups. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "xoIeVdFO7U@OpenReview",
      "index": 177,
      "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning",
      "authors": [
        "Chongyi Zheng",
        "Jens Tuyls",
        "Joanne Peng",
        "Benjamin Eysenbach"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "misl",
        "metra",
        "skill",
        "mutual",
        "successor",
        "ingredients",
        "learning",
        "contrastive",
        "fly",
        "information"
      ],
      "summary": "Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL).Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xoIeVdFO7U"
        ],
        "venue": [
          "/venue/xoIeVdFO7U@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xoIeVdFO7U"
        ],
        "detail": [
          "https://openreview.net/forum?id=xoIeVdFO7U"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 9
      },
      "raw_excerpt": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning [PDF 9 ] [Copy] [Kimi 9 ] [REL] Authors : Chongyi Zheng , Jens Tuyls , Joanne Peng , Benjamin Eysenbach Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL).Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "trKNi4IUiP@OpenReview",
      "index": 178,
      "title": "Robustness Inspired Graph Backdoor Defense",
      "authors": [
        "Zhiwei Zhang",
        "Minhua Lin",
        "Junjie Xu",
        "Zongyu Wu",
        "Enyan Dai",
        "Suhang Wang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "backdoor",
        "poisoned",
        "graph",
        "attacks",
        "defending",
        "dropping",
        "nodes",
        "gnns",
        "triggers",
        "clean"
      ],
      "summary": "Graph Neural Networks (GNNs) have achieved promising results in tasks such as node classification and graph classification. However, recent studies reveal that GNNs are vulnerable to backdoor attacks, posing a significant threat to their real-world adoption. Despite initial efforts to defend against specific graph backdoor attacks, there is no work on defending against various types of backdoor attacks where generated triggers have different properties. Hence, we first empirically verify that prediction variance under edge dropping is a crucial indicator for identifying poisoned nodes. With this observation, we propose using random edge dropping to detect backdoors and theoretically show that it can efficiently distinguish poisoned nodes from clean ones. Furthermore, we introduce a novel robust training strategy to efficiently counteract the impact of the triggers. Extensive experiments on real-world datasets show that our framework can effectively identify poisoned nodes, significantly degrade the attack success rate, and maintain clean accuracy when defending against various types of graph backdoor attacks with different properties. Our code is available at: https://anonymous.4open.science/r/RIGBD-A670.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=trKNi4IUiP"
        ],
        "venue": [
          "/venue/trKNi4IUiP@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=trKNi4IUiP"
        ],
        "detail": [
          "https://openreview.net/forum?id=trKNi4IUiP"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 5
      },
      "raw_excerpt": "Robustness Inspired Graph Backdoor Defense [PDF 7 ] [Copy] [Kimi 5 ] [REL] Authors : Zhiwei Zhang , Minhua Lin , Junjie Xu , Zongyu Wu , Enyan Dai , Suhang Wang Graph Neural Networks (GNNs) have achieved promising results in tasks such as node classification and graph classification. However, recent studies reveal that GNNs are vulnerable to backdoor attacks, posing a significant threat to their real-world adoption. Despite initial efforts to defend against specific graph backdoor attacks, there is no work on defending against various types of backdoor attacks where generated triggers have different properties. Hence, we first empirically verify that prediction variance under edge dropping is a crucial indicator for identifying poisoned nodes. With this observation, we propose using random edge dropping to detect backdoors and theoretically show that it can efficiently distinguish poisoned nodes from clean ones. Furthermore, we introduce a novel robust training strategy to efficiently counteract the impact of the triggers. Extensive experiments on real-world datasets show that our framework can effectively identify poisoned nodes, significantly degrade the attack success rate, and maintain clean accuracy when defending against various types of graph backdoor attacks with different properties. Our code is available at: https://anonymous.4open.science/r/RIGBD-A670. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "kX8h23UG6v@OpenReview",
      "index": 179,
      "title": "Standard Gaussian Process Can Be Excellent for High-Dimensional Bayesian Optimization",
      "authors": [
        "Zhitong Xu",
        "Haitao Wang",
        "Jeff Phillips",
        "Shandian Zhe"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "optimization",
        "matérn",
        "standard",
        "kernel",
        "initialization",
        "dimensional",
        "bayesian",
        "belief",
        "kernels",
        "gaussian"
      ],
      "summary": "A long-standing belief holds that Bayesian Optimization (BO) with standard Gaussian processes (GP) --- referred to as standard BO --- underperforms in high-dimensional optimization problems. While this belief seems plausible, it lacks both robust empirical evidence and theoretical justification. To address this gap, we present a systematic investigation. First, through a comprehensive evaluation across eleven widely used benchmarks, we found that while the popular Square Exponential (SE) kernel often leads to poor performance, using Matérn kernels enables standard BO to consistently achieve top-tier results, frequently surpassing methods specifically designed for high-dimensional optimization. Second, our theoretical analysis reveals that the SE kernel’s failure primarily stems from improper initialization of the length-scale parameters, which are commonly used in practice but can cause gradient vanishing in training. We provide a probabilistic bound to characterize this issue, showing that Matérn kernels are less susceptible and can robustly handle much higher dimensions. Third, we propose a simple robust initialization strategy that dramatically improves the performance of the SE kernel, bringing it close to state-of-the-art methods, without requiring any additional priors or regularization. We prove another probabilistic bound that demonstrates how the gradient vanishing issue can be effectively mitigated with our method. Our findings advocate for a re-evaluation of standard BO’s potential in high-dimensional settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kX8h23UG6v"
        ],
        "venue": [
          "/venue/kX8h23UG6v@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kX8h23UG6v"
        ],
        "detail": [
          "https://openreview.net/forum?id=kX8h23UG6v"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Standard Gaussian Process Can Be Excellent for High-Dimensional Bayesian Optimization [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Zhitong Xu , Haitao Wang , Jeff Phillips , Shandian Zhe A long-standing belief holds that Bayesian Optimization (BO) with standard Gaussian processes (GP) --- referred to as standard BO --- underperforms in high-dimensional optimization problems. While this belief seems plausible, it lacks both robust empirical evidence and theoretical justification. To address this gap, we present a systematic investigation. First, through a comprehensive evaluation across eleven widely used benchmarks, we found that while the popular Square Exponential (SE) kernel often leads to poor performance, using Matérn kernels enables standard BO to consistently achieve top-tier results, frequently surpassing methods specifically designed for high-dimensional optimization. Second, our theoretical analysis reveals that the SE kernel’s failure primarily stems from improper initialization of the length-scale parameters, which are commonly used in practice but can cause gradient vanishing in training. We provide a probabilistic bound to characterize this issue, showing that Matérn kernels are less susceptible and can robustly handle much higher dimensions. Third, we propose a simple robust initialization strategy that dramatically improves the performance of the SE kernel, bringing it close to state-of-the-art methods, without requiring any additional priors or regularization. We prove another probabilistic bound that demonstrates how the gradient vanishing issue can be effectively mitigated with our method. Our findings advocate for a re-evaluation of standard BO’s potential in high-dimensional settings. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "esYrEndGsr@OpenReview",
      "index": 180,
      "title": "Influence Functions for Scalable Data Attribution in Diffusion Models",
      "authors": [
        "Bruno Mlodozeniec",
        "Runa Eschenhagen",
        "Juhan Bae",
        "Alexander Immer",
        "David Krueger",
        "Richard E Turner"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "attribution",
        "influence",
        "diffusion",
        "data",
        "functions",
        "models",
        "modelling",
        "predicting",
        "fac",
        "lds"
      ],
      "summary": "Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by extending influence functions. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we use a K-FAC approximation based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We show that our recommended method outperforms previously proposed data attribution methods on common data attribution evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=esYrEndGsr"
        ],
        "venue": [
          "/venue/esYrEndGsr@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=esYrEndGsr"
        ],
        "detail": [
          "https://openreview.net/forum?id=esYrEndGsr"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 9
      },
      "raw_excerpt": "Influence Functions for Scalable Data Attribution in Diffusion Models [PDF 11 ] [Copy] [Kimi 9 ] [REL] Authors : Bruno Mlodozeniec , Runa Eschenhagen , Juhan Bae , Alexander Immer , David Krueger , Richard E Turner Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by extending influence functions. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we use a K-FAC approximation based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We show that our recommended method outperforms previously proposed data attribution methods on common data attribution evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "Pujt3ADZgI@OpenReview",
      "index": 181,
      "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning",
      "authors": [
        "Yuheng Zhang",
        "Dian Yu",
        "Baolin Peng",
        "Linfeng Song",
        "Ye Tian",
        "Mingyue Huo",
        "Nan Jiang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "inpo",
        "rlhf",
        "nash",
        "preferences",
        "policy",
        "aligning",
        "win",
        "regret",
        "policyoptimization",
        "winrate"
      ],
      "summary": "Reinforcement Learning with Human Feedback (RLHF) has achieved great successin aligning large language models (LLMs) with human preferences. PrevalentRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. Inthis paper, we explore RLHF under a general preference framework and approachit from a game-theoretic perspective. Specifically, we formulate the problem asa two-player game and propose a novel online algorithm, iterative Nash policyoptimization (INPO). The key idea is to let the policy play against itself via no-regret learning, thereby approximating the Nash policy. Unlike previous methods,INPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,we introduce a new loss objective that is directly minimized over a preferencedataset. We provide theoretical analysis for our approach and demonstrate itseffectiveness through experiments on various representative benchmarks. With anLLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled winrate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantialimprovement over the state-of-the-art online RLHF algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Pujt3ADZgI"
        ],
        "venue": [
          "/venue/Pujt3ADZgI@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Pujt3ADZgI"
        ],
        "detail": [
          "https://openreview.net/forum?id=Pujt3ADZgI"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 14
      },
      "raw_excerpt": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning [PDF 8 ] [Copy] [Kimi 14 ] [REL] Authors : Yuheng Zhang , Dian Yu , Baolin Peng , Linfeng Song , Ye Tian , Mingyue Huo , Nan Jiang , Haitao Mi , Dong Yu Reinforcement Learning with Human Feedback (RLHF) has achieved great successin aligning large language models (LLMs) with human preferences. PrevalentRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. Inthis paper, we explore RLHF under a general preference framework and approachit from a game-theoretic perspective. Specifically, we formulate the problem asa two-player game and propose a novel online algorithm, iterative Nash policyoptimization (INPO). The key idea is to let the policy play against itself via no-regret learning, thereby approximating the Nash policy. Unlike previous methods,INPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,we introduce a new loss objective that is directly minimized over a preferencedataset. We provide theoretical analysis for our approach and demonstrate itseffectiveness through experiments on various representative benchmarks. With anLLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled winrate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantialimprovement over the state-of-the-art online RLHF algorithms. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "eBS3dQQ8GV@OpenReview",
      "index": 182,
      "title": "Emergence of meta-stable clustering in mean-field transformer models",
      "authors": [
        "Giuseppe Bruno",
        "Federico Pasqualotto",
        "Andrea Agazzi"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "meta",
        "stable",
        "mean",
        "geshkovski",
        "field",
        "emergence",
        "clustering",
        "transformer",
        "tokens",
        "pde"
      ],
      "summary": "We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in Geshkovski et al. (2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as a function of the inverse temperature parameter of the model, by the index maximizing a certain rescaling of Gegenbauer polynomials.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eBS3dQQ8GV"
        ],
        "venue": [
          "/venue/eBS3dQQ8GV@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eBS3dQQ8GV"
        ],
        "detail": [
          "https://openreview.net/forum?id=eBS3dQQ8GV"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 9
      },
      "raw_excerpt": "Emergence of meta-stable clustering in mean-field transformer models [PDF 4 ] [Copy] [Kimi 9 ] [REL] Authors : Giuseppe Bruno , Federico Pasqualotto , Andrea Agazzi We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in Geshkovski et al. (2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as a function of the inverse temperature parameter of the model, by the index maximizing a certain rescaling of Gegenbauer polynomials. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "WOzffPgVjF@OpenReview",
      "index": 183,
      "title": "Knowing Your Target : Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding",
      "authors": [
        "Xin Gu",
        "Yaojie Shen",
        "Chenxi Luo",
        "Tiejian Luo",
        "Yan Huang",
        "YUEWEI LIN",
        "Heng Fan",
        "Libo Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "stvg",
        "target",
        "cues",
        "queries",
        "video",
        "temporal",
        "object",
        "aware",
        "transformer",
        "multimodal"
      ],
      "summary": "Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in the complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and largely outperforms the baseline, validating its efficacy. Code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WOzffPgVjF"
        ],
        "venue": [
          "/venue/WOzffPgVjF@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WOzffPgVjF"
        ],
        "detail": [
          "https://openreview.net/forum?id=WOzffPgVjF"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 14
      },
      "raw_excerpt": "Knowing Your Target : Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding [PDF 12 ] [Copy] [Kimi 14 ] [REL] Authors : Xin Gu , Yaojie Shen , Chenxi Luo , Tiejian Luo , Yan Huang , YUEWEI LIN , Heng Fan , Libo Zhang Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in the complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and largely outperforms the baseline, validating its efficacy. Code will be released. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "4xWQS2z77v@OpenReview",
      "index": 184,
      "title": "Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality",
      "authors": [
        "Sungyoon Kim",
        "Aaron Mishkin",
        "Mert Pilanci"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "convex",
        "neural",
        "networks",
        "landscape",
        "regularized",
        "loss",
        "connectivity",
        "problem",
        "layer",
        "stationary"
      ],
      "summary": "We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with non-increasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two layer vector-valued neural networks and parallel three-layer neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4xWQS2z77v"
        ],
        "venue": [
          "/venue/4xWQS2z77v@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4xWQS2z77v"
        ],
        "detail": [
          "https://openreview.net/forum?id=4xWQS2z77v"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 9
      },
      "raw_excerpt": "Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality [PDF 6 ] [Copy] [Kimi 9 ] [REL] Authors : Sungyoon Kim , Aaron Mishkin , Mert Pilanci We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with non-increasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two layer vector-valued neural networks and parallel three-layer neural networks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "stUKwWBuBm@OpenReview",
      "index": 185,
      "title": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics",
      "authors": [
        "Eric Mazumdar",
        "Kishan Panaganti",
        "Laixi Shi"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "rqe",
        "economics",
        "tractable",
        "games",
        "equilibria",
        "aversion",
        "agent",
        "reinforcement",
        "behavioral",
        "rationality"
      ],
      "summary": "A significant roadblock to the development of principled multi-agent reinforcement learning is the fact that desired solution concepts like Nash equilibria may be intractable to compute. To overcome this obstacle, we take inspiration from behavioral economics and show that---by imbuing agents with important features of human decision-making like risk aversion and bounded rationality---a class of risk-averse quantal response equilibria (RQE) become tractable to compute in all n n -player matrix and finite-horizon Markov games. In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degree of risk-aversion and bounded rationality. To validate the richness of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model and validate our findings on a simple multi-agent reinforcement learning benchmark.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=stUKwWBuBm"
        ],
        "venue": [
          "/venue/stUKwWBuBm@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=stUKwWBuBm"
        ],
        "detail": [
          "https://openreview.net/forum?id=stUKwWBuBm"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 21
      },
      "raw_excerpt": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics [PDF 7 ] [Copy] [Kimi 21 ] [REL] Authors : Eric Mazumdar , Kishan Panaganti , Laixi Shi A significant roadblock to the development of principled multi-agent reinforcement learning is the fact that desired solution concepts like Nash equilibria may be intractable to compute. To overcome this obstacle, we take inspiration from behavioral economics and show that---by imbuing agents with important features of human decision-making like risk aversion and bounded rationality---a class of risk-averse quantal response equilibria (RQE) become tractable to compute in all n n -player matrix and finite-horizon Markov games. In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degree of risk-aversion and bounded rationality. To validate the richness of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model and validate our findings on a simple multi-agent reinforcement learning benchmark. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "is4nCVkSFA@OpenReview",
      "index": 186,
      "title": "Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model",
      "authors": [
        "Siyu Chen",
        "Beining Wu",
        "Miao Lu",
        "Zhuoran Yang",
        "Tianhao Wang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "star",
        "lor",
        "tradeoff",
        "index",
        "polylogarithmic",
        "achieve",
        "statistical",
        "tilde",
        "neural",
        "gradient"
      ],
      "summary": "In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? Prior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires Ω ( d s ⋆ / 2 ∨ d ) Ω ( d s ⋆ / 2 ∨ d ) samples, where s ⋆ s ⋆ is the generative exponent representing the intrinsic difficulty of learning the underlying model.However, it remains unknown whether neural networks can achieve this sample complexity. Inspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.Our method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.We show that our algorithm learns a feature representation that strongly aligns with the unknown signal θ ⋆ θ ⋆ , with sample complexity O ~ ( d s ⋆ / 2 ∨ d ) O ~ ( d s ⋆ / 2 ∨ d ) , matching the SQ lower bound up to a polylogarithmic factor for all generative exponents s ⋆ ≥ 1 s ⋆ ≥ 1 .Furthermore, we extend our approach to the setting where θ ⋆ θ ⋆ is k k -sparse for k = o ( d − − √ ) k = o ( d ) by introducing a novel weight perturbation technique that leverages the sparsity structure. We derive a corresponding SQ lower bound of order Ω ~ ( k s ⋆ ) Ω ~ ( k s ⋆ ) , matched by our method up to a polylogarithmic factor.Our framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=is4nCVkSFA"
        ],
        "venue": [
          "/venue/is4nCVkSFA@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=is4nCVkSFA"
        ],
        "detail": [
          "https://openreview.net/forum?id=is4nCVkSFA"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 8
      },
      "raw_excerpt": "Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model [PDF 4 ] [Copy] [Kimi 8 ] [REL] Authors : Siyu Chen , Beining Wu , Miao Lu , Zhuoran Yang , Tianhao Wang In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? Prior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires Ω ( d s ⋆ / 2 ∨ d ) Ω ( d s ⋆ / 2 ∨ d ) samples, where s ⋆ s ⋆ is the generative exponent representing the intrinsic difficulty of learning the underlying model.However, it remains unknown whether neural networks can achieve this sample complexity. Inspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.Our method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.We show that our algorithm learns a feature representation that strongly aligns with the unknown signal θ ⋆ θ ⋆ , with sample complexity O ~ ( d s ⋆ / 2 ∨ d ) O ~ ( d s ⋆ / 2 ∨ d ) , matching the SQ lower bound up to a polylogarithmic factor for all generative exponents s ⋆ ≥ 1 s ⋆ ≥ 1 .Furthermore, we extend our approach to the setting where θ ⋆ θ ⋆ is k k -sparse for k = o ( d − − √ ) k = o ( d ) by introducing a novel weight perturbation technique that leverages the sparsity structure. We derive a corresponding SQ lower bound of order Ω ~ ( k s ⋆ ) Ω ~ ( k s ⋆ ) , matched by our method up to a polylogarithmic factor.Our framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "7BLXhmWvwF@OpenReview",
      "index": 187,
      "title": "Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects",
      "authors": [
        "Tai Hoang",
        "Huy Le",
        "Philipp Becker",
        "Vien A Ngo",
        "Gerhard Neumann"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "objects",
        "hepi",
        "deformable",
        "cloth",
        "equivariant",
        "tasks",
        "actuators",
        "manipulation",
        "heterogeneous",
        "insertion"
      ],
      "summary": "Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing S E ( 3 ) S E ( 3 ) equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7BLXhmWvwF"
        ],
        "venue": [
          "/venue/7BLXhmWvwF@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7BLXhmWvwF"
        ],
        "detail": [
          "https://openreview.net/forum?id=7BLXhmWvwF"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 14
      },
      "raw_excerpt": "Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects [PDF 8 ] [Copy] [Kimi 14 ] [REL] Authors : Tai Hoang , Huy Le , Philipp Becker , Vien A Ngo , Gerhard Neumann Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing S E ( 3 ) S E ( 3 ) equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "g3xuCtrG6H@OpenReview",
      "index": 188,
      "title": "A Computational Framework for Modeling Emergence of Color Vision in the Human Brain",
      "authors": [
        "Atsunobu Kotani",
        "Yi-Ren Ng"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "color",
        "vision",
        "nerve",
        "optic",
        "dimensionality",
        "cortex",
        "eye",
        "signals",
        "brain",
        "human"
      ],
      "summary": "It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a bio-plausible model of cortical learning based on self-supervised prediction of optic nerve signal fluctuations under natural eye motions. We show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains N N types of color photoreceptors, our simulation shows that N N -dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=g3xuCtrG6H"
        ],
        "venue": [
          "/venue/g3xuCtrG6H@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=g3xuCtrG6H"
        ],
        "detail": [
          "https://openreview.net/forum?id=g3xuCtrG6H"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 11
      },
      "raw_excerpt": "A Computational Framework for Modeling Emergence of Color Vision in the Human Brain [PDF 12 ] [Copy] [Kimi 11 ] [REL] Authors : Atsunobu Kotani , Yi-Ren Ng It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a bio-plausible model of cortical learning based on self-supervised prediction of optic nerve signal fluctuations under natural eye motions. We show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains N N types of color photoreceptors, our simulation shows that N N -dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "j7cyANIAxV@OpenReview",
      "index": 189,
      "title": "Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation",
      "authors": [
        "Chenbin Zhang",
        "Zhiqiang Hu",
        "Jiang Chuchu",
        "Wen Chen",
        "JIE XU",
        "Shaoting Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "similarity",
        "drug",
        "evaluation",
        "split",
        "affinity",
        "target",
        "aware",
        "rethinking",
        "set",
        "samples"
      ],
      "summary": "Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=j7cyANIAxV"
        ],
        "venue": [
          "/venue/j7cyANIAxV@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=j7cyANIAxV"
        ],
        "detail": [
          "https://openreview.net/forum?id=j7cyANIAxV"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : Chenbin Zhang , Zhiqiang Hu , Jiang Chuchu , Wen Chen , JIE XU , Shaoting Zhang Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "xByvdb3DCm@OpenReview",
      "index": 190,
      "title": "When Selection meets Intervention: Additional Complexities in Causal Discovery",
      "authors": [
        "Haoyue Dai",
        "Ignavier Ng",
        "Jianle Sun",
        "Zeyu Tang",
        "Gongxu Luo",
        "Xinshuai Dong",
        "Peter Spirtes",
        "Kun Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "interventions",
        "causal",
        "selection",
        "interventional",
        "discovery",
        "bias",
        "world",
        "meets",
        "intervention",
        "complexities"
      ],
      "summary": "We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xByvdb3DCm"
        ],
        "venue": [
          "/venue/xByvdb3DCm@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xByvdb3DCm"
        ],
        "detail": [
          "https://openreview.net/forum?id=xByvdb3DCm"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 13
      },
      "raw_excerpt": "When Selection meets Intervention: Additional Complexities in Causal Discovery [PDF 8 ] [Copy] [Kimi 13 ] [REL] Authors : Haoyue Dai , Ignavier Ng , Jianle Sun , Zeyu Tang , Gongxu Luo , Xinshuai Dong , Peter Spirtes , Kun Zhang We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "07yvxWDSla@OpenReview",
      "index": 191,
      "title": "Synthetic continued pretraining",
      "authors": [
        "Zitong Yang",
        "Neil Band",
        "Shuangping Li",
        "Emmanuel Candes",
        "Tatsunori Hashimoto"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "entigraph",
        "pretraining",
        "continued",
        "corpus",
        "synthetic",
        "documents",
        "knowledge",
        "augmentation",
        "entities",
        "source"
      ],
      "summary": "Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge.However, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it.This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once.We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus.We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source corpus and then generates diverse text by drawing connections between those entities.Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them.If the source documents are instead available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation.To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can \"rearrange\" knowledge to enable more data-efficient learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=07yvxWDSla"
        ],
        "venue": [
          "/venue/07yvxWDSla@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=07yvxWDSla"
        ],
        "detail": [
          "https://openreview.net/forum?id=07yvxWDSla"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 23
      },
      "raw_excerpt": "Synthetic continued pretraining [PDF 19 ] [Copy] [Kimi 23 ] [REL] Authors : Zitong Yang , Neil Band , Shuangping Li , Emmanuel Candes , Tatsunori Hashimoto Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge.However, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it.This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once.We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus.We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source corpus and then generates diverse text by drawing connections between those entities.Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them.If the source documents are instead available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation.To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can \"rearrange\" knowledge to enable more data-efficient learning. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "pQqeQpMkE7@OpenReview",
      "index": 192,
      "title": "On Scaling Up 3D Gaussian Splatting Training",
      "authors": [
        "Hexu Zhao",
        "Haoyang Weng",
        "Daohan Lu",
        "Ang Li",
        "Jinyang Li",
        "Aurojit Panda",
        "Saining Xie"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "grendel",
        "3dgs",
        "gaussians",
        "splatting",
        "scaling",
        "gpu",
        "psnr",
        "gaussian",
        "gpus",
        "rendering"
      ],
      "summary": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K ``Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pQqeQpMkE7"
        ],
        "venue": [
          "/venue/pQqeQpMkE7@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pQqeQpMkE7"
        ],
        "detail": [
          "https://openreview.net/forum?id=pQqeQpMkE7"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 7
      },
      "raw_excerpt": "On Scaling Up 3D Gaussian Splatting Training [PDF 6 ] [Copy] [Kimi 7 ] [REL] Authors : Hexu Zhao , Haoyang Weng , Daohan Lu , Ang Li , Jinyang Li , Aurojit Panda , Saining Xie 3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K ``Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "vRvVVb0NAz@OpenReview",
      "index": 193,
      "title": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
      "authors": [
        "Hongkang Li",
        "Yihua Zhang",
        "shuai ZHANG",
        "Meng Wang",
        "Sijia Liu",
        "Pin-Yu Chen"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "task",
        "editing",
        "generalization",
        "conceptual",
        "tasks",
        "theoretical",
        "unlearning",
        "transformers",
        "irrelevant",
        "arithmetic"
      ],
      "summary": "Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vRvVVb0NAz"
        ],
        "venue": [
          "/venue/vRvVVb0NAz@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vRvVVb0NAz"
        ],
        "detail": [
          "https://openreview.net/forum?id=vRvVVb0NAz"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 10
      },
      "raw_excerpt": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers [PDF 10 ] [Copy] [Kimi 10 ] [REL] Authors : Hongkang Li , Yihua Zhang , shuai ZHANG , Meng Wang , Sijia Liu , Pin-Yu Chen Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B). Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "t7P5BUKcYv@OpenReview",
      "index": 194,
      "title": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts",
      "authors": [
        "Peng Jin",
        "Bo Zhu",
        "Yuan Li",
        "Shuicheng YAN"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "moe",
        "experts",
        "ffn",
        "expert",
        "zero",
        "computation",
        "vanilla",
        "tokens",
        "skip",
        "mixture"
      ],
      "summary": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) **Low Computing Overhead**: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) **High Performance**: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) **Deployment Friendly**: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1 ∼ ∼ 2.1 × × expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=t7P5BUKcYv"
        ],
        "venue": [
          "/venue/t7P5BUKcYv@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=t7P5BUKcYv"
        ],
        "detail": [
          "https://openreview.net/forum?id=t7P5BUKcYv"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 22
      },
      "raw_excerpt": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts [PDF 19 ] [Copy] [Kimi 22 ] [REL] Authors : Peng Jin , Bo Zhu , Yuan Li , Shuicheng YAN In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) **Low Computing Overhead**: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) **High Performance**: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) **Deployment Friendly**: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1 ∼ ∼ 2.1 × × expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "tcsZt9ZNKD@OpenReview",
      "index": 195,
      "title": "Scaling and evaluating sparse autoencoders",
      "authors": [
        "Leo Gao",
        "Tom Dupre la Tour",
        "Henk Tillman",
        "Gabriel Goh",
        "Rajan Troll",
        "Alec Radford",
        "Ilya Sutskever",
        "Jan Leike",
        "Jeffrey Wu"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "autoencoders",
        "sparsity",
        "autoencoder",
        "sparse",
        "latents",
        "dead",
        "scaling",
        "activations",
        "makhzani",
        "evaluating"
      ],
      "summary": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tcsZt9ZNKD"
        ],
        "venue": [
          "/venue/tcsZt9ZNKD@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tcsZt9ZNKD"
        ],
        "detail": [
          "https://openreview.net/forum?id=tcsZt9ZNKD"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 18
      },
      "raw_excerpt": "Scaling and evaluating sparse autoencoders [PDF 19 ] [Copy] [Kimi 18 ] [REL] Authors : Leo Gao , Tom Dupre la Tour , Henk Tillman , Gabriel Goh , Rajan Troll , Alec Radford , Ilya Sutskever , Jan Leike , Jeffrey Wu Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "ijbA5swmoK@OpenReview",
      "index": 196,
      "title": "Second-Order Min-Max Optimization with Lazy Hessians",
      "authors": [
        "Lesi Chen",
        "Chengchang Liu",
        "Jingzhao Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "complexity",
        "epsilon",
        "mathcal",
        "hessians",
        "concave",
        "minimax",
        "thecomputational",
        "doikov",
        "oracle",
        "tilde"
      ],
      "summary": "This paper studies second-order methods for convex-concave minimax optimization. Monteiro & Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of O ( ϵ − 3 / 2 ) O ( ϵ − 3 / 2 ) to find an ϵ ϵ -saddle point. However, it is unclear whether thecomputational complexity, O ( ( N + d 2 ) d ϵ − 2 / 3 ) O ( ( N + d 2 ) d ϵ − 2 / 3 ) , can be improved. In the above, we follow Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as N N and the complexity of obtaining a second-order oracle as d N d N . In this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of O ~ ( ( N + d 2 ) ( d + d 2 / 3 ϵ − 2 / 3 ) ) O ~ ( ( N + d 2 ) ( d + d 2 / 3 ϵ − 2 / 3 ) ) , which improves those of previous methods by a factor of d 1 / 3 d 1 / 3 . Furthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of O ~ ( ( N + d 2 ) ( d + d 2 / 3 κ 2 / 3 ) ) O ~ ( ( N + d 2 ) ( d + d 2 / 3 κ 2 / 3 ) ) when the condition number of the problem is κ κ , enjoying a similar speedup upon the state-of-the-art method. Numerical experiments on both real and synthetic datasets also verify the efficiency of our method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ijbA5swmoK"
        ],
        "venue": [
          "/venue/ijbA5swmoK@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ijbA5swmoK"
        ],
        "detail": [
          "https://openreview.net/forum?id=ijbA5swmoK"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 7
      },
      "raw_excerpt": "Second-Order Min-Max Optimization with Lazy Hessians [PDF 11 ] [Copy] [Kimi 7 ] [REL] Authors : Lesi Chen , Chengchang Liu , Jingzhao Zhang This paper studies second-order methods for convex-concave minimax optimization. Monteiro & Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of O ( ϵ − 3 / 2 ) O ( ϵ − 3 / 2 ) to find an ϵ ϵ -saddle point. However, it is unclear whether thecomputational complexity, O ( ( N + d 2 ) d ϵ − 2 / 3 ) O ( ( N + d 2 ) d ϵ − 2 / 3 ) , can be improved. In the above, we follow Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as N N and the complexity of obtaining a second-order oracle as d N d N . In this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of O ~ ( ( N + d 2 ) ( d + d 2 / 3 ϵ − 2 / 3 ) ) O ~ ( ( N + d 2 ) ( d + d 2 / 3 ϵ − 2 / 3 ) ) , which improves those of previous methods by a factor of d 1 / 3 d 1 / 3 . Furthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of O ~ ( ( N + d 2 ) ( d + d 2 / 3 κ 2 / 3 ) ) O ~ ( ( N + d 2 ) ( d + d 2 / 3 κ 2 / 3 ) ) when the condition number of the problem is κ κ , enjoying a similar speedup upon the state-of-the-art method. Numerical experiments on both real and synthetic datasets also verify the efficiency of our method. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "gQlxd3Mtru@OpenReview",
      "index": 197,
      "title": "Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport",
      "authors": [
        "Zhenyi Zhang",
        "Tiejun Li",
        "Peijie Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "ruot",
        "snapshots",
        "unbalanced",
        "regularized",
        "dynamics",
        "waddington",
        "stochastic",
        "transport",
        "seq",
        "sparsely"
      ],
      "summary": "Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learnt directly from data. Theoretically, we explore the connections between the RUOT and Schrödinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gQlxd3Mtru"
        ],
        "venue": [
          "/venue/gQlxd3Mtru@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gQlxd3Mtru"
        ],
        "detail": [
          "https://openreview.net/forum?id=gQlxd3Mtru"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 8
      },
      "raw_excerpt": "Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport [PDF 7 ] [Copy] [Kimi 8 ] [REL] Authors : Zhenyi Zhang , Tiejun Li , Peijie Zhou Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learnt directly from data. Theoretically, we explore the connections between the RUOT and Schrödinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "ny8T8OuNHe@OpenReview",
      "index": 198,
      "title": "Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model",
      "authors": [
        "Han Lin",
        "Jaemin Cho",
        "Abhay Zala",
        "Mohit Bansal"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "controlnets",
        "ctrl",
        "video",
        "adapter",
        "control",
        "diverse",
        "diffusion",
        "backbones",
        "versatile",
        "controls"
      ],
      "summary": "ControlNets are widely used for adding spatial control to text-to-image diffusion models. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames can not effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control, zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-α, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (< 10 GPU hours). We provide video examples in https://ctrladapterexamples.github.io and code in the supplementary material.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ny8T8OuNHe"
        ],
        "venue": [
          "/venue/ny8T8OuNHe@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ny8T8OuNHe"
        ],
        "detail": [
          "https://openreview.net/forum?id=ny8T8OuNHe"
        ]
      },
      "scores": {
        "pdf": 24,
        "kimi": 16
      },
      "raw_excerpt": "Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model [PDF 24 ] [Copy] [Kimi 16 ] [REL] Authors : Han Lin , Jaemin Cho , Abhay Zala , Mohit Bansal ControlNets are widely used for adding spatial control to text-to-image diffusion models. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames can not effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control, zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-α, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (< 10 GPU hours). We provide video examples in https://ctrladapterexamples.github.io and code in the supplementary material. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "OIvg3MqWX2@OpenReview",
      "index": 199,
      "title": "A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules",
      "authors": [
        "Shih-Hsin Wang",
        "Yuhao Huang",
        "Justin Baker",
        "Yuan-En Sun",
        "Qi Tang",
        "Bao Wang"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "graph",
        "molecules",
        "gnns",
        "graphs",
        "sparsity",
        "connectivity",
        "rigidity",
        "guarantees",
        "sparse",
        "principled"
      ],
      "summary": "Graph neural networks (GNNs) -- learn graph representations by exploiting graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical guarantees for achieving connectivity and sparsity simultaneously, which are essential for the performance and scalability of GNNs. Furthermore, existing widely used graph construction methods for molecules lack rigidity, limiting GNNs' ability to exploit graph nodes' spatial arrangement. In this paper, we introduce a new hyperparameter-free graph construction of molecules and beyond with sparsity, connectivity, and rigidity guarantees. Remarkably, our method consistently generates connected and sparse graphs with the edge-to-node ratio being bounded above by 3. Our graphs' rigidity guarantees that edge distances and dihedral angles are sufficient to uniquely determine general spatial arrangements of atoms. We substantiate the effectiveness and efficiency of our proposed graphs in various molecular modeling benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OIvg3MqWX2"
        ],
        "venue": [
          "/venue/OIvg3MqWX2@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OIvg3MqWX2"
        ],
        "detail": [
          "https://openreview.net/forum?id=OIvg3MqWX2"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 7
      },
      "raw_excerpt": "A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules [PDF 15 ] [Copy] [Kimi 7 ] [REL] Authors : Shih-Hsin Wang , Yuhao Huang , Justin Baker , Yuan-En Sun , Qi Tang , Bao Wang Graph neural networks (GNNs) -- learn graph representations by exploiting graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical guarantees for achieving connectivity and sparsity simultaneously, which are essential for the performance and scalability of GNNs. Furthermore, existing widely used graph construction methods for molecules lack rigidity, limiting GNNs' ability to exploit graph nodes' spatial arrangement. In this paper, we introduce a new hyperparameter-free graph construction of molecules and beyond with sparsity, connectivity, and rigidity guarantees. Remarkably, our method consistently generates connected and sparse graphs with the edge-to-node ratio being bounded above by 3. Our graphs' rigidity guarantees that edge distances and dihedral angles are sufficient to uniquely determine general spatial arrangements of atoms. We substantiate the effectiveness and efficiency of our proposed graphs in various molecular modeling benchmarks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "o2Igqm95SJ@OpenReview",
      "index": 200,
      "title": "CAX: Cellular Automata Accelerated in JAX",
      "authors": [
        "Maxence Faldor",
        "Antoine Cully"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "cax",
        "cellular",
        "automata",
        "jax",
        "accelerated",
        "library",
        "research",
        "accelerate",
        "life",
        "modular"
      ],
      "summary": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines, spanning neuroscience, artificial life, and theoretical physics. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX offers cutting-edge performance and a modular design through a user-friendly interface, and can support both discrete and continuous cellular automata with any number of dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o2Igqm95SJ"
        ],
        "venue": [
          "/venue/o2Igqm95SJ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o2Igqm95SJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=o2Igqm95SJ"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 9
      },
      "raw_excerpt": "CAX: Cellular Automata Accelerated in JAX [PDF 8 ] [Copy] [Kimi 9 ] [REL] Authors : Maxence Faldor , Antoine Cully Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines, spanning neuroscience, artificial life, and theoretical physics. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX offers cutting-edge performance and a modular design through a user-friendly interface, and can support both discrete and continuous cellular automata with any number of dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "A3YUPeJTNR@OpenReview",
      "index": 201,
      "title": "The Hidden Cost of Waiting for Accurate Predictions",
      "authors": [
        "Ali Shirali",
        "Ariel Procaccia",
        "Rediet Abebe"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "allocations",
        "predictions",
        "individuals",
        "planner",
        "wait",
        "observations",
        "improve",
        "allocation",
        "ranking",
        "waiting"
      ],
      "summary": "Algorithmic predictions are increasingly informing societal resource allocations by identifying individuals for targeting. Policymakers often build these systems with the assumption that by gathering more observations on individuals, they can improve predictive accuracy and, consequently, allocation efficiency. An overlooked yet consequential aspect of prediction-driven allocations is that of timing. The planner has to trade off relying on earlier and potentially noisier predictions to intervene before individuals experience undesirable outcomes, or they may wait to gather more observations to make more precise allocations. We examine this tension using a simple mathematical model, where the planner collects observations on individuals to improve predictions over time. We analyze both the ranking induced by these predictions and optimal resource allocation. We show that though individual prediction accuracy improves over time, counter-intuitively, the average ranking loss can worsen. As a result, the planner's ability to improve social welfare can decline. We identify inequality as a driving factor behind this phenomenon. Our findings provide a nuanced perspective and challenge the conventional wisdom that it is preferable to wait for more accurate predictions to ensure the most efficient allocations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A3YUPeJTNR"
        ],
        "venue": [
          "/venue/A3YUPeJTNR@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A3YUPeJTNR"
        ],
        "detail": [
          "https://openreview.net/forum?id=A3YUPeJTNR"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 6
      },
      "raw_excerpt": "The Hidden Cost of Waiting for Accurate Predictions [PDF 3 ] [Copy] [Kimi 6 ] [REL] Authors : Ali Shirali , Ariel Procaccia , Rediet Abebe Algorithmic predictions are increasingly informing societal resource allocations by identifying individuals for targeting. Policymakers often build these systems with the assumption that by gathering more observations on individuals, they can improve predictive accuracy and, consequently, allocation efficiency. An overlooked yet consequential aspect of prediction-driven allocations is that of timing. The planner has to trade off relying on earlier and potentially noisier predictions to intervene before individuals experience undesirable outcomes, or they may wait to gather more observations to make more precise allocations. We examine this tension using a simple mathematical model, where the planner collects observations on individuals to improve predictions over time. We analyze both the ranking induced by these predictions and optimal resource allocation. We show that though individual prediction accuracy improves over time, counter-intuitively, the average ranking loss can worsen. As a result, the planner's ability to improve social welfare can decline. We identify inequality as a driving factor behind this phenomenon. Our findings provide a nuanced perspective and challenge the conventional wisdom that it is preferable to wait for more accurate predictions to ensure the most efficient allocations. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "n2NidsYDop@OpenReview",
      "index": 202,
      "title": "Transformers Provably Solve Parity Efficiently with Chain of Thought",
      "authors": [
        "Juno Kim",
        "Taiji Suzuki"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "parity",
        "cot",
        "transformers",
        "intermediate",
        "reasoning",
        "solve",
        "chain",
        "thought",
        "teacher",
        "efficiently"
      ],
      "summary": "This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental k k -parity problem, extending the work on RNNs by \\citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \\emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. Our findings, supported by numerical experiments, show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve multi-step reasoning ability, aligning with empirical studies of CoT.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n2NidsYDop"
        ],
        "venue": [
          "/venue/n2NidsYDop@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n2NidsYDop"
        ],
        "detail": [
          "https://openreview.net/forum?id=n2NidsYDop"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 19
      },
      "raw_excerpt": "Transformers Provably Solve Parity Efficiently with Chain of Thought [PDF 12 ] [Copy] [Kimi 19 ] [REL] Authors : Juno Kim , Taiji Suzuki This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental k k -parity problem, extending the work on RNNs by \\citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \\emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. Our findings, supported by numerical experiments, show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve multi-step reasoning ability, aligning with empirical studies of CoT. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "v593OaNePQ@OpenReview",
      "index": 203,
      "title": "Learning to Search from Demonstration Sequences",
      "authors": [
        "Dixant Mittal",
        "Liwei Kang",
        "Wee Sun Lee"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "tsn",
        "search",
        "sequences",
        "tree",
        "world",
        "demonstrations",
        "scenarios",
        "learning",
        "procgen",
        "planning"
      ],
      "summary": "Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=v593OaNePQ"
        ],
        "venue": [
          "/venue/v593OaNePQ@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=v593OaNePQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=v593OaNePQ"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 12
      },
      "raw_excerpt": "Learning to Search from Demonstration Sequences [PDF 7 ] [Copy] [Kimi 12 ] [REL] Authors : Dixant Mittal , Liwei Kang , Wee Sun Lee Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "je3GZissZc@OpenReview",
      "index": 204,
      "title": "Instant Policy: In-Context Imitation Learning via Graph Diffusion",
      "authors": [
        "Vitalis Vosylius",
        "Edward Johns"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "icil",
        "instant",
        "demonstrations",
        "policy",
        "imitation",
        "context",
        "graph",
        "tasks",
        "diffusion",
        "learning"
      ],
      "summary": "Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=je3GZissZc"
        ],
        "venue": [
          "/venue/je3GZissZc@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=je3GZissZc"
        ],
        "detail": [
          "https://openreview.net/forum?id=je3GZissZc"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 12
      },
      "raw_excerpt": "Instant Policy: In-Context Imitation Learning via Graph Diffusion [PDF 14 ] [Copy] [Kimi 12 ] [REL] Authors : Vitalis Vosylius , Edward Johns Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "eFGQ97z5Cd@OpenReview",
      "index": 205,
      "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free",
      "authors": [
        "Ziyue Li",
        "Tianyi Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "embedding",
        "moee",
        "moe",
        "finetuning",
        "llms",
        "experts",
        "llm",
        "secretly",
        "mixture",
        "tasks"
      ],
      "summary": "While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eFGQ97z5Cd"
        ],
        "venue": [
          "/venue/eFGQ97z5Cd@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eFGQ97z5Cd"
        ],
        "detail": [
          "https://openreview.net/forum?id=eFGQ97z5Cd"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 33
      },
      "raw_excerpt": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free [PDF 19 ] [Copy] [Kimi 33 ] [REL] Authors : Ziyue Li , Tianyi Zhou While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "P7KIGdgW8S@OpenReview",
      "index": 206,
      "title": "On the Hölder Stability of Multiset and Graph Neural Networks",
      "authors": [
        "Yair Davidson",
        "Nadav Dym"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "mpnns",
        "separation",
        "hölder",
        "multiset",
        "expectation",
        "quality",
        "graph",
        "maximally",
        "analysis",
        "lipschitz"
      ],
      "summary": "Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough.In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and Hölder stability to parametric functions. The proposed framework, which we name Hölder in expectation, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-Hölder in expectation, with an exponent that decays rapidly with the network's depth . Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=P7KIGdgW8S"
        ],
        "venue": [
          "/venue/P7KIGdgW8S@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=P7KIGdgW8S"
        ],
        "detail": [
          "https://openreview.net/forum?id=P7KIGdgW8S"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 11
      },
      "raw_excerpt": "On the Hölder Stability of Multiset and Graph Neural Networks [PDF 7 ] [Copy] [Kimi 11 ] [REL] Authors : Yair Davidson , Nadav Dym Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough.In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and Hölder stability to parametric functions. The proposed framework, which we name Hölder in expectation, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-Hölder in expectation, with an exponent that decays rapidly with the network's depth . Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks. Subject : ICLR.2025 - Oral"
    },
    {
      "paper_id": "LyJi5ugyJx@OpenReview",
      "index": 207,
      "title": "Simplifying, Stabilizing and Scaling Continuous-time Consistency Models",
      "authors": [
        "Cheng Lu",
        "Yang Song"
      ],
      "subjects": [
        "ICLR.2025 - Oral"
      ],
      "keywords": [
        "512",
        "cms",
        "imagenet",
        "diffusion",
        "fid",
        "continuous",
        "consistency",
        "scores",
        "models",
        "simplifying"
      ],
      "summary": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512×512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512, narrowing the gap in FID scores with the best existing diffusion models to within 10\\%.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LyJi5ugyJx"
        ],
        "venue": [
          "/venue/LyJi5ugyJx@OpenReview",
          "/venue/ICLR.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LyJi5ugyJx"
        ],
        "detail": [
          "https://openreview.net/forum?id=LyJi5ugyJx"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 22
      },
      "raw_excerpt": "Simplifying, Stabilizing and Scaling Continuous-time Consistency Models [PDF 21 ] [Copy] [Kimi 22 ] [REL] Authors : Cheng Lu , Yang Song Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512×512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512, narrowing the gap in FID scores with the best existing diffusion models to within 10\\%. Subject : ICLR.2025 - Oral"
    }
  ]
}