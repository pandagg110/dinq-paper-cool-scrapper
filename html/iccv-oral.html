<html><head>
    <title>ICCV.2025 - Oral | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for ICCV.2025 - Oral, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body id="venue"><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">ICCV.2025 - Oral</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 64
    </p>
    <div class="papers">
        <div id="Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="panel paper" keywords="tam,mllms,tokens,activation,token,visualization,mllm,multimodal,map,redundant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs_ICCV_2025_paper.html" target="_blank" title="1/64"><span class="index notranslate">#1</span></a>
                <a id="title-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="title-link" href="/venue/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" target="_blank">Token Activation Map to Visually Explain Multimodal LLMs</a>
                <a id="pdf-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF">72</sup>]</a>
                <a id="copy-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF">66</sup>]</a>
                <a id="rel-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Li" target="_blank">Yi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hualiang Wang" target="_blank">Hualiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinpeng Ding" target="_blank">Xinpeng Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haonan Wang" target="_blank">Haonan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaomeng Li" target="_blank">Xiaomeng Li</a>
            </p>
            <p id="summary-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="summary">Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available at github.com/xmed-lab/TAM.</p>
            <p id="subjects-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" onclick="foldPdfKimi('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="panel paper" keywords="swapping,identity,proactive,deepfake,face,nullswap,cloaking,identities,images,source">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping_ICCV_2025_paper.html" target="_blank" title="2/64"><span class="index notranslate">#2</span></a>
                <a id="title-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="title-link" href="/venue/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" target="_blank">NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping</a>
                <a id="pdf-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF">18</sup>]</a>
                <a id="copy-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF">14</sup>]</a>
                <a id="rel-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Wang" target="_blank">Tianyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuaicheng Niu" target="_blank">Shuaicheng Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harry Cheng" target="_blank">Harry Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Zhang" target="_blank">Xiao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinglong Wang" target="_blank">Yinglong Wang</a>
            </p>
            <p id="summary-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="summary">Suffering from performance bottlenecks in passively detecting high-quality Deepfake images due to the advancement of generative models, proactive perturbations offer a promising approach to disabling Deepfake manipulations by inserting signals into benign images. However, existing proactive perturbation approaches remain unsatisfactory in several aspects: 1) visual degradation due to direct element-wise addition; 2) limited effectiveness against face swapping manipulation; 3) unavoidable reliance on white- and grey-box settings to involve generative models during training. In this study, we analyze the essence of Deepfake face swapping and argue the necessity of protecting source identities rather than target images, and we propose NullSwap, a novel proactive defense approach that cloaks source image identities and nullifies face swapping under a pure black-box scenario. We design an Identity Extraction module to obtain facial identity features from the source image, while a Perturbation Block is then devised to generate identity-guided perturbations accordingly. Meanwhile, a Feature Block extracts shallow-level image features, which are then fused with the perturbation in the Cloaking Block for image reconstruction. Furthermore, to ensure adaptability across different identity extractors in face swapping algorithms, we propose Dynamic Loss Weighting to adaptively balance identity losses. Experiments demonstrate the outstanding ability of our approach to fool various identity recognition models, outperforming state-of-the-art proactive perturbations in preventing face swapping models from generating images with correct source identities.</p>
            <p id="subjects-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" onclick="foldPdfKimi('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="panel paper" keywords="corrclip,patch,correlations,segmentation,clip,sam,semantic,vocabulary,inter,masks">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.html" target="_blank" title="3/64"><span class="index notranslate">#3</span></a>
                <a id="title-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" target="_blank">CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation</a>
                <a id="pdf-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF">23</sup>]</a>
                <a id="copy-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF">23</sup>]</a>
                <a id="rel-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dengke Zhang" target="_blank">Dengke Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fagui Liu" target="_blank">Fagui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Tang" target="_blank">Quan Tang</a>
            </p>
            <p id="summary-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="summary">Open-vocabulary semantic segmentation aims to assign semantic labels to each pixel without being constrained by a predefined set of categories. While Contrastive Language-Image Pre-training (CLIP) excels in zero-shot classification, it struggles to align image patches with category embeddings because of its incoherent patch correlations. This study reveals that inter-class correlations are the main reason for impairing CLIP's segmentation performance. Accordingly, we propose CorrCLIP, which reconstructs the scope and value of patch correlations. Specifically, CorrCLIP leverages the Segment Anything Model (SAM) to define the scope of patch interactions, reducing inter-class correlations. To mitigate the problem that SAM-generated masks may contain patches belonging to different classes, CorrCLIP incorporates self-supervised models to compute coherent similarity values, suppressing the weight of inter-class correlations. Additionally, we introduce two additional branches to strengthen patch features' spatial details and semantic representation. Finally, we update segmentation maps with SAM-generated masks to improve spatial consistency. Based on the improvement across patch correlations, feature representations, and segmentation maps, CorrCLIP achieves superior performance across eight benchmarks. Codes are available at: https://github.com/zdk258/CorrCLIP.</p>
            <p id="subjects-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="panel paper" keywords="maskcontrol,logits,motion,control,masked,regularizer,joint,generation,align,positions">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis_ICCV_2025_paper.html" target="_blank" title="4/64"><span class="index notranslate">#4</span></a>
                <a id="title-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="title-link" href="/venue/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" target="_blank">MaskControl: Spatio-Temporal Control for Masked Motion Synthesis</a>
                <a id="pdf-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF">19</sup>]</a>
                <a id="copy-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF">11</sup>]</a>
                <a id="rel-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ekkasit Pinyoanuntapong" target="_blank">Ekkasit Pinyoanuntapong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Saleem" target="_blank">Muhammad Saleem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Korrawe Karunratanakul" target="_blank">Korrawe Karunratanakul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pu Wang" target="_blank">Pu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongfei Xue" target="_blank">Hongfei Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Chen" target="_blank">Chen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuan Guo" target="_blank">Chuan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junli Cao" target="_blank">Junli Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Ren" target="_blank">Jian Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Tulyakov" target="_blank">Sergey Tulyakov</a>
            </p>
            <p id="summary-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="summary">Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, Logits Regularizer implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, Logit Optimization explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce Differentiable Expectation Sampling (DES) to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by 77%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://anonymous-ai-agent.github.io/CAM</p>
            <p id="subjects-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" onclick="foldPdfKimi('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="panel paper" keywords="volumes,cost,removing,estimators,optical,flow,frames,memory,raft,2xfaster">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.html" target="_blank" title="5/64"><span class="index notranslate">#5</span></a>
                <a id="title-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="title-link" href="/venue/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" target="_blank">Removing Cost Volumes from Optical Flow Estimators</a>
                <a id="pdf-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF">16</sup>]</a>
                <a id="copy-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF">12</sup>]</a>
                <a id="rel-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Kiefhaber" target="_blank">Simon Kiefhaber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Roth" target="_blank">Stefan Roth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simone Schaub-Meyer" target="_blank">Simone Schaub-Meyer</a>
            </p>
            <p id="summary-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="summary">Cost volumes are used in every modern optical flow estimator, but due to their computational and space complexity, they are often a limiting factor regarding both processing speed and the resolution of input frames. Motivated by our empirical observation that cost volumes lose their importance once all other network parts of, e.g., a RAFT-based pipeline have been sufficiently trained, we introduce a training strategy that allows removing the cost volume from optical flow estimators throughout training. This leads to significantly improved inference speed and reduced memory requirements. Using our training strategy, we create three different models covering different compute budgets. Our most accurate model reaches state-of-the-art accuracy while being 1.2xfaster and having a 6xlower memory footprint than comparable models; our fastest model is capable of processing Full HD frames at 20\,\mathrm FPS using only 500\,\mathrm MB of GPU memory.</p>
            <p id="subjects-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" onclick="foldPdfKimi('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="panel paper" keywords="occlusion,image,rendering,objects,larender,relationships,control,generation,latent,opacities">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering_ICCV_2025_paper.html" target="_blank" title="6/64"><span class="index notranslate">#6</span></a>
                <a id="title-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="title-link" href="/venue/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" target="_blank">LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering</a>
                <a id="pdf-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF">28</sup>]</a>
                <a id="copy-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF">12</sup>]</a>
                <a id="rel-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohang Zhan" target="_blank">Xiaohang Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dingming Liu" target="_blank">Dingming Liu</a>
            </p>
            <p id="summary-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="summary">We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to "render" the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc.</p>
            <p id="subjects-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" onclick="foldPdfKimi('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="panel paper" keywords="autoeval,boxes,pcr,reliability,nms,consistency,evaluation,object,detection,automated">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and_ICCV_2025_paper.html" target="_blank" title="7/64"><span class="index notranslate">#7</span></a>
                <a id="title-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="title-link" href="/venue/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" target="_blank">Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability</a>
                <a id="pdf-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF">16</sup>]</a>
                <a id="copy-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF">10</sup>]</a>
                <a id="rel-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seungju Yoo" target="_blank">Seungju Yoo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyuk Kwon" target="_blank">Hyuk Kwon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joong-Won Hwang" target="_blank">Joong-Won Hwang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kibok Lee" target="_blank">Kibok Lee</a>
            </p>
            <p id="summary-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="summary">Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.</p>
            <p id="subjects-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" onclick="foldPdfKimi('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="panel paper" keywords="upsampler,feature,loftup,upsampling,resolution,coordinate,foundation,vision,vfms,vfm">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models_ICCV_2025_paper.html" target="_blank" title="8/64"><span class="index notranslate">#8</span></a>
                <a id="title-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="title-link" href="/venue/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" target="_blank">LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models</a>
                <a id="pdf-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF">16</sup>]</a>
                <a id="copy-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF">8</sup>]</a>
                <a id="rel-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haiwen Huang" target="_blank">Haiwen Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anpei Chen" target="_blank">Anpei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Volodymyr Havrylov" target="_blank">Volodymyr Havrylov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Geiger" target="_blank">Andreas Geiger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Zhang" target="_blank">Dan Zhang</a>
            </p>
            <p id="summary-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="summary">Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup.</p>
            <p id="subjects-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" onclick="foldPdfKimi('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="panel paper" keywords="sketch,lots,fashion,text,conditioning,localized,generation,sketches,image,fashionpedia">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing_ICCV_2025_paper.html" target="_blank" title="9/64"><span class="index notranslate">#9</span></a>
                <a id="title-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="title-link" href="/venue/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" target="_blank">LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</a>
                <a id="pdf-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF">14</sup>]</a>
                <a id="copy-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Girella" target="_blank">Federico Girella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Davide Talon" target="_blank">Davide Talon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyue Liu" target="_blank">Ziyue Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zanxi Ruan" target="_blank">Zanxi Ruan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Wang" target="_blank">Yiming Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Cristani" target="_blank">Marco Cristani</a>
            </p>
            <p id="summary-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="summary">Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.</p>
            <p id="subjects-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" onclick="foldPdfKimi('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="panel paper" keywords="copernicus,foundation,earth,sentinel,metadata,xlab,unified,spectral,towards,unlocked">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision_ICCV_2025_paper.html" target="_blank" title="10/64"><span class="index notranslate">#10</span></a>
                <a id="title-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="title-link" href="/venue/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" target="_blank">Towards a Unified Copernicus Foundation Model for Earth Vision</a>
                <a id="pdf-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Wang" target="_blank">Yi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhitong Xiong" target="_blank">Zhitong Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenying Liu" target="_blank">Chenying Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam J. Stewart" target="_blank">Adam J. Stewart</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Dujardin" target="_blank">Thomas Dujardin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikolaos Ioannis Bountos" target="_blank">Nikolaos Ioannis Bountos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angelos Zavras" target="_blank">Angelos Zavras</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Franziska Gerken" target="_blank">Franziska Gerken</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ioannis Papoutsis" target="_blank">Ioannis Papoutsis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laura Leal-Taix" target="_blank">Laura Leal-Taix</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Xiang Zhu" target="_blank">Xiao Xiang Zhu</a>
            </p>
            <p id="summary-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="summary">Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes at https://github.com/zhu-xlab/Copernicus-FM.</p>
            <p id="subjects-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="panel paper" keywords="sam,masks,entity,segmentation,amg,emr,mask,level,outputs,segment">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model_ICCV_2025_paper.html" target="_blank" title="11/64"><span class="index notranslate">#11</span></a>
                <a id="title-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="title-link" href="/venue/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" target="_blank">E-SAM: Training-Free Segment Every Entity Model</a>
                <a id="pdf-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF">23</sup>]</a>
                <a id="copy-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF">7</sup>]</a>
                <a id="rel-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Zhang" target="_blank">Weiming Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dingwen Xiao" target="_blank">Dingwen Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Chen" target="_blank">Lei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Wang" target="_blank">Lin Wang</a>
            </p>
            <p id="summary-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="summary">Entity Segmentation (ES) aims at identifying and segmenting distinct entities within an image without the need for predefined class labels. This characteristic makes ES well-suited to open-world applications with adaptation to diverse and dynamically changing environments, where new and previously unseen entities may appear frequently. Existing ES methods either require large annotated datasets or high training costs, limiting their scalability and adaptability. Recently, the Segment Anything Model (SAM), especially in its Automatic Mask Generation (AMG) mode, has shown potential for holistic image segmentation. However, it struggles with over-segmentation and under-segmentation, making it less effective for ES. In this paper, we introduce E-SAM, a novel training-free framework that exhibits exceptional ES capability. Specifically, we first propose Multi-level Mask Generation (MMG) that hierarchically processes SAM's AMG outputs to generate reliable object-level masks while preserving fine details at other levels. Entity-level Mask Refinement (EMR) then refines these object-level masks into accurate entity-level masks. That is, it separates overlapping masks to address the redundancy issues inherent in SAM's outputs and merges similar masks by evaluating entity-level consistency. Lastly, Under-Segmentation Refinement (USR) addresses under-segmentation by generating additional high-confidence masks fused with EMR outputs to produce the final ES map. These three modules are seamlessly optimized to achieve the best ES without additional training overhead. Extensive experiments demonstrate that E-SAM achieves state-of-the-art performance compared to prior ES methods, demonstrating a significant improvement by +30.1 on benchmark metrics.</p>
            <p id="subjects-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="panel paper" keywords="conservative,extrapolation,trajectories,noisy,savitzky,conservation,unknown,forces,golay,forecasting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3_ICCV_2025_paper.html" target="_blank" title="12/64"><span class="index notranslate">#12</span></a>
                <a id="title-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="title-link" href="/venue/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" target="_blank">Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)</a>
                <a id="pdf-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lennart Bastian" target="_blank">Lennart Bastian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Rashed" target="_blank">Mohammad Rashed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nassir Navab" target="_blank">Nassir Navab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tolga Birdal" target="_blank">Tolga Birdal</a>
            </p>
            <p id="summary-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="summary">Modeling the rotation of moving objects is a fundamental task in computer vision, yet SO(3) extrapolation still presents numerous challenges: (1) unknown quantities such as the moment of inertia complicate dynamics, (2) the presence of external forces and torques can lead to non-conservative kinematics, and (3) estimating evolving state trajectories under sparse, noisy observations requires robustness. We propose modeling trajectories of noisy pose estimates on the manifold of 3D rotations in a physically and geometrically meaningful way by leveraging Neural Controlled Differential Equations guided with SO(3) Savitzky-Golay paths. Existing extrapolation methods often rely on energy conservation or constant velocity assumptions, limiting their applicability in real-world scenarios involving non-conservative forces. In contrast, our approach is agnostic to energy and momentum conservation while being robust to input noise, making it applicable to complex, non-inertial systems. Our approach is easily integrated as a module in existing pipelines and generalizes well to trajectories with unknown physical parameters. By learning to approximate object dynamics from noisy states during training, our model attains robust extrapolation capabilities in simulation and various real-world settings.</p>
            <p id="subjects-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" onclick="foldPdfKimi('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="panel paper" keywords="identity,talking,fixtalk,head,leakage,quality,generation,taming,extreme,rendering">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in_ICCV_2025_paper.html" target="_blank" title="13/64"><span class="index notranslate">#13</span></a>
                <a id="title-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="title-link" href="/venue/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" target="_blank">FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases</a>
                <a id="pdf-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF">12</sup>]</a>
                <a id="copy-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF">8</sup>]</a>
                <a id="rel-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Tan" target="_blank">Shuai Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bill Gong" target="_blank">Bill Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Ji" target="_blank">Bin Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Pan" target="_blank">Ye Pan</a>
            </p>
            <p id="summary-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="summary">Talking head generation is gaining significant importance across various domains, with a growing demand for high-quality rendering. However, existing methods often suffer from identity leakage (IL) and rendering artifacts (RA), particularly in extreme cases. Through an in-depth analysis of previous approaches, we identify two key insights: (1) IL arises from identity information embedded within motion features, and (2) this identity information can be leveraged to address RA. Building on these findings, this paper introduces FixTalk, a novel framework designed to simultaneously resolve both issues for high-quality talking head generation. Firstly, we propose an Enhanced Motion Indicator (EMI) to effectively decouple identity information from motion features, mitigating the impact of IL on generated talking heads. To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes the leaked identity information to supplement missing details, thus fixing the artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates IL and RA, achieving superior performance compared to state-of-the-art methods.</p>
            <p id="subjects-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" onclick="foldPdfKimi('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="panel paper" keywords="videos,monocular,trajectorycrafter,view,camera,redirecting,diffusion,redirect,trajectories,transformations">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models_ICCV_2025_paper.html" target="_blank" title="14/64"><span class="index notranslate">#14</span></a>
                <a id="title-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="title-link" href="/venue/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" target="_blank">TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</a>
                <a id="pdf-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF">15</sup>]</a>
                <a id="copy-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF">9</sup>]</a>
                <a id="rel-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Yu" target="_blank">Mark Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbo Hu" target="_blank">Wenbo Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinbo Xing" target="_blank">Jinbo Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Shan" target="_blank">Ying Shan</a>
            </p>
            <p id="summary-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="summary">We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method. Code and pre-trained model will be released.</p>
            <p id="subjects-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" onclick="foldPdfKimi('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="panel paper" keywords="inversion,flowedit,editing,text,t2i,pre,trained,flow,free,intervene">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models_ICCV_2025_paper.html" target="_blank" title="15/64"><span class="index notranslate">#15</span></a>
                <a id="title-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="title-link" href="/venue/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" target="_blank">FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models</a>
                <a id="pdf-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF">18</sup>]</a>
                <a id="copy-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF">8</sup>]</a>
                <a id="rel-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vladimir Kulikov" target="_blank">Vladimir Kulikov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matan Kleiner" target="_blank">Matan Kleiner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Inbar Huberman-Spiegelglas" target="_blank">Inbar Huberman-Spiegelglas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomer Michaeli" target="_blank">Tomer Michaeli</a>
            </p>
            <p id="summary-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="summary">Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX.</p>
            <p id="subjects-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" onclick="foldPdfKimi('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="panel paper" keywords="hierarchies,visual,retrieval,hierarchical,image,hyperbolic,hierarchy,similarity,metrics,transcends">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval_ICCV_2025_paper.html" target="_blank" title="16/64"><span class="index notranslate">#16</span></a>
                <a id="title-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="title-link" href="/venue/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" target="_blank">Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval</a>
                <a id="pdf-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF">16</sup>]</a>
                <a id="copy-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF">10</sup>]</a>
                <a id="rel-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Wang" target="_blank">Ziwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sameera Ramasinghe" target="_blank">Sameera Ramasinghe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenchen Xu" target="_blank">Chenchen Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julien Monteil" target="_blank">Julien Monteil</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Loris Bazzani" target="_blank">Loris Bazzani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thalaiyasingam Ajanthan" target="_blank">Thalaiyasingam Ajanthan</a>
            </p>
            <p id="summary-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="summary">Structuring latent representations in a hierarchical manner enables models to learn patterns at multiple levels of abstraction. However, most prevalent image understanding models focus on visual similarity, and learning visual hierarchies is relatively unexplored. In this work, for the first time, we introduce a learning paradigm that can encode user-defined multi-level complex visual hierarchies in hyperbolic space without requiring explicit hierarchical labels. As a concrete example, first, we define a part-based image hierarchy using object-level annotations within and across images. Then, we introduce an approach to enforce the hierarchy using contrastive loss with pairwise entailment metrics. Finally, we discuss new evaluation metrics to effectively measure hierarchical image retrieval. Encoding these complex relationships ensures that the learned representations capture semantic and structural information that transcends mere visual similarity. Experiments in part-based image retrieval show significant improvements in hierarchical retrieval tasks, demonstrating the capability of our model in capturing visual hierarchies.</p>
            <p id="subjects-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="panel paper" keywords="animation,typography,text,video,animations,canonical,dynamic,prompts,infuses,bringing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior_ICCV_2025_paper.html" target="_blank" title="17/64"><span class="index notranslate">#17</span></a>
                <a id="title-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="title-link" href="/venue/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" target="_blank">Dynamic Typography: Bringing Text to Life via Video Diffusion Prior</a>
                <a id="pdf-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF">16</sup>]</a>
                <a id="copy-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Liu" target="_blank">Zichen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yihao Meng" target="_blank">Yihao Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Ouyang" target="_blank">Hao Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Yu" target="_blank">Yue Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bolin Zhao" target="_blank">Bolin Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Cohen-Or" target="_blank">Daniel Cohen-Or</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huamin Qu" target="_blank">Huamin Qu</a>
            </p>
            <p id="summary-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="summary">Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives. Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation. We present an automated text animation scheme, termed "Dynamic Typography", which deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts. The animation is represented by a canonical field that aggregates the semantic content in a canonical shape and a deformation field that applies per-frame motion to deform the canonical shape. Two fields are jointly optimized by the priors from a large pretrained text-to-video diffusion model using score-distillation loss with designed regularization, encouraging the video coherence with the intended textual concept while maintaining legibility and structural integrity throughout the animation process. We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our methodology over baselines. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability.</p>
            <p id="subjects-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" onclick="foldPdfKimi('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="panel paper" keywords="motion,spherical,uncalibrated,camera,jonathanventura,focal,outstretched,sphere,hemi,structure">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.html" target="_blank" title="18/64"><span class="index notranslate">#18</span></a>
                <a id="title-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="title-link" href="/venue/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" target="_blank">Uncalibrated Structure from Motion on a Sphere</a>
                <a id="pdf-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF">16</sup>]</a>
                <a id="copy-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF">7</sup>]</a>
                <a id="rel-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Ventura" target="_blank">Jonathan Ventura</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viktor Larsson" target="_blank">Viktor Larsson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fredrik Kahl" target="_blank">Fredrik Kahl</a>
            </p>
            <p id="summary-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="summary">Spherical motion is a special case of camera motion where the camera moves on the imaginary surface of a sphere with the optical axis normal to the surface. Common sources of spherical motion are a person capturing a stereo panorama with a phone held in an outstretched hand, or a hemi-spherical camera rig used for multi-view scene capture. However, traditional structure-from-motion pipelines tend to fail on spherical camera motion sequences, especially when the camera is facing outward. Building upon prior work addressing the calibrated case, we explore uncalibrated reconstruction from spherical motion, assuming a fixed but unknown focal length parameter. We show that, although two-view spherical motion is always a critical case, self-calibration is possible from three or more views. Through analysis of the relationship between focal length and spherical relative pose, we devise a global structure-from-motion approach for uncalibrated reconstruction. We demonstrate the effectiveness of our approach on real-world captures in various settings, even when the camera motion deviates from perfect spherical motion. Code and data for our method are available at https://github.com/jonathanventura/spherical-sfm.</p>
            <p id="subjects-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" onclick="foldPdfKimi('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="panel paper" keywords="acoustic,room,dar,rendering,differentiable,audio,visual,physics,view,environments">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors_ICCV_2025_paper.html" target="_blank" title="19/64"><span class="index notranslate">#19</span></a>
                <a id="title-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="title-link" href="/venue/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" target="_blank">Differentiable Room Acoustic Rendering with Multi-View Vision Priors</a>
                <a id="pdf-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Derong Jin" target="_blank">Derong Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruohan Gao" target="_blank">Ruohan Gao</a>
            </p>
            <p id="summary-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="summary">An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.</p>
            <p id="subjects-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" onclick="foldPdfKimi('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="panel paper" keywords="geo,loc,timestamp,cyclical,retrieval,location,cues,joint,embedding,prediction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint_ICCV_2025_paper.html" target="_blank" title="20/64"><span class="index notranslate">#20</span></a>
                <a id="title-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="title-link" href="/venue/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" target="_blank">GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space</a>
                <a id="pdf-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF">15</sup>]</a>
                <a id="copy-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=David G. Shatwell" target="_blank">David G. Shatwell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ishan Rajendrakumar Dave" target="_blank">Ishan Rajendrakumar Dave</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sirnam Swetha" target="_blank">Sirnam Swetha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mubarak Shah" target="_blank">Mubarak Shah</a>
            </p>
            <p id="summary-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="summary">Timestamp prediction aims to determine when an image was captured using only visual information, supporting applications such as metadata correction, retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely on cues like brightness, hue, and shadow positioning, while seasonal changes and weather inform date estimation. However, these visual cues significantly depend on geographic context, closely linking timestamp prediction to geo-localization. To address this interdependence, we introduce GT-Loc, a novel retrieval-based method that jointly predicts the capture time (hour and month) and geo-location (GPS coordinates) of an image. Our approach employs separate encoders for images, time, and location, aligning their embeddings within a shared high-dimensional feature space. Recognizing the cyclical nature of time, instead of conventional contrastive learning with hard positives and negatives, we propose a temporal metric-learning objective providing soft targets by modeling pairwise time differences over a cyclical toroidal surface. We present new benchmarks demonstrating that our joint optimization surpasses previous time prediction methods, even those using the ground-truth geo-location as an input during inference. Additionally, our approach achieves competitive results on standard geo-localization tasks, and the unified embedding space facilitates compositional and text-based image retrieval.</p>
            <p id="subjects-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" onclick="foldPdfKimi('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="panel paper" keywords="moto,motion,robot,token,manipulation,latent,pre,language,gpt,bridging">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning_ICCV_2025_paper.html" target="_blank" title="21/64"><span class="index notranslate">#21</span></a>
                <a id="title-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="title-link" href="/venue/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" target="_blank">Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos</a>
                <a id="pdf-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Chen" target="_blank">Yi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuying Ge" target="_blank">Yuying Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiliang Tang" target="_blank">Weiliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhuo Li" target="_blank">Yizhuo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixiao Ge" target="_blank">Yixiao Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyu Ding" target="_blank">Mingyu Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Shan" target="_blank">Ying Shan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xihui Liu" target="_blank">Xihui Liu</a>
            </p>
            <p id="summary-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="summary">Recent developments in Large Language Models (LLMs) pre-trained on extensive corpora have shown significant success in various natural language processing (NLP) tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich "corpus", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks.Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging "language" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood.To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.</p>
            <p id="subjects-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="panel paper" keywords="ever,volumetric,ellipsoid,zip,ray,rendering,raytraced,nerf,rtx4090,popping">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis_ICCV_2025_paper.html" target="_blank" title="22/64"><span class="index notranslate">#22</span></a>
                <a id="title-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="title-link" href="/venue/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" target="_blank">EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis</a>
                <a id="pdf-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Mai" target="_blank">Alexander Mai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Hedman" target="_blank">Peter Hedman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=George Kopanas" target="_blank">George Kopanas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dor Verbin" target="_blank">Dor Verbin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Futschik" target="_blank">David Futschik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiangeng Xu" target="_blank">Qiangeng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Falko Kuester" target="_blank">Falko Kuester</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan T. Barron" target="_blank">Jonathan T. Barron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinda Zhang" target="_blank">Yinda Zhang</a>
            </p>
            <p id="summary-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="summary">We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time 3D reconstruction.EVER accurately blends an unlimited number of overlapping primitives together in 3D space, eliminating the popping artifacts that 3D Gaussian Splatting (3DGS) and other related methods exhibit.EVER represents a radiance field as a set of constant-density volumetric ellipsoids, which are raytraced by intersecting each primitive twice (once upon ray entrance and another on ray exit) and accumulating the derivatives of the densities and colors along the ray.Because EVER is built around ray tracing, it also enables effects such as defocus blur and fish-eye camera distortion, while still achieving frame rates of 30 FPS at 720p on an NVIDIA RTX4090. We show that our method is more accurate on the challenging large-scale scenes from the Zip-NeRF dataset, where it achieves state of the art SSIM, even higher than Zip-NeRF.</p>
            <p id="subjects-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" onclick="foldPdfKimi('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="panel paper" keywords="view,multi,tracker,camera,tracking,occlusion,correspondences,alongside,kubric,0cm">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Rajic_Multi-View_3D_Point_Tracking_ICCV_2025_paper.html" target="_blank" title="23/64"><span class="index notranslate">#23</span></a>
                <a id="title-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="title-link" href="/venue/Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" target="_blank">Multi-View 3D Point Tracking</a>
                <a id="pdf-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Rajic_Multi-View_3D_Point_Tracking_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Frano Raji" target="_blank">Frano Raji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haofei Xu" target="_blank">Haofei Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marko Mihajlovic" target="_blank">Marko Mihajlovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Li" target="_blank">Siyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Irem Demir" target="_blank">Irem Demir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emircan Gndodu" target="_blank">Emircan Gndodu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Ke" target="_blank">Lei Ke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Prokudin" target="_blank">Sergey Prokudin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Tang" target="_blank">Siyu Tang</a>
            </p>
            <p id="summary-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="summary">We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks--Panoptic Studio and DexYCB--achieving median trajectory errors of 3.1 cm and 2.0cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page: https://ethz-vlg.github.io/mvtracker.</p>
            <p id="subjects-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" onclick="foldPdfKimi('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="panel paper" keywords="monocular,stereo,fusion,disparity,matching,depth,optima,vfm,ill,posed">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo_ICCV_2025_paper.html" target="_blank" title="24/64"><span class="index notranslate">#24</span></a>
                <a id="title-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="title-link" href="/venue/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" target="_blank">Diving into the Fusion of Monocular Priors for Generalized Stereo Matching</a>
                <a id="pdf-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chengtang Yao" target="_blank">Chengtang Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lidong Yu" target="_blank">Lidong Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhidan Liu" target="_blank">Zhidan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxi Zeng" target="_blank">Jiaxi Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuwei Wu" target="_blank">Yuwei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunde Jia" target="_blank">Yunde Jia</a>
            </p>
            <p id="summary-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="summary">The matching formulation makes it naturally hard for the stereo matching to handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing monocular priors has been proven helpful for ill-posed matching, but the biased monocular prior learned from small stereo datasets constrains the generalization. Recently, stereo matching has progressed by leveraging the unbiased monocular prior from the vision foundation model (VFM) to improve the generalization in ill-posed regions. We dive into the fusion process and observe three main problems limiting the fusion of the VFM monocular prior. The first problem is the misalignment between affine-invariant relative monocular depth and absolute depth of disparity. Besides, when we use the monocular feature in an iterative update structure, the over-confidence in the disparity update leads to local optima results. A direct fusion of a monocular depth map could alleviate the local optima problem, but noisy disparity results computed at the first several iterations will misguide the fusion. In this paper, we propose a binary local ordering map to guide the fusion, which converts the depth map into a binary relative format, unifying the relative and absolute depth representation. The computed local ordering map is also used to re-weight the initial disparity update, resolving the local optima and noisy problem. In addition, we formulate the final direct fusion of monocular depth to the disparity as a registration problem, where a pixel-wise linear regression module can globally and adaptively align them. Our method fully exploits the monocular prior to support stereo matching results effectively and efficiently. We significantly improve the performance from the experiments when generalizing from SceneFlow to Middlebury and Booster datasets while barely reducing the efficiency.</p>
            <p id="subjects-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" onclick="foldPdfKimi('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="panel paper" keywords="hair,avatars,compositionality,hairless,face,head,prior,haircup,compositional,holistic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars_ICCV_2025_paper.html" target="_blank" title="25/64"><span class="index notranslate">#25</span></a>
                <a id="title-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="title-link" href="/venue/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" target="_blank">HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</a>
                <a id="pdf-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF">9</sup>]</a>
                <a id="copy-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF">7</sup>]</a>
                <a id="rel-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Byungjun Kim" target="_blank">Byungjun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunsuke Saito" target="_blank">Shunsuke Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giljoo Nam" target="_blank">Giljoo Nam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Simon" target="_blank">Tomas Simon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Saragih" target="_blank">Jason Saragih</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanbyul Joo" target="_blank">Hanbyul Joo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junxuan Li" target="_blank">Junxuan Li</a>
            </p>
            <p id="summary-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="summary">We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of original and synthetic hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a data-efficient manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.</p>
            <p id="subjects-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" onclick="foldPdfKimi('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF', this)" class="hr hr-fold">
        </div>
    <div id="Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="panel paper" keywords="video,camera,recammaster,generative,rendering,controlled,filming,trajectories,unreal,diverse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video_ICCV_2025_paper.html" target="_blank" title="26/64"><span class="index notranslate">#26</span></a>
                <a id="title-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="title-link" href="/venue/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" target="_blank">ReCamMaster: Camera-Controlled Generative Rendering from A Single Video</a>
                <a id="pdf-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianhong Bai" target="_blank">Jianhong Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Menghan Xia" target="_blank">Menghan Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Fu" target="_blank">Xiao Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xintao Wang" target="_blank">Xintao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lianrui Mu" target="_blank">Lianrui Mu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwen Cao" target="_blank">Jinwen Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zuozhu Liu" target="_blank">Zuozhu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoji Hu" target="_blank">Haoji Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Bai" target="_blank">Xiang Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Wan" target="_blank">Pengfei Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Zhang" target="_blank">Di Zhang</a>
            </p>
            <p id="summary-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="summary">Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through an elegant yet powerful video conditioning mechanism--an aspect often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments show that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Our code and dataset are publicly available.</p>
            <p id="subjects-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" onclick="foldPdfKimi('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="panel paper" keywords="depth,solvers,mdes,cameras,pose,monocular,unknown,reposed,relative,kocurvik">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.html" target="_blank" title="27/64"><span class="index notranslate">#27</span></a>
                <a id="title-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="title-link" href="/venue/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" target="_blank">RePoseD: Efficient Relative Pose Estimation With Known Depth Information</a>
                <a id="pdf-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF">8</sup>]</a>
                <a id="copy-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yaqing Ding" target="_blank">Yaqing Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viktor Kocur" target="_blank">Viktor Kocur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vclav Vvra" target="_blank">Vclav Vvra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zuzana Berger Haladov" target="_blank">Zuzana Berger Haladov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Torsten Sattler" target="_blank">Torsten Sattler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zuzana Kukelova" target="_blank">Zuzana Kukelova</a>
            </p>
            <p id="summary-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="summary">Recent advances in monocular depth estimation methods (MDE) and their improved accuracy open new possibilities for their applications. In this paper, we investigate how monocular depth estimates can be used for relative pose estimation. In particular, we are interested in answering the question whether using MDEs improves results over traditional point-based methods. We propose a novel framework for estimating the relative pose of two cameras from point correspondences with associated monocular depths. Since depth predictions are typically defined up to an unknown scale or even both unknown scale and shift parameters, our solvers jointly estimate the scale or both the scale and shift parameters along with the relative pose. We derive efficient solvers considering different types of depths for three camera configurations: (1) two calibrated cameras, (2) two cameras with an unknown shared focal length, and (3) two cameras with unknown different focal lengths. Our new solvers outperform state-of-the-art depth-aware solvers in terms of speed and accuracy. In extensive real experiments on multiple datasets and with various MDEs, we discuss which depth-aware solvers are preferable in which situation. The code is available at https://github.com/kocurvik/mdrp.</p>
            <p id="subjects-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" onclick="foldPdfKimi('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="panel paper" keywords="sparseflex,rendering,reconstruction,resolution,shape,mesh,topology,interiors,high,arbitrary">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling_ICCV_2025_paper.html" target="_blank" title="28/64"><span class="index notranslate">#28</span></a>
                <a id="title-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="title-link" href="/venue/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" target="_blank">SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</a>
                <a id="pdf-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF">7</sup>]</a>
                <a id="copy-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xianglong He" target="_blank">Xianglong He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zi-Xin Zou" target="_blank">Zi-Xin Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chia-Hao Chen" target="_blank">Chia-Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan-Chen Guo" target="_blank">Yuan-Chen Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ding Liang" target="_blank">Ding Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun Yuan" target="_blank">Chun Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanli Ouyang" target="_blank">Wanli Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan-Pei Cao" target="_blank">Yan-Pei Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yangguang Li" target="_blank">Yangguang Li</a>
            </p>
            <p id="summary-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="summary">Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a 82% reduction in Chamfer Distance and a 88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.</p>
            <p id="subjects-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" onclick="foldPdfKimi('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="panel paper" keywords="rayzer,camera,scene,self,supervised,view,awareness,synthesis,supervision,poses">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model_ICCV_2025_paper.html" target="_blank" title="29/64"><span class="index notranslate">#29</span></a>
                <a id="title-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="title-link" href="/venue/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" target="_blank">RayZer: A Self-supervised Large View Synthesis Model</a>
                <a id="pdf-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF">12</sup>]</a>
                <a id="copy-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF">8</sup>]</a>
                <a id="rel-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwen Jiang" target="_blank">Hanwen Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tan" target="_blank">Hao Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Wang" target="_blank">Peng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haian Jin" target="_blank">Haian Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Zhao" target="_blank">Yue Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai Bi" target="_blank">Sai Bi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fujun Luan" target="_blank">Fujun Luan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kalyan Sunkavalli" target="_blank">Kalyan Sunkavalli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixing Huang" target="_blank">Qixing Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Pavlakos" target="_blank">Georgios Pavlakos</a>
            </p>
            <p id="summary-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="summary">We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than "oracle" methods that rely on pose annotations in both training and testing.</p>
            <p id="subjects-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" onclick="foldPdfKimi('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="panel paper" keywords="adjustment,bundle,motion,dynamic,camera,elements,track,slam,reconstructions,tracker">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction_ICCV_2025_paper.html" target="_blank" title="30/64"><span class="index notranslate">#30</span></a>
                <a id="title-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="title-link" href="/venue/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" target="_blank">Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction</a>
                <a id="pdf-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weirong Chen" target="_blank">Weirong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ganlin Zhang" target="_blank">Ganlin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Wimbauer" target="_blank">Felix Wimbauer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Wang" target="_blank">Rui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikita Araslanov" target="_blank">Nikita Araslanov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Cremers" target="_blank">Daniel Cremers</a>
            </p>
            <p id="summary-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="summary">Traditional SLAM systems, which rely on bundle adjustment, struggle with the highly dynamic scenes commonly found in casual videos. Such videos entangle the motion of dynamic elements, undermining the assumption of static environments required by traditional systems. Existing techniques either filter out dynamic elements or model their motion independently. However, the former often results in incomplete reconstructions, while the latter can lead to inconsistent motion estimates. Taking a novel approach, this work leverages a 3D point tracker to separate camera-induced motion from the observed motion of dynamic objects. By considering only the camera-induced component, bundle adjustment can operate reliably on all scene elements. We further ensure depth consistency across video frames with lightweight post-processing based on scale maps. Our framework combines the core of traditional SLAM---bundle adjustment---with a robust learning-based 3D tracker. Integrating motion decomposition, bundle adjustment, and depth refinement, our unified framework, BA-Track, accurately tracks camera motion and produces temporally coherent and scale-consistent dense reconstructions, accommodating both static and dynamic elements. Our experiments on challenging datasets reveal significant improvements in camera pose estimation and 3D reconstruction accuracy.</p>
            <p id="subjects-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="panel paper" keywords="3dgs,ensembling,splatting,nvs,perturbed,sigma,self,gaussian,view,segs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis_ICCV_2025_paper.html" target="_blank" title="31/64"><span class="index notranslate">#31</span></a>
                <a id="title-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="title-link" href="/venue/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" target="_blank">Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis</a>
                <a id="pdf-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF">5</sup>]</a>
                <a id="rel-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Zhao" target="_blank">Chen Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Wang" target="_blank">Xuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Zhang" target="_blank">Tong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saqib Javed" target="_blank">Saqib Javed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathieu Salzmann" target="_blank">Mathieu Salzmann</a>
            </p>
            <p id="summary-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="summary">3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in novel view synthesis (NVS). However, 3DGS tends to overfit when trained with sparse views, limiting its generalization to novel viewpoints. In this paper, we address this overfitting issue by introducing Self-Ensembling Gaussian Splatting (SE-GS). We achieve self-ensembling by incorporating an uncertainty-aware perturbation strategy during training. A \Delta-model and a \Sigma-model are jointly trained on the available images. The \Delta-model is dynamically perturbed based on rendering uncertainty across training steps, generating diverse perturbed models with negligible computational overhead. Discrepancies between the \Sigma-model and these perturbed models are minimized throughout training, forming a robust ensemble of 3DGS models. This ensemble, represented by the \Sigma-model, is then used to generate novel-view images during inference. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets demonstrate that our approach enhances NVS quality under few-shot training conditions, outperforming existing state-of-the-art methods. The code is released at: https://sailor-z.github.io/projects/SEGS.html.</p>
            <p id="subjects-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" onclick="foldPdfKimi('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="panel paper" keywords="token,merging,tokens,importance,generation,animatediff,video,zero123,pixart,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation_ICCV_2025_paper.html" target="_blank" title="32/64"><span class="index notranslate">#32</span></a>
                <a id="title-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="title-link" href="/venue/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" target="_blank">Importance-Based Token Merging for Efficient Image and Video Generation</a>
                <a id="pdf-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF">17</sup>]</a>
                <a id="copy-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF">7</sup>]</a>
                <a id="rel-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyu Wu" target="_blank">Haoyu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Xu" target="_blank">Jingyi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hieu Le" target="_blank">Hieu Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitris Samaras" target="_blank">Dimitris Samaras</a>
            </p>
            <p id="summary-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="summary">Token merging can effectively accelerate various vision systems by processing groups of similar tokens only once and sharing the results across them. However, existing token grouping methods are often ad hoc and random, disregarding the actual content of the samples. We show that preserving high-information tokens during merging--those essential for semantic fidelity and structural details--significantly improves sample quality, producing finer details and more coherent, realistic generations. To do so, we propose an importance-based token merging method that prioritizes the most critical tokens in computational resource allocation, leveraging readily available importance scores, such as those from classifier-free guidance in diffusion models. Experiments show that our approach significantly outperforms baseline methods across multiple applications, including text-to-image synthesis, multi-view image generation, and video generation with various model architectures such as Stable Diffusion, Zero123++, AnimateDiff, or PixArt-\alpha.</p>
            <p id="subjects-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" onclick="foldPdfKimi('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="panel paper" keywords="lic,student,distillation,knowledge,teacher,kdic,learned,compression,image,stage">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Knowledge_Distillation_for_Learned_Image_Compression_ICCV_2025_paper.html" target="_blank" title="33/64"><span class="index notranslate">#33</span></a>
                <a id="title-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="title-link" href="/venue/Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" target="_blank">Knowledge Distillation for Learned Image Compression</a>
                <a id="pdf-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Knowledge_Distillation_for_Learned_Image_Compression_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF">11</sup>]</a>
                <a id="copy-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF">7</sup>]</a>
                <a id="rel-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunuo Chen" target="_blank">Yunuo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zezheng Lyu" target="_blank">Zezheng Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bing He" target="_blank">Bing He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Cao" target="_blank">Ning Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gang Chen" target="_blank">Gang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guo Lu" target="_blank">Guo Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjun Zhang" target="_blank">Wenjun Zhang</a>
            </p>
            <p id="summary-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="summary">Recently, learned image compression (LIC) models have achieved remarkable rate-distortion (RD) performance, yet their high computational complexity severely limits practical deployment. To overcome this challenge, we propose a novel Stage-wise Modular Distillation framework, SMoDi, which efficiently compresses LIC models while preserving RD performance. This framework treats each stage of LIC models as an independent sub-task, mirroring the teacher model's task decomposition to the student, thereby simplifying knowledge transfer. We identify two crucial factors determining the effectiveness of knowledge distillation: student model construction and loss function design. Specifically, we first propose Teacher-Guided Student Model Construction, a pruning-like method ensuring architectural consistency between teacher and student models. Next, we introduce Implicit End-to-end Supervision, facilitating adaptive energy compaction and bitrate regularization. Based on these insights, we develop KDIC, a lightweight student model derived from the state-of-the-art S2CFormer model. Experimental results demonstrate that KDIC achieves top-tier RD performance with significantly reduced computational complexity. To our knowledge, this work is among the first successful applications of knowledge distillation to learned image compression.</p>
            <p id="subjects-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="panel paper" keywords="pruning,compressing,structured,trained,transfomers,retraining,epochs,variance,networks,simultaneously">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks_ICCV_2025_paper.html" target="_blank" title="34/64"><span class="index notranslate">#34</span></a>
                <a id="title-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="title-link" href="/venue/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" target="_blank">Variance-Based Pruning for Accelerating and Compressing Trained Networks</a>
                <a id="pdf-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF">12</sup>]</a>
                <a id="copy-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Uranik Berisha" target="_blank">Uranik Berisha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jens Mehnert" target="_blank">Jens Mehnert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandru Paul Condurache" target="_blank">Alexandru Paul Condurache</a>
            </p>
            <p id="summary-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="summary">Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are then used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44 times.</p>
            <p id="subjects-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" onclick="foldPdfKimi('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="panel paper" keywords="vheat,remote,sensing,heat,conduction,foundation,improves,across,tasks,computational">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model_ICCV_2025_paper.html" target="_blank" title="35/64"><span class="index notranslate">#35</span></a>
                <a id="title-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="title-link" href="/venue/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" target="_blank">RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model</a>
                <a id="pdf-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF">11</sup>]</a>
                <a id="copy-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huiyang Hu" target="_blank">Huiyang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peijin Wang" target="_blank">Peijin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanbo Bi" target="_blank">Hanbo Bi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyuan Tong" target="_blank">Boyuan Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaozhi Wang" target="_blank">Zhaozhi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhui Diao" target="_blank">Wenhui Diao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chang" target="_blank">Hao Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingchao Feng" target="_blank">Yingchao Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Zhang" target="_blank">Ziqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaowei Wang" target="_blank">Yaowei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixiang Ye" target="_blank">Qixiang Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Fu" target="_blank">Kun Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xian Sun" target="_blank">Xian Sun</a>
            </p>
            <p id="summary-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="summary">Remote sensing foundation models largely break away from the traditional paradigm of designing task-specific models, offering greater scalability across multiple tasks. However, they face challenges such as low computational efficiency and limited interpretability, especially when dealing with large-scale remote sensing images. To overcome these, we draw inspiration from heat conduction, a physical process modeling local heat diffusion. Building on this idea, we are the first to explore the potential of using the parallel computing model of heat conduction to simulate the local region correlations in high-resolution remote sensing images, and introduce RS-vHeat, an efficient multi-modal remote sensing foundation model. Specifically, RS-vHeat 1) applies the Heat Conduction Operator (HCO) with a complexity of O(N^ 1.5 ) and a global receptive field, reducing computational overhead while capturing remote sensing object structure information to guide heat diffusion; 2) learns the frequency distribution representations of various scenes through a self-supervised strategy based on frequency domain hierarchical masking and multi-domain reconstruction; 3) significantly improves efficiency and performance over state-of-the-art techniques across 4 tasks and 10 datasets. Compared to attention-based remote sensing foundation models, we reduce memory usage by 84%, FLOPs by 24% and improves throughput by 2.7 times. The code will be made publicly available.</p>
            <p id="subjects-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" onclick="foldPdfKimi('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="panel paper" keywords="gesture,speech,gestures,wild,tri,gestured,text,understanding,word,modal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Hegde_Understanding_Co-speech_Gestures_in-the-wild_ICCV_2025_paper.html" target="_blank" title="36/64"><span class="index notranslate">#36</span></a>
                <a id="title-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="title-link" href="/venue/Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" target="_blank">Understanding Co-speech Gestures in-the-wild</a>
                <a id="pdf-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Hegde_Understanding_Co-speech_Gestures_in-the-wild_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sindhu B Hegde" target="_blank">Sindhu B Hegde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=K R Prajwal" target="_blank">K R Prajwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taein Kwon" target="_blank">Taein Kwon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Zisserman" target="_blank">Andrew Zisserman</a>
            </p>
            <p id="summary-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="summary">Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-speech-text associations: (i) gesture based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal video-gesture-speech-text representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs). Further analysis reveals that speech and text modalities capture distinct gesture related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/ vgg/research/jegal.</p>
            <p id="subjects-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" onclick="foldPdfKimi('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="panel paper" keywords="dposer,pose,body,whole,human,prior,diffusion,poses,modeling,model">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior_ICCV_2025_paper.html" target="_blank" title="37/64"><span class="index notranslate">#37</span></a>
                <a id="title-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="title-link" href="/venue/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" target="_blank">DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior</a>
                <a id="pdf-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junzhe Lu" target="_blank">Junzhe Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Lin" target="_blank">Jing Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongkun Dou" target="_blank">Hongkun Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ailing Zeng" target="_blank">Ailing Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Deng" target="_blank">Yue Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xian Liu" target="_blank">Xian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongang Cai" target="_blank">Zhongang Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Yang" target="_blank">Lei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulun Zhang" target="_blank">Yulun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoqian Wang" target="_blank">Haoqian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Liu" target="_blank">Ziwei Liu</a>
            </p>
            <p id="summary-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="summary">We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling.Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions.Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.</p>
            <p id="subjects-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" onclick="foldPdfKimi('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="panel paper" keywords="teeth,facial,dental,capture,phone,reconstruction,camera,rendering,democratizes,performance">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera_ICCV_2025_paper.html" target="_blank" title="38/64"><span class="index notranslate">#38</span></a>
                <a id="title-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="title-link" href="/venue/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" target="_blank">Teeth Reconstruction and Performance Capture Using a Phone Camera</a>
                <a id="pdf-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weixi Zheng" target="_blank">Weixi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingwang Ling" target="_blank">Jingwang Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhibo Wang" target="_blank">Zhibo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Wang" target="_blank">Quan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Xu" target="_blank">Feng Xu</a>
            </p>
            <p id="summary-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="summary">We present the first method for personalized dental shape reconstruction and teeth-inclusive facial performance capture using only a single phone camera. Our approach democratizes high-quality facial avatars through a non-invasive, low-cost setup by addressing the ill-posed monocular capture problem with an analysis-by-synthesis approach. We introduce a representation adaptation technique that maintains both mesh and SDF representations of teeth, enabling efficient differentiable rendering while preventing teeth-lip interpenetration. To overcome alignment challenges with similar-appearing dental components, we leverage foundation models for semantic teeth segmentation and design specialized optimization objectives. Our method addresses the challenging occlusions of teeth during facial performance through optimization strategies that leverage facial structural priors, while our semantic mask rendering loss with optimal transport-based matching ensures convergence despite significant variations in initial positioning.</p>
            <p id="subjects-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" onclick="foldPdfKimi('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="panel paper" keywords="averaging,certifiably,anisotropic,rotation,incorporated,isotropic,uncertainties,subproblem,setting,optimization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging_ICCV_2025_paper.html" target="_blank" title="39/64"><span class="index notranslate">#39</span></a>
                <a id="title-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="title-link" href="/venue/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" target="_blank">Certifiably Optimal Anisotropic Rotation Averaging</a>
                <a id="pdf-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Carl Olsson" target="_blank">Carl Olsson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaroslava Lochman" target="_blank">Yaroslava Lochman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johan Malmport" target="_blank">Johan Malmport</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Zach" target="_blank">Christopher Zach</a>
            </p>
            <p id="summary-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="summary">Rotation averaging is a key subproblem in applications of computer vision and robotics. Many methods for solving this problem exist, and there are also several theoretical results analyzing difficulty and optimality. However, one aspect that most of these have in common is a focus on the isotropic setting, where the intrinsic uncertainties in the measurements are not fully incorporated into the resulting optimization task. Recent empirical results suggest that moving to an anisotropic framework, where these uncertainties are explicitly included, can result in an improvement of solution quality. However, global optimization for rotation averaging has remained a challenge in this scenario.In this paper we show how anisotropic costs can be incorporated in certifiably optimal rotation averaging. We also demonstrate how existing solvers, designed for isotropic situations, fail in the anisotropic setting. Finally, we propose a stronger relaxation and show empirically that it is able to recover global optima in all tested datasets and leads to a more accurate reconstruction in all but one of the scenes.</p>
            <p id="subjects-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" onclick="foldPdfKimi('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="panel paper" keywords="miore,var,restoration,motion,blur,benchmarks,push,frame,video,truths">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration_ICCV_2025_paper.html" target="_blank" title="40/64"><span class="index notranslate">#40</span></a>
                <a id="title-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="title-link" href="/venue/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" target="_blank">MIORe &amp; VAR-MIORe: Benchmarks to Push the Boundaries of Restoration</a>
                <a id="pdf-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=George Ciubotariu" target="_blank">George Ciubotariu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuyun Zhou" target="_blank">Zhuyun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongwei Wu" target="_blank">Zongwei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Radu Timofte" target="_blank">Radu Timofte</a>
            </p>
            <p id="summary-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="summary">We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address critical limitations in current motion restoration benchmarks. Designed with high-frame-rate (1000 FPS) acquisition and professional-grade optics, our datasets capture a broad spectrum of motion scenarios, which include complex ego-camera movements, dynamic multi-subject interactions, and depth-dependent blur effects. By adaptively averaging frames based on computed optical flow metrics, MIORe generates consistent motion blur, and preserves sharp inputs for video frame interpolation and optical flow estimation. VAR-MIORe further extends by spanning a variable range of motion magnitudes, from minimal to extreme, establishing the first benchmark to offer explicit control over motion amplitude. We provide high-resolution, scalable ground truths that challenge existing algorithms under both controlled and adverse conditions, paving the way for next-generation research of various image and video restoration tasks.</p>
            <p id="subjects-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" onclick="foldPdfKimi('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="panel paper" keywords="motion,mikudance,character,mixed,guidance,art,animation,animating,scene,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics_ICCV_2025_paper.html" target="_blank" title="41/64"><span class="index notranslate">#41</span></a>
                <a id="title-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="title-link" href="/venue/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" target="_blank">MikuDance: Animating Character Art with Mixed Motion Dynamics</a>
                <a id="pdf-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxu Zhang" target="_blank">Jiaxu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianfang Zeng" target="_blank">Xianfang Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Chen" target="_blank">Xin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Zuo" target="_blank">Wei Zuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gang Yu" target="_blank">Gang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhigang Tu" target="_blank">Zhigang Tu</a>
            </p>
            <p id="summary-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="summary">We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics.</p>
            <p id="subjects-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="panel paper" keywords="inversion,roar,errors,watermarking,latent,watermark,generative,domain,embedding,distortions">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.html" target="_blank" title="42/64"><span class="index notranslate">#42</span></a>
                <a id="title-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="title-link" href="/venue/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" target="_blank">ROAR: Reducing Inversion Error in Generative Image Watermarking</a>
                <a id="pdf-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF">9</sup>]</a>
                <a id="copy-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyi Wang" target="_blank">Hanyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Fang" target="_blank">Han Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shi-Lin Wang" target="_blank">Shi-Lin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ee-Chien Chang" target="_blank">Ee-Chien Chang</a>
            </p>
            <p id="summary-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="summary">Generative image watermarking enables the proactive detection and traceability of generated images. Among existing methods, inversion-based frameworks achieve highly conceal ed watermark embedding by injecting watermarks into the latent representation before the diffusion process. The robustness of this approach hinges on both the embedding mechanism and inversion accuracy. However, prior works have predominantly focused on optimizing the embedding process while overlooking inversion errors, which significantly affect extraction fidelity. In this paper, we address the challenge of inversion errors and propose ROAR, a dual-domain optimization-based framework designed to mitigate errors arising from two key sources: 1) Latent-domain errors, which accumulate across inversion steps due to inherent approximation assumptions. 2) Pixel-domain errors, which result from channel distortions such as JPEG compression. To tackle these issues, we introduce two novel components: A Regeneration-based Optimization (RO) mechanism, which incorporates an optimizable starting latent to minimize latent-domain errors; A Mixture of Experts (MoE)-based distortion-adaptive restoration (AR) network, which effectively recovers watermarked distributions from pixel-level distortions.Extensive experiments demonstrate that ROAR significantly reduces inversion errors and enhances watermark extraction robustness, thereby improving the reliability of generative image watermarking.</p>
            <p id="subjects-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" onclick="foldPdfKimi('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="panel paper" keywords="wavelet,frequency,dwt,dtwsr,interrelations,transformer,image,bands,mdwt,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution_ICCV_2025_paper.html" target="_blank" title="43/64"><span class="index notranslate">#43</span></a>
                <a id="title-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="title-link" href="/venue/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" target="_blank">Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution</a>
                <a id="pdf-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF">20</sup>]</a>
                <a id="copy-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Du" target="_blank">Peng Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Li" target="_blank">Hui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Xu" target="_blank">Han Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Barom Jeon" target="_blank">Paul Barom Jeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongwook Lee" target="_blank">Dongwook Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daehyun Ji" target="_blank">Daehyun Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Yang" target="_blank">Ran Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Zhu" target="_blank">Feng Zhu</a>
            </p>
            <p id="summary-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="summary">Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image super-resolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multi-scale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multi-scale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform (MDWT) to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in low-frequency (LF) and high-frequency (HF) sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity.</p>
            <p id="subjects-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" onclick="foldPdfKimi('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="panel paper" keywords="layertracer,svg,svgs,layered,dit,vectorization,cognitive,aligned,rasterized,blueprints">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer_ICCV_2025_paper.html" target="_blank" title="44/64"><span class="index notranslate">#44</span></a>
                <a id="title-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="title-link" href="/venue/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" target="_blank">LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer</a>
                <a id="pdf-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF">1</sup>]</a>
                <a id="rel-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiren Song" target="_blank">Yiren Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danze Chen" target="_blank">Danze Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mike Zheng Shou" target="_blank">Mike Zheng Shou</a>
            </p>
            <p id="summary-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="summary">Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a DiT based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments show that LayerTracer surpasses optimization-based and neural baselines in generation quality and editability.</p>
            <p id="subjects-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" onclick="foldPdfKimi('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="panel paper" keywords="depth,lens,focus,defocus,spatially,autofocus,scene,lohmann,precept,swathe">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Spatially-Varying_Autofocus_ICCV_2025_paper.html" target="_blank" title="45/64"><span class="index notranslate">#45</span></a>
                <a id="title-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="title-link" href="/venue/Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" target="_blank">Spatially-Varying Autofocus</a>
                <a id="pdf-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Spatially-Varying_Autofocus_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yingsi Qin" target="_blank">Yingsi Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aswin C. Sankaranarayanan" target="_blank">Aswin C. Sankaranarayanan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew O'Toole" target="_blank">Matthew O'Toole</a>
            </p>
            <p id="summary-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="summary">A lens brings a single plane into focus on a planar sensor; hence, parts of the scene that are outside this planar focus plane are resolved on the sensor under defocus. Can we break this precept by enabling a "lens" that can change its depth-of-field arbitrarily? This work investigates the design and implementation of such a computational lens with spatially-selective focusing. Our design uses an optical arrangement of a Lohmann lens and a phase-only spatial light modulator to allow each pixel to focus at a different depth. We extend classical techniques used in autofocusing to the spatially-varying scenario where the depth map is iteratively estimated using contrast and disparity cues, enabling the camera to progressively shape its depth-of-field to the scene's depth. By obtaining an optical all-in-focus image, our technique advances upon a broad swathe of prior work ranging from depth-from-focus/defocus to coded aperture techniques in two key aspects: the ability to bring an entire scene in focus simultaneously, and the ability to maintain the highest possible spatial resolution.</p>
            <p id="subjects-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" onclick="foldPdfKimi('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="panel paper" keywords="vibrometry,event,visual,vibration,based,camera,lighting,audio,speed,high">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Event-based_Visual_Vibrometry_ICCV_2025_paper.html" target="_blank" title="46/64"><span class="index notranslate">#46</span></a>
                <a id="title-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="title-link" href="/venue/Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" target="_blank">Event-based Visual Vibrometry</a>
                <a id="pdf-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Event-based_Visual_Vibrometry_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF">15</sup>]</a>
                <a id="copy-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Zhou" target="_blank">Xinyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiqi Duan" target="_blank">Peiqi Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yeliduosi Xiaokaiti" target="_blank">Yeliduosi Xiaokaiti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Xu" target="_blank">Chao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>
            </p>
            <p id="summary-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="summary">Visual vibrometry has emerged as a powerful technique for remote acquisition of audio and the physical properties of materials. To capture high-frequency vibrations, frame-based approaches often require a high-speed video camera and bright lighting to compensate for the short exposure time. In this paper, we introduce event-based visual vibrometry, a new high-speed visual vibration sensing method using an event camera. By leveraging the high temporal resolution and low bandwidth characteristics of event cameras, event-based visual vibrometry enables high-speed vibration sensing under ambient lighting conditions with improved data efficiency. Specifically, we leverage a hybrid camera system and propose an event-based subtle motion estimation framework that integrates an optimization-based approach based on the event generation model and a motion refinement network. We demonstrate our method by capturing vibration caused by audio sources and estimating material properties for various objects.</p>
            <p id="subjects-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" onclick="foldPdfKimi('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="panel paper" keywords="superquadrics,superdec,primitives,scene,compact,leverage,scenes,capabilities,shapenet,scannet">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives_ICCV_2025_paper.html" target="_blank" title="47/64"><span class="index notranslate">#47</span></a>
                <a id="title-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="title-link" href="/venue/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" target="_blank">SuperDec: 3D Scene Decomposition with Superquadrics Primitives</a>
                <a id="pdf-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Elisabetta Fedele" target="_blank">Elisabetta Fedele</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyang Sun" target="_blank">Boyang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas Guibas" target="_blank">Leonidas Guibas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francis Engelmann" target="_blank">Francis Engelmann</a>
            </p>
            <p id="summary-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="summary">We present SuperDec, an approach for compact 3D scene representations based on geometric primitives, namely superquadrics.While most recent works leverage geometric primitives to obtain photorealistic 3D scene representations, we propose to leverage them to obtain a compact yet expressive representation. We propose to solve the problem locally on individual objects and leverage the capabilities of instance segmentation methods to scale our solution to full 3D scenes. In doing that, we design a new architecture which efficiently decompose point clouds of arbitrary objects in a compact set of superquadrics. We train our architecture on ShapeNet and we prove its generalization capabilities on object instances extracted from the ScanNet++ dataset as well as on full Replica scenes. Finally, we show how a compact representation based on superquadrics can be useful for a diverse range of downstream applications, including robotic tasks and controllable visual content generation and editing.</p>
            <p id="subjects-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" onclick="foldPdfKimi('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="panel paper" keywords="reasoning,video,llm,twin,digital,queries,perception,specialist,online,segmentation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins_ICCV_2025_paper.html" target="_blank" title="48/64"><span class="index notranslate">#48</span></a>
                <a id="title-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="title-link" href="/venue/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" target="_blank">Online Reasoning Video Segmentation with Just-in-Time Digital Twins</a>
                <a id="pdf-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF">7</sup>]</a>
                <a id="copy-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF">7</sup>]</a>
                <a id="rel-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiqing Shen" target="_blank">Yiqing Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bohan Liu" target="_blank">Bohan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenjia Li" target="_blank">Chenjia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lalithkumar Seenivasan" target="_blank">Lalithkumar Seenivasan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathias Unberath" target="_blank">Mathias Unberath</a>
            </p>
            <p id="summary-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="summary">Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- an LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as "just-in-time" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity. Experimental results demonstrate that our method performs best across all reasoning categories, suggesting that our just-in-time digital twin can bridge the gap between high-level reasoning and low-level perception in embodied AI. Benchmark is available at https://github.com/yiqings/jitbench/.</p>
            <p id="subjects-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" onclick="foldPdfKimi('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="panel paper" keywords="radar,foundational,grt,chip,radars,inexpensive,mmwave,raw,hours,single">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Towards_Foundational_Models_for_Single-Chip_Radar_ICCV_2025_paper.html" target="_blank" title="49/64"><span class="index notranslate">#49</span></a>
                <a id="title-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="title-link" href="/venue/Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" target="_blank">Towards Foundational Models for Single-Chip Radar</a>
                <a id="pdf-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Towards_Foundational_Models_for_Single-Chip_Radar_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianshu Huang" target="_blank">Tianshu Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akarsh Prabhakara" target="_blank">Akarsh Prabhakara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuhan Chen" target="_blank">Chuhan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jay Karhade" target="_blank">Jay Karhade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deva Ramanan" target="_blank">Deva Ramanan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew O'toole" target="_blank">Matthew O'toole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anthony Rowe" target="_blank">Anthony Rowe</a>
            </p>
            <p id="summary-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="summary">mmWave radars are compact, inexpensive, and durable sensors that are robust to occlusions and work regardless of environmental conditions, such as weather and darkness. However, this comes at the cost of poor angular resolution, especially for inexpensive single-chip radars, which are typically used in automotive and indoor sensing applications. Although many have proposed learning-based methods to mitigate this weakness, no standardized foundational models or large datasets for the mmWave radar have emerged, and practitioners have largely trained task-specific models from scratch using relatively small datasets.In this paper, we collect (to our knowledge) the largest available raw radar dataset with 1M samples (29 hours) and train a foundational model for 4D single-chip radar, which can predict 3D occupancy and semantic segmentation with quality that is typically only possible with much higher resolution sensors. We demonstrate that our Generalizable Radar Transformer (GRT) generalizes across diverse settings, can be fine-tuned for different tasks, and shows logarithmic data scaling of 20% per 10xdata. We also run extensive ablations on common design decisions, and find that using raw radar data significantly outperforms widely-used lossy representations, equivalent to a 10xincrease in training data. Finally, we estimate a total data requirement of ~100M samples (3000 hours) to fully exploit the potential of GRT.</p>
            <p id="subjects-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" onclick="foldPdfKimi('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="panel paper" keywords="scenesplat,3dgs,scenes,scene,indoor,splatting,understanding,6868,vision,gpu">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining_ICCV_2025_paper.html" target="_blank" title="50/64"><span class="index notranslate">#50</span></a>
                <a id="title-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="title-link" href="/venue/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" target="_blank">SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining</a>
                <a id="pdf-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF">9</sup>]</a>
                <a id="copy-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF">3</sup>]</a>
                <a id="rel-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Li" target="_blank">Yue Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Ma" target="_blank">Qi Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runyi Yang" target="_blank">Runyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huapeng Li" target="_blank">Huapeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengjiao Ma" target="_blank">Mengjiao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Ren" target="_blank">Bin Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikola Popovic" target="_blank">Nikola Popovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicu Sebe" target="_blank">Nicu Sebe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ender Konukoglu" target="_blank">Ender Konukoglu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Theo Gevers" target="_blank">Theo Gevers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luc Van Gool" target="_blank">Luc Van Gool</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin R. Oswald" target="_blank">Martin R. Oswald</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danda Pani Paudel" target="_blank">Danda Pani Paudel</a>
            </p>
            <p id="summary-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="summary">Recognizing arbitrary or previously unseen categories is essential for comprehensive real-world 3D scene understanding. Currently, all existing methods rely on 2D or textual modalities during training, or together at inference. This highlights a clear absence of a model capable of processing 3D data alone for learning semantics end-to-end, along with the necessary data to train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the de facto standard for 3D scene representation across various vision tasks. However, effectively integrating semantic reasoning into 3DGS in a generalizable fashion remains an open challenge.To address these limitations we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene understanding approach that operates natively on 3DGS. Furthermore, we propose a self-supervised learning scheme that unlocks rich 3D feature learning from unlabeled scenes. In order to power the proposed methods, we introduce SceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising of 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D, etc. Generating SceneSplat-7K required computational resources equivalent to 119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning for indoor scenes.Our exhaustive experiments on SceneSplat-7K demonstrate the significant benefit of the proposed methods over the established baselines. Our code, model, and datasets will be released to facilitate further research.</p>
            <p id="subjects-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" onclick="foldPdfKimi('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="panel paper" keywords="forestformer3d,forest,segmentation,end,instancev2,lidar,tree,lautx,wytham,unified">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR_ICCV_2025_paper.html" target="_blank" title="51/64"><span class="index notranslate">#51</span></a>
                <a id="title-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="title-link" href="/venue/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" target="_blank">ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds</a>
                <a id="pdf-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF">3</sup>]</a>
                <a id="copy-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Binbin Xiang" target="_blank">Binbin Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maciej Wielgosz" target="_blank">Maciej Wielgosz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Puliti" target="_blank">Stefano Puliti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kamil Krl" target="_blank">Kamil Krl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Krek" target="_blank">Martin Krek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azim Missarov" target="_blank">Azim Missarov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rasmus Astrup" target="_blank">Rasmus Astrup</a>
            </p>
            <p id="summary-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="summary">The segmentation of forest LiDAR 3D point clouds, including both individual tree and semantic segmentation, is fundamental for advancing forest management and ecological research. However, current approaches often struggle with the complexity and variability of natural forest environments. We present ForestFormer3D, a new unified and end-to-end framework designed for precise individual tree and semantic segmentation. ForestFormer3D incorporates ISA-guided query point selection, a score-based block merging strategy during inference, and a one-to-many association mechanism for effective training. By combining these new components, our model achieves state-of-the-art performance for individual tree segmentation on the newly introduced FOR-instanceV2 dataset, which spans diverse forest types and regions. Additionally, ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx), showcasing its robustness across different forest conditions and sensor modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly available at https://bxiang233.github.io/FF3D/.</p>
            <p id="subjects-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" onclick="foldPdfKimi('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="panel paper" keywords="scannet,easy3d,interactive,segmentation,unseen,environments,consistently,s3dis,unfamiliar,across">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation_ICCV_2025_paper.html" target="_blank" title="52/64"><span class="index notranslate">#52</span></a>
                <a id="title-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="title-link" href="/venue/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" target="_blank">Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation</a>
                <a id="pdf-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Simonelli" target="_blank">Andrea Simonelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Norman Mller" target="_blank">Norman Mller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Kontschieder" target="_blank">Peter Kontschieder</a>
            </p>
            <p id="summary-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="summary">The increasing availability of digital 3D environments, whether through image reconstruction, generation, or scans obtained via lasers or robots, is driving innovation across various fields. Among the numerous applications, there is a significant demand for those that enable 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and consistently perform well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as Gaussian Splatting.</p>
            <p id="subjects-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" onclick="foldPdfKimi('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="panel paper" keywords="diip,degradation,dip,prior,image,diffusion,restoration,waterdrop,pretrained,blind">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chihaoui_Diffusion_Image_Prior_ICCV_2025_paper.html" target="_blank" title="53/64"><span class="index notranslate">#53</span></a>
                <a id="title-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="title-link" href="/venue/Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" target="_blank">Diffusion Image Prior</a>
                <a id="pdf-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chihaoui_Diffusion_Image_Prior_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF">14</sup>]</a>
                <a id="copy-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF">8</sup>]</a>
                <a id="rel-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hamadi Chihaoui" target="_blank">Hamadi Chihaoui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paolo Favaro" target="_blank">Paolo Favaro</a>
            </p>
            <p id="summary-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="summary">Zero-shot image restoration (IR) methods based on pretrained diffusion models have recently achieved significant success. These methods typically require at least a parametric form of the degradation model. However, in real-world scenarios, the degradation may be too complex to define explicitly without relying on crude approximations. To handle this general case, we introduce the DIffusion Image Prior (DIIP). We take inspiration from the Deep Image Prior (DIP), since it can be used to remove artifacts without the need for an explicit degradation model. However, in contrast to DIP, we find that pretrained diffusion models offer a much stronger prior, despite being trained without knowledge from corrupted data. We show that, the optimization process in DIIP first reconstructs a clean version of the image before eventually overfitting to the degraded input, but it does so for a broader range of degradations than DIP. In light of this result, we propose a blind image restoration (IR) method based on early stopping, which does not require prior knowledge of the degradation model. We validate DIIP on various degradation-blind IR tasks, including JPEG artifact removal, waterdrop removal, denoising and super-resolution with state-of-the-art results.</p>
            <p id="subjects-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" onclick="foldPdfKimi('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="panel paper" keywords="completeness,attributions,subregions,faithful,attribution,soft,rethinking,xai,contend,standalone">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI_ICCV_2025_paper.html" target="_blank" title="54/64"><span class="index notranslate">#54</span></a>
                <a id="title-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="title-link" href="/venue/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" target="_blank">Soft Local Completeness: Rethinking Completeness in XAI</a>
                <a id="pdf-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF">14</sup>]</a>
                <a id="copy-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziv Weiss Haddad" target="_blank">Ziv Weiss Haddad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oren Barkan" target="_blank">Oren Barkan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yehonatan Elisha" target="_blank">Yehonatan Elisha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noam Koenigstein" target="_blank">Noam Koenigstein</a>
            </p>
            <p id="summary-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="summary">Completeness is a widely discussed property in explainability research, requiring that the attributions sum to the model's response to the input. While completeness intuitively suggests that the model's prediction is "completely explained" by the attributions, its global formulation alone is insufficient to ensure faithful explanations. We contend that promoting completeness locally within attribution subregions, in a soft manner, can serve as a standalone guiding principle for producing faithful attributions. To this end, we introduce the concept of the completeness gap as a flexible measure of completeness and propose an optimization procedure that minimizes this gap across subregions within the attribution map. Extensive evaluations across various model architectures demonstrate that our method produces state-of-the-art results.</p>
            <p id="subjects-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" onclick="foldPdfKimi('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="panel paper" keywords="counting,stacked,objects,biomedicine,wildlife,object,task,underpinning,world,underexplored">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Dumery_Counting_Stacked_Objects_ICCV_2025_paper.html" target="_blank" title="55/64"><span class="index notranslate">#55</span></a>
                <a id="title-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="title-link" href="/venue/Dumery_Counting_Stacked_Objects@ICCV2025@CVF" target="_blank">Counting Stacked Objects</a>
                <a id="pdf-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dumery_Counting_Stacked_Objects@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Dumery_Counting_Stacked_Objects_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Dumery_Counting_Stacked_Objects@ICCV2025@CVF">5</sup>]</a>
                <a id="copy-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dumery_Counting_Stacked_Objects@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dumery_Counting_Stacked_Objects@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Dumery_Counting_Stacked_Objects@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dumery_Counting_Stacked_Objects@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Corentin Dumery" target="_blank">Corentin Dumery</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noa Ett" target="_blank">Noa Ett</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aoxiang Fan" target="_blank">Aoxiang Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ren Li" target="_blank">Ren Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Xu" target="_blank">Jingyi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hieu Le" target="_blank">Hieu Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Fua" target="_blank">Pascal Fua</a>
            </p>
            <p id="summary-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="summary">Visual object counting is a fundamental computer vision task underpinning numerous real-world applications, from cell counting in biomedicine to traffic and wildlife monitoring. However, existing methods struggle to handle the challenge of stacked 3D objects in which most objects are hidden by those above them. To address this important yet underexplored problem, we propose a novel 3D counting approach that decomposes the task into two complementary subproblems - estimating the 3D geometry of the object stack and the occupancy ratio from multi-view images. By combining geometric reconstruction and deep learning-based depth analysis, our method can accurately count identical objects within containers, even when they are irregularly stacked. We validate our 3D Counting pipeline on large-scale synthetic and diverse real-world datasets with manually verified total counts.</p>
            <p id="subjects-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" onclick="foldPdfKimi('Dumery_Counting_Stacked_Objects@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="panel paper" keywords="confidence,pose,regions,keypoint,region,estimation,deterministic,poses,sampling,regressed">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation_ICCV_2025_paper.html" target="_blank" title="56/64"><span class="index notranslate">#56</span></a>
                <a id="title-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="title-link" href="/venue/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" target="_blank">Deterministic Object Pose Confidence Region Estimation</a>
                <a id="pdf-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Deterministic_Object_Pose_Confidence_Region_Estimation_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF">6</sup>]</a>
                <a id="copy-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinghao Wang" target="_blank">Jinghao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhang Li" target="_blank">Zhang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zi Wang" target="_blank">Zi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Banglei Guan" target="_blank">Banglei Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Shang" target="_blank">Yang Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifeng Yu" target="_blank">Qifeng Yu</a>
            </p>
            <p id="summary-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="summary">6D pose confidence region estimation has emerged as a critical direction, aiming to perform uncertainty quantification for assessing the reliability of estimated poses. However, current sampling-based approach suffers from critical limitations that severely impede their practical deployment: 1) the sampling speed significantly decreases as the number of samples increases. 2) the derived confidence regions are often excessively large. To address these challenges, we propose a deterministic and efficient method for estimating pose confidence regions. Our approach uses inductive conformal prediction to calibrate the deterministically regressed Gaussian keypoint distributions into 2D keypoint confidence regions. We then leverage the implicit function theorem to propagate these keypoint confidence regions directly into 6D pose confidence regions. This method avoids the inefficiency and inflated region sizes associated with sampling and ensembling. It provides compact confidence regions that cover the ground-truth poses with a user-defined confidence level. Experimental results on the LineMOD Occlusion and SPEED datasets show that our method achieves higher pose estimation accuracy with reduced computational time. For the same coverage rate, our method yields significantly smaller confidence region volumes, reducing them by up to 99.9% for rotations and 99.8% for translations. The code will be available soon.</p>
            <p id="subjects-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" onclick="foldPdfKimi('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="panel paper" keywords="wir3d,shape,visually,abstraction,curves,geometry,shapes,texture,bezier,deformation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction_ICCV_2025_paper.html" target="_blank" title="57/64"><span class="index notranslate">#57</span></a>
                <a id="title-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="title-link" href="/venue/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" target="_blank">WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</a>
                <a id="pdf-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF">4</sup>]</a>
                <a id="copy-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Liu" target="_blank">Richard Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Fu" target="_blank">Daniel Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Tan" target="_blank">Noah Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Itai Lang" target="_blank">Itai Lang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rana Hanocka" target="_blank">Rana Hanocka</a>
            </p>
            <p id="summary-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="summary">In this work we present WIR3D, a technique for abstracting 3D shapes through a sparse set of visually meaningful curves in 3D. We optimize the parameters of Bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. We leverage the intermediate activations of a pre-trained foundation model (CLIP) to guide our optimization process. We divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. Our second phase supervision is spatially guided by a novel localized keypoint loss. This spatial guidance enables user control over abstracted features. We ensure fidelity to the original surface through a neural SDF loss, which allows the curves to be used as intuitive deformation handles. We successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation.</p>
            <p id="subjects-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" onclick="foldPdfKimi('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="panel paper" keywords="brick,brickgpt,physically,buildable,stable,structures,text,designs,captions,generating">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text_ICCV_2025_paper.html" target="_blank" title="58/64"><span class="index notranslate">#58</span></a>
                <a id="title-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="title-link" href="/venue/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" target="_blank">Generating Physically Stable and Buildable Brick Structures from Text</a>
                <a id="pdf-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF">8</sup>]</a>
                <a id="copy-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ava Pun" target="_blank">Ava Pun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangle Deng" target="_blank">Kangle Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixuan Liu" target="_blank">Ruixuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deva Ramanan" target="_blank">Deva Ramanan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changliu Liu" target="_blank">Changliu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun-Yan Zhu" target="_blank">Jun-Yan Zhu</a>
            </p>
            <p id="summary-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="summary">We introduce BrickGPT, the first approach for generating physically stable interconnecting brick assembly models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of brick structures, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that BrickGPT produces stable, diverse, and aesthetically pleasing brick structures that align closely with the input text prompts. We also develop a text-based brick texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We release our new dataset, StableText2Brick, containing over 47,000 brick structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/BrickGPT/.</p>
            <p id="subjects-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" onclick="foldPdfKimi('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="panel paper" keywords="mamba,gmmamba,group,masking,slide,imm,tcga,wsis,uninformative,css">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification_ICCV_2025_paper.html" target="_blank" title="59/64"><span class="index notranslate">#59</span></a>
                <a id="title-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="title-link" href="/venue/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" target="_blank">GMMamba: Group Masking Mamba for Whole Slide Image Classification</a>
                <a id="pdf-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF">9</sup>]</a>
                <a id="copy-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tingting Zheng" target="_blank">Tingting Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxun Yao" target="_blank">Hongxun Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kui Jiang" target="_blank">Kui Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Xiao" target="_blank">Yi Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sicheng Zhao" target="_blank">Sicheng Zhao</a>
            </p>
            <p id="summary-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="summary">Recent advances in selective state space models (Mamba) have shown great promise in whole slide image (WSI) classification. Despite this, WSIs contain explicit local redundancy (similar patches) and irrelevant regions (uninformative instances), posing significant challenges for Mamba-based multi-instance learning (MIL) methods in capturing global representations. Furthermore, bag-level approaches struggle to extract critical features from all instances, while group-level methods fail to adequately account for tumor dispersion and intrinsic correlations across groups, leading to suboptimal global representations. To address these issues, we propose group masking Mamba (GMMamba), a novel framework that combines two elaborate modules: (1) intra-group masking Mamba (IMM) for selective instance exploration within groups, and (2) cross-group super-feature sampling (CSS) to ameliorate long-range relation learning. Specifically, IMM adaptively predicts sparse masks to filter out features with low attention scores (i.e., uninformative patterns) during bidirectional Mamba modeling, facilitating the removal of instance redundancies for compact local representation. For improved bag prediction, the CSS module further aggregates sparse group representations into discriminative features, effectively grasping comprehensive dependencies among dispersed and sparse tumor regions inherent in large-scale WSIs. Extensive experiments on four datasets demonstrate that GMMamba outperforms the state-of-the-art ACMIL by 2.2% and 6.4% in accuracy on the TCGA-BRCA and TCGA-ESCA datasets, respectively.</p>
            <p id="subjects-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" onclick="foldPdfKimi('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="panel paper" keywords="video,streamformer,streaming,multitask,understanding,streams,temporal,representation,frame,maintaining">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training_ICCV_2025_paper.html" target="_blank" title="60/64"><span class="index notranslate">#60</span></a>
                <a id="title-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="title-link" href="/venue/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" target="_blank">Learning Streaming Video Representation via Multitask Training</a>
                <a id="pdf-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yan_Learning_Streaming_Video_Representation_via_Multitask_Training_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF">8</sup>]</a>
                <a id="copy-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF">2</sup>]</a>
                <a id="rel-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yibin Yan" target="_blank">Yibin Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jilan Xu" target="_blank">Jilan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangzhe Di" target="_blank">Shangzhe Di</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yikun Liu" target="_blank">Yikun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yudi Shi" target="_blank">Yudi Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qirui Chen" target="_blank">Qirui Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeqian Li" target="_blank">Zeqian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Huang" target="_blank">Yifei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weidi Xie" target="_blank">Weidi Xie</a>
            </p>
            <p id="summary-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="summary">Understanding continuous video streams plays a fundamental role in real-time applications, including embodied AI and autonomous driving. Unlike offline video processing, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions. To address these challenges, our main contributions are three-fold. (i) we develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability. (ii) to train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) we conduct extensive experiments for online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive performance while maintaining efficiency, demonstrating its potential for real-time applications.</p>
            <p id="subjects-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" onclick="foldPdfKimi('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="panel paper" keywords="label,lightening,distilled,labels,hello,distillation,dataset,original,projectors,synthetic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening_ICCV_2025_paper.html" target="_blank" title="61/64"><span class="index notranslate">#61</span></a>
                <a id="title-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="title-link" href="/venue/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" target="_blank">Heavy Labels Out! Dataset Distillation with Label Space Lightening</a>
                <a id="pdf-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF">10</sup>]</a>
                <a id="copy-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruonan Yu" target="_blank">Ruonan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Songhua Liu" target="_blank">Songhua Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zigeng Chen" target="_blank">Zigeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingwen Ye" target="_blank">Jingwen Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchao Wang" target="_blank">Xinchao Wang</a>
            </p>
            <p id="summary-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="summary">Dataset distillation or condensation aims to condense a large-scale training dataset into a much smaller synthetic one such that the training performance of distilled and original sets on neural networks are similar. Although the number of training samples can be reduced substantially, current state-of-the-art methods heavily rely on enormous soft labels to achieve satisfactory performance. As a result, the required storage can be comparable even to original datasets, especially for large-scale ones. To solve this problem, instead of storing these heavy labels, we propose a novel label-lightening framework termed HeLlO aiming at effective image-to-label projectors, with which synthetic labels can be directly generated online from synthetic images. Specifically, to construct such projectors, we leverage prior knowledge in open-source foundation models, e.g., CLIP, and introduce a LoRA-like fine-tuning strategy to mitigate the gap between pre-trained and target distributions, so that original models for soft-label generation can be distilled into a group of low-rank matrices. Moreover, an effective image optimization method is proposed to further mitigate the potential error between the original and distilled label generators. Extensive experiments show that our method significantly reduces the storage cost to merely 0.001% compared to full soft-label storage methods while achieving comparable performance to state-of-the-art dataset distillation methods on large-scale datasets. Our codes are available at https://github.com/Lexie-YU/HeLlO.</p>
            <p id="subjects-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" onclick="foldPdfKimi('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="panel paper" keywords="completion,scene,lidar,scorelidar,distilled,diffusion,distilling,quality,distillation,constraining">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion_ICCV_2025_paper.html" target="_blank" title="62/64"><span class="index notranslate">#62</span></a>
                <a id="title-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="title-link" href="/venue/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" target="_blank">Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</a>
                <a id="pdf-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF">9</sup>]</a>
                <a id="copy-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF">4</sup>]</a>
                <a id="rel-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shengyuan Zhang" target="_blank">Shengyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=An Zhao" target="_blank">An Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Yang" target="_blank">Ling Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zejian Li" target="_blank">Zejian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenye Meng" target="_blank">Chenye Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoran Xu" target="_blank">Haoran Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianrun Chen" target="_blank">Tianrun Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=AnYang Wei" target="_blank">AnYang Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Perry Pengyun Gu" target="_blank">Perry Pengyun Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingyun Sun" target="_blank">Lingyun Sun</a>
            </p>
            <p id="summary-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="summary">Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. Score- LiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (&gt;5x) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our model and code are publicly available on https://github.com/happyw1nd/ScoreLiDAR.</p>
            <p id="subjects-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" onclick="foldPdfKimi('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="panel paper" keywords="motion,camera,blur,imu,blurred,mast3r,image,colmap,scannet,fast">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single_ICCV_2025_paper.html" target="_blank" title="63/64"><span class="index notranslate">#63</span></a>
                <a id="title-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="title-link" href="/venue/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" target="_blank">Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</a>
                <a id="pdf-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF">8</sup>]</a>
                <a id="copy-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF">6</sup>]</a>
                <a id="rel-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jerred Chen" target="_blank">Jerred Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronald Clark" target="_blank">Ronald Clark</a>
            </p>
            <p id="summary-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="summary">In many robotics and VR/AR applications, fast camera motions lead to a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.</p>
            <p id="subjects-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" onclick="foldPdfKimi('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="panel paper" keywords="multimodal,fusion,architectures,native,early,encoders,nmms,laws,trained,late">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Shukor_Scaling_Laws_for_Native_Multimodal_Models_ICCV_2025_paper.html" target="_blank" title="64/64"><span class="index notranslate">#64</span></a>
                <a id="title-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="title-link" href="/venue/Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" target="_blank">Scaling Laws for Native Multimodal Models</a>
                <a id="pdf-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF', this)" data="https://openaccess.thecvf.com/content/ICCV2025/papers/Shukor_Scaling_Laws_for_Native_Multimodal_Models_ICCV_2025_paper.pdf">[PDF<sup id="pdf-stars-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF">15</sup>]</a>
                <a id="copy-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF')">[Copy]</a>
                <a id="kimi-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF', this)">[Kimi<sup id="kimi-stars-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF">10</sup>]</a>
                <a id="rel-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mustafa Shukor" target="_blank">Mustafa Shukor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enrico Fini" target="_blank">Enrico Fini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victor Guilherme Turrisi da Costa" target="_blank">Victor Guilherme Turrisi da Costa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthieu Cord" target="_blank">Matthieu Cord</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua Susskind" target="_blank">Joshua Susskind</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alaaeldin El-Nouby" target="_blank">Alaaeldin El-Nouby</a>
            </p>
            <p id="summary-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="summary">Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders or tokenizers. On the contrary, early fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows models to learn modality-specific weights, significantly benefiting performance.</p>
            <p id="subjects-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICCV.2025?group=Oral" target="_blank">ICCV.2025 - Oral</a>
            </p>
            <div id="pdf-container-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" onclick="foldPdfKimi('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF">Token Activation Map to Visually Explain Multimodal LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF">NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF">CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF">MaskControl: Spatio-Temporal Control for Masked Motion Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF">Removing Cost Volumes from Optical Flow Estimators</a>
                    <a class="i-star" onclick="toggleAppStar('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF">LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering</a>
                    <a class="i-star" onclick="toggleAppStar('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF">Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability</a>
                    <a class="i-star" onclick="toggleAppStar('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF">LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF">LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</a>
                    <a class="i-star" onclick="toggleAppStar('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF">Towards a Unified Copernicus Foundation Model for Earth Vision</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF">E-SAM: Training-Free Segment Every Entity Model</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF">Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)</a>
                    <a class="i-star" onclick="toggleAppStar('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF">FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases</a>
                    <a class="i-star" onclick="toggleAppStar('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF">TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF">FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models</a>
                    <a class="i-star" onclick="toggleAppStar('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF">Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF">Dynamic Typography: Bringing Text to Life via Video Diffusion Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF">Uncalibrated Structure from Motion on a Sphere</a>
                    <a class="i-star" onclick="toggleAppStar('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF">Differentiable Room Acoustic Rendering with Multi-View Vision Priors</a>
                    <a class="i-star" onclick="toggleAppStar('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF">GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space</a>
                    <a class="i-star" onclick="toggleAppStar('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF">Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF">EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF">Multi-View 3D Point Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF">Diving into the Fusion of Monocular Priors for Generalized Stereo Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF">HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF">ReCamMaster: Camera-Controlled Generative Rendering from A Single Video</a>
                    <a class="i-star" onclick="toggleAppStar('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bai_ReCamMaster_Camera-Controlled_Generative_Rendering_from_A_Single_Video@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF">RePoseD: Efficient Relative Pose Estimation With Known Depth Information</a>
                    <a class="i-star" onclick="toggleAppStar('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ding_RePoseD_Efficient_Relative_Pose_Estimation_With_Known_Depth_Information@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF">SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('He_SparseFlex_High-Resolution_and_Arbitrary-Topology_3D_Shape_Modeling@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF">RayZer: A Self-supervised Large View Synthesis Model</a>
                    <a class="i-star" onclick="toggleAppStar('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jiang_RayZer_A_Self-supervised_Large_View_Synthesis_Model@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF">Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Back_on_Track_Bundle_Adjustment_for_Dynamic_Scene_Reconstruction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF">Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_Self-Ensembling_Gaussian_Splatting_for_Few-Shot_Novel_View_Synthesis@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF">Importance-Based Token Merging for Efficient Image and Video Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Importance-Based_Token_Merging_for_Efficient_Image_and_Video_Generation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF">Knowledge Distillation for Learned Image Compression</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Knowledge_Distillation_for_Learned_Image_Compression@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF">Variance-Based Pruning for Accelerating and Compressing Trained Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Berisha_Variance-Based_Pruning_for_Accelerating_and_Compressing_Trained_Networks@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF">RS-vHeat: Heat Conduction Guided Efficient Remote Sensing Foundation Model</a>
                    <a class="i-star" onclick="toggleAppStar('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hu_RS-vHeat_Heat_Conduction_Guided_Efficient_Remote_Sensing_Foundation_Model@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF">Understanding Co-speech Gestures in-the-wild</a>
                    <a class="i-star" onclick="toggleAppStar('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hegde_Understanding_Co-speech_Gestures_in-the-wild@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF">DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lu_DPoser-X_Diffusion_Model_as_Robust_3D_Whole-body_Human_Pose_Prior@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF">Teeth Reconstruction and Performance Capture Using a Phone Camera</a>
                    <a class="i-star" onclick="toggleAppStar('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zheng_Teeth_Reconstruction_and_Performance_Capture_Using_a_Phone_Camera@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF">Certifiably Optimal Anisotropic Rotation Averaging</a>
                    <a class="i-star" onclick="toggleAppStar('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Olsson_Certifiably_Optimal_Anisotropic_Rotation_Averaging@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF">MIORe &amp; VAR-MIORe: Benchmarks to Push the Boundaries of Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ciubotariu_MIORe__VAR-MIORe_Benchmarks_to_Push_the_Boundaries_of_Restoration@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF">MikuDance: Animating Character Art with Mixed Motion Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_MikuDance_Animating_Character_Art_with_Mixed_Motion_Dynamics@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF">ROAR: Reducing Inversion Error in Generative Image Watermarking</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF">Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Du_Diffusion_Transformer_meets_Multi-level_Wavelet_Spectrum_for_Single_Image_Super-Resolution@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF">LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Song_LayerTracer_Cognitive-Aligned_Layered_SVG_Synthesis_via_Diffusion_Transformer@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qin_Spatially-Varying_Autofocus@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#Qin_Spatially-Varying_Autofocus@ICCV2025@CVF">Spatially-Varying Autofocus</a>
                    <a class="i-star" onclick="toggleAppStar('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qin_Spatially-Varying_Autofocus@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF">Event-based Visual Vibrometry</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_Event-based_Visual_Vibrometry@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF">SuperDec: 3D Scene Decomposition with Superquadrics Primitives</a>
                    <a class="i-star" onclick="toggleAppStar('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fedele_SuperDec_3D_Scene_Decomposition_with_Superquadrics_Primitives@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF">Online Reasoning Video Segmentation with Just-in-Time Digital Twins</a>
                    <a class="i-star" onclick="toggleAppStar('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shen_Online_Reasoning_Video_Segmentation_with_Just-in-Time_Digital_Twins@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF">Towards Foundational Models for Single-Chip Radar</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_Towards_Foundational_Models_for_Single-Chip_Radar@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF">SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining</a>
                    <a class="i-star" onclick="toggleAppStar('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_SceneSplat_Gaussian_Splatting-based_Scene_Understanding_with_Vision-Language_Pretraining@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF">ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds</a>
                    <a class="i-star" onclick="toggleAppStar('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xiang_ForestFormer3D_A_Unified_Framework_for_End-to-End_Segmentation_of_Forest_LiDAR@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF">Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Simonelli_Easy3D_A_Simple_Yet_Effective_Method_for_3D_Interactive_Segmentation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF">Diffusion Image Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chihaoui_Diffusion_Image_Prior@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF">Soft Local Completeness: Rethinking Completeness in XAI</a>
                    <a class="i-star" onclick="toggleAppStar('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Haddad_Soft_Local_Completeness_Rethinking_Completeness_in_XAI@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dumery_Counting_Stacked_Objects@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#Dumery_Counting_Stacked_Objects@ICCV2025@CVF">Counting Stacked Objects</a>
                    <a class="i-star" onclick="toggleAppStar('Dumery_Counting_Stacked_Objects@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dumery_Counting_Stacked_Objects@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF">Deterministic Object Pose Confidence Region Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Deterministic_Object_Pose_Confidence_Region_Estimation@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF">WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_WIR3D_Visually-Informed_and_Geometry-Aware_3D_Shape_Abstraction@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF">Generating Physically Stable and Buildable Brick Structures from Text</a>
                    <a class="i-star" onclick="toggleAppStar('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pun_Generating_Physically_Stable_and_Buildable_Brick_Structures_from_Text@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF">GMMamba: Group Masking Mamba for Whole Slide Image Classification</a>
                    <a class="i-star" onclick="toggleAppStar('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zheng_GMMamba_Group_Masking_Mamba_for_Whole_Slide_Image_Classification@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF">Learning Streaming Video Representation via Multitask Training</a>
                    <a class="i-star" onclick="toggleAppStar('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yan_Learning_Streaming_Video_Representation_via_Multitask_Training@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF">Heavy Labels Out! Dataset Distillation with Label Space Lightening</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Heavy_Labels_Out_Dataset_Distillation_with_Label_Space_Lightening@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF">Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Distilling_Diffusion_Models_to_Efficient_3D_LiDAR_Scene_Completion@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF">Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Image_as_an_IMU_Estimating_Camera_Motion_from_a_Single@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF">Scaling Laws for Native Multimodal Models</a>
                    <a class="i-star" onclick="toggleAppStar('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shukor_Scaling_Laws_for_Native_Multimodal_Models@ICCV2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



</body></html>