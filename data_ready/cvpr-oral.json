{
  "source_html": "html\\cvpr-oral.html",
  "paper_count": 95,
  "conference": "cvpr",
  "year": 2025,
  "status": "oral",
  "papers": [
    {
      "paper_id": "Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF",
      "index": 1,
      "title": "Camera Resection from Known Line Pencils and a Radially Distorted Scanline",
      "authors": [
        "Juan C. Dibene",
        "Enrique Dunn"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "scanline",
        "pencils",
        "radially",
        "distorted",
        "camera",
        "geometric",
        "resection",
        "solver",
        "measurements",
        "eight"
      ],
      "summary": "We present a marker-based geometric estimation framework for the absolute pose of a camera by analyzing the 1D observations in a single radially distorted pixel scanline.We leverage a pair of known co-planar pencils of lines, along with lens distortion parameters, to propose an ensemble of solvers exploring the space of estimation strategies applicable to our setup.First, we present a minimal algebraic solver requiring only six measurements and yielding eight solutions, which relies on the intersection of two conics defined by one of the pencils of lines.Then, we present a unique closed-form geometric solver from seven measurements.Finally, we present an homography-based formulation amenable to linear least-squares from eight or more measurements.Our geometric framework constitutes a theoretical analysis on the minimum geometric context necessary to solve in closed form for the absolute pose of a single camera from a single radially distorted scanline.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 494,
        "kimi": 370
      },
      "raw_excerpt": "Camera Resection from Known Line Pencils and a Radially Distorted Scanline [PDF 494 ] [Copy] [Kimi 370 ] [REL] Authors : Juan C. Dibene , Enrique Dunn We present a marker-based geometric estimation framework for the absolute pose of a camera by analyzing the 1D observations in a single radially distorted pixel scanline.We leverage a pair of known co-planar pencils of lines, along with lens distortion parameters, to propose an ensemble of solvers exploring the space of estimation strategies applicable to our setup.First, we present a minimal algebraic solver requiring only six measurements and yielding eight solutions, which relies on the intersection of two conics defined by one of the pencils of lines.Then, we present a unique closed-form geometric solver from seven measurements.Finally, we present an homography-based formulation amenable to linear least-squares from eight or more measurements.Our geometric framework constitutes a theoretical analysis on the minimum geometric context necessary to solve in closed form for the absolute pose of a single camera from a single radially distorted scanline. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF",
      "index": 2,
      "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding",
      "authors": [
        "Feilong Tang",
        "Chengzhi Liu",
        "Zhongxing Xu",
        "Ming Hu",
        "Zile Huang",
        "Haochen Xue",
        "Ziyang Chen",
        "Zelin Peng",
        "Zhiwei Yang",
        "Sijin Zhou",
        "Wenxue Li",
        "Yulong Li",
        "Wenxuan Song",
        "Shiyan Su",
        "Wei Feng",
        "Jionglong Su",
        "Mingquan Lin",
        "Yifan Peng",
        "Xuelian Cheng",
        "Imran Razzak",
        "Zongyuan Ge"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "hallucinations",
        "tokens",
        "mllms",
        "outlier",
        "causal",
        "farsight",
        "attention",
        "decoding",
        "mitigating",
        "propagation"
      ],
      "summary": "Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 450,
        "kimi": 236
      },
      "raw_excerpt": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding [PDF 450 ] [Copy] [Kimi 236 ] [REL] Authors : Feilong Tang , Chengzhi Liu , Zhongxing Xu , Ming Hu , Zile Huang , Haochen Xue , Ziyang Chen , Zelin Peng , Zhiwei Yang , Sijin Zhou , Wenxue Li , Yulong Li , Wenxuan Song , Shiyan Su , Wei Feng , Jionglong Su , Mingquan Lin , Yifan Peng , Xuelian Cheng , Imran Razzak , Zongyuan Ge Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF",
      "index": 3,
      "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
      "authors": [
        "Hao Lin",
        "Ke Wu",
        "Jie Li",
        "Jun Li",
        "Wu-Jun Li"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "uniap",
        "parallelism",
        "intra",
        "parallel",
        "layer",
        "inter",
        "automatic",
        "mixed",
        "integer",
        "quadratic"
      ],
      "summary": "Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 3.80 × × in throughput and reduces strategy optimization time by up to 107 × × across five Transformer-based models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 165,
        "kimi": 109
      },
      "raw_excerpt": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming [PDF 165 ] [Copy] [Kimi 109 ] [REL] Authors : Hao Lin , Ke Wu , Jie Li , Jun Li , Wu-Jun Li Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 3.80 × × in throughput and reduces strategy optimization time by up to 107 × × across five Transformer-based models. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF",
      "index": 4,
      "title": "SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images",
      "authors": [
        "Kaiyu Li",
        "Ruixun Liu",
        "Xiangyong Cao",
        "Xueru Bai",
        "Feng Zhou",
        "Deyu Meng",
        "Zhi Wang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "remote",
        "sensing",
        "simfeatup",
        "segearth",
        "segmentation",
        "vocabulary",
        "images",
        "ovss",
        "semantic",
        "patch"
      ],
      "summary": "Current remote sensing semantic segmentation methods are mostly built on the close-set assumption, meaning that the model can only recognize pre-defined categories that exist in the training set. However, in practical Earth observation, there are countless unseen categories, and manual annotation is impractical. To address this challenge, we first attempt to introduce training-free open-vocabulary semantic segmentation (OVSS) into the remote sensing context. However, due to the sensitivity of remote sensing images to low-resolution features, distorted target shapes and ill-fitting boundaries are exhibited in the prediction mask. To tackle these issues, we propose a simple and universal upsampler, i.e. SimFeatUp, to restore lost spatial information of deep features. Specifically, SimFeatUp only needs to learn from a few unlabeled images, and can upsample arbitrary remote sensing image features. Furthermore, based on the observation of the abnormal response of patch tokens to the [CLS] token in CLIP, we propose to execute a simple subtraction operation to alleviate the global bias in patch tokens. Extensive experiments are conducted on 17 remote sensing datasets of 4 tasks, including semantic segmentation, building extraction, road detection, and flood detection. Our method achieves an average of 5.8\\%, 8.2\\%, 4.0\\%, and 15.3\\% improvement over state-of-the-art methods on the 4 tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 300,
        "kimi": 135
      },
      "raw_excerpt": "SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images [PDF 300 ] [Copy] [Kimi 135 ] [REL] Authors : Kaiyu Li , Ruixun Liu , Xiangyong Cao , Xueru Bai , Feng Zhou , Deyu Meng , Zhi Wang Current remote sensing semantic segmentation methods are mostly built on the close-set assumption, meaning that the model can only recognize pre-defined categories that exist in the training set. However, in practical Earth observation, there are countless unseen categories, and manual annotation is impractical. To address this challenge, we first attempt to introduce training-free open-vocabulary semantic segmentation (OVSS) into the remote sensing context. However, due to the sensitivity of remote sensing images to low-resolution features, distorted target shapes and ill-fitting boundaries are exhibited in the prediction mask. To tackle these issues, we propose a simple and universal upsampler, i.e. SimFeatUp, to restore lost spatial information of deep features. Specifically, SimFeatUp only needs to learn from a few unlabeled images, and can upsample arbitrary remote sensing image features. Furthermore, based on the observation of the abnormal response of patch tokens to the [CLS] token in CLIP, we propose to execute a simple subtraction operation to alleviate the global bias in patch tokens. Extensive experiments are conducted on 17 remote sensing datasets of 4 tasks, including semantic segmentation, building extraction, road detection, and flood detection. Our method achieves an average of 5.8\\%, 8.2\\%, 4.0\\%, and 15.3\\% improvement over state-of-the-art methods on the 4 tasks. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF",
      "index": 5,
      "title": "VGGT: Visual Geometry Grounded Transformer",
      "authors": [
        "Jianyuan Wang",
        "Minghao Chen",
        "Nikita Karaev",
        "Andrea Vedaldi",
        "Christian Rupprecht",
        "David Novotny"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "vggn",
        "vggt",
        "point",
        "hundreds",
        "grounded",
        "visual",
        "geometry",
        "forward",
        "camera",
        "depth"
      ],
      "summary": "We present VGGN, a feed-forward neural network that infers directly all key 3D attributes of a scene, such as camera poses, point maps, depth maps, and 3D point tracks, from few or hundreds of its views. Unlike recent alternatives, VGGN does not need to use visual geometry optimization techniques to refine the results in post-processing, obtaining all quantities of interest directly. This approach is simple and more efficient, reconstructing hundreds of images in seconds. We train VGGN on a large number of publicly available datasets with 3D annotations and demonstrate its ability to achieve state-of-the-art results in multiple 3D tasks, including camera pose estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. This is a step forward in 3D computer vision, where models have been typically constrained to and specialized for single tasks. We extensively evaluate our method on unseen datasets to demonstrate its superior performance. We will release the code and trained model.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 293,
        "kimi": 159
      },
      "raw_excerpt": "VGGT: Visual Geometry Grounded Transformer [PDF 293 ] [Copy] [Kimi 159 ] [REL] Authors : Jianyuan Wang , Minghao Chen , Nikita Karaev , Andrea Vedaldi , Christian Rupprecht , David Novotny We present VGGN, a feed-forward neural network that infers directly all key 3D attributes of a scene, such as camera poses, point maps, depth maps, and 3D point tracks, from few or hundreds of its views. Unlike recent alternatives, VGGN does not need to use visual geometry optimization techniques to refine the results in post-processing, obtaining all quantities of interest directly. This approach is simple and more efficient, reconstructing hundreds of images in seconds. We train VGGN on a large number of publicly available datasets with 3D annotations and demonstrate its ability to achieve state-of-the-art results in multiple 3D tasks, including camera pose estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. This is a step forward in 3D computer vision, where models have been typically constrained to and specialized for single tasks. We extensively evaluate our method on unseen datasets to demonstrate its superior performance. We will release the code and trained model. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF",
      "index": 6,
      "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
      "authors": [
        "Yan Xia",
        "Xiaowei Zhou",
        "Etienne Vouga",
        "Qixing Huang",
        "Georgios Pavlakos"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "biomechanically",
        "skeleton",
        "reconstructing",
        "humans",
        "pose",
        "estimation",
        "accurate",
        "pseudo",
        "benchmarks",
        "realistic"
      ],
      "summary": "In this paper, we introduce a method for reconstructing humans in 3D from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to generate pseudo ground truth data and implement a training procedure that iteratively refines these pseudo labels for improved accuracy. Compared to state-of-the-art methods in 3D human pose estimation, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. This result highlights the benefits of using a biomechanical skeleton with realistic degrees of freedom for robust pose estimation. Additionally, we show that previous models frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom leading to more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We will make all code, models and data publicly available upon publication.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 213,
        "kimi": 88
      },
      "raw_excerpt": "Reconstructing Humans with a Biomechanically Accurate Skeleton [PDF 213 ] [Copy] [Kimi 88 ] [REL] Authors : Yan Xia , Xiaowei Zhou , Etienne Vouga , Qixing Huang , Georgios Pavlakos In this paper, we introduce a method for reconstructing humans in 3D from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to generate pseudo ground truth data and implement a training procedure that iteratively refines these pseudo labels for improved accuracy. Compared to state-of-the-art methods in 3D human pose estimation, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. This result highlights the benefits of using a biomechanical skeleton with realistic degrees of freedom for robust pose estimation. Additionally, we show that previous models frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom leading to more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We will make all code, models and data publicly available upon publication. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF",
      "index": 7,
      "title": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner",
      "authors": [
        "Weiyu Li",
        "Jiarui Liu",
        "Hongyu Yan",
        "Rui Chen",
        "Yixun Liang",
        "Xuelin Chen",
        "Ping Tan",
        "Xiaoxiao Long"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "mesh",
        "craftsman",
        "refiner",
        "craftsman3d",
        "native",
        "interactive",
        "mumber",
        "geometry",
        "roughs",
        "fidelity"
      ],
      "summary": "We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, self-occlusion, irregular mesh topologies, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling softwares. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we first introduce a robust data preprocessing pipeline that utilizes visibility check and winding mumber to maximize the use of existing 3D data. Leveraging this data, we employ a 3D-native DiT model that directly models the distribution of 3D data in latent space, generating coarse geometries with regular mesh topology in seconds. Subsequently, a normal-based geometry refiner enhances local surface details, which can be applied automatically or interactively with user input. Extensive experiments demonstrate that our method achieves high efficacy in producing superior quality 3D assets compared to existing methods.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 171,
        "kimi": 71
      },
      "raw_excerpt": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner [PDF 171 ] [Copy] [Kimi 71 ] [REL] Authors : Weiyu Li , Jiarui Liu , Hongyu Yan , Rui Chen , Yixun Liang , Xuelin Chen , Ping Tan , Xiaoxiao Long We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, self-occlusion, irregular mesh topologies, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling softwares. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we first introduce a robust data preprocessing pipeline that utilizes visibility check and winding mumber to maximize the use of existing 3D data. Leveraging this data, we employ a 3D-native DiT model that directly models the distribution of 3D data in latent space, generating coarse geometries with regular mesh topology in seconds. Subsequently, a normal-based geometry refiner enhances local surface details, which can be applied automatically or interactively with user input. Extensive experiments demonstrate that our method achieves high efficacy in producing superior quality 3D assets compared to existing methods. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF",
      "index": 8,
      "title": "DNF: Unconditional 4D Generation with Dictionary-based Neural Fields",
      "authors": [
        "Xinyi Zhang",
        "Naiqi Li",
        "Angela Dai"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "dictionary",
        "shape",
        "dnf",
        "unconditional",
        "motion",
        "generative",
        "fidelity",
        "deformable",
        "achived",
        "shapes"
      ],
      "summary": "While remarkable success has been achived through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields.Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 149,
        "kimi": 65
      },
      "raw_excerpt": "DNF: Unconditional 4D Generation with Dictionary-based Neural Fields [PDF 149 ] [Copy] [Kimi 65 ] [REL] Authors : Xinyi Zhang , Naiqi Li , Angela Dai While remarkable success has been achived through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields.Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF",
      "index": 9,
      "title": "Removing Reflections from RAW Photos",
      "authors": [
        "Eric Kee",
        "Adam Pikielny",
        "Kevin Blackburn-Matzen",
        "Marc Levoy"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "photos",
        "raw",
        "256p",
        "optional",
        "photo",
        "accepts",
        "reflections",
        "images",
        "system",
        "consumer"
      ],
      "summary": "We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, and accepts an optional contextual photo looking in the opposite direction (e.g., the \"selfie\" camera on a mobile device). This optional photo helps disambiguate what should be considered the reflection. The system is trained solely on synthetic mixtures of real-world RAW images, which we combine using a reflection simulation that is photometrically and geometrically accurate. Our system comprises a base model that accepts the captured photo and optional context photo as input, and runs at 256p, followed by an up-sampling model that transforms 256p images to full resolution. The system can produce images for review at 1K in 4.5 to 6.5 seconds on a MacBook or iPhone 14 Pro. We test on RAW photos that were captured in the field and embody typical consumer photos, and show that our RAW-image simulation yields SOTA performance.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Kee_Removing_Reflections_from_RAW_Photos_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Kee_Removing_Reflections_from_RAW_Photos_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 220,
        "kimi": 94
      },
      "raw_excerpt": "Removing Reflections from RAW Photos [PDF 220 ] [Copy] [Kimi 94 ] [REL] Authors : Eric Kee , Adam Pikielny , Kevin Blackburn-Matzen , Marc Levoy We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, and accepts an optional contextual photo looking in the opposite direction (e.g., the \"selfie\" camera on a mobile device). This optional photo helps disambiguate what should be considered the reflection. The system is trained solely on synthetic mixtures of real-world RAW images, which we combine using a reflection simulation that is photometrically and geometrically accurate. Our system comprises a base model that accepts the captured photo and optional context photo as input, and runs at 256p, followed by an up-sampling model that transforms 256p images to full resolution. The system can produce images for review at 1K in 4.5 to 6.5 seconds on a MacBook or iPhone 14 Pro. We test on RAW photos that were captured in the field and embody typical consumer photos, and show that our RAW-image simulation yields SOTA performance. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF",
      "index": 10,
      "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation",
      "authors": [
        "Aishik Konwer",
        "Zhijian Yang",
        "Erhan Bas",
        "Cao Xiao",
        "Prateek Prasanna",
        "Parminder Bhatia",
        "Taha Kass-Hout"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "sam",
        "segmentation",
        "annotation",
        "anything",
        "preference",
        "medical",
        "segment",
        "still",
        "prompts",
        "prompting"
      ],
      "summary": "Foundational models such as the Segment Anything Model (SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 238,
        "kimi": 99
      },
      "raw_excerpt": "Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation [PDF 238 ] [Copy] [Kimi 99 ] [REL] Authors : Aishik Konwer , Zhijian Yang , Erhan Bas , Cao Xiao , Prateek Prasanna , Parminder Bhatia , Taha Kass-Hout Foundational models such as the Segment Anything Model (SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF",
      "index": 11,
      "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
      "authors": [
        "Zhendong Wang",
        "Jianmin Bao",
        "Shuyang Gu",
        "Dong Chen",
        "Wengang Zhou",
        "Houqiang Li"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "designdiffusion",
        "text",
        "generation",
        "textual",
        "visual",
        "design",
        "style",
        "image",
        "quality",
        "diffusion"
      ],
      "summary": "In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 293,
        "kimi": 100
      },
      "raw_excerpt": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models [PDF 293 ] [Copy] [Kimi 100 ] [REL] Authors : Zhendong Wang , Jianmin Bao , Shuyang Gu , Dong Chen , Wengang Zhou , Houqiang Li In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF",
      "index": 12,
      "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
      "authors": [
        "Boseung Jeong",
        "Jicheol Park",
        "Sungyeon Kim",
        "Suha Kwak"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "video",
        "audio",
        "gated",
        "retrieval",
        "text",
        "avigate",
        "representation",
        "attention",
        "guided",
        "textual"
      ],
      "summary": "Video-text retrieval, the task of retrieving videos based on a textual query or vice versa, is of paramount importance for video understanding and multimodal information retrieval. Recent methods in this area rely primarily on visual and textual features and often ignore audio, although it helps enhance overall comprehension of video content.Moreover, traditional models that incorporate audio blindly utilize the audio input regardless of whether it is useful or not, resulting in suboptimal video representation. To address these limitations, we propose a novel video-text retrieval framework, Audio-guided VIdeo representation learning with GATEd attention (AVIGATE), that effectively leverages audio cues through a gated attention mechanism that selectively filters out uninformative audio signals.In addition, we propose an adaptive margin-based contrastive loss to deal with the inherently unclear positive-negative relationship between video and text, which facilitates learning better video-text alignment.Our extensive experiments demonstrate that AVIGATE achieves state-of-the-art performance on all the public benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.html",
          "/venue/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 142,
        "kimi": 65
      },
      "raw_excerpt": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval [PDF 142 ] [Copy] [Kimi 65 ] [REL] Authors : Boseung Jeong , Jicheol Park , Sungyeon Kim , Suha Kwak Video-text retrieval, the task of retrieving videos based on a textual query or vice versa, is of paramount importance for video understanding and multimodal information retrieval. Recent methods in this area rely primarily on visual and textual features and often ignore audio, although it helps enhance overall comprehension of video content.Moreover, traditional models that incorporate audio blindly utilize the audio input regardless of whether it is useful or not, resulting in suboptimal video representation. To address these limitations, we propose a novel video-text retrieval framework, Audio-guided VIdeo representation learning with GATEd attention (AVIGATE), that effectively leverages audio cues through a gated attention mechanism that selectively filters out uninformative audio signals.In addition, we propose an adaptive margin-based contrastive loss to deal with the inherently unclear positive-negative relationship between video and text, which facilitates learning better video-text alignment.Our extensive experiments demonstrate that AVIGATE achieves state-of-the-art performance on all the public benchmarks. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF",
      "index": 13,
      "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
      "authors": [
        "Ziqi Pang",
        "Tianyuan Zhang",
        "Fujun Luan",
        "Yunze Man",
        "Hao Tan",
        "Kai Zhang",
        "William T. Freeman",
        "Yu-Xiong Wang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "randar",
        "decoder",
        "token",
        "generation",
        "orders",
        "autoregressive",
        "visual",
        "random",
        "order",
        "generatng"
      ],
      "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generatng images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enabling random order is to insert a \"position instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports in-painting, outpainting and resolution extrapolation in a zero-shot manner.We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 127,
        "kimi": 70
      },
      "raw_excerpt": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders [PDF 127 ] [Copy] [Kimi 70 ] [REL] Authors : Ziqi Pang , Tianyuan Zhang , Fujun Luan , Yunze Man , Hao Tan , Kai Zhang , William T. Freeman , Yu-Xiong Wang We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generatng images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enabling random order is to insert a \"position instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports in-painting, outpainting and resolution extrapolation in a zero-shot manner.We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF",
      "index": 14,
      "title": "MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos",
      "authors": [
        "Zhengqi Li",
        "Richard Tucker",
        "Forrester Cole",
        "Qianqian Wang",
        "Linyi Jin",
        "Vickie Ye",
        "Angjoo Kanazawa",
        "Aleksander Holynski",
        "Noah Snavely"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "videos",
        "camera",
        "casual",
        "dynamic",
        "scenes",
        "parallax",
        "megasam",
        "accurate",
        "robust",
        "motion"
      ],
      "summary": "We present a system that allows for accurate, fast, and robust estimation of camera parameters and depth maps from casual monocular videos of dynamic scenes. Most conventional structure from motion and monocular SLAM techniques assume input videos that feature predominantly static scenes with large amounts of parallax. Such methods tend to produce erroneous estimates in the absence of these conditions. Recent neural network based approaches attempt to overcome these challenges; however, such methods are either computationally expensive or brittle when run on dynamic videos with uncontrolled camera motion or unknown field of view. We demonstrate the surprising effectiveness of the deep visual SLAM framework, and with careful modifications to its training and inference schemes, this system can scale to real-world videos of complex dynamic scenes with unconstrained camera paths, including videos with little camera parallax. Extensive experiments on both synthetic and real videos demonstrate that our system is significantly more accurate and robust at camera pose and depth estimation when compared with prior and concurrent work, with faster or comparable running times.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 123,
        "kimi": 58
      },
      "raw_excerpt": "MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos [PDF 123 ] [Copy] [Kimi 58 ] [REL] Authors : Zhengqi Li , Richard Tucker , Forrester Cole , Qianqian Wang , Linyi Jin , Vickie Ye , Angjoo Kanazawa , Aleksander Holynski , Noah Snavely We present a system that allows for accurate, fast, and robust estimation of camera parameters and depth maps from casual monocular videos of dynamic scenes. Most conventional structure from motion and monocular SLAM techniques assume input videos that feature predominantly static scenes with large amounts of parallax. Such methods tend to produce erroneous estimates in the absence of these conditions. Recent neural network based approaches attempt to overcome these challenges; however, such methods are either computationally expensive or brittle when run on dynamic videos with uncontrolled camera motion or unknown field of view. We demonstrate the surprising effectiveness of the deep visual SLAM framework, and with careful modifications to its training and inference schemes, this system can scale to real-world videos of complex dynamic scenes with unconstrained camera paths, including videos with little camera parallax. Extensive experiments on both synthetic and real videos demonstrate that our system is significantly more accurate and robust at camera pose and depth estimation when compared with prior and concurrent work, with faster or comparable running times. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF",
      "index": 15,
      "title": "FoundationStereo: Zero-Shot Stereo Matching",
      "authors": [
        "Bowen Wen",
        "Matthew Trepte",
        "Joseph Aribido",
        "Jan Kautz",
        "Orazio Gallo",
        "Stan Birchfield"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "stereo",
        "shot",
        "zero",
        "foundation",
        "matching",
        "foundationstereo",
        "tuning",
        "vision",
        "stereoanything",
        "depth"
      ],
      "summary": "Tremendous progress has been made in deep stereo matching to excel on benchmark datasets through per-domain fine-tuning. However, achieving strong zero-shot generalization — a hallmark of foundation models in other computer vision tasks — remains challenging for stereo matching. We introduce StereoAnything, a foundation model for stereo depth estimation designed to achieve strong zero-shot generalization. To this end, we first construct a large-scale (1M stereo pairs) synthetic training dataset featuring large diversity and high photorealism, followed by an automatic self-curation pipeline to remove ambiguous samples. We then design a number of network architecture components to enhance scalability, including a side-tuning feature backbone that adapts rich monocular priors from vision foundation models to mitigate the sim-to-real gap, and long-range context reasoning for effective cost volume filtering. Together, these components lead to strong robustness and accuracy across domains, establishing a new standard in zero-shot stereo depth estimation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 133,
        "kimi": 90
      },
      "raw_excerpt": "FoundationStereo: Zero-Shot Stereo Matching [PDF 133 ] [Copy] [Kimi 90 ] [REL] Authors : Bowen Wen , Matthew Trepte , Joseph Aribido , Jan Kautz , Orazio Gallo , Stan Birchfield Tremendous progress has been made in deep stereo matching to excel on benchmark datasets through per-domain fine-tuning. However, achieving strong zero-shot generalization — a hallmark of foundation models in other computer vision tasks — remains challenging for stereo matching. We introduce StereoAnything, a foundation model for stereo depth estimation designed to achieve strong zero-shot generalization. To this end, we first construct a large-scale (1M stereo pairs) synthetic training dataset featuring large diversity and high photorealism, followed by an automatic self-curation pipeline to remove ambiguous samples. We then design a number of network architecture components to enhance scalability, including a side-tuning feature backbone that adapts rich monocular priors from vision foundation models to mitigate the sim-to-real gap, and long-range context reasoning for effective cost volume filtering. Together, these components lead to strong robustness and accuracy across domains, establishing a new standard in zero-shot stereo depth estimation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF",
      "index": 16,
      "title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
      "authors": [
        "Daniel Geng",
        "Charles Herrmann",
        "Junhwa Hur",
        "Forrester Cole",
        "Serena Zhang",
        "Tobias Pfaff",
        "Tatiana Lopez-Guevara",
        "Yusuf Aytar",
        "Michael Rubinstein",
        "Chen Sun",
        "Oliver Wang",
        "Andrew Owens",
        "Deqing Sun"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "motion",
        "video",
        "prompts",
        "trajectories",
        "sparse",
        "generation",
        "temporally",
        "conditioning",
        "control",
        "prompting"
      ],
      "summary": "Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse _or_ dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as _motion prompts_. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term _motion prompt expansion_. We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.html",
          "/venue/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 149,
        "kimi": 55
      },
      "raw_excerpt": "Motion Prompting: Controlling Video Generation with Motion Trajectories [PDF 149 ] [Copy] [Kimi 55 ] [REL] Authors : Daniel Geng , Charles Herrmann , Junhwa Hur , Forrester Cole , Serena Zhang , Tobias Pfaff , Tatiana Lopez-Guevara , Yusuf Aytar , Michael Rubinstein , Chen Sun , Oliver Wang , Andrew Owens , Deqing Sun Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse _or_ dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as _motion prompts_. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term _motion prompt expansion_. We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF",
      "index": 17,
      "title": "Language-Guided Image Tokenization for Generation",
      "authors": [
        "Kaiwen Zha",
        "Lijun Yu",
        "Alireza Fathi",
        "David A. Ross",
        "Cordelia Schmid",
        "Dina Katabi",
        "Xiuye Gu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "tokenization",
        "textok",
        "512",
        "dit",
        "image",
        "fid",
        "256",
        "generation",
        "text",
        "tokens"
      ],
      "summary": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focusing on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2\\% and 48.1\\% on ImageNet 256 × × 256 and 512 × × 512 benchmarks respectively, across varying number of tokens. These tokenization improvements consistently translate to 16.3\\% and 34.3\\% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve 93.5 × × inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_Language-Guided_Image_Tokenization_for_Generation_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zha_Language-Guided_Image_Tokenization_for_Generation_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 206,
        "kimi": 84
      },
      "raw_excerpt": "Language-Guided Image Tokenization for Generation [PDF 206 ] [Copy] [Kimi 84 ] [REL] Authors : Kaiwen Zha , Lijun Yu , Alireza Fathi , David A. Ross , Cordelia Schmid , Dina Katabi , Xiuye Gu Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focusing on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2\\% and 48.1\\% on ImageNet 256 × × 256 and 512 × × 512 benchmarks respectively, across varying number of tokens. These tokenization improvements consistently translate to 16.3\\% and 34.3\\% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve 93.5 × × inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF",
      "index": 18,
      "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens",
      "authors": [
        "Kaihang Pan",
        "Wang Lin",
        "Zhongqi Yue",
        "Tenglong Ao",
        "Liyu Jia",
        "Wei Zhao",
        "Juncheng Li",
        "Siliang Tang",
        "Hanwang Zhang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "tokens",
        "multimodal",
        "timestep",
        "diffusion",
        "comprehension",
        "mllms",
        "timesteps",
        "visual",
        "generation",
        "language"
      ],
      "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to effectively integrate the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. Extensive experiments show that we achieve a new SOTA for multimodal comprehension and generation simultaneously compared with other MLLMs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 167,
        "kimi": 73
      },
      "raw_excerpt": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens [PDF 167 ] [Copy] [Kimi 73 ] [REL] Authors : Kaihang Pan , Wang Lin , Zhongqi Yue , Tenglong Ao , Liyu Jia , Wei Zhao , Juncheng Li , Siliang Tang , Hanwang Zhang Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to effectively integrate the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. Extensive experiments show that we achieve a new SOTA for multimodal comprehension and generation simultaneously compared with other MLLMs. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF",
      "index": 19,
      "title": "Temporally Consistent Object-Centric Learning by Contrasting Slots",
      "authors": [
        "Anna Manasyan",
        "Maximilian Seitzer",
        "Filip Radovic",
        "Georg Martius",
        "Andrii Zadaianchuk"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "object",
        "centric",
        "temporally",
        "temporal",
        "consistency",
        "slots",
        "videos",
        "representations",
        "contrasting",
        "consistent"
      ],
      "summary": "Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 93,
        "kimi": 40
      },
      "raw_excerpt": "Temporally Consistent Object-Centric Learning by Contrasting Slots [PDF 93 ] [Copy] [Kimi 40 ] [REL] Authors : Anna Manasyan , Maximilian Seitzer , Filip Radovic , Georg Martius , Andrii Zadaianchuk Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF",
      "index": 20,
      "title": "Reanimating Images using Neural Representations of Dynamic Stimuli",
      "authors": [
        "Jacob Yeung",
        "Andrew F. Luo",
        "Gabriel Sarch",
        "Margaret M. Henderson",
        "Deva Ramanan",
        "Michael J. Tarr"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "brain",
        "video",
        "stimuli",
        "motion",
        "reanimating",
        "activity",
        "dynamic",
        "decoded",
        "reanimation",
        "understanding"
      ],
      "summary": "While computer vision models have made incredible strides in static image recognition, they still do not match human performance in tasks that require the understanding of complex, dynamic motion. This is notably true for real-world scenarios where embodied agents face complex and motion-rich environments. Our approach leverages state-of-the-art video diffusion models to decouple static image representation from motion generation, enabling us to utilize fMRI brain activity for a deeper understanding of human responses to dynamic visual stimuli. Conversely, we also demonstrate that information about the brain's representation of motion can enhance the prediction of optical flow in artificial systems. Our novel approach leads to four main findings: (1) Visual motion, represented as fine-grained, object-level resolution optical flow, can be decoded from brain activity generated by participants viewing video stimuli; (2) Video encoders outperform image-based models in predicting video-driven brain activity; (3) Brain-decoded motion signals enable realistic video reanimation based only on the initial frame of the video; and (4) We extend prior work to achieve full video decoding from video-driven brain activity. This framework advances our understanding of how the brain represents spatial and temporal information in dynamic visual scenes. Our findings demonstrate the potential of combining brain imaging with video diffusion models for developing more robust and biologically-inspired computer vision systems.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 95,
        "kimi": 40
      },
      "raw_excerpt": "Reanimating Images using Neural Representations of Dynamic Stimuli [PDF 95 ] [Copy] [Kimi 40 ] [REL] Authors : Jacob Yeung , Andrew F. Luo , Gabriel Sarch , Margaret M. Henderson , Deva Ramanan , Michael J. Tarr While computer vision models have made incredible strides in static image recognition, they still do not match human performance in tasks that require the understanding of complex, dynamic motion. This is notably true for real-world scenarios where embodied agents face complex and motion-rich environments. Our approach leverages state-of-the-art video diffusion models to decouple static image representation from motion generation, enabling us to utilize fMRI brain activity for a deeper understanding of human responses to dynamic visual stimuli. Conversely, we also demonstrate that information about the brain's representation of motion can enhance the prediction of optical flow in artificial systems. Our novel approach leads to four main findings: (1) Visual motion, represented as fine-grained, object-level resolution optical flow, can be decoded from brain activity generated by participants viewing video stimuli; (2) Video encoders outperform image-based models in predicting video-driven brain activity; (3) Brain-decoded motion signals enable realistic video reanimation based only on the initial frame of the video; and (4) We extend prior work to achieve full video decoding from video-driven brain activity. This framework advances our understanding of how the brain represents spatial and temporal information in dynamic visual scenes. Our findings demonstrate the potential of combining brain imaging with video diffusion models for developing more robust and biologically-inspired computer vision systems. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF",
      "index": 21,
      "title": "Towards Universal Dataset Distillation via Task-Driven Diffusion",
      "authors": [
        "Ding Qi",
        "Jian Li",
        "Junyao Gao",
        "Shuguang Dou",
        "Ying Tai",
        "Jianlong Hu",
        "Bo Zhao",
        "Yabiao Wang",
        "Chengjie Wang",
        "Cairong Zhao"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "task",
        "unidd",
        "universal",
        "diffusion",
        "distillation",
        "dataset",
        "driven",
        "methods",
        "specific",
        "imagenet"
      ],
      "summary": "Dataset distillation (DD) condenses key information from large-scale datasets into smaller synthetic datasets, reducing storage and computational costs for training networks. However, recent research has primarily focused on image classification tasks, with limited expansion to detection and segmentation. Two key challenges remain: (i) Task Optimization Heterogeneity, where existing methods focus on class-level information and fail to address the diverse needs of detection and segmentation and (ii) Inflexible Image Generation, where current generation methods rely on global updates for single-class targets and lack localized optimization for specific object regions.To address these challenges, we propose a universal dataset distillation framework, named UniDD, a task-driven diffusion model for diverse DD tasks, as illustrated in Fig.1. Our approach operates in two stages: Universal Task Knowledge Mining, which captures task-relevant information through task-specific proxy model training, and Universal Task-Driven Diffusion, where these proxies guide the diffusion process to generate task-specific synthetic images.Extensive experiments across ImageNet-1K, Pascal VOC, and MS COCO demonstrate that UniDD consistently outperforms state-of-the-art methods. In particular, on ImageNet-1K with IPC-10, UniDD surpasses previous diffusion-based methods by 6.1\\%, while also reducing deployment costs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 108,
        "kimi": 44
      },
      "raw_excerpt": "Towards Universal Dataset Distillation via Task-Driven Diffusion [PDF 108 ] [Copy] [Kimi 44 ] [REL] Authors : Ding Qi , Jian Li , Junyao Gao , Shuguang Dou , Ying Tai , Jianlong Hu , Bo Zhao , Yabiao Wang , Chengjie Wang , Cairong Zhao Dataset distillation (DD) condenses key information from large-scale datasets into smaller synthetic datasets, reducing storage and computational costs for training networks. However, recent research has primarily focused on image classification tasks, with limited expansion to detection and segmentation. Two key challenges remain: (i) Task Optimization Heterogeneity, where existing methods focus on class-level information and fail to address the diverse needs of detection and segmentation and (ii) Inflexible Image Generation, where current generation methods rely on global updates for single-class targets and lack localized optimization for specific object regions.To address these challenges, we propose a universal dataset distillation framework, named UniDD, a task-driven diffusion model for diverse DD tasks, as illustrated in Fig.1. Our approach operates in two stages: Universal Task Knowledge Mining, which captures task-relevant information through task-specific proxy model training, and Universal Task-Driven Diffusion, where these proxies guide the diffusion process to generate task-specific synthetic images.Extensive experiments across ImageNet-1K, Pascal VOC, and MS COCO demonstrate that UniDD consistently outperforms state-of-the-art methods. In particular, on ImageNet-1K with IPC-10, UniDD surpasses previous diffusion-based methods by 6.1\\%, while also reducing deployment costs. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF",
      "index": 22,
      "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models",
      "authors": [
        "Xinyu Tian",
        "Shu Zou",
        "Zhaoyuan Yang",
        "Jing Zhang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "lvlms",
        "position",
        "bias",
        "sofa",
        "reasoning",
        "image",
        "middle",
        "beginning",
        "vision",
        "struggle"
      ],
      "summary": "The evolution of Large Vision-Language Models (LVLMs) has progressed from single-image understanding to multi-image reasoning. Despite this advancement, our findings indicate that LVLMs struggle to robustly utilize information across multiple images, with predictions significantly affected by the alteration of image positions. To further explore this issue, we introduce Position-wise Question Answering (PQA), a meticulously designed task to quantify reasoning capabilities at each position. Our analysis reveals a pronounced position bias in LVLMs: open-source models excel in reasoning with images positioned later but underperform with those in the middle or at the beginning, while proprietary models like GPT-4o show improved comprehension for images at the beginning and end but struggle with those in the middle. Motivated by these insights, we propose SoFt Attention (SoFA), a simple, training-free approach that mitigates this bias by employing linear interpolation between inter-image causal attention and bidirectional counterparts. Experimental results demonstrate that SoFA effectively reduces position bias and significantly enhances the reasoning performance of existing LVLMs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 134,
        "kimi": 53
      },
      "raw_excerpt": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models [PDF 134 ] [Copy] [Kimi 53 ] [REL] Authors : Xinyu Tian , Shu Zou , Zhaoyuan Yang , Jing Zhang The evolution of Large Vision-Language Models (LVLMs) has progressed from single-image understanding to multi-image reasoning. Despite this advancement, our findings indicate that LVLMs struggle to robustly utilize information across multiple images, with predictions significantly affected by the alteration of image positions. To further explore this issue, we introduce Position-wise Question Answering (PQA), a meticulously designed task to quantify reasoning capabilities at each position. Our analysis reveals a pronounced position bias in LVLMs: open-source models excel in reasoning with images positioned later but underperform with those in the middle or at the beginning, while proprietary models like GPT-4o show improved comprehension for images at the beginning and end but struggle with those in the middle. Motivated by these insights, we propose SoFt Attention (SoFA), a simple, training-free approach that mitigates this bias by employing linear interpolation between inter-image causal attention and bidirectional counterparts. Experimental results demonstrate that SoFA effectively reduces position bias and significantly enhances the reasoning performance of existing LVLMs. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF",
      "index": 23,
      "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
      "authors": [
        "Matt Deitke",
        "Christopher Clark",
        "Sangho Lee",
        "Rohun Tripathi",
        "Yue Yang",
        "Jae Sung Park",
        "Mohammadreza Salehi",
        "Niklas Muennighoff",
        "Kyle Lo",
        "Luca Soldaini",
        "Jiasen Lu",
        "Taira Anderson",
        "Erin Bransom",
        "Kiana Ehsani",
        "Huong Ngo",
        "YenSung Chen",
        "Ajay Patel",
        "Mark Yatskar",
        "Chris Callison-Burch",
        "Andrew Head",
        "Rose Hendrix",
        "Favyen Bastani",
        "Eli VanderBilt",
        "Nathan Lambert",
        "Yvonne Chou",
        "Arnavi Chheda",
        "Jenna Sparks",
        "Sam Skjonsberg",
        "Michael Schmitz",
        "Aaron Sarnat",
        "Byron Bischoff",
        "Pete Walsh",
        "Chris Newell",
        "Piper Wolters",
        "Tanmay Gupta",
        "Kuo-Hao Zeng",
        "Jon Borchardt",
        "Dirk Groeneveld",
        "Crystal Nam",
        "Sophie Lebrecht",
        "Caitlin Wittlif",
        "Carissa Schoenick",
        "Oscar Michel",
        "Ranjay Krishna",
        "Luca Weihs",
        "Noah A. Smith",
        "Hannaneh Hajishirzi",
        "Ross Girshick",
        "Ali Farhadi",
        "Aniruddha Kembhavi"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "vlms",
        "molmo",
        "pixmo",
        "open",
        "proprietary",
        "textbf",
        "weights",
        "vision",
        "datasets",
        "dataset"
      ],
      "summary": "Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present \\textbf{Molmo}, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets, including a dataset of highly detailed image captions for pre-training called \\textbf{PixMo}, a free-form image Q\\&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code will all be released.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 126,
        "kimi": 60
      },
      "raw_excerpt": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models [PDF 126 ] [Copy] [Kimi 60 ] [REL] Authors : Matt Deitke , Christopher Clark , Sangho Lee , Rohun Tripathi , Yue Yang , Jae Sung Park , Mohammadreza Salehi , Niklas Muennighoff , Kyle Lo , Luca Soldaini , Jiasen Lu , Taira Anderson , Erin Bransom , Kiana Ehsani , Huong Ngo , YenSung Chen , Ajay Patel , Mark Yatskar , Chris Callison-Burch , Andrew Head , Rose Hendrix , Favyen Bastani , Eli VanderBilt , Nathan Lambert , Yvonne Chou , Arnavi Chheda , Jenna Sparks , Sam Skjonsberg , Michael Schmitz , Aaron Sarnat , Byron Bischoff , Pete Walsh , Chris Newell , Piper Wolters , Tanmay Gupta , Kuo-Hao Zeng , Jon Borchardt , Dirk Groeneveld , Crystal Nam , Sophie Lebrecht , Caitlin Wittlif , Carissa Schoenick , Oscar Michel , Ranjay Krishna , Luca Weihs , Noah A. Smith , Hannaneh Hajishirzi , Ross Girshick , Ali Farhadi , Aniruddha Kembhavi et al. (20 additional authors not shown) Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present \\textbf{Molmo}, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets, including a dataset of highly detailed image captions for pre-training called \\textbf{PixMo}, a free-form image Q\\&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code will all be released. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF",
      "index": 24,
      "title": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition",
      "authors": [
        "Otto Brookes",
        "Maksim Kukushkin",
        "Majid Mirmehdi",
        "Colleen Stephens",
        "Paula Dieguez",
        "Thurston C. Hicks",
        "Sorrel Jones",
        "Kevin Lee",
        "Maureen S. McCarthy",
        "Amelia Meier",
        "Emmanuelle Normand",
        "Erin G. Wessling",
        "Roman M. Wittig",
        "Kevin Langergraber",
        "Klaus Zuberbühler",
        "Lukas Boesch",
        "Thomas Schmid",
        "Mimi Arandjelovic",
        "Hjalmar Kühl",
        "Tilo Burghardt"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "panaf",
        "chimpanzee",
        "behaviour",
        "fgbg",
        "camera",
        "wildlife",
        "backgrounds",
        "video",
        "dataset",
        "impact"
      ],
      "summary": "Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42\\% mAP for convolutional and +3.75\\% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos). The full dataset, baseline models, and weights will be available at `anonymous'.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 61,
        "kimi": 23
      },
      "raw_excerpt": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition [PDF 61 ] [Copy] [Kimi 23 ] [REL] Authors : Otto Brookes , Maksim Kukushkin , Majid Mirmehdi , Colleen Stephens , Paula Dieguez , Thurston C. Hicks , Sorrel Jones , Kevin Lee , Maureen S. McCarthy , Amelia Meier , Emmanuelle Normand , Erin G. Wessling , Roman M. Wittig , Kevin Langergraber , Klaus Zuberbühler , Lukas Boesch , Thomas Schmid , Mimi Arandjelovic , Hjalmar Kühl , Tilo Burghardt Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42\\% mAP for convolutional and +3.75\\% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos). The full dataset, baseline models, and weights will be available at `anonymous'. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF",
      "index": 25,
      "title": "Opportunistic Single-Photon Time of Flight",
      "authors": [
        "Sotiris Nousias",
        "Mian Wei",
        "Howard Xiao",
        "Maxx Wu",
        "Shahmeer Athar",
        "Kevin J. Wang",
        "Anagh Malik",
        "David A. Barmherzig",
        "David B. Lindell",
        "Kyros N. Kutulakos"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "lasers",
        "passive",
        "mhz",
        "camera",
        "pulsed",
        "opportunistic",
        "light",
        "ambient",
        "laser",
        "scene"
      ],
      "summary": "Scattered light from pulsed lasers is increasingly part of our ambient illumination, as many devices rely on them for active 3D sensing. In this work, we ask: can these “ambient” light signals be detected and leveraged for passive 3D vision? We show that pulsed lasers, despite being weak and fluctuating at MHz to GHz frequencies, leave a distinctive sinc comb pattern in the temporal frequency domain of incident flux that is specific to each laser and invariant to the scene. This enables their passive detection and analysis with a free-running SPAD camera, even when they are unknown, asynchronous, out of sight, and emitting concurrently. We show how to synchronize with such lasers computationally, characterize their pulse emissions, separate their contributions, and—if many are present—localize them in 3D and recover a depth map of the camera’s field of view. We use our camera prototype to demonstrate (1) a first-of-its-kind visualization of asynchronously propagating light pulses from multiple lasers through the same scene, (2) passive estimation of a laser’s MHz-scale pulse repetition frequency with mHz precision, and (3) mm-scale 3D imaging over room-scale distances by passively harvesting photons from two or more out-of-view lasers.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Nousias_Opportunistic_Single-Photon_Time_of_Flight_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Nousias_Opportunistic_Single-Photon_Time_of_Flight_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 68,
        "kimi": 30
      },
      "raw_excerpt": "Opportunistic Single-Photon Time of Flight [PDF 68 ] [Copy] [Kimi 30 ] [REL] Authors : Sotiris Nousias , Mian Wei , Howard Xiao , Maxx Wu , Shahmeer Athar , Kevin J. Wang , Anagh Malik , David A. Barmherzig , David B. Lindell , Kyros N. Kutulakos Scattered light from pulsed lasers is increasingly part of our ambient illumination, as many devices rely on them for active 3D sensing. In this work, we ask: can these “ambient” light signals be detected and leveraged for passive 3D vision? We show that pulsed lasers, despite being weak and fluctuating at MHz to GHz frequencies, leave a distinctive sinc comb pattern in the temporal frequency domain of incident flux that is specific to each laser and invariant to the scene. This enables their passive detection and analysis with a free-running SPAD camera, even when they are unknown, asynchronous, out of sight, and emitting concurrently. We show how to synchronize with such lasers computationally, characterize their pulse emissions, separate their contributions, and—if many are present—localize them in 3D and recover a depth map of the camera’s field of view. We use our camera prototype to demonstrate (1) a first-of-its-kind visualization of asynchronously propagating light pulses from multiple lasers through the same scene, (2) passive estimation of a laser’s MHz-scale pulse repetition frequency with mHz precision, and (3) mm-scale 3D imaging over room-scale distances by passively harvesting photons from two or more out-of-view lasers. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF",
      "index": 26,
      "title": "Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models",
      "authors": [
        "Ruofan Liang",
        "Zan Gojcic",
        "Huan Ling",
        "Jacob Munkberg",
        "Jon Hasselgren",
        "Chih-Hao Lin",
        "Jun Gao",
        "Alexander Keller",
        "Nandita Vijaykumar",
        "Sanja Fidler",
        "Zian Wang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "rendering",
        "renderer",
        "diffusion",
        "inverse",
        "video",
        "world",
        "buffers",
        "videos",
        "real",
        "lighting"
      ],
      "summary": "Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce Diffusion Renderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Specifically, we first train a video diffusion model for inverse rendering on synthetic data, which generalizes well to real-world videos and allows us to auto-label diverse real-world videos. We then co-train our rendering model using both synthetic and auto-labeled real-world data. Experiments demonstrate that Diffusion Renderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input—including relighting, material editing, and realistic object insertion.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.html",
          "/venue/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 99,
        "kimi": 38
      },
      "raw_excerpt": "Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models [PDF 99 ] [Copy] [Kimi 38 ] [REL] Authors : Ruofan Liang , Zan Gojcic , Huan Ling , Jacob Munkberg , Jon Hasselgren , Chih-Hao Lin , Jun Gao , Alexander Keller , Nandita Vijaykumar , Sanja Fidler , Zian Wang Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce Diffusion Renderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Specifically, we first train a video diffusion model for inverse rendering on synthetic data, which generalizes well to real-world videos and allows us to auto-label diverse real-world videos. We then co-train our rendering model using both synthetic and auto-labeled real-world data. Experiments demonstrate that Diffusion Renderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input—including relighting, material editing, and realistic object insertion. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF",
      "index": 27,
      "title": "Neural Inverse Rendering from Propagating Light",
      "authors": [
        "Anagh Malik",
        "Benjamin Attal",
        "Andrew Xie",
        "Matthew O'Toole",
        "David B. Lindell"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "light",
        "rendering",
        "propagating",
        "indirect",
        "captured",
        "inverse",
        "radiance",
        "neural",
        "relighting",
        "arriving"
      ],
      "summary": "We present the first system for physically based, neural inverse rendering from multi-viewpoint videos of propagating light. Our approach relies on a time-resolved extension of neural radiance caching --- a technique that accelerates inverse rendering by storing infinite-bounce radiance arriving at any point from any direction. The resulting model accurately accounts for direct and indirect light transport effects and, when applied to captured measurements from a flash lidar system, enables state-of-the-art 3D reconstruction in the presence of strong indirect light. Further, we demonstrate view synthesis of propagating light, automatic decomposition of captured measurements into direct and indirect components, as well as novel capabilities such as multi-view transient relighting of captured scenes.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Malik_Neural_Inverse_Rendering_from_Propagating_Light@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 74,
        "kimi": 28
      },
      "raw_excerpt": "Neural Inverse Rendering from Propagating Light [PDF 74 ] [Copy] [Kimi 28 ] [REL] Authors : Anagh Malik , Benjamin Attal , Andrew Xie , Matthew O'Toole , David B. Lindell We present the first system for physically based, neural inverse rendering from multi-viewpoint videos of propagating light. Our approach relies on a time-resolved extension of neural radiance caching --- a technique that accelerates inverse rendering by storing infinite-bounce radiance arriving at any point from any direction. The resulting model accurately accounts for direct and indirect light transport effects and, when applied to captured measurements from a flash lidar system, enables state-of-the-art 3D reconstruction in the presence of strong indirect light. Further, we demonstrate view synthesis of propagating light, automatic decomposition of captured measurements into direct and indirect components, as well as novel capabilities such as multi-view transient relighting of captured scenes. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF",
      "index": 28,
      "title": "OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
      "authors": [
        "Pengfei Zhou",
        "Xiaopeng Peng",
        "Jiajun Song",
        "Chuanhao Li",
        "Zhaopan Xu",
        "Yue Yang",
        "Ziyao Guo",
        "Hao Zhang",
        "Yuqi Lin",
        "Yefei He",
        "Lirui Zhao",
        "Shuo Liu",
        "Tianhua Li",
        "Yuxuan Xie",
        "Xiaojun Chang",
        "Yu Qiao",
        "Wenqi Shao",
        "Kaipeng Zhang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "interleaved",
        "generation",
        "opening",
        "intjudge",
        "multimodal",
        "ended",
        "judge",
        "benchmark",
        "text",
        "judging"
      ],
      "summary": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce OpenING, a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The benchmark, code and judge models will be released.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 63,
        "kimi": 30
      },
      "raw_excerpt": "OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation [PDF 63 ] [Copy] [Kimi 30 ] [REL] Authors : Pengfei Zhou , Xiaopeng Peng , Jiajun Song , Chuanhao Li , Zhaopan Xu , Yue Yang , Ziyao Guo , Hao Zhang , Yuqi Lin , Yefei He , Lirui Zhao , Shuo Liu , Tianhua Li , Yuxuan Xie , Xiaojun Chang , Yu Qiao , Wenqi Shao , Kaipeng Zhang Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge, which requires integrated multimodal understanding and generation abilities. While the progress in unified models offers new solutions, existing benchmarks are insufficient for evaluating these methods due to data size and diversity limitations. To bridge this gap, we introduce OpenING, a comprehensive benchmark comprising 5,400 high-quality human-annotated instances across 56 real-world tasks. OpenING covers diverse daily scenarios such as travel guide, design, and brainstorming, offering a robust platform for challenging interleaved generation methods. In addition, we present IntJudge, a judge model for evaluating open-ended multimodal generation methods. Trained with a novel data pipeline, our IntJudge achieves an agreement rate of 82. 42% with human judgments, outperforming GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that current interleaved generation methods still have substantial room for improvement. Key findings on interleaved image-text generation are further presented to guide the development of next-generation models. The benchmark, code and judge models will be released. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF",
      "index": 29,
      "title": "CustAny: Customizing Anything from A Single Example",
      "authors": [
        "Lingjie Kong",
        "Kai Wu",
        "Chengming Xu",
        "Xiaobin Hu",
        "Wenhui Han",
        "Jinlong Peng",
        "Donghao Luo",
        "Mengtian Li",
        "Jiangning Zhang",
        "Chengjie Wang",
        "Yanwei Fu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "custany",
        "customization",
        "object",
        "customizing",
        "idc",
        "anything",
        "general",
        "text",
        "dataset",
        "shot"
      ],
      "summary": "Recent advances in diffusion-based text-to-image models have simplified creating high-fidelity images, but preserving the identity (ID) of specific elements, like a personal dog, is still challenging.Object customization, using reference images and textual descriptions, is key to addressing this issue. Current object customization methods are either object-specific, requiring extensive fine-tuning, or object-agnostic, offering zero-shot customization but limited to specialized domains. The primary issue of promoting zero-shot object customization from specific domains to the general domain is to establish a large-scale general ID dataset for model pre-training, which is time-consuming and labor-intensive. In this paper, we propose a novel pipeline to construct a large dataset of general objects and build the Multi-Category ID-Consistent (MC-IDC) dataset, featuring 315k text-image samples across 10k categories. With the help of MC-IDC, we introduce Customizing Anything (CustAny), a zero-shot framework that maintains ID fidelity and supports flexible text editing for general objects. CustAny features three key components: a general ID extraction module, a dual-level ID injection module, and an ID-aware decoupling module, allowing it to customize any object from a single reference image and text prompt. Experiments demonstrate that CustAny outperforms existing methods in both general object customization and specialized domains like human customization and virtual try-on. Our contributions include a large-scale dataset, the CustAny framework and novel ID processing to advance this field.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_CustAny_Customizing_Anything_from_A_Single_Example_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Kong_CustAny_Customizing_Anything_from_A_Single_Example@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Kong_CustAny_Customizing_Anything_from_A_Single_Example_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 100,
        "kimi": 30
      },
      "raw_excerpt": "CustAny: Customizing Anything from A Single Example [PDF 100 ] [Copy] [Kimi 30 ] [REL] Authors : Lingjie Kong , Kai Wu , Chengming Xu , Xiaobin Hu , Wenhui Han , Jinlong Peng , Donghao Luo , Mengtian Li , Jiangning Zhang , Chengjie Wang , Yanwei Fu Recent advances in diffusion-based text-to-image models have simplified creating high-fidelity images, but preserving the identity (ID) of specific elements, like a personal dog, is still challenging.Object customization, using reference images and textual descriptions, is key to addressing this issue. Current object customization methods are either object-specific, requiring extensive fine-tuning, or object-agnostic, offering zero-shot customization but limited to specialized domains. The primary issue of promoting zero-shot object customization from specific domains to the general domain is to establish a large-scale general ID dataset for model pre-training, which is time-consuming and labor-intensive. In this paper, we propose a novel pipeline to construct a large dataset of general objects and build the Multi-Category ID-Consistent (MC-IDC) dataset, featuring 315k text-image samples across 10k categories. With the help of MC-IDC, we introduce Customizing Anything (CustAny), a zero-shot framework that maintains ID fidelity and supports flexible text editing for general objects. CustAny features three key components: a general ID extraction module, a dual-level ID injection module, and an ID-aware decoupling module, allowing it to customize any object from a single reference image and text prompt. Experiments demonstrate that CustAny outperforms existing methods in both general object customization and specialized domains like human customization and virtual try-on. Our contributions include a large-scale dataset, the CustAny framework and novel ID processing to advance this field. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF",
      "index": 30,
      "title": "Birth and Death of a Rose",
      "authors": [
        "Chen Geng",
        "Yunzhi Zhang",
        "Shangzhe Wu",
        "Jiajun Wu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "intrinsics",
        "rose",
        "temporal",
        "object",
        "blooming",
        "birth",
        "death",
        "lifespan",
        "reflectance",
        "animation"
      ],
      "summary": "We study the problem of generating temporal object intrinsics—temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose—from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pretrained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Birth_and_Death_of_a_Rose_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Geng_Birth_and_Death_of_a_Rose@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Geng_Birth_and_Death_of_a_Rose_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 164,
        "kimi": 67
      },
      "raw_excerpt": "Birth and Death of a Rose [PDF 164 ] [Copy] [Kimi 67 ] [REL] Authors : Chen Geng , Yunzhi Zhang , Shangzhe Wu , Jiajun Wu We study the problem of generating temporal object intrinsics—temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose—from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pretrained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF",
      "index": 31,
      "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
      "authors": [
        "Ryan Burgert",
        "Yuancheng Xu",
        "Wenqi Xian",
        "Oliver Pilarski",
        "Pascal Clausen",
        "Mingming He",
        "Li Ma",
        "Yitong Deng",
        "Lingxiao Li",
        "Mohsen Mousavi",
        "Michael Ryoo",
        "Paul Debevec",
        "Ning Yu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "motion",
        "control",
        "video",
        "noise",
        "warped",
        "temporal",
        "controllable",
        "diffusion",
        "warping",
        "gaussianity"
      ],
      "summary": "Generative modeling aims to transform chaotic noise into structured outputs that align with training data distributions. In this work, we enhance video diffusion generative models by introducing motion control as a structured component within latent space sampling. Specifically, we propose a novel real-time noise warping method that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, enabling fine-grained motion control independent of model architecture and guidance type. We fine-tune modern video diffusion base models and provide a unified paradigm for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. By leveraging a real-time noise-warping algorithm that preserves spatial Gaussianity while efficiently maintaining temporal consistency, we enable flexible and diverse motion control applications with minimal trade-offs in pixel quality and temporal coherence. Extensive experiments and user studies demonstrate the advantages of our method in terms of visual quality, motion controllability, and temporal consistency, making it a robust and scalable solution for motion-controllable video synthesis.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.html",
          "/venue/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 81,
        "kimi": 27
      },
      "raw_excerpt": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise [PDF 81 ] [Copy] [Kimi 27 ] [REL] Authors : Ryan Burgert , Yuancheng Xu , Wenqi Xian , Oliver Pilarski , Pascal Clausen , Mingming He , Li Ma , Yitong Deng , Lingxiao Li , Mohsen Mousavi , Michael Ryoo , Paul Debevec , Ning Yu Generative modeling aims to transform chaotic noise into structured outputs that align with training data distributions. In this work, we enhance video diffusion generative models by introducing motion control as a structured component within latent space sampling. Specifically, we propose a novel real-time noise warping method that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, enabling fine-grained motion control independent of model architecture and guidance type. We fine-tune modern video diffusion base models and provide a unified paradigm for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. By leveraging a real-time noise-warping algorithm that preserves spatial Gaussianity while efficiently maintaining temporal consistency, we enable flexible and diverse motion control applications with minimal trade-offs in pixel quality and temporal coherence. Extensive experiments and user studies demonstrate the advantages of our method in terms of visual quality, motion controllability, and temporal consistency, making it a robust and scalable solution for motion-controllable video synthesis. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF",
      "index": 32,
      "title": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models",
      "authors": [
        "Andreas Müller",
        "Denis Lukovnikov",
        "Jonas Thietke",
        "Asja Fischer",
        "Erwin Quiring"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "watermarks",
        "watermark",
        "forgery",
        "attacks",
        "latent",
        "watermarked",
        "semantic",
        "watermarking",
        "attackers",
        "unrelated"
      ],
      "summary": "Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 62,
        "kimi": 36
      },
      "raw_excerpt": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models [PDF 62 ] [Copy] [Kimi 36 ] [REL] Authors : Andreas Müller , Denis Lukovnikov , Jonas Thietke , Asja Fischer , Erwin Quiring Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF",
      "index": 33,
      "title": "Continuous 3D Perception Model with Persistent State",
      "authors": [
        "Qianqian Wang",
        "Yifei Zhang",
        "Aleksander Holynski",
        "Alexei A. Efros",
        "Angjoo Kanazawa"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "pointmaps",
        "state",
        "online",
        "reconstruction",
        "raymap",
        "video",
        "perception",
        "persistent",
        "stateful",
        "accepting"
      ],
      "summary": "We propose a novel unified framework capable of solving a broad range of 3D tasks. At the core of our approach is an online stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, our method leverages the evolving state to generate metric-scale pointmaps for each input in an online manner. These pointmaps reside within a common coordinate system, accumulating into a coherent 3D scene reconstruction. Our model captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen structures beyond the coverage of the input images through a raymap probe. Our method is simple yet highly flexible, naturally accepting varying lengths of image sequences and working seamlessly with both video streams and unordered photo collections. We evaluate our method on various 3D/4D tasks including monocular/video depth estimation, camera estimation, multi-view reconstruction, and achieve competitive or state-of-the-art performance. Additionally, we showcase intriguing behaviors enabled by our state representation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Continuous_3D_Perception_Model_with_Persistent_State_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Continuous_3D_Perception_Model_with_Persistent_State@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Continuous_3D_Perception_Model_with_Persistent_State_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 73,
        "kimi": 32
      },
      "raw_excerpt": "Continuous 3D Perception Model with Persistent State [PDF 73 ] [Copy] [Kimi 32 ] [REL] Authors : Qianqian Wang , Yifei Zhang , Aleksander Holynski , Alexei A. Efros , Angjoo Kanazawa We propose a novel unified framework capable of solving a broad range of 3D tasks. At the core of our approach is an online stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, our method leverages the evolving state to generate metric-scale pointmaps for each input in an online manner. These pointmaps reside within a common coordinate system, accumulating into a coherent 3D scene reconstruction. Our model captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen structures beyond the coverage of the input images through a raymap probe. Our method is simple yet highly flexible, naturally accepting varying lengths of image sequences and working seamlessly with both video streams and unordered photo collections. We evaluate our method on various 3D/4D tasks including monocular/video depth estimation, camera estimation, multi-view reconstruction, and achieve competitive or state-of-the-art performance. Additionally, we showcase intriguing behaviors enabled by our state representation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF",
      "index": 34,
      "title": "AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea",
      "authors": [
        "Qifan Yu",
        "Wei Chow",
        "Zhongqi Yue",
        "Kaihang Pan",
        "Yang Wu",
        "Xiaoyang Wan",
        "Juncheng Li",
        "Siliang Tang",
        "Hanwang Zhang",
        "Yueting Zhuang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "editing",
        "anyedit",
        "quality",
        "instruction",
        "image",
        "instructions",
        "mastering",
        "unified",
        "comprehensive",
        "diversity"
      ],
      "summary": "Instruction-based image editing aims to modify specific image elements with natural language instructions. However, current models in this domain often struggle to accurately execute complex user instructions, as they are trained on low-quality data with limited editing types. We present AnyEdit, a comprehensive multi-modal instruction editing dataset, comprising 2.5 million high-quality editing pairs spanning over 20 editing types and five domains. We ensure the diversity and quality of the AnyEdit collection through three aspects: initial data diversity, adaptive editing process, and automated selection of editing results. Using the dataset, we further train a novel AnyEdit Stable Diffusion with task-aware routing and learnable task embedding for unified image editing. Comprehensive experiments on three benchmark datasets show that AnyEdit consistently boosts the performance of diffusion-based editing models. This presents prospects for developing instruction-driven image editing models that support human creativity. The code is available in \\url{https://anonymous.4open.science/r/AnyEdit-C53B}.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 106,
        "kimi": 41
      },
      "raw_excerpt": "AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea [PDF 106 ] [Copy] [Kimi 41 ] [REL] Authors : Qifan Yu , Wei Chow , Zhongqi Yue , Kaihang Pan , Yang Wu , Xiaoyang Wan , Juncheng Li , Siliang Tang , Hanwang Zhang , Yueting Zhuang Instruction-based image editing aims to modify specific image elements with natural language instructions. However, current models in this domain often struggle to accurately execute complex user instructions, as they are trained on low-quality data with limited editing types. We present AnyEdit, a comprehensive multi-modal instruction editing dataset, comprising 2.5 million high-quality editing pairs spanning over 20 editing types and five domains. We ensure the diversity and quality of the AnyEdit collection through three aspects: initial data diversity, adaptive editing process, and automated selection of editing results. Using the dataset, we further train a novel AnyEdit Stable Diffusion with task-aware routing and learnable task embedding for unified image editing. Comprehensive experiments on three benchmark datasets show that AnyEdit consistently boosts the performance of diffusion-based editing models. This presents prospects for developing instruction-driven image editing models that support human creativity. The code is available in \\url{https://anonymous.4open.science/r/AnyEdit-C53B}. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF",
      "index": 35,
      "title": "LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions",
      "authors": [
        "Faridoun Mehri",
        "Mahdieh Soleymani Baghshah",
        "Mohammad Taher Pilehvar"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "libragrad",
        "gradient",
        "imbalances",
        "universally",
        "faithfulness",
        "fullgrad",
        "attribution",
        "attributions",
        "transformer",
        "struggle"
      ],
      "summary": "Why do gradient-based explanations struggle with Transformers, and how can we improve them? We identify gradient flow imbalances in Transformers that violate FullGrad-completeness, a critical property for attribution faithfulness that CNNs naturally possess. To address this issue, we introduce LibraGrad—a theoretically grounded post-hoc approach that corrects gradient imbalances through pruning and scaling of backward paths, without changing the forward pass or adding computational overhead. We evaluate LibraGrad using three metric families: Faithfulness, which quantifies prediction changes under perturbations of the most and least relevant features; Completeness Error, which measures attribution conservation relative to model outputs; and Segmentation AP, which assesses alignment with human perception. Extensive experiments across 8 architectures, 4 model sizes, and 4 datasets show that LibraGrad universally enhances gradient-based methods, outperforming existing white-box methods—including Transformer-specific approaches—across all metrics. We demonstrate superior qualitative results through two complementary evaluations: precise text-prompted region highlighting on CLIP models and accurate class discrimination between co-occurring animals on ImageNet-finetuned models—two settings on which existing methods often struggle. LibraGrad is effective even on the attention-free MLP-Mixer architecture, indicating potential for extension to other modern architectures. Our code is freely available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 48,
        "kimi": 29
      },
      "raw_excerpt": "LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions [PDF 48 ] [Copy] [Kimi 29 ] [REL] Authors : Faridoun Mehri , Mahdieh Soleymani Baghshah , Mohammad Taher Pilehvar Why do gradient-based explanations struggle with Transformers, and how can we improve them? We identify gradient flow imbalances in Transformers that violate FullGrad-completeness, a critical property for attribution faithfulness that CNNs naturally possess. To address this issue, we introduce LibraGrad—a theoretically grounded post-hoc approach that corrects gradient imbalances through pruning and scaling of backward paths, without changing the forward pass or adding computational overhead. We evaluate LibraGrad using three metric families: Faithfulness, which quantifies prediction changes under perturbations of the most and least relevant features; Completeness Error, which measures attribution conservation relative to model outputs; and Segmentation AP, which assesses alignment with human perception. Extensive experiments across 8 architectures, 4 model sizes, and 4 datasets show that LibraGrad universally enhances gradient-based methods, outperforming existing white-box methods—including Transformer-specific approaches—across all metrics. We demonstrate superior qualitative results through two complementary evaluations: precise text-prompted region highlighting on CLIP models and accurate class discrimination between co-occurring animals on ImageNet-finetuned models—two settings on which existing methods often struggle. LibraGrad is effective even on the attention-free MLP-Mixer architecture, indicating potential for extension to other modern architectures. Our code is freely available. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF",
      "index": 36,
      "title": "3D Student Splatting and Scooping",
      "authors": [
        "Jialin Zhu",
        "Jiangbei Yue",
        "Feixiang He",
        "He Wang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "splatting",
        "scooping",
        "3dgs",
        "sss",
        "student",
        "new",
        "mixture",
        "spiked",
        "unnormalized",
        "component"
      ],
      "summary": "Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_3D_Student_Splatting_and_Scooping_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhu_3D_Student_Splatting_and_Scooping@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_3D_Student_Splatting_and_Scooping_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 99,
        "kimi": 40
      },
      "raw_excerpt": "3D Student Splatting and Scooping [PDF 99 ] [Copy] [Kimi 40 ] [REL] Authors : Jialin Zhu , Jiangbei Yue , Feixiang He , He Wang Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF",
      "index": 37,
      "title": "Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing",
      "authors": [
        "Bingliang Zhang",
        "Wenda Chu",
        "Julius Berner",
        "Chenlin Meng",
        "Anima Anandkumar",
        "Yang Song"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "annealing",
        "daps",
        "inverse",
        "diffusion",
        "decoupled",
        "sampling",
        "noise",
        "improving",
        "complicated",
        "posterior"
      ],
      "summary": "Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 80,
        "kimi": 31
      },
      "raw_excerpt": "Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing [PDF 80 ] [Copy] [Kimi 31 ] [REL] Authors : Bingliang Zhang , Wenda Chu , Julius Berner , Chenlin Meng , Anima Anandkumar , Yang Song Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF",
      "index": 38,
      "title": "CleanDIFT: Diffusion Features without Noise",
      "authors": [
        "Nick Stracke",
        "Stefan Andreas Baumann",
        "Kolja Bauer",
        "Frank Fundel",
        "Björn Ommer"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "features",
        "noise",
        "diffusion",
        "cleandift",
        "semantic",
        "wide",
        "downstream",
        "remedied",
        "tasks",
        "images"
      ],
      "summary": "Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Stracke_CleanDIFT_Diffusion_Features_without_Noise_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Stracke_CleanDIFT_Diffusion_Features_without_Noise@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Stracke_CleanDIFT_Diffusion_Features_without_Noise_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 101,
        "kimi": 33
      },
      "raw_excerpt": "CleanDIFT: Diffusion Features without Noise [PDF 101 ] [Copy] [Kimi 33 ] [REL] Authors : Nick Stracke , Stefan Andreas Baumann , Kolja Bauer , Frank Fundel , Björn Ommer Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF",
      "index": 39,
      "title": "CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models",
      "authors": [
        "Felix Taubner",
        "Ruihang Zhang",
        "Mathieu Tuli",
        "David B. Lindell"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "portrait",
        "avatars",
        "cap4d",
        "avatar",
        "morphable",
        "view",
        "reconstruction",
        "reference",
        "animate",
        "multi"
      ],
      "summary": "Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints — for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 52,
        "kimi": 19
      },
      "raw_excerpt": "CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models [PDF 52 ] [Copy] [Kimi 19 ] [REL] Authors : Felix Taubner , Ruihang Zhang , Mathieu Tuli , David B. Lindell Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints — for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF",
      "index": 40,
      "title": "IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior",
      "authors": [
        "Jingyi Xu",
        "Siwei Tu",
        "Weidong Yang",
        "Ben Fei",
        "Shuhao Li",
        "Keyi Liu",
        "Yeqi Luo",
        "Lipeng Ma",
        "Lei Bai"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "sea",
        "icediff",
        "ice",
        "forecasting",
        "arctic",
        "25km",
        "resolution",
        "concentration",
        "finer",
        "utilized"
      ],
      "summary": "Variation of Arctic sea ice has significant impacts on polar ecosystems, transporting routes, coastal communities, and global climate. Tracing the change of sea ice at a finer scale is paramount for both operational applications and scientific studies. Recent pan-Arctic sea ice forecasting methods that leverage advances in artificial intelligence have made promising progress over numerical models. However, forecasting sea ice at higher resolutions is still under-explored. To bridge the gap, we propose a two-module cooperative deep learning framework, IceDiff, to forecast sea ice concentration at finer scales. IceDiff first leverages a vision transformer to generate coarse yet superior forecasting results over previous methods at a regular 25km grid. This high-quality sea ice forecasting can be utilized as reliable guidance for the next module. Subsequently, an unconditional diffusion model pre-trained on low-resolution sea ice concentration maps is utilized for sampling down-scaled sea ice forecasting via a zero-shot guided sampling strategy and a patch-based method. For the first time, IceDiff demonstrates sea ice forecasting with a 6.25km resolution. IceDiff extends the boundary of existing sea ice forecasting models and more importantly, its capability to generate high-resolution sea ice concentration data is vital for pragmatic usages and research.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 54,
        "kimi": 25
      },
      "raw_excerpt": "IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior [PDF 54 ] [Copy] [Kimi 25 ] [REL] Authors : Jingyi Xu , Siwei Tu , Weidong Yang , Ben Fei , Shuhao Li , Keyi Liu , Yeqi Luo , Lipeng Ma , Lei Bai Variation of Arctic sea ice has significant impacts on polar ecosystems, transporting routes, coastal communities, and global climate. Tracing the change of sea ice at a finer scale is paramount for both operational applications and scientific studies. Recent pan-Arctic sea ice forecasting methods that leverage advances in artificial intelligence have made promising progress over numerical models. However, forecasting sea ice at higher resolutions is still under-explored. To bridge the gap, we propose a two-module cooperative deep learning framework, IceDiff, to forecast sea ice concentration at finer scales. IceDiff first leverages a vision transformer to generate coarse yet superior forecasting results over previous methods at a regular 25km grid. This high-quality sea ice forecasting can be utilized as reliable guidance for the next module. Subsequently, an unconditional diffusion model pre-trained on low-resolution sea ice concentration maps is utilized for sampling down-scaled sea ice forecasting via a zero-shot guided sampling strategy and a patch-based method. For the first time, IceDiff demonstrates sea ice forecasting with a 6.25km resolution. IceDiff extends the boundary of existing sea ice forecasting models and more importantly, its capability to generate high-resolution sea ice concentration data is vital for pragmatic usages and research. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF",
      "index": 41,
      "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition",
      "authors": [
        "SuBeen Lee",
        "WonJun Moon",
        "Hyun Seok Seong",
        "Jae-Pil Heo"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "alignment",
        "video",
        "fsar",
        "team",
        "matching",
        "temporal",
        "action",
        "shot",
        "units",
        "flexibility"
      ],
      "summary": "Few-Shot Action Recognition (FSAR) aims to train a model with only a few labeled video instances.A key challenge in FSAR is handling divergent narrative trajectories for precise video matching.While the frame- and tuple-level alignment approaches have been promising, their methods heavily rely on pre-defined and length-dependent alignment units (e.g., frames or tuples), which limits flexibility for actions of varying lengths and speeds. In this work, we introduce a novel TEmporal Alignment-free Matching (TEAM) approach, which eliminates the need for temporal units in action representation and brute-force alignment during matching.Specifically, TEAM represents each video with a fixed set of pattern tokens that capture globally discriminative clues within the video instance regardless of action length or speed, ensuring its flexibility.Furthermore, TEAM is inherently efficient, using token-wise comparisons to measure similarity between videos, unlike existing methods that rely on pairwise comparisons for temporal alignment. Additionally, we propose an adaptation process that identifies and removes common information across novel classes, establishing clear boundaries even between novel categories. Extensive experiments demonstrate the effectiveness of TEAM.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.html",
          "/venue/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 69,
        "kimi": 25
      },
      "raw_excerpt": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition [PDF 69 ] [Copy] [Kimi 25 ] [REL] Authors : SuBeen Lee , WonJun Moon , Hyun Seok Seong , Jae-Pil Heo Few-Shot Action Recognition (FSAR) aims to train a model with only a few labeled video instances.A key challenge in FSAR is handling divergent narrative trajectories for precise video matching.While the frame- and tuple-level alignment approaches have been promising, their methods heavily rely on pre-defined and length-dependent alignment units (e.g., frames or tuples), which limits flexibility for actions of varying lengths and speeds. In this work, we introduce a novel TEmporal Alignment-free Matching (TEAM) approach, which eliminates the need for temporal units in action representation and brute-force alignment during matching.Specifically, TEAM represents each video with a fixed set of pattern tokens that capture globally discriminative clues within the video instance regardless of action length or speed, ensuring its flexibility.Furthermore, TEAM is inherently efficient, using token-wise comparisons to measure similarity between videos, unlike existing methods that rely on pairwise comparisons for temporal alignment. Additionally, we propose an adaptation process that identifies and removes common information across novel classes, establishing clear boundaries even between novel categories. Extensive experiments demonstrate the effectiveness of TEAM. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF",
      "index": 42,
      "title": "Gromov-Wasserstein Problem with Cyclic Symmetry",
      "authors": [
        "Shoichiro Takeda",
        "Yasunori Akagi"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "cyclic",
        "symmetry",
        "algorithms",
        "gromov",
        "wasserstein",
        "gradient",
        "registration",
        "problem",
        "iteration",
        "matching"
      ],
      "summary": "We propose novel fast algorithms for the Gromov–Wasserstein problem (GW) using cyclic symmetry of input data. Such GW with cyclic symmetry naturally appears as an object matching task underlying various real-world computer vision applications, e.g., image registration, point cloud registration, stereo matching, and 3D reconstruction. Gradient-based algorithms have been used to solve GW, and our main idea is to use the following remarkable and non-trivial property: By setting the initial solution to have cyclic symmetry, all intermediate solutions and matrices appearing in the gradient-based algorithms have the same cyclic symmetry until convergence. Based on this property, our gradient-based algorithms restrict the solution space to have cyclic symmetry and update only one of the symmetric parts of solutions and matrices at each iteration, which results in fast computation. Furthermore, the original gradient-based algorithms and ours must solve the Optimal Transport problem (OT) at each iteration, but only in ours does this problem exhibit cyclic symmetry. This cyclic OT can be solved efficiently, and as a result, the total computational time of our algorithms is dramatically faster than the original ones. Experiments showed the effectiveness of our algorithms in synthetic and real-world data with strict and approximate cyclic symmetry, respectively.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 12
      },
      "raw_excerpt": "Gromov-Wasserstein Problem with Cyclic Symmetry [PDF 33 ] [Copy] [Kimi 12 ] [REL] Authors : Shoichiro Takeda , Yasunori Akagi We propose novel fast algorithms for the Gromov–Wasserstein problem (GW) using cyclic symmetry of input data. Such GW with cyclic symmetry naturally appears as an object matching task underlying various real-world computer vision applications, e.g., image registration, point cloud registration, stereo matching, and 3D reconstruction. Gradient-based algorithms have been used to solve GW, and our main idea is to use the following remarkable and non-trivial property: By setting the initial solution to have cyclic symmetry, all intermediate solutions and matrices appearing in the gradient-based algorithms have the same cyclic symmetry until convergence. Based on this property, our gradient-based algorithms restrict the solution space to have cyclic symmetry and update only one of the symmetric parts of solutions and matrices at each iteration, which results in fast computation. Furthermore, the original gradient-based algorithms and ours must solve the Optimal Transport problem (OT) at each iteration, but only in ours does this problem exhibit cyclic symmetry. This cyclic OT can be solved efficiently, and as a result, the total computational time of our algorithms is dramatically faster than the original ones. Experiments showed the effectiveness of our algorithms in synthetic and real-world data with strict and approximate cyclic symmetry, respectively. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF",
      "index": 43,
      "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
      "authors": [
        "Jian Han",
        "Jinlai Liu",
        "Yi Jiang",
        "Bin Yan",
        "Yuqi Zhang",
        "Zehuan Yuan",
        "Bingyue Peng",
        "Xiaobing Liu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "bitwise",
        "infinity",
        "autoregressive",
        "1024",
        "vocabulary",
        "sd3",
        "image",
        "refactors",
        "visual",
        "unleashes"
      ],
      "summary": "We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity refactors visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary classifier and bitwise self-correction mechanism. By theoretically expanding the tokenizer vocabulary size to infinity in Transformer, our method significantly unleashes powerful scaling capabilities to infinity compared to vanilla VAR. Extensive experiments indicate Infinity outperforms AutoRegressive Text-to-Image models by large margins, matches or surpasses leading diffusion models. Without extra optimization, Infinity generates a 1024 × × 1024 image in 0.8s, 2.6 × × faster than SD3-Medium, making it the fastest Text-to-Image model. Models and codes will be released to promote the further exploration of Infinity for visual generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 62,
        "kimi": 27
      },
      "raw_excerpt": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis [PDF 62 ] [Copy] [Kimi 27 ] [REL] Authors : Jian Han , Jinlai Liu , Yi Jiang , Bin Yan , Yuqi Zhang , Zehuan Yuan , Bingyue Peng , Xiaobing Liu We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity refactors visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary classifier and bitwise self-correction mechanism. By theoretically expanding the tokenizer vocabulary size to infinity in Transformer, our method significantly unleashes powerful scaling capabilities to infinity compared to vanilla VAR. Extensive experiments indicate Infinity outperforms AutoRegressive Text-to-Image models by large margins, matches or surpasses leading diffusion models. Without extra optimization, Infinity generates a 1024 × × 1024 image in 0.8s, 2.6 × × faster than SD3-Medium, making it the fastest Text-to-Image model. Models and codes will be released to promote the further exploration of Infinity for visual generation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF",
      "index": 44,
      "title": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds",
      "authors": [
        "Zhenggang Tang",
        "Yuchen Fan",
        "Dilin Wang",
        "Hongyu Xu",
        "Rakesh Ranjan",
        "Alexander Schwing",
        "Zhicheng Yan"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "dust3r",
        "view",
        "views",
        "reconstruction",
        "reference",
        "pointmaps",
        "mast3r",
        "stage",
        "scene",
        "sparse"
      ],
      "summary": "Recent sparse view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 59,
        "kimi": 22
      },
      "raw_excerpt": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds [PDF 59 ] [Copy] [Kimi 22 ] [REL] Authors : Zhenggang Tang , Yuchen Fan , Dilin Wang , Hongyu Xu , Rakesh Ranjan , Alexander Schwing , Zhicheng Yan Recent sparse view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF",
      "index": 45,
      "title": "Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content",
      "authors": [
        "Zicheng Zhang",
        "Tengchuan Kou",
        "Shushi Wang",
        "Chunyi Li",
        "Wei Sun",
        "Wei Wang",
        "Xiaoyu Li",
        "Zongyu Wang",
        "Xuezhi Cao",
        "Xiongkuo Min",
        "Xiaohong Liu",
        "Guangtao Zhai"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "eval",
        "100k",
        "alignment",
        "quality",
        "text",
        "visual",
        "evaluating",
        "vision",
        "content",
        "human"
      ],
      "summary": "Evaluating text-to-vision content hinges on two crucial aspects: **visual quality** and **alignment**. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to **Scaling Law**, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models.Therefore, we introduce a comprehensive dataset designed to **E**valuate **V**isual quality and **A**lignment **L**evel for text-to-vision content (**Q-EVAL-100K**), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects.The **Q-EVAL-100K** dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose **Q-Eval-Score**, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment.Experimental results indicate that the proposed **Q-Eval-Score** achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the **Q-EVAL-100K** dataset. **The data and code will be released** to help promote the generation models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 48,
        "kimi": 18
      },
      "raw_excerpt": "Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content [PDF 48 ] [Copy] [Kimi 18 ] [REL] Authors : Zicheng Zhang , Tengchuan Kou , Shushi Wang , Chunyi Li , Wei Sun , Wei Wang , Xiaoyu Li , Zongyu Wang , Xuezhi Cao , Xiongkuo Min , Xiaohong Liu , Guangtao Zhai Evaluating text-to-vision content hinges on two crucial aspects: **visual quality** and **alignment**. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to **Scaling Law**, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models.Therefore, we introduce a comprehensive dataset designed to **E**valuate **V**isual quality and **A**lignment **L**evel for text-to-vision content (**Q-EVAL-100K**), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects.The **Q-EVAL-100K** dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose **Q-Eval-Score**, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment.Experimental results indicate that the proposed **Q-Eval-Score** achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the **Q-EVAL-100K** dataset. **The data and code will be released** to help promote the generation models. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF",
      "index": 46,
      "title": "MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision",
      "authors": [
        "Ruicheng Wang",
        "Sicheng Xu",
        "Cassie Dai",
        "Jianfeng Xiang",
        "Yu Deng",
        "Xin Tong",
        "Jiaolong Yang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "geometry",
        "moge",
        "supervision",
        "monocular",
        "unlocking",
        "map",
        "global",
        "accurate",
        "estimation",
        "open"
      ],
      "summary": "We present MoGe, a powerful model for recovering 3D geometry from monocular open-domain images. Given a single image, our model directly predicts a 3D point map of the captured scene with an affine-invariant representation, which is agnostic to true global scale and shift. This new representation precludes ambiguous supervision in training and facilitates effective geometry learning. Furthermore, we propose a set of novel global and local geometry supervision techniques that empower the model to learn high-quality geometry. These include a robust, optimal, and efficient point cloud alignment solver for accurate global shape learning, and a multi-scale local geometry loss promoting precise local geometry supervision. We train our model on a large, mixed dataset and demonstrate its strong generalizability and high accuracy. In our comprehensive evaluation on diverse unseen datasets, our model significantly outperforms state-of-the-art methods across all tasks, including monocular estimation of 3D point map, depth map, and camera field of view.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 51,
        "kimi": 21
      },
      "raw_excerpt": "MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision [PDF 51 ] [Copy] [Kimi 21 ] [REL] Authors : Ruicheng Wang , Sicheng Xu , Cassie Dai , Jianfeng Xiang , Yu Deng , Xin Tong , Jiaolong Yang We present MoGe, a powerful model for recovering 3D geometry from monocular open-domain images. Given a single image, our model directly predicts a 3D point map of the captured scene with an affine-invariant representation, which is agnostic to true global scale and shift. This new representation precludes ambiguous supervision in training and facilitates effective geometry learning. Furthermore, we propose a set of novel global and local geometry supervision techniques that empower the model to learn high-quality geometry. These include a robust, optimal, and efficient point cloud alignment solver for accurate global shape learning, and a multi-scale local geometry loss promoting precise local geometry supervision. We train our model on a large, mixed dataset and demonstrate its strong generalizability and high accuracy. In our comprehensive evaluation on diverse unseen datasets, our model significantly outperforms state-of-the-art methods across all tasks, including monocular estimation of 3D point map, depth map, and camera field of view. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF",
      "index": 47,
      "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields",
      "authors": [
        "Runfeng Li",
        "Mikhail Okunev",
        "Zixuan Guo",
        "Anh Ha Duong",
        "Christian Richardt",
        "Matthew O'Toole",
        "James Tompkin"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "gaussians",
        "flight",
        "radiance",
        "dynamic",
        "depth",
        "underappreciated",
        "indirectly",
        "reconstruction",
        "scene",
        "constrained"
      ],
      "summary": "We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight cameras using raw sensor samples that is as accurate as past methods and is 100 × × faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. Recent 3D Gaussian splatting methods often depend on multi-view data to produce satisfactory results and are brittle in their optimizations otherwise.In time-of-flight radiance field reconstruction, the property of interest---depth---is not directly optimized, causing additional challenges.We describe how these problems have a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussians.Then, we incorporate two heuristics into our optimization to improve the accuracy of scene geometry for under-constrained time-of-flight Gaussians.Experimental results show that our approach produces accurate reconstructions under constrained sensing conditions, including for fast motions like swinging baseball bats.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 39,
        "kimi": 18
      },
      "raw_excerpt": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields [PDF 39 ] [Copy] [Kimi 18 ] [REL] Authors : Runfeng Li , Mikhail Okunev , Zixuan Guo , Anh Ha Duong , Christian Richardt , Matthew O'Toole , James Tompkin We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight cameras using raw sensor samples that is as accurate as past methods and is 100 × × faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. Recent 3D Gaussian splatting methods often depend on multi-view data to produce satisfactory results and are brittle in their optimizations otherwise.In time-of-flight radiance field reconstruction, the property of interest---depth---is not directly optimized, causing additional challenges.We describe how these problems have a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussians.Then, we incorporate two heuristics into our optimization to improve the accuracy of scene geometry for under-constrained time-of-flight Gaussians.Experimental results show that our approach produces accurate reconstructions under constrained sensing conditions, including for fast motions like swinging baseball bats. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF",
      "index": 48,
      "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning",
      "authors": [
        "Kunyu Wang",
        "Xueyang Fu",
        "Xin Lu",
        "Chengjie Ge",
        "Chengzhi Cao",
        "Wei Zhai",
        "Zheng-Jun Zha"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "ctta",
        "pruning",
        "sensitivity",
        "domain",
        "channels",
        "guided",
        "channel",
        "test",
        "adaptive",
        "object"
      ],
      "summary": "Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 76,
        "kimi": 34
      },
      "raw_excerpt": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning [PDF 76 ] [Copy] [Kimi 34 ] [REL] Authors : Kunyu Wang , Xueyang Fu , Xin Lu , Chengjie Ge , Chengzhi Cao , Wei Zhai , Zheng-Jun Zha Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF",
      "index": 49,
      "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
      "authors": [
        "Songhao Han",
        "Wei Huang",
        "Hairong Shi",
        "Le Zhuo",
        "Xiu Su",
        "Shifeng Zhang",
        "Xu Zhou",
        "Xiaojuan Qi",
        "Yue Liao",
        "Si Liu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "reasoning",
        "video",
        "lvlms",
        "videoqa",
        "videoespresso",
        "frame",
        "multimodal",
        "annotations",
        "pairs",
        "cot"
      ],
      "summary": "The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html",
          "/venue/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 60,
        "kimi": 28
      },
      "raw_excerpt": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection [PDF 60 ] [Copy] [Kimi 28 ] [REL] Authors : Songhao Han , Wei Huang , Hairong Shi , Le Zhuo , Xiu Su , Shifeng Zhang , Xu Zhou , Xiaojuan Qi , Yue Liao , Si Liu The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF",
      "index": 50,
      "title": "Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining",
      "authors": [
        "Shangquan Sun",
        "Wenqi Ren",
        "Juxiang Zhou",
        "Shu Wang",
        "Jianhou Gan",
        "Xiaochun Cao"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "rainy",
        "stacking",
        "rain",
        "deraining",
        "world",
        "video",
        "real",
        "filter",
        "temporal",
        "space"
      ],
      "summary": "Significant progress has been made in video restoration under rainy conditions over the past decade, largely propelled by advancements in deep learning. Nevertheless, existing methods that depend on paired data struggle to generalize effectively to real-world scenarios, primarily due to the disparity between synthetic and authentic rain effects. To address these limitations, we propose a dual-branch spatio-temporal state-space model to enhance rain streak removal in video sequences. Specifically, we design spatial and temporal state-space model layers to extract spatial features and incorporate temporal dependencies across frames, respectively. To improve multi-frame feature fusion, we derive a dynamic stacking filter, which adaptively approximates statistical filters for superior pixel-wise feature refinement. Moreover, we integrate a median stacking loss to enable semi-supervised learning by generating pseudo-clean patches based on the sparsity prior of rain. To further explore the capacity of deraining models in supporting other vision-based tasks in rainy environments, we introduce a novel real-world benchmark focused on object detection and tracking in rainy conditions. Our method is extensively evaluated across multiple benchmarks containing numerous synthetic and real-world rainy videos, consistently demonstrating its superiority in quantitative metrics, visual quality, efficiency, and its utility for downstream tasks. Our code will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.html",
          "/venue/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 52,
        "kimi": 16
      },
      "raw_excerpt": "Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining [PDF 52 ] [Copy] [Kimi 16 ] [REL] Authors : Shangquan Sun , Wenqi Ren , Juxiang Zhou , Shu Wang , Jianhou Gan , Xiaochun Cao Significant progress has been made in video restoration under rainy conditions over the past decade, largely propelled by advancements in deep learning. Nevertheless, existing methods that depend on paired data struggle to generalize effectively to real-world scenarios, primarily due to the disparity between synthetic and authentic rain effects. To address these limitations, we propose a dual-branch spatio-temporal state-space model to enhance rain streak removal in video sequences. Specifically, we design spatial and temporal state-space model layers to extract spatial features and incorporate temporal dependencies across frames, respectively. To improve multi-frame feature fusion, we derive a dynamic stacking filter, which adaptively approximates statistical filters for superior pixel-wise feature refinement. Moreover, we integrate a median stacking loss to enable semi-supervised learning by generating pseudo-clean patches based on the sparsity prior of rain. To further explore the capacity of deraining models in supporting other vision-based tasks in rainy environments, we introduce a novel real-world benchmark focused on object detection and tracking in rainy conditions. Our method is extensively evaluated across multiple benchmarks containing numerous synthetic and real-world rainy videos, consistently demonstrating its superiority in quantitative metrics, visual quality, efficiency, and its utility for downstream tasks. Our code will be made publicly available. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF",
      "index": 51,
      "title": "PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation",
      "authors": [
        "Jingyi Tian",
        "Le Wang",
        "Sanping Zhou",
        "Sen Wang",
        "Jiayi Li",
        "Haowen Sun",
        "Wei Tang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "pdfactor",
        "manipulation",
        "tri",
        "action",
        "robotic",
        "latent",
        "diffusion",
        "perspective",
        "accuracy",
        "representation"
      ],
      "summary": "Robotic manipulation based on visual observations and natural language instructions is a long-standing challenge in robotics. Yet prevailing approaches model action distribution by adopting explicit or implicit representations, which often struggle to achieve a trade-off between accuracy and efficiency. In response, we propose PDFactor, a novel framework that models action distribution with a hybrid triplane representation. In particular, PDFactor decomposes 3D point cloud into three orthogonal feature planes and leverages a tri-perspective view transformer to produce dense cubic features as a latent diffusion field aligned with observation space representing 6-DoF action probability distribution at an arbitrary location. We employ a small denoising network conceptually as both a parameterized loss function measuring the quality of the learned latent features and an action gradient decoder to sample actions from the latent diffusion field during inference. This design enables our PDFactor to benefit from spatial awareness of explicit representation and arbitrary resolution of implicit representation, rendering it with manipulation accuracy, inference efficiency, and model scalability. Experiments demonstrate that PDFactor outperforms state-of-the-art approaches across a diverse range of manipulation tasks in RLBench simulation. Moreover, PDFactor can effectively learn multi-task policies from a limited number of human demonstrations, achieving promising accuracy in a variety of real-world manipulation tasks.",
      "session": null,
      "time": null,
      "links": {
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.html",
          "/venue/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF",
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 51,
        "kimi": 15
      },
      "raw_excerpt": "PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation [PDF 51 ] [Copy] [Kimi 15 ] [REL] Authors : Jingyi Tian , Le Wang , Sanping Zhou , Sen Wang , Jiayi Li , Haowen Sun , Wei Tang Robotic manipulation based on visual observations and natural language instructions is a long-standing challenge in robotics. Yet prevailing approaches model action distribution by adopting explicit or implicit representations, which often struggle to achieve a trade-off between accuracy and efficiency. In response, we propose PDFactor, a novel framework that models action distribution with a hybrid triplane representation. In particular, PDFactor decomposes 3D point cloud into three orthogonal feature planes and leverages a tri-perspective view transformer to produce dense cubic features as a latent diffusion field aligned with observation space representing 6-DoF action probability distribution at an arbitrary location. We employ a small denoising network conceptually as both a parameterized loss function measuring the quality of the learned latent features and an action gradient decoder to sample actions from the latent diffusion field during inference. This design enables our PDFactor to benefit from spatial awareness of explicit representation and arbitrary resolution of implicit representation, rendering it with manipulation accuracy, inference efficiency, and model scalability. Experiments demonstrate that PDFactor outperforms state-of-the-art approaches across a diverse range of manipulation tasks in RLBench simulation. Moreover, PDFactor can effectively learn multi-task policies from a limited number of human demonstrations, achieving promising accuracy in a variety of real-world manipulation tasks. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF",
      "index": 52,
      "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
      "authors": [
        "Linyi Jin",
        "Richard Tucker",
        "Zhengqi Li",
        "David Fouhey",
        "Noah Snavely",
        "Aleksander Holynski"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "stereo",
        "stereo4d",
        "internet",
        "videos",
        "reconstructions",
        "dust3r",
        "scenes",
        "motion",
        "world",
        "supervising"
      ],
      "summary": "Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3r to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 12
      },
      "raw_excerpt": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos [PDF 35 ] [Copy] [Kimi 12 ] [REL] Authors : Linyi Jin , Richard Tucker , Zhengqi Li , David Fouhey , Noah Snavely , Aleksander Holynski Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3r to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF",
      "index": 53,
      "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization",
      "authors": [
        "Liang Pan",
        "Zeshi Yang",
        "Zhiyang Dou",
        "Wenjia Wang",
        "Buzhen Huang",
        "Bo Dai",
        "Taku Komura",
        "Jingbo Wang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "skills",
        "hsi",
        "tokenhsi",
        "task",
        "unified",
        "policy",
        "scene",
        "tokenization",
        "tasks",
        "human"
      ],
      "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 11
      },
      "raw_excerpt": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization [PDF 33 ] [Copy] [Kimi 11 ] [REL] Authors : Liang Pan , Zeshi Yang , Zhiyang Dou , Wenjia Wang , Buzhen Huang , Bo Dai , Taku Komura , Jingbo Wang Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF",
      "index": 54,
      "title": "SEAL: Semantic Attention Learning for Long Video Representation",
      "authors": [
        "Lan Wang",
        "Yujia Chen",
        "Du Tran",
        "Vishnu Naresh Boddeti",
        "Wen-Sheng Chu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "seal",
        "video",
        "long",
        "representation",
        "videos",
        "lvbench",
        "redundancy",
        "mantic",
        "moviechat",
        "tasks"
      ],
      "summary": "Long video understanding presents challenges due to the inherent high computational complexity and redundant temporal information. An effective representation for long videos must process such redundancy efficiently while preserving essential contents for downstream tasks. This paper introduces **SE**mantic **A**ttention **L**earning (SEAL), a novel unified representation for long videos. To reduce computational complexity, long videos are decomposed into three distinct types of semantic entities: scenes, objects, and actions, allowing models to operate on a handful of entities rather than a large number of frames or pixels. To further address redundancy, we propose an attention learning module that balances token relevance with diversity formulated as a subset selection optimization problem. Our representation is versatile, enabling applications across various long video understanding tasks. Extensive experiments show that SEAL significantly outperforms state-of-the-art methods in video question answering and temporal grounding tasks and benchmarks including LVBench, MovieChat-1K, and Ego4D.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.html",
          "/venue/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 69,
        "kimi": 21
      },
      "raw_excerpt": "SEAL: Semantic Attention Learning for Long Video Representation [PDF 69 ] [Copy] [Kimi 21 ] [REL] Authors : Lan Wang , Yujia Chen , Du Tran , Vishnu Naresh Boddeti , Wen-Sheng Chu Long video understanding presents challenges due to the inherent high computational complexity and redundant temporal information. An effective representation for long videos must process such redundancy efficiently while preserving essential contents for downstream tasks. This paper introduces **SE**mantic **A**ttention **L**earning (SEAL), a novel unified representation for long videos. To reduce computational complexity, long videos are decomposed into three distinct types of semantic entities: scenes, objects, and actions, allowing models to operate on a handful of entities rather than a large number of frames or pixels. To further address redundancy, we propose an attention learning module that balances token relevance with diversity formulated as a subset selection optimization problem. Our representation is versatile, enabling applications across various long video understanding tasks. Extensive experiments show that SEAL significantly outperforms state-of-the-art methods in video question answering and temporal grounding tasks and benchmarks including LVBench, MovieChat-1K, and Ego4D. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF",
      "index": 55,
      "title": "Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers",
      "authors": [
        "Yichen Xiao",
        "Shuai Wang",
        "Dehao Zhang",
        "Wenjie Wei",
        "Yimeng Shan",
        "Xiaoli Liu",
        "Yulin Jiang",
        "Malu Zhang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "spiking",
        "xnor",
        "similarity",
        "spikes",
        "spike",
        "transformers",
        "trains",
        "calculation",
        "rethinking",
        "dot"
      ],
      "summary": "Transformers significantly raise the performance limits across various tasks, spurring research into integrating them into spiking neural networks. However, a notable performance gap remains between existing spiking Transformers and their artificial neural network counterparts. Here, we first analyze the reason for this gap and identify that the dot product ineffectively calculates similarity between spiking Queries (Q) and Keys (K). To address this challenge, we introduce an innovative α-XNOR similarity calculation method tailored for spike trains. α-XNOR similarity redefines the correlation of non-spike pairs as a specific value α, effectively overcoming the limitations of dot-product similarity caused by numerous non-spiking events. Additionally, considering the sparse nature of spike trains where spikes carry more information than non-spikes, the α-XNOR similarity correspondingly highlights the distinct importance of spikes over non-spikes. Extensive experiments demonstrate that our α-XNOR similarity significantly improves performance across different spiking Transformer architectures in various static and neuromorphic datasets. This is the first attempt to develop a spiking self-attention paradigm tailored for the binary characteristics of spike trains.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 34,
        "kimi": 19
      },
      "raw_excerpt": "Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers [PDF 34 ] [Copy] [Kimi 19 ] [REL] Authors : Yichen Xiao , Shuai Wang , Dehao Zhang , Wenjie Wei , Yimeng Shan , Xiaoli Liu , Yulin Jiang , Malu Zhang Transformers significantly raise the performance limits across various tasks, spurring research into integrating them into spiking neural networks. However, a notable performance gap remains between existing spiking Transformers and their artificial neural network counterparts. Here, we first analyze the reason for this gap and identify that the dot product ineffectively calculates similarity between spiking Queries (Q) and Keys (K). To address this challenge, we introduce an innovative α-XNOR similarity calculation method tailored for spike trains. α-XNOR similarity redefines the correlation of non-spike pairs as a specific value α, effectively overcoming the limitations of dot-product similarity caused by numerous non-spiking events. Additionally, considering the sparse nature of spike trains where spikes carry more information than non-spikes, the α-XNOR similarity correspondingly highlights the distinct importance of spikes over non-spikes. Extensive experiments demonstrate that our α-XNOR similarity significantly improves performance across different spiking Transformer architectures in various static and neuromorphic datasets. This is the first attempt to develop a spiking self-attention paradigm tailored for the binary characteristics of spike trains. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF",
      "index": 56,
      "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion",
      "authors": [
        "Yiran Wang",
        "Jiaqi Li",
        "Chaoyi Hong",
        "Ruibo Li",
        "Liusheng Sun",
        "Xiao Song",
        "Zhe Wang",
        "Zhiguo Cao",
        "Guosheng Lin"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "radar",
        "tacodepth",
        "depth",
        "camera",
        "estimation",
        "stage",
        "fusion",
        "efficient",
        "dense",
        "intermediate"
      ],
      "summary": "Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 57,
        "kimi": 17
      },
      "raw_excerpt": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion [PDF 57 ] [Copy] [Kimi 17 ] [REL] Authors : Yiran Wang , Jiaqi Li , Chaoyi Hong , Ruibo Li , Liusheng Sun , Xiao Song , Zhe Wang , Zhiguo Cao , Guosheng Lin Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF",
      "index": 57,
      "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
      "authors": [
        "Qi Wu",
        "Janick Martinez Esturo",
        "Ashkan Mirzaei",
        "Nicolas Moënne-Loccoz",
        "Zan Gojcic"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "3dgs",
        "splatting",
        "3dgut",
        "cameras",
        "secondary",
        "rasterization",
        "rendering",
        "tracing",
        "unscented",
        "distorted"
      ],
      "summary": "3D Gaussian Splatting (3DGS) has shown great potential for efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing volumetric particles instead, however, this comes at the cost of significantly slower rendering speeds. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 15
      },
      "raw_excerpt": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting [PDF 35 ] [Copy] [Kimi 15 ] [REL] Authors : Qi Wu , Janick Martinez Esturo , Ashkan Mirzaei , Nicolas Moënne-Loccoz , Zan Gojcic 3D Gaussian Splatting (3DGS) has shown great potential for efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing volumetric particles instead, however, this comes at the cost of significantly slower rendering speeds. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF",
      "index": 58,
      "title": "MEGA: Masked Generative Autoencoder for Human Mesh Recovery",
      "authors": [
        "Guénolé Fiche",
        "Simon Leglaive",
        "Xavier Alameda-Pineda",
        "Francesc Moreno-Noguer"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "human",
        "hmr",
        "mega",
        "mesh",
        "masked",
        "meshes",
        "single",
        "generative",
        "autoencoder",
        "recovery"
      ],
      "summary": "Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as an infinite set of 3D interpretations can explain the 2D observation equally well. Nevertheless, most HMR methods overlook this issue and make a single prediction without accounting for this ambiguity. A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction. This work proposes a new approach based on masked generative modeling. By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image. We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences. Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches. Code and trained models will be released upon acceptance.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 51,
        "kimi": 21
      },
      "raw_excerpt": "MEGA: Masked Generative Autoencoder for Human Mesh Recovery [PDF 51 ] [Copy] [Kimi 21 ] [REL] Authors : Guénolé Fiche , Simon Leglaive , Xavier Alameda-Pineda , Francesc Moreno-Noguer Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as an infinite set of 3D interpretations can explain the 2D observation equally well. Nevertheless, most HMR methods overlook this issue and make a single prediction without accounting for this ambiguity. A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction. This work proposes a new approach based on masked generative modeling. By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image. We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences. Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches. Code and trained models will be released upon acceptance. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF",
      "index": 59,
      "title": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild",
      "authors": [
        "Damien Teney",
        "Liangze Jiang",
        "Florin Gogianu",
        "Ehsan Abbasnejad"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "simplicity",
        "bias",
        "tasks",
        "activation",
        "architectures",
        "gelus",
        "relu",
        "suboptimal",
        "functions",
        "cases"
      ],
      "summary": "Common choices of architecture give neural networks a preference for fitting data with simple functions. This simplicity bias is known as key to their success. This paper explores the limits of this assumption. Building on recent work that showed that activation functions are the origin of the simplicity bias (Teney, 2024), we introduce a method to meta-learn activation functions to modulate this bias.**Findings.** We discover multiple tasks where the assumption of simplicity is inadequate, and standard ReLU architectures are therefore suboptimal. In these cases, we find activation functions that perform better by inducing a prior of higher complexity. Interestingly, these cases correspond to domains where neural networks have historically struggled: tabular data, regression tasks, cases of shortcut learning, and algorithmic grokking tasks. In comparison, the simplicity bias proves adequate on image tasks, where learned activations are nearly identical to ReLUs and GeLUs.**Implications.** (1) Contrary to common belief, the simplicity bias is not universally useful. There exist real tasks where it is suboptimal. (2) The suitability of ReLU models for image classification is not accidental. (3) The success of ML ultimately depends on the adequacy between data and architectures, and there may be benefits for architectures tailored to specific distributions of tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 48,
        "kimi": 19
      },
      "raw_excerpt": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild [PDF 48 ] [Copy] [Kimi 19 ] [REL] Authors : Damien Teney , Liangze Jiang , Florin Gogianu , Ehsan Abbasnejad Common choices of architecture give neural networks a preference for fitting data with simple functions. This simplicity bias is known as key to their success. This paper explores the limits of this assumption. Building on recent work that showed that activation functions are the origin of the simplicity bias (Teney, 2024), we introduce a method to meta-learn activation functions to modulate this bias.**Findings.** We discover multiple tasks where the assumption of simplicity is inadequate, and standard ReLU architectures are therefore suboptimal. In these cases, we find activation functions that perform better by inducing a prior of higher complexity. Interestingly, these cases correspond to domains where neural networks have historically struggled: tabular data, regression tasks, cases of shortcut learning, and algorithmic grokking tasks. In comparison, the simplicity bias proves adequate on image tasks, where learned activations are nearly identical to ReLUs and GeLUs.**Implications.** (1) Contrary to common belief, the simplicity bias is not universally useful. There exist real tasks where it is suboptimal. (2) The suitability of ReLU models for image classification is not accidental. (3) The success of ML ultimately depends on the adequacy between data and architectures, and there may be benefits for architectures tailored to specific distributions of tasks. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF",
      "index": 60,
      "title": "TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model",
      "authors": [
        "Meilong Xu",
        "Saumya Gupta",
        "Xiaoling Hu",
        "Chen Li",
        "Shahira Abousamra",
        "Dimitris Samaras",
        "Prateek Prasanna",
        "Chao Chen"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "cell",
        "topological",
        "topology",
        "pathology",
        "layouts",
        "topocellgen",
        "tissue",
        "generating",
        "histopathology",
        "diffusion"
      ],
      "summary": "Accurately modeling multi-class cell topology is crucial in digital pathology, as it provides critical insights into tissue structure and pathology. The synthetic generation of cell topology enables realistic simulations of complex tissue environments, enhances downstream tasks by augmenting training data, aligns more closely with pathologists' domain knowledge, and offers new opportunities for controlling and generalizing the tumor microenvironment. In this paper, we propose a novel approach that integrates topological constraints into a diffusion model to improve the generation of realistic, contextually accurate cell topologies. Our method refines the simulation of cell distributions and interactions, increasing the precision and interpretability of results in downstream tasks such as cell detection and classification. To assess the topological fidelity of generated layouts, we introduce a new metric, Topological Fréchet Distance (TopoFD), which overcomes the limitations of traditional metrics like FID in evaluating topological structure. Experimental results demonstrate the effectiveness of our approach in generating multi-class cell layouts that capture intricate topological relationships.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 34,
        "kimi": 13
      },
      "raw_excerpt": "TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model [PDF 34 ] [Copy] [Kimi 13 ] [REL] Authors : Meilong Xu , Saumya Gupta , Xiaoling Hu , Chen Li , Shahira Abousamra , Dimitris Samaras , Prateek Prasanna , Chao Chen Accurately modeling multi-class cell topology is crucial in digital pathology, as it provides critical insights into tissue structure and pathology. The synthetic generation of cell topology enables realistic simulations of complex tissue environments, enhances downstream tasks by augmenting training data, aligns more closely with pathologists' domain knowledge, and offers new opportunities for controlling and generalizing the tumor microenvironment. In this paper, we propose a novel approach that integrates topological constraints into a diffusion model to improve the generation of realistic, contextually accurate cell topologies. Our method refines the simulation of cell distributions and interactions, increasing the precision and interpretability of results in downstream tasks such as cell detection and classification. To assess the topological fidelity of generated layouts, we introduce a new metric, Topological Fréchet Distance (TopoFD), which overcomes the limitations of traditional metrics like FID in evaluating topological structure. Experimental results demonstrate the effectiveness of our approach in generating multi-class cell layouts that capture intricate topological relationships. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF",
      "index": 61,
      "title": "Enhancing Diversity for Data-free Quantization",
      "authors": [
        "Kai Zhao",
        "Zhihao Zhuang",
        "Miao Zhang",
        "Chenjuan Guo",
        "Yang Shu",
        "Bin Yang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "quantization",
        "calibration",
        "data",
        "diverse",
        "diversity",
        "dfq",
        "collapse",
        "4open",
        "84e6",
        "mixer"
      ],
      "summary": "Model quantization is an effective way to compress deep neural networks and accelerate the inference time on edge devices. Existing quantization methods usually require original data for calibration during the compressing process, which may be inaccessible due to privacy issues. A common way is to generate calibration data to mimic the origin data. However, the generators in these methods have the mode collapse problem, making them unable to synthesize diverse data. To solve this problem, we leverage the information from the full-precision model and enhance both inter-class and intra-class diversity for generating better calibration data, by devising a multi-layer features mixer and normalization flow based attention. Besides, novel regulation losses are proposed to make the generator produce diverse data with more patterns from the perspective of activated feature values and for the quantized model to learn better clip ranges adaptive to our diverse calibration data. Extensive experiments show that our method achieves state-of-the-art quantization results for both Transformer and CNN architectures. In addition, we visualize the generated data to verify that our strategies can effectively handle the mode collapse issue. Our codes are available at https://anonymous.4open.science/r/DFQ-84E6 and will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Enhancing_Diversity_for_Data-free_Quantization_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhao_Enhancing_Diversity_for_Data-free_Quantization@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Enhancing_Diversity_for_Data-free_Quantization_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 15
      },
      "raw_excerpt": "Enhancing Diversity for Data-free Quantization [PDF 35 ] [Copy] [Kimi 15 ] [REL] Authors : Kai Zhao , Zhihao Zhuang , Miao Zhang , Chenjuan Guo , Yang Shu , Bin Yang Model quantization is an effective way to compress deep neural networks and accelerate the inference time on edge devices. Existing quantization methods usually require original data for calibration during the compressing process, which may be inaccessible due to privacy issues. A common way is to generate calibration data to mimic the origin data. However, the generators in these methods have the mode collapse problem, making them unable to synthesize diverse data. To solve this problem, we leverage the information from the full-precision model and enhance both inter-class and intra-class diversity for generating better calibration data, by devising a multi-layer features mixer and normalization flow based attention. Besides, novel regulation losses are proposed to make the generator produce diverse data with more patterns from the perspective of activated feature values and for the quantized model to learn better clip ranges adaptive to our diverse calibration data. Extensive experiments show that our method achieves state-of-the-art quantization results for both Transformer and CNN architectures. In addition, we visualize the generated data to verify that our strategies can effectively handle the mode collapse issue. Our codes are available at https://anonymous.4open.science/r/DFQ-84E6 and will be publicly available. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Bar_Navigation_World_Models@CVPR2025@CVF",
      "index": 62,
      "title": "Navigation World Models",
      "authors": [
        "Amir Bar",
        "Gaoyue Zhou",
        "Danny Tran",
        "Trevor Darrell",
        "Yann LeCun"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "nwm",
        "navigation",
        "video",
        "egocentric",
        "planning",
        "cdit",
        "visual",
        "unlabeled",
        "world",
        "agents"
      ],
      "summary": "Navigation is a fundamental skill of agents with visual-motor capabilities. We propose a Navigation World Model (NWM), a controllable video generation model that predicts the future visual observation given the past observations and navigation actions. NWM is a Conditional Diffusion Transformer (CDiT) trained on the video footage of robots as well as unlabeled egocentric video data. We scale the model up to 1B parameters and train it over human and robot agents data from numerous environments and embodiments. Our model scales favorably on known and unknown environments and can leverage unlabeled egocentric video data. NWM exhibits improved navigation planning skills either by planning from scratch or by ranking proposals from an external navigation policy. Compared to existing supervised navigation models which are ``hard coded'', NWM can incorporate new constraints when planning trajectories. NWM learns visual priors that enable it to imagine navigation trajectories based on just a single input image.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Bar_Navigation_World_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 80,
        "kimi": 41
      },
      "raw_excerpt": "Navigation World Models [PDF 80 ] [Copy] [Kimi 41 ] [REL] Authors : Amir Bar , Gaoyue Zhou , Danny Tran , Trevor Darrell , Yann LeCun Navigation is a fundamental skill of agents with visual-motor capabilities. We propose a Navigation World Model (NWM), a controllable video generation model that predicts the future visual observation given the past observations and navigation actions. NWM is a Conditional Diffusion Transformer (CDiT) trained on the video footage of robots as well as unlabeled egocentric video data. We scale the model up to 1B parameters and train it over human and robot agents data from numerous environments and embodiments. Our model scales favorably on known and unknown environments and can leverage unlabeled egocentric video data. NWM exhibits improved navigation planning skills either by planning from scratch or by ranking proposals from an external navigation policy. Compared to existing supervised navigation models which are ``hard coded'', NWM can incorporate new constraints when planning trajectories. NWM learns visual priors that enable it to imagine navigation trajectories based on just a single input image. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF",
      "index": 63,
      "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
      "authors": [
        "Rundi Wu",
        "Ruiqi Gao",
        "Ben Poole",
        "Alex Trevithick",
        "Changxi Zheng",
        "Jonathan T. Barron",
        "Aleksander Holynski"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "cat4d",
        "video",
        "view",
        "monocular",
        "anything",
        "multi",
        "diffusion",
        "synthesis",
        "reconstruction",
        "scene"
      ],
      "summary": "We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.html",
          "/venue/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 39,
        "kimi": 11
      },
      "raw_excerpt": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models [PDF 39 ] [Copy] [Kimi 11 ] [REL] Authors : Rundi Wu , Ruiqi Gao , Ben Poole , Alex Trevithick , Changxi Zheng , Jonathan T. Barron , Aleksander Holynski We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF",
      "index": 64,
      "title": "FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video",
      "authors": [
        "Yue Gao",
        "Hong-Xing Yu",
        "Bo Zhu",
        "Jiajun Wu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "fluidnexus",
        "fluid",
        "video",
        "reconstruction",
        "view",
        "videos",
        "prediction",
        "novel",
        "simulation",
        "single"
      ],
      "summary": "We study reconstructing and predicting 3D fluid appearance and velocity from a single video. Current methods require multi-view videos for fluid reconstruction. We present FluidNexus, a novel framework that bridges video generation and physics simulation to tackle this task. Our key insight is to synthesize multiple novel-view videos as references for reconstruction. FluidNexus consists of two key components: (1) a novel-view video synthesizer that combines frame-wise view synthesis with video diffusion refinement for generating realistic videos, and (2) a physics-integrated particle representation coupling differentiable simulation and rendering to simultaneously facilitate 3D fluid reconstruction and prediction. To evaluate our approach, we collect two new real-world fluid datasets featuring textured backgrounds and object interactions. Our method enables dynamic novel view synthesis, future prediction, and interaction simulation from a single fluid video. we will release code and datasets.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.html",
          "/venue/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 43,
        "kimi": 11
      },
      "raw_excerpt": "FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video [PDF 43 ] [Copy] [Kimi 11 ] [REL] Authors : Yue Gao , Hong-Xing Yu , Bo Zhu , Jiajun Wu We study reconstructing and predicting 3D fluid appearance and velocity from a single video. Current methods require multi-view videos for fluid reconstruction. We present FluidNexus, a novel framework that bridges video generation and physics simulation to tackle this task. Our key insight is to synthesize multiple novel-view videos as references for reconstruction. FluidNexus consists of two key components: (1) a novel-view video synthesizer that combines frame-wise view synthesis with video diffusion refinement for generating realistic videos, and (2) a physics-integrated particle representation coupling differentiable simulation and rendering to simultaneously facilitate 3D fluid reconstruction and prediction. To evaluate our approach, we collect two new real-world fluid datasets featuring textured backgrounds and object interactions. Our method enables dynamic novel view synthesis, future prediction, and interaction simulation from a single fluid video. we will release code and datasets. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF",
      "index": 65,
      "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding",
      "authors": [
        "Yan Shu",
        "Zheng Liu",
        "Peitian Zhang",
        "Minghao Qin",
        "Junjie Zhou",
        "Zhengyang Liang",
        "Tiejun Huang",
        "Bo Zhao"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "video",
        "mllms",
        "vst",
        "compression",
        "visual",
        "long",
        "understanding",
        "token",
        "instruction",
        "customizes"
      ],
      "summary": "Long video understanding poses a significant challenge for current Multi-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained by their limited context lengths and the substantial costs while processing long videos. Although several existing methods attempt to reduce visual tokens, their strategies encounter severe bottleneck, restricting MLLMs' ability to perceive fine-grained visual details. In this work, we propose Video-XL, a novel approach that leverages MLLMs' inherent key-value (KV) sparsification capacity to condense the visual input. Specifically, we introduce a new special token, the Visual Summarization Token (VST), for each interval of the video, which summarizes the visual information within the interval as its associated KV. The VST module is trained by instruction fine-tuning, where two optimizing strategies are offered. 1. Curriculum learning, where VST learns to make small (easy) and large compression (hard) progressively. 2. Composite data curation, which integrates single-image, multi-image, and synthetic data to overcome the scarcity of long-video instruction data. The compression quality is further improved by dynamic compression, which customizes compression granularity based on the information density of different video intervals. Video-XL's effectiveness is verified from three aspects. First, it achieves a superior long-video understanding capability, outperforming state-of-the-art models of comparable sizes across multiple popular benchmarks. Second, it effectively preserves video information, with minimal compression loss even at 16x compression ratio. Third, it realizes outstanding cost-effectiveness, enabling high-quality processing of thousands of frames on a single A100 GPU.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.html",
          "/venue/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 44,
        "kimi": 18
      },
      "raw_excerpt": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding [PDF 44 ] [Copy] [Kimi 18 ] [REL] Authors : Yan Shu , Zheng Liu , Peitian Zhang , Minghao Qin , Junjie Zhou , Zhengyang Liang , Tiejun Huang , Bo Zhao Long video understanding poses a significant challenge for current Multi-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained by their limited context lengths and the substantial costs while processing long videos. Although several existing methods attempt to reduce visual tokens, their strategies encounter severe bottleneck, restricting MLLMs' ability to perceive fine-grained visual details. In this work, we propose Video-XL, a novel approach that leverages MLLMs' inherent key-value (KV) sparsification capacity to condense the visual input. Specifically, we introduce a new special token, the Visual Summarization Token (VST), for each interval of the video, which summarizes the visual information within the interval as its associated KV. The VST module is trained by instruction fine-tuning, where two optimizing strategies are offered. 1. Curriculum learning, where VST learns to make small (easy) and large compression (hard) progressively. 2. Composite data curation, which integrates single-image, multi-image, and synthetic data to overcome the scarcity of long-video instruction data. The compression quality is further improved by dynamic compression, which customizes compression granularity based on the information density of different video intervals. Video-XL's effectiveness is verified from three aspects. First, it achieves a superior long-video understanding capability, outperforming state-of-the-art models of comparable sizes across multiple popular benchmarks. Second, it effectively preserves video information, with minimal compression loss even at 16x compression ratio. Third, it realizes outstanding cost-effectiveness, enabling high-quality processing of thousands of frames on a single A100 GPU. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF",
      "index": 66,
      "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather",
      "authors": [
        "Longyu Yang",
        "Ping Hu",
        "Shangbo Yuan",
        "Lu Zhang",
        "Jun Liu",
        "Hengtao Shen",
        "Xiaofeng Zhu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "grc",
        "reflectance",
        "weather",
        "adverse",
        "collaboration",
        "lidar",
        "geometry",
        "segmentation",
        "augmentation",
        "capitalizing"
      ],
      "summary": "Existing LiDAR semantic segmentation models often suffer from decreased accuracy when exposed to adverse weather conditions. Recent methods addressing this issue focus on enhancing training data through weather simulation or universal augmentation techniques. However, few works have studied the negative impacts caused by the heterogeneous domain shifts in the geometric structure and reflectance intensity of point clouds. In this paper, we delve into this challenge and address it with a novel Geometry-Reflectance Collaboration (GRC) framework that explicitly separates feature extraction for geometry and reflectance. Specifically, GRC employs a dual-branch architecture designed to process geometric and reflectance features independently initially, thereby capitalizing on their distinct characteristic. Then, GRC adopts a robust multi-level feature collaboration module to suppress redundant and unreliable information from both branches. Consequently, without complex simulation or augmentation, our method effectively extracts intrinsic information about the scene while suppressing interference, thus achieving better robustness and generalization in adverse weather conditions. We demonstrate the effectiveness of GRC through comprehensive experiments on challenging benchmarks, showing that our method outperforms previous approaches and establishes new state-of-the-art results.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 10
      },
      "raw_excerpt": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather [PDF 33 ] [Copy] [Kimi 10 ] [REL] Authors : Longyu Yang , Ping Hu , Shangbo Yuan , Lu Zhang , Jun Liu , Hengtao Shen , Xiaofeng Zhu Existing LiDAR semantic segmentation models often suffer from decreased accuracy when exposed to adverse weather conditions. Recent methods addressing this issue focus on enhancing training data through weather simulation or universal augmentation techniques. However, few works have studied the negative impacts caused by the heterogeneous domain shifts in the geometric structure and reflectance intensity of point clouds. In this paper, we delve into this challenge and address it with a novel Geometry-Reflectance Collaboration (GRC) framework that explicitly separates feature extraction for geometry and reflectance. Specifically, GRC employs a dual-branch architecture designed to process geometric and reflectance features independently initially, thereby capitalizing on their distinct characteristic. Then, GRC adopts a robust multi-level feature collaboration module to suppress redundant and unreliable information from both branches. Consequently, without complex simulation or augmentation, our method effectively extracts intrinsic information about the scene while suppressing interference, thus achieving better robustness and generalization in adverse weather conditions. We demonstrate the effectiveness of GRC through comprehensive experiments on challenging benchmarks, showing that our method outperforms previous approaches and establishes new state-of-the-art results. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF",
      "index": 67,
      "title": "Minority-Focused Text-to-Image Generation via Prompt Optimization",
      "authors": [
        "Soobin Um",
        "Jong Chul Ye"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "t2i",
        "minority",
        "prompt",
        "generation",
        "samplers",
        "text",
        "pretrained",
        "diffusion",
        "optimization",
        "instances"
      ],
      "summary": "We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of *text-conditional* data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for producing high-quality generations. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that can encourage the emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes the generation of minority features by incorporating a carefully-crafted likelihood objective. Our comprehensive experiments, conducted across various types of T2I models, demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 59,
        "kimi": 27
      },
      "raw_excerpt": "Minority-Focused Text-to-Image Generation via Prompt Optimization [PDF 59 ] [Copy] [Kimi 27 ] [REL] Authors : Soobin Um , Jong Chul Ye We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of *text-conditional* data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for producing high-quality generations. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that can encourage the emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes the generation of minority features by incorporating a carefully-crafted likelihood objective. Our comprehensive experiments, conducted across various types of T2I models, demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF",
      "index": 68,
      "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
      "authors": [
        "Fangzhou Hong",
        "Vladimir Guzov",
        "Hyo Jin Kim",
        "Yuting Ye",
        "Richard Newcombe",
        "Ziwei Liu",
        "Lingni Ma"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "egolm",
        "egocentric",
        "motion",
        "wearable",
        "language",
        "motions",
        "sensors",
        "modal",
        "devices",
        "understanding"
      ],
      "summary": "As wearable devices become more prevalent, understanding the user's motion is crucial for improving contextual AI systems. We introduce EgoLM, a versatile framework designed for egocentric motion understanding using multi-modal data. EgoLM integrates the rich contextual information from egocentric videos and motion sensors afforded by wearable devices. It also combines dense supervision signals from motion and language, leveraging the vast knowledge encoded in pre-trained large language models (LLMs). EgoLM models the joint distribution of egocentric motions and natural language using LLMs, conditioned on observations from egocentric videos and motion sensors. It unifies a range of motion understanding tasks, including motion narration from video or motion data, as well as motion generation from text or sparse sensor data. Unique to wearable devices, it also enables a novel task to generate text descriptions from sparse sensors. Through extensive experiments, we validate the effectiveness of EgoLM in addressing the challenges of under-constrained egocentric motion learning, and demonstrate its capability as a generalist model through a variety of applications.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 37,
        "kimi": 19
      },
      "raw_excerpt": "EgoLM: Multi-Modal Language Model of Egocentric Motions [PDF 37 ] [Copy] [Kimi 19 ] [REL] Authors : Fangzhou Hong , Vladimir Guzov , Hyo Jin Kim , Yuting Ye , Richard Newcombe , Ziwei Liu , Lingni Ma As wearable devices become more prevalent, understanding the user's motion is crucial for improving contextual AI systems. We introduce EgoLM, a versatile framework designed for egocentric motion understanding using multi-modal data. EgoLM integrates the rich contextual information from egocentric videos and motion sensors afforded by wearable devices. It also combines dense supervision signals from motion and language, leveraging the vast knowledge encoded in pre-trained large language models (LLMs). EgoLM models the joint distribution of egocentric motions and natural language using LLMs, conditioned on observations from egocentric videos and motion sensors. It unifies a range of motion understanding tasks, including motion narration from video or motion data, as well as motion generation from text or sparse sensor data. Unique to wearable devices, it also enables a novel task to generate text descriptions from sparse sensors. Through extensive experiments, we validate the effectiveness of EgoLM in addressing the challenges of under-constrained egocentric motion learning, and demonstrate its capability as a generalist model through a variety of applications. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF",
      "index": 69,
      "title": "Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models",
      "authors": [
        "Zhejun Zhang",
        "Peter Karkus",
        "Maximilian Igl",
        "Wenhao Ding",
        "Yuxiao Chen",
        "Boris Ivanovic",
        "Marco Pavone"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "tokenized",
        "loop",
        "traffic",
        "tuning",
        "cat",
        "closed",
        "fine",
        "simulation",
        "covariate",
        "102m"
      ],
      "summary": "Traffic simulation aims to learn a policy for traffic agents that, when unrolled in closed-loop, faithfully recovers the joint distribution of trajectories observed in the real world. Inspired by large language models, tokenized multi-agent policies have recently become the state-of-the-art in traffic simulation. However, they are typically trained through open-loop behavior cloning, and thus suffer from covariate shift when executed in closed-loop during simulation. In this work, we present Closest Among Top-K (CAT-K) rollouts, a simple yet effective closed-loop fine-tuning strategy to mitigate covariate shift. CAT-K fine-tuning only requires existing trajectory data, without reinforcement learning or generative adversarial imitation. Concretely, CAT-K fine-tuning enables a small 7M-parameter tokenized traffic simulation policy to outperform a 102M-parameter model from the same model family, achieving the top spot on the Waymo Sim Agent Challenge leaderboard at the time of submission. Our code will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 26,
        "kimi": 15
      },
      "raw_excerpt": "Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models [PDF 26 ] [Copy] [Kimi 15 ] [REL] Authors : Zhejun Zhang , Peter Karkus , Maximilian Igl , Wenhao Ding , Yuxiao Chen , Boris Ivanovic , Marco Pavone Traffic simulation aims to learn a policy for traffic agents that, when unrolled in closed-loop, faithfully recovers the joint distribution of trajectories observed in the real world. Inspired by large language models, tokenized multi-agent policies have recently become the state-of-the-art in traffic simulation. However, they are typically trained through open-loop behavior cloning, and thus suffer from covariate shift when executed in closed-loop during simulation. In this work, we present Closest Among Top-K (CAT-K) rollouts, a simple yet effective closed-loop fine-tuning strategy to mitigate covariate shift. CAT-K fine-tuning only requires existing trajectory data, without reinforcement learning or generative adversarial imitation. Concretely, CAT-K fine-tuning enables a small 7M-parameter tokenized traffic simulation policy to outperform a 102M-parameter model from the same model family, achieving the top spot on the Waymo Sim Agent Challenge leaderboard at the time of submission. Our code will be made publicly available. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF",
      "index": 70,
      "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
      "authors": [
        "Jian Liang",
        "Wenke Huang",
        "Guancheng Wan",
        "Qu Yang",
        "Mang Ye"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "lora",
        "knowledge",
        "specialized",
        "mllms",
        "lorasculpt",
        "harmonizing",
        "sculpting",
        "multimodal",
        "downstream",
        "harmful"
      ],
      "summary": "While Multimodal Large Language Models (MLLMs) excel at generalizing across modalities and tasks, effectively adapting them to specific downstream tasks while simultaneously retaining both general and specialized knowledge remains challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently acquire specialized knowledge in MLLMs, it introduces substantial harmful redundancy during visual instruction tuning, which exacerbates the forgetting of general knowledge and degrades downstream task performance.To address this issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby harmonizing general and specialized knowledge.Specifically, under theoretical guarantees, we introduce sparse updates into LoRA to discard redundant parameters effectively. Furthermore, we propose a Conflict Mitigation Regularizer to refine the update trajectory of LoRA, mitigating knowledge conflicts with the pretrained weights.Extensive experimental results demonstrate that even at very high degree of sparsity ( ≤ ≤ 5\\%), our method simultaneously enhances generalization and downstream task performance. This confirms that our approach effectively mitigates the catastrophic forgetting issue and further promotes knowledge harmonization in MLLMs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 61,
        "kimi": 34
      },
      "raw_excerpt": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models [PDF 61 ] [Copy] [Kimi 34 ] [REL] Authors : Jian Liang , Wenke Huang , Guancheng Wan , Qu Yang , Mang Ye While Multimodal Large Language Models (MLLMs) excel at generalizing across modalities and tasks, effectively adapting them to specific downstream tasks while simultaneously retaining both general and specialized knowledge remains challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently acquire specialized knowledge in MLLMs, it introduces substantial harmful redundancy during visual instruction tuning, which exacerbates the forgetting of general knowledge and degrades downstream task performance.To address this issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby harmonizing general and specialized knowledge.Specifically, under theoretical guarantees, we introduce sparse updates into LoRA to discard redundant parameters effectively. Furthermore, we propose a Conflict Mitigation Regularizer to refine the update trajectory of LoRA, mitigating knowledge conflicts with the pretrained weights.Extensive experimental results demonstrate that even at very high degree of sparsity ( ≤ ≤ 5\\%), our method simultaneously enhances generalization and downstream task performance. This confirms that our approach effectively mitigates the catastrophic forgetting issue and further promotes knowledge harmonization in MLLMs. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF",
      "index": 71,
      "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
      "authors": [
        "Jingfeng Yao",
        "Bin Yang",
        "Xinggang Wang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "latent",
        "tokenizers",
        "dit",
        "gfid",
        "generation",
        "diffusion",
        "epochs",
        "rfid",
        "reconstruction",
        "dilemma"
      ],
      "summary": "Latent diffusion models (LDM) with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: increasing the per-token feature dimension in visual tokenizers improves reconstruction quality but requires substantially larger diffusion models and extended training time to maintain generation performance. This results in prohibitively high computational costs, making high-dimensional tokenizers impractical. In this paper, we argue that this limitation stems from the inherent difficulty of learning unconstrained high-dimensional latent spaces and address this limitation by aligning the latent space with pre-trained vision foundation models. Our VA-VAE (Vision foundation model Aligned Variational AutoEncoder) expands the Pareto frontier of visual tokenizers, enabling 2.7 times faster Diffusion Transformers (DiT) convergence in high-dimensional latent space. To further validate our approach, we optimize a DiT baseline, referred to as LightningDiT, achieving superior performance on class conditional generation with only 6% of the original training epochs. The integrated system demonstrates the effectiveness of VA-VAE, achieving 0.28 rFID and 1.73 gFID on ImageNet-256 generation in 400 epochs—outperforming the original DiT's 0.71 rFID and 2.27 gFID in 1400 epochs, without more complex designs. To our knowledge, this marks the first latent diffusion system to achieve both superior generation and reconstruction without increasing training costs. Our codes and weights will be open source.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 56,
        "kimi": 29
      },
      "raw_excerpt": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models [PDF 56 ] [Copy] [Kimi 29 ] [REL] Authors : Jingfeng Yao , Bin Yang , Xinggang Wang Latent diffusion models (LDM) with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: increasing the per-token feature dimension in visual tokenizers improves reconstruction quality but requires substantially larger diffusion models and extended training time to maintain generation performance. This results in prohibitively high computational costs, making high-dimensional tokenizers impractical. In this paper, we argue that this limitation stems from the inherent difficulty of learning unconstrained high-dimensional latent spaces and address this limitation by aligning the latent space with pre-trained vision foundation models. Our VA-VAE (Vision foundation model Aligned Variational AutoEncoder) expands the Pareto frontier of visual tokenizers, enabling 2.7 times faster Diffusion Transformers (DiT) convergence in high-dimensional latent space. To further validate our approach, we optimize a DiT baseline, referred to as LightningDiT, achieving superior performance on class conditional generation with only 6% of the original training epochs. The integrated system demonstrates the effectiveness of VA-VAE, achieving 0.28 rFID and 1.73 gFID on ImageNet-256 generation in 400 epochs—outperforming the original DiT's 0.71 rFID and 2.27 gFID in 1400 epochs, without more complex designs. To our knowledge, this marks the first latent diffusion system to achieve both superior generation and reconstruction without increasing training costs. Our codes and weights will be open source. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF",
      "index": 72,
      "title": "Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation",
      "authors": [
        "Jiaxin Cai",
        "Jingze Su",
        "Qi Li",
        "Wenjie Yang",
        "Shu Wang",
        "Tiesong Zhao",
        "Shengfeng He",
        "Wenxi Liu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "modality",
        "rgb",
        "modalities",
        "segmentation",
        "multimodal",
        "fusion",
        "symmetrical",
        "cross",
        "efficient",
        "semantic"
      ],
      "summary": "Multimodal semantic segmentation is a critical challenge in computer vision, with early methods suffering from high computational costs and limited transferability due to full fine-tuning of RGB-based pre-trained parameters. Recent studies, while leveraging additional modalities as supplementary prompts to RGB, still predominantly rely on RGB, which restricts the full potential of other modalities. To address these issues, we propose a novel symmetric parameter-efficient fine-tuning framework for multimodal segmentation, featuring with a modality-aware prompting and adaptation scheme, to simultaneously adapt the capabilities of a powerful pre-trained model to both RGB and X modalities. Furthermore, prevalent approaches use the global cross-modality correlations of attention mechanism for modality fusion, which inadvertently introduces noise across modalities. To mitigate this noise, we propose a dynamic sparse cross-modality fusion module to facilitate effective and efficient cross-modality fusion. To further strengthen the above two modules, we propose a training strategy that leverages accurately predicted dual-modality results to self-teach the single-modality outcomes. In comprehensive experiments, we demonstrate that our method outperforms previous state-of-the-art approaches across six multimodal segmentation scenarios with minimal computation cost.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 49,
        "kimi": 21
      },
      "raw_excerpt": "Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation [PDF 49 ] [Copy] [Kimi 21 ] [REL] Authors : Jiaxin Cai , Jingze Su , Qi Li , Wenjie Yang , Shu Wang , Tiesong Zhao , Shengfeng He , Wenxi Liu Multimodal semantic segmentation is a critical challenge in computer vision, with early methods suffering from high computational costs and limited transferability due to full fine-tuning of RGB-based pre-trained parameters. Recent studies, while leveraging additional modalities as supplementary prompts to RGB, still predominantly rely on RGB, which restricts the full potential of other modalities. To address these issues, we propose a novel symmetric parameter-efficient fine-tuning framework for multimodal segmentation, featuring with a modality-aware prompting and adaptation scheme, to simultaneously adapt the capabilities of a powerful pre-trained model to both RGB and X modalities. Furthermore, prevalent approaches use the global cross-modality correlations of attention mechanism for modality fusion, which inadvertently introduces noise across modalities. To mitigate this noise, we propose a dynamic sparse cross-modality fusion module to facilitate effective and efficient cross-modality fusion. To further strengthen the above two modules, we propose a training strategy that leverages accurately predicted dual-modality results to self-teach the single-modality outcomes. In comprehensive experiments, we demonstrate that our method outperforms previous state-of-the-art approaches across six multimodal segmentation scenarios with minimal computation cost. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF",
      "index": 73,
      "title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
      "authors": [
        "Minhyeok Lee",
        "Suhwan Cho",
        "Jungho Lee",
        "Sunghun Yang",
        "Heeseung Choi",
        "Ig-Jae Kim",
        "Sangyoun Lee"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "sam",
        "segmentation",
        "esc",
        "vocabulary",
        "mask",
        "net",
        "open",
        "pascal",
        "vlf",
        "semantic"
      ],
      "summary": "Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAM’s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. Additionally, a Vision-Language Fusion (VLF) module enhances the final mask prediction through image and text guidance. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 77,
        "kimi": 29
      },
      "raw_excerpt": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation [PDF 77 ] [Copy] [Kimi 29 ] [REL] Authors : Minhyeok Lee , Suhwan Cho , Jungho Lee , Sunghun Yang , Heeseung Choi , Ig-Jae Kim , Sangyoun Lee Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAM’s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. Additionally, a Vision-Language Fusion (VLF) module enhances the final mask prediction through image and text guidance. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF",
      "index": 74,
      "title": "Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays",
      "authors": [
        "Laurie Bose",
        "Jianing Chen",
        "Piotr Dudek"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "pixel",
        "ppa",
        "processor",
        "tracking",
        "descriptors",
        "point",
        "feature",
        "descriptor",
        "sensor",
        "processors"
      ],
      "summary": "This paper presents a novel approach for joint point-feature detection and tracking, specifically designed for Pixel Processor Array sensors (PPA). Instead of standard pixels, PPA sensors consists of thousands of \"pixel-processors\", enabling massive parallel computation of visual data at the point of light capture. Our approach performs all computation within these pixel-processors, meaning no raw image data need ever leave the sensor. Instead, sensor output can be reduced to merely the locations of tracked features, and the descriptors of newly initialized features, minimizing data transfer between sensor and external processing. To achieve this we store feature descriptors inside every pixel-processor, adjusting the layout of these descriptors every frame. The PPA's architecture enables us to compute the response of every stored descriptor in parallel. This \"response map\" is utilized for both detection and tracking of point-features across the pixel-processor array. This approach is very fast, our implementation upon the SCAMP-7 PPA prototype runs at over 3000 FPS (Frames Per Second), tracking point-features reliably even under violent motion. This is the first work performing point-feature detection and tracking entirely \"in-pixel\".",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 36,
        "kimi": 13
      },
      "raw_excerpt": "Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays [PDF 36 ] [Copy] [Kimi 13 ] [REL] Authors : Laurie Bose , Jianing Chen , Piotr Dudek This paper presents a novel approach for joint point-feature detection and tracking, specifically designed for Pixel Processor Array sensors (PPA). Instead of standard pixels, PPA sensors consists of thousands of \"pixel-processors\", enabling massive parallel computation of visual data at the point of light capture. Our approach performs all computation within these pixel-processors, meaning no raw image data need ever leave the sensor. Instead, sensor output can be reduced to merely the locations of tracked features, and the descriptors of newly initialized features, minimizing data transfer between sensor and external processing. To achieve this we store feature descriptors inside every pixel-processor, adjusting the layout of these descriptors every frame. The PPA's architecture enables us to compute the response of every stored descriptor in parallel. This \"response map\" is utilized for both detection and tracking of point-features across the pixel-processor array. This approach is very fast, our implementation upon the SCAMP-7 PPA prototype runs at over 3000 FPS (Frames Per Second), tracking point-features reliably even under violent motion. This is the first work performing point-feature detection and tracking entirely \"in-pixel\". Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF",
      "index": 75,
      "title": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation",
      "authors": [
        "Haoyu Guo",
        "He Zhu",
        "Sida Peng",
        "Haotong Lin",
        "Yunzhi Yan",
        "Tao Xie",
        "Wenguan Wang",
        "Xiaowei Zhou",
        "Hujun Bao"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "murre",
        "sfm",
        "view",
        "reconstruction",
        "depth",
        "multi",
        "mvs",
        "scenes",
        "monocular",
        "scene"
      ],
      "summary": "This paper aims to reconstruct the scene geometry from multi-view images with strong robustness and high quality. Previous learning-based methods incorporate neural networks into the multi-view stereo matching and have shown impressive reconstruction results. However, due to the reliance on matching across input images, they typically suffer from high GPU memory consumption and tend to fail in sparse view scenarios. To overcome this problem, we develop a new pipeline, named Murre, for multi-view geometry reconstruction of 3D scenes based on SfM-guided monocular depth estimation. For input images, Murre first recover the SfM point cloud that captures the global scene structure, and then use it to guide a conditional diffusion model to produce multi-view metric depth maps for the final TSDF fusion. By predicting the depth map from a single image, Murre bypasses the multi-view matching step and naturally resolves the issues of previous MVS-based methods. In addition, the diffusion-based model can easily leverage the powerful priors of 2D foundation models, achieving good generalization ability across diverse real-world scenes. To obtain multi-view consistent depth maps, our key design is providing effective guidance on the diffusion model through the SfM point cloud, which is a condensed form of multi-view information, highlighting the scene's salient structure, and can be readily transformed into point maps to drive the image-space estimation process. We evaluate the reconstruction quality of Murre in various types of real-world datasets including indoor, streetscapes, and aerial scenes, surpassing state-of-the-art MVS-based and implicit neural reconstruction-based methods. The code will be released for reproducibility.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 49,
        "kimi": 16
      },
      "raw_excerpt": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation [PDF 49 ] [Copy] [Kimi 16 ] [REL] Authors : Haoyu Guo , He Zhu , Sida Peng , Haotong Lin , Yunzhi Yan , Tao Xie , Wenguan Wang , Xiaowei Zhou , Hujun Bao This paper aims to reconstruct the scene geometry from multi-view images with strong robustness and high quality. Previous learning-based methods incorporate neural networks into the multi-view stereo matching and have shown impressive reconstruction results. However, due to the reliance on matching across input images, they typically suffer from high GPU memory consumption and tend to fail in sparse view scenarios. To overcome this problem, we develop a new pipeline, named Murre, for multi-view geometry reconstruction of 3D scenes based on SfM-guided monocular depth estimation. For input images, Murre first recover the SfM point cloud that captures the global scene structure, and then use it to guide a conditional diffusion model to produce multi-view metric depth maps for the final TSDF fusion. By predicting the depth map from a single image, Murre bypasses the multi-view matching step and naturally resolves the issues of previous MVS-based methods. In addition, the diffusion-based model can easily leverage the powerful priors of 2D foundation models, achieving good generalization ability across diverse real-world scenes. To obtain multi-view consistent depth maps, our key design is providing effective guidance on the diffusion model through the SfM point cloud, which is a condensed form of multi-view information, highlighting the scene's salient structure, and can be readily transformed into point maps to drive the image-space estimation process. We evaluate the reconstruction quality of Murre in various types of real-world datasets including indoor, streetscapes, and aerial scenes, surpassing state-of-the-art MVS-based and implicit neural reconstruction-based methods. The code will be released for reproducibility. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF",
      "index": 76,
      "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
      "authors": [
        "Junying Wang",
        "Hongyuan Zhang",
        "Yuan Yuan"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "cpg",
        "adv",
        "portrait",
        "facial",
        "customized",
        "generation",
        "portraits",
        "adversarial",
        "attacks",
        "attack"
      ],
      "summary": "Recent personalized portrait generation methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and customized portrait generation, we develop a multi-modal image customizer capable of generating controllable fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into customized portrait generation. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 30,
        "kimi": 7
      },
      "raw_excerpt": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks [PDF 30 ] [Copy] [Kimi 7 ] [REL] Authors : Junying Wang , Hongyuan Zhang , Yuan Yuan Recent personalized portrait generation methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and customized portrait generation, we develop a multi-modal image customizer capable of generating controllable fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into customized portrait generation. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF",
      "index": 77,
      "title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning",
      "authors": [
        "Yanbiao Ma",
        "Wei Dai",
        "Wenke Huang",
        "Jiayi Chen"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "global",
        "skew",
        "geometric",
        "shapes",
        "distribution",
        "scenarios",
        "domain",
        "federated",
        "guided",
        "handling"
      ],
      "summary": "Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 31,
        "kimi": 12
      },
      "raw_excerpt": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning [PDF 31 ] [Copy] [Kimi 12 ] [REL] Authors : Yanbiao Ma , Wei Dai , Wenke Huang , Jiayi Chen Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF",
      "index": 78,
      "title": "DreamRelation: Bridging Customization and Relation Generation",
      "authors": [
        "Qingyu Shi",
        "Lu Qi",
        "Jianzong Wu",
        "Jinbin Bai",
        "Jingbo Wang",
        "Yunhai Tong",
        "Xiangtai Li"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "dreamrelation",
        "prompts",
        "relation",
        "customized",
        "generation",
        "relations",
        "object",
        "image",
        "confusion",
        "customization"
      ],
      "summary": "Customized image generation is essential for delivering personalized content based on user-provided prompts, enabling large-scale text-to-image diffusion models to better align with individual needs. However, existing models often neglect the relationships between customized objects in generated images. In contrast, this work addresses this gap by focusing on relation-aware customized image generation, which seeks to preserve the identities from image prompts while maintaining the predicate relations specified in text prompts. Specifically, we introduce DreamRelation, a framework that disentangles identity and relation learning using a carefully curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges—generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on our proposed benchmarks demonstrate the superiority of DreamRelation in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 45,
        "kimi": 17
      },
      "raw_excerpt": "DreamRelation: Bridging Customization and Relation Generation [PDF 45 ] [Copy] [Kimi 17 ] [REL] Authors : Qingyu Shi , Lu Qi , Jianzong Wu , Jinbin Bai , Jingbo Wang , Yunhai Tong , Xiangtai Li Customized image generation is essential for delivering personalized content based on user-provided prompts, enabling large-scale text-to-image diffusion models to better align with individual needs. However, existing models often neglect the relationships between customized objects in generated images. In contrast, this work addresses this gap by focusing on relation-aware customized image generation, which seeks to preserve the identities from image prompts while maintaining the predicate relations specified in text prompts. Specifically, we introduce DreamRelation, a framework that disentangles identity and relation learning using a carefully curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges—generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on our proposed benchmarks demonstrate the superiority of DreamRelation in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF",
      "index": 79,
      "title": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector",
      "authors": [
        "Xiao Guo",
        "Xiufeng Song",
        "Yue Zhang",
        "Xiaohong Liu",
        "Xiaoming Liu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "m2f2",
        "det",
        "forgery",
        "face",
        "deepfake",
        "forgeries",
        "detection",
        "interpretability",
        "modal",
        "forged"
      ],
      "summary": "Deepfake detection is a long-established research topic crucial for combating the spread of malicious misinformation. Unlike previous methods that provide either binary classification results or textual explanations for deepfake detection, we propose a novel method that delivers both simultaneously. Our method harnesses the multi-modal learning power of the pre-trained CLIP and the unprecedented interpretability of large language models (LLMs) to enhance both the generalization and interpretability of deepfake detection. Specifically, we introduce a multi-modal face forgery detector (M2F2-Det) that employs specially designed face forgery prompt learning, integrating zero-shot learning capabilities of the pre-trained CLIP to improve generalization to unseen forgeries.Also, M2F2-Det incorporates the LLM to provide detailed explanations for detection decisions, offering strong interpretability by bridging the gap between natural language and the subtle nuances of facial forgery detection. Empirically, we evaluate M2F2-Det for both detection and sentence generation tasks, on both of which M2F2-Det achieves state-of-the-art performance, showing its effectiveness in detecting and explaining diverse and unseen forgeries. Code and models will be released upon publication.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 56,
        "kimi": 15
      },
      "raw_excerpt": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector [PDF 56 ] [Copy] [Kimi 15 ] [REL] Authors : Xiao Guo , Xiufeng Song , Yue Zhang , Xiaohong Liu , Xiaoming Liu Deepfake detection is a long-established research topic crucial for combating the spread of malicious misinformation. Unlike previous methods that provide either binary classification results or textual explanations for deepfake detection, we propose a novel method that delivers both simultaneously. Our method harnesses the multi-modal learning power of the pre-trained CLIP and the unprecedented interpretability of large language models (LLMs) to enhance both the generalization and interpretability of deepfake detection. Specifically, we introduce a multi-modal face forgery detector (M2F2-Det) that employs specially designed face forgery prompt learning, integrating zero-shot learning capabilities of the pre-trained CLIP to improve generalization to unseen forgeries.Also, M2F2-Det incorporates the LLM to provide detailed explanations for detection decisions, offering strong interpretability by bridging the gap between natural language and the subtle nuances of facial forgery detection. Empirically, we evaluate M2F2-Det for both detection and sentence generation tasks, on both of which M2F2-Det achieves state-of-the-art performance, showing its effectiveness in detecting and explaining diverse and unseen forgeries. Code and models will be released upon publication. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF",
      "index": 80,
      "title": "Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues",
      "authors": [
        "Yuhui Liu",
        "Liangxun Ou",
        "Qiang Fu",
        "Hadi Amata",
        "Wolfgang Heidrich",
        "Yifan Peng"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "stereo",
        "rgbd",
        "depth",
        "imaging",
        "cues",
        "igev",
        "binocular",
        "encoding",
        "information",
        "focus"
      ],
      "summary": "Extracting high-fidelity RGBD information from two-dimensional (2D) images is essential for various visual computing applications. Stereo imaging, as a reliable passive imaging technique for obtaining three-dimensional (3D) scene information, has benefited greatly from deep learning advancements. However, existing stereo depth estimation algorithms struggle to perceive high-frequency information and resolve high-resolution depth maps in realistic camera settings with large depth variations. These algorithms commonly neglect the hardware parameter configuration, limiting the potential for achieving optimal solutions solely through software-based design strategies.This work presents a hardware-software co-designed RGBD imaging framework that leverages both stereo and focus cues to reconstruct texture-rich color images along with detailed depth maps over a wide depth range. A pair of rank-2 parameterized diffractive optical elements (DOEs) is employed to encode perpendicular complementary information optically during stereo acquisitions. Additionally, we employ an IGEV-UNet-fused neural network tailored to the proposed rank-2 encoding for stereo matching and image reconstruction. Through prototyping a stereo camera with customized DOEs, our deep stereo imaging paradigm has demonstrated superior performance over existing monocular and stereo imaging systems in both image PSNR by 2.96 dB gain and depth accuracy in high-frequency details across distances from 0.67 to 8 meters.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 9
      },
      "raw_excerpt": "Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues [PDF 18 ] [Copy] [Kimi 9 ] [REL] Authors : Yuhui Liu , Liangxun Ou , Qiang Fu , Hadi Amata , Wolfgang Heidrich , Yifan Peng Extracting high-fidelity RGBD information from two-dimensional (2D) images is essential for various visual computing applications. Stereo imaging, as a reliable passive imaging technique for obtaining three-dimensional (3D) scene information, has benefited greatly from deep learning advancements. However, existing stereo depth estimation algorithms struggle to perceive high-frequency information and resolve high-resolution depth maps in realistic camera settings with large depth variations. These algorithms commonly neglect the hardware parameter configuration, limiting the potential for achieving optimal solutions solely through software-based design strategies.This work presents a hardware-software co-designed RGBD imaging framework that leverages both stereo and focus cues to reconstruct texture-rich color images along with detailed depth maps over a wide depth range. A pair of rank-2 parameterized diffractive optical elements (DOEs) is employed to encode perpendicular complementary information optically during stereo acquisitions. Additionally, we employ an IGEV-UNet-fused neural network tailored to the proposed rank-2 encoding for stereo matching and image reconstruction. Through prototyping a stereo camera with customized DOEs, our deep stereo imaging paradigm has demonstrated superior performance over existing monocular and stereo imaging systems in both image PSNR by 2.96 dB gain and depth accuracy in high-frequency details across distances from 0.67 to 8 meters. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF",
      "index": 81,
      "title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics",
      "authors": [
        "Chan Hee Song",
        "Valts Blukis",
        "Jonathan Tremblay",
        "Stephen Tyree",
        "Yu Su",
        "Stan Birchfield"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "spatial",
        "robospatial",
        "robotics",
        "centric",
        "understanding",
        "scans",
        "ego",
        "robots",
        "world",
        "images"
      ],
      "summary": "Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment. This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world. In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources. These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities. For example, the datasets do not address reference frame comprehension — spatial relationships require clear contextual understanding, whether from a ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction. To address this issue, we introduce RoboSpatial, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and ego-centric images, annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 54,
        "kimi": 15
      },
      "raw_excerpt": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics [PDF 54 ] [Copy] [Kimi 15 ] [REL] Authors : Chan Hee Song , Valts Blukis , Jonathan Tremblay , Stephen Tyree , Yu Su , Stan Birchfield Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment. This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world. In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources. These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities. For example, the datasets do not address reference frame comprehension — spatial relationships require clear contextual understanding, whether from a ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction. To address this issue, we introduce RoboSpatial, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and ego-centric images, annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF",
      "index": 82,
      "title": "Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning",
      "authors": [
        "Mi Luo",
        "Zihui Xue",
        "Alex Dimakis",
        "Kristen Grauman"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "unpaired",
        "ego",
        "exo",
        "view",
        "rosetta",
        "viewpoint",
        "stone",
        "video",
        "rst",
        "translator"
      ],
      "summary": "Egocentric and exocentric perspectives of human action differ significantly, yet overcoming this extreme viewpoint gap is critical for applications in augmented reality and robotics. We propose ViewpointRosetta, an approach that unlocks large-scale unpaired ego and exo video data to learn clip-level viewpoint-invariant video representations. Our framework introduces (1) a diffusion-based Rosetta Stone Translator (RST), which, leveraging a moderate amount of synchronized multi-view videos, serves as a translator in feature space to decipher the alignments between unpaired ego and exo data, and (2) a dual encoder that aligns unpaired data representations through contrastive learning with RST-based synthetic feature augmentation and soft alignment. To evaluate the learned features in a standardized setting, we construct a new cross-view benchmark using Ego-Exo4D, covering cross-view retrieval, action recognition, and skill assessment. Our framework demonstrates superior cross-view understanding compared to previous view-invariant learning and egocentric video representation learning approaches, and opens the door to bringing vast amounts of traditional third-person video to bear on the more nascent first-person setting.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.html",
          "/venue/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.pdf"
        ],
        "venue": [
          "/venue/CVPR.2025?group=Oral",
          "/venue/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation@CVPR2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 26,
        "kimi": 6
      },
      "raw_excerpt": "Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning [PDF 26 ] [Copy] [Kimi 6 ] [REL] Authors : Mi Luo , Zihui Xue , Alex Dimakis , Kristen Grauman Egocentric and exocentric perspectives of human action differ significantly, yet overcoming this extreme viewpoint gap is critical for applications in augmented reality and robotics. We propose ViewpointRosetta, an approach that unlocks large-scale unpaired ego and exo video data to learn clip-level viewpoint-invariant video representations. Our framework introduces (1) a diffusion-based Rosetta Stone Translator (RST), which, leveraging a moderate amount of synchronized multi-view videos, serves as a translator in feature space to decipher the alignments between unpaired ego and exo data, and (2) a dual encoder that aligns unpaired data representations through contrastive learning with RST-based synthetic feature augmentation and soft alignment. To evaluate the learned features in a standardized setting, we construct a new cross-view benchmark using Ego-Exo4D, covering cross-view retrieval, action recognition, and skill assessment. Our framework demonstrates superior cross-view understanding compared to previous view-invariant learning and egocentric video representation learning approaches, and opens the door to bringing vast amounts of traditional third-person video to bear on the more nascent first-person setting. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF",
      "index": 83,
      "title": "Autoregressive Distillation of Diffusion Transformers",
      "authors": [
        "Yeongmin Kim",
        "Sotiris Anagnostidis",
        "Yuming Du",
        "Edgar Schönfeld",
        "Jonas Kohler",
        "Markos Georgopoulos",
        "Albert Pumarola",
        "Ali Thabet",
        "Artsiom Sanakoyeu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "ard",
        "fid",
        "trajectory",
        "historical",
        "imagenet",
        "256",
        "autoregressive",
        "distillation",
        "transformer",
        "susceptible"
      ],
      "summary": "Diffusion models with transformer architectures have demonstrated promising capabilities in generating high-fidelity images and scalability for high resolution. However, iterative sampling process required for synthesis is very resource-intensive. A line of work has focused on distilling solutions to probability flow ODEs into few-step student models. Nevertheless, existing methods have been limited by their reliance on the most recent denoised samples as input, rendering them susceptible to exposure bias. To address this limitation, we propose AutoRegressive Distillation (ARD), a novel approach that leverages the historical trajectory of the ODE to predict future steps. ARD offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted historical trajectory that is less susceptible to accumulated errors, and 2) it leverages the previous history of the ODE trajectory as a more effective source of coarse-grained information. ARD modifies the teacher transformer architecture by adding token-wise time embedding to mark each input from the trajectory history and employs a block-wise causal attention mask for training. Furthermore, incorporating historical inputs only in lower transformer layers enhances performance and efficiency. We validate the effectiveness of ARD in a class-conditioned generation on ImageNet and T2I synthesis. Our model achieves a 5 × 5 × reduction in FID degradation compared to the baseline methods while requiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of 1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available 1024p text-to-image distilled models in prompt adherence score with a minimal drop in FID compared to the teacher.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Autoregressive_Distillation_of_Diffusion_Transformers_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Kim_Autoregressive_Distillation_of_Diffusion_Transformers@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Autoregressive_Distillation_of_Diffusion_Transformers_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 63,
        "kimi": 28
      },
      "raw_excerpt": "Autoregressive Distillation of Diffusion Transformers [PDF 63 ] [Copy] [Kimi 28 ] [REL] Authors : Yeongmin Kim , Sotiris Anagnostidis , Yuming Du , Edgar Schönfeld , Jonas Kohler , Markos Georgopoulos , Albert Pumarola , Ali Thabet , Artsiom Sanakoyeu Diffusion models with transformer architectures have demonstrated promising capabilities in generating high-fidelity images and scalability for high resolution. However, iterative sampling process required for synthesis is very resource-intensive. A line of work has focused on distilling solutions to probability flow ODEs into few-step student models. Nevertheless, existing methods have been limited by their reliance on the most recent denoised samples as input, rendering them susceptible to exposure bias. To address this limitation, we propose AutoRegressive Distillation (ARD), a novel approach that leverages the historical trajectory of the ODE to predict future steps. ARD offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted historical trajectory that is less susceptible to accumulated errors, and 2) it leverages the previous history of the ODE trajectory as a more effective source of coarse-grained information. ARD modifies the teacher transformer architecture by adding token-wise time embedding to mark each input from the trajectory history and employs a block-wise causal attention mask for training. Furthermore, incorporating historical inputs only in lower transformer layers enhances performance and efficiency. We validate the effectiveness of ARD in a class-conditioned generation on ImageNet and T2I synthesis. Our model achieves a 5 × 5 × reduction in FID degradation compared to the baseline methods while requiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of 1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available 1024p text-to-image distilled models in prompt adherence score with a minimal drop in FID compared to the teacher. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF",
      "index": 84,
      "title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
      "authors": [
        "Jihan Yang",
        "Shusheng Yang",
        "Anjali W. Gupta",
        "Rilyn Han",
        "Li Fei-Fei",
        "Saining Xie"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "mllms",
        "spatial",
        "remember",
        "visual",
        "multimodal",
        "intelligence",
        "subhuman",
        "think",
        "awareness",
        "models"
      ],
      "summary": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also \"think in space\" from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive—though subhuman—visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance awareness.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 79,
        "kimi": 26
      },
      "raw_excerpt": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces [PDF 79 ] [Copy] [Kimi 26 ] [REL] Authors : Jihan Yang , Shusheng Yang , Anjali W. Gupta , Rilyn Han , Li Fei-Fei , Saining Xie Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also \"think in space\" from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive—though subhuman—visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance awareness. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF",
      "index": 85,
      "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World",
      "authors": [
        "Bangyan Liao",
        "Zhenjun Zhao",
        "Haoang Li",
        "Yi Zhou",
        "Yingping Zeng",
        "Hao Li",
        "Peidong Liu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "vps",
        "manhattan",
        "globustvp",
        "world",
        "convex",
        "sdp",
        "relaxation",
        "vanishing",
        "association",
        "optimality"
      ],
      "summary": "Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a “soft” association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs’ locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called GlobustVP), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that GlobustVP achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. We will release our code for further study.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 30,
        "kimi": 21
      },
      "raw_excerpt": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World [PDF 30 ] [Copy] [Kimi 21 ] [REL] Authors : Bangyan Liao , Zhenjun Zhao , Haoang Li , Yi Zhou , Yingping Zeng , Hao Li , Peidong Liu Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a “soft” association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs’ locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called GlobustVP), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that GlobustVP achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. We will release our code for further study. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF",
      "index": 86,
      "title": "Zero-Shot Monocular Scene Flow Estimation in the Wild",
      "authors": [
        "Yiqing Liang",
        "Abhishek Badki",
        "Hang Su",
        "James Tompkin",
        "Orazio Gallo"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "scene",
        "flow",
        "wild",
        "robotap",
        "shot",
        "foundation",
        "prediction",
        "scenes",
        "casually",
        "monocular"
      ],
      "summary": "Foundation models have shown generalization across datasets for many low-level vision tasks, like depth estimation, but no such model exists for scene flow.Even though scene flow has wide potential use, it is not used in practice because current predictive models do not generalize well.We solve three challenges to fix this problem.First, we create a method that jointly estimates geometry and motion for accurate prediction.Second, we alleviate scene flow data scarcity with a data recipe that affords us 1M annotated training samples across diverse synthetic scenes.Third, we evaluate different parameterizations for scene flow prediction and identify a natural and effective parameterization.Our resulting model outperforms existing methods as well baselines built on foundation models in term of 3D end-point error, and shows zero-shot generalization to the casually captured videos from DAVIS and the robotic manipulation scenes from RoboTAP.Overall, this makes scene flow prediction significantly more practical for in-the-wild use.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 52,
        "kimi": 19
      },
      "raw_excerpt": "Zero-Shot Monocular Scene Flow Estimation in the Wild [PDF 52 ] [Copy] [Kimi 19 ] [REL] Authors : Yiqing Liang , Abhishek Badki , Hang Su , James Tompkin , Orazio Gallo Foundation models have shown generalization across datasets for many low-level vision tasks, like depth estimation, but no such model exists for scene flow.Even though scene flow has wide potential use, it is not used in practice because current predictive models do not generalize well.We solve three challenges to fix this problem.First, we create a method that jointly estimates geometry and motion for accurate prediction.Second, we alleviate scene flow data scarcity with a data recipe that affords us 1M annotated training samples across diverse synthetic scenes.Third, we evaluate different parameterizations for scene flow prediction and identify a natural and effective parameterization.Our resulting model outperforms existing methods as well baselines built on foundation models in term of 3D end-point error, and shows zero-shot generalization to the casually captured videos from DAVIS and the robotic manipulation scenes from RoboTAP.Overall, this makes scene flow prediction significantly more practical for in-the-wild use. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF",
      "index": 87,
      "title": "OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels",
      "authors": [
        "Meng Lou",
        "Yizhou Yu"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "overlock",
        "convnet",
        "top",
        "dds",
        "contmix",
        "context",
        "biomimetic",
        "overview",
        "look",
        "dynamic"
      ],
      "summary": "In the human vision system, top-down attention plays a crucial role in perception, wherein the brain initially performs an overall but rough scene analysis to extract salient cues (i.e., overview first), followed by a finer-grained examination to make more accurate judgments (i.e., look closely next). However, recent efforts in ConvNet designs primarily focused on increasing kernel size to obtain a larger receptive field without considering this crucial biomimetic mechanism to further improve performance. To this end, we propose a novel pure ConvNet vision backbone, termed OverLoCK, which is carefully devised from both the architecture and mixer perspectives. Specifically, we introduce a biomimetic Deep-stage Decomposition Strategy (DDS) that fuses semantically meaningful context representations into middle and deep layers by providing dynamic top-down context guidance at both feature and kernel weight levels. To fully unleash the power of top-down context guidance, we further propose a novel **Cont**ext-**Mix**ing Dynamic Convolution (ContMix) that effectively models long-range dependencies while preserving inherent local inductive biases even when the input resolution increases. These properties are absent in previous convolutions. With the support from both DDS and ContMix, our OverLoCK exhibits notable performance improvement over existing methods. For instance, OverLoCK-T achieves a Top-1 accuracy of 84.2\\%, significantly surpassing ConvNeXt-B while only using around one-third of the FLOPs/parameters. On object detection with Cascade Mask R-CNN, our OverLoCK-S surpasses MogaNet-B by a significant 1\\% in AP b b . On semantic segmentation with UperNet, our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7\\% in mIoU.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 46,
        "kimi": 20
      },
      "raw_excerpt": "OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels [PDF 46 ] [Copy] [Kimi 20 ] [REL] Authors : Meng Lou , Yizhou Yu In the human vision system, top-down attention plays a crucial role in perception, wherein the brain initially performs an overall but rough scene analysis to extract salient cues (i.e., overview first), followed by a finer-grained examination to make more accurate judgments (i.e., look closely next). However, recent efforts in ConvNet designs primarily focused on increasing kernel size to obtain a larger receptive field without considering this crucial biomimetic mechanism to further improve performance. To this end, we propose a novel pure ConvNet vision backbone, termed OverLoCK, which is carefully devised from both the architecture and mixer perspectives. Specifically, we introduce a biomimetic Deep-stage Decomposition Strategy (DDS) that fuses semantically meaningful context representations into middle and deep layers by providing dynamic top-down context guidance at both feature and kernel weight levels. To fully unleash the power of top-down context guidance, we further propose a novel **Cont**ext-**Mix**ing Dynamic Convolution (ContMix) that effectively models long-range dependencies while preserving inherent local inductive biases even when the input resolution increases. These properties are absent in previous convolutions. With the support from both DDS and ContMix, our OverLoCK exhibits notable performance improvement over existing methods. For instance, OverLoCK-T achieves a Top-1 accuracy of 84.2\\%, significantly surpassing ConvNeXt-B while only using around one-third of the FLOPs/parameters. On object detection with Cascade Mask R-CNN, our OverLoCK-S surpasses MogaNet-B by a significant 1\\% in AP b b . On semantic segmentation with UperNet, our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7\\% in mIoU. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF",
      "index": 88,
      "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
      "authors": [
        "Jieming Cui",
        "Tengyu Liu",
        "Ziyu Meng",
        "Jiale Yu",
        "Ran Song",
        "Wei Zhang",
        "Yixin Zhu",
        "Siyuan Huang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "grove",
        "vocabulary",
        "reward",
        "skill",
        "open",
        "physical",
        "vlm",
        "learning",
        "generalized",
        "pose2clip"
      ],
      "summary": "Learning open-vocabulary physical skills for simulated agents remains challenging due to the limitations of reinforcement learning approaches: manually designed rewards lack scalability, while demonstration-based methods struggle to cover arbitrary tasks. We propose GROVE, a generalized reward framework for open-vocabulary physical skill learning without manual reward design or task-specific demonstrations. GROVE uniquely combines Large Language Models (LLMs) for generating precise constraints with Vision Language Models (VLMs) for semantic evaluation. Through an iterative reward design process, VLM-based feedback guides the refinement of LLM-generated constraints, significantly enhancing the reliability of our method. Central to our approach is Pose2CLIP, a lightweight pose-to-semantic feature mapper that significantly enhances the quality and efficiency of VLM evaluation. Extensive experiments demonstrate GROVE's versatility across diverse tasks and learning paradigms. Our approach achieves 22.2% higher naturalness and 25.7% better task completion score while training 8.4 times faster than previous open-vocabulary methods, establishing a new foundation for scalable physical skill acquisition.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 36,
        "kimi": 20
      },
      "raw_excerpt": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill [PDF 36 ] [Copy] [Kimi 20 ] [REL] Authors : Jieming Cui , Tengyu Liu , Ziyu Meng , Jiale Yu , Ran Song , Wei Zhang , Yixin Zhu , Siyuan Huang Learning open-vocabulary physical skills for simulated agents remains challenging due to the limitations of reinforcement learning approaches: manually designed rewards lack scalability, while demonstration-based methods struggle to cover arbitrary tasks. We propose GROVE, a generalized reward framework for open-vocabulary physical skill learning without manual reward design or task-specific demonstrations. GROVE uniquely combines Large Language Models (LLMs) for generating precise constraints with Vision Language Models (VLMs) for semantic evaluation. Through an iterative reward design process, VLM-based feedback guides the refinement of LLM-generated constraints, significantly enhancing the reliability of our method. Central to our approach is Pose2CLIP, a lightweight pose-to-semantic feature mapper that significantly enhances the quality and efficiency of VLM evaluation. Extensive experiments demonstrate GROVE's versatility across diverse tasks and learning paradigms. Our approach achieves 22.2% higher naturalness and 25.7% better task completion score while training 8.4 times faster than previous open-vocabulary methods, establishing a new foundation for scalable physical skill acquisition. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF",
      "index": 89,
      "title": "Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space",
      "authors": [
        "Yifan Zhou",
        "Zeqi Xiao",
        "Shuai Yang",
        "Xingang Pan"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "equivariance",
        "ldm",
        "ldms",
        "shift",
        "aliasing",
        "alias",
        "latent",
        "redesign",
        "diffusion",
        "equivariant"
      ],
      "summary": "Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 36,
        "kimi": 20
      },
      "raw_excerpt": "Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space [PDF 36 ] [Copy] [Kimi 20 ] [REL] Authors : Yifan Zhou , Zeqi Xiao , Shuai Yang , Xingang Pan Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF",
      "index": 90,
      "title": "Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key",
      "authors": [
        "Zhihe Yang",
        "Xufang Luo",
        "Dongqi Han",
        "Yunjian Xu",
        "Dongsheng Li"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "dpo",
        "policy",
        "hallucination",
        "hallucinations",
        "opa",
        "responses",
        "aligns",
        "vision",
        "preference",
        "expert"
      ],
      "summary": "Hallucination remains a major challenge for Large Vision-Language Models (LVLMs). Direct Preference Optimization (DPO) has gained increasing attention as a simple solution to hallucination issues. It directly learns from constructed preference pairs that reflect the severity of hallucinations in responses to the same prompt and image. Nonetheless, different data construction methods in existing works bring notable performance variations. We identify a crucial factor here: outcomes are largely contingent on whether the constructed data aligns on-policy w.r.t the initial (reference) policy of DPO. Theoretical analysis suggests that learning from off-policy data is impeded by the presence of KL-divergence between the updated policy and the reference policy. From the perspective of dataset distribution, we systematically summarize the inherent flaws in existing algorithms that employ DPO to address hallucination issues. To alleviate the problems, we propose On-Policy Alignment (OPA)-DPO framework, which uniquely leverages expert feedback to correct hallucinated responses and aligns both the original and expert-revised responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B: 13.26\\% on the AMBER benchmark and 5.39\\% on the Object-Hal benchmark, compared to the previous SOTA algorithm trained with 16k samples.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 45,
        "kimi": 17
      },
      "raw_excerpt": "Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key [PDF 45 ] [Copy] [Kimi 17 ] [REL] Authors : Zhihe Yang , Xufang Luo , Dongqi Han , Yunjian Xu , Dongsheng Li Hallucination remains a major challenge for Large Vision-Language Models (LVLMs). Direct Preference Optimization (DPO) has gained increasing attention as a simple solution to hallucination issues. It directly learns from constructed preference pairs that reflect the severity of hallucinations in responses to the same prompt and image. Nonetheless, different data construction methods in existing works bring notable performance variations. We identify a crucial factor here: outcomes are largely contingent on whether the constructed data aligns on-policy w.r.t the initial (reference) policy of DPO. Theoretical analysis suggests that learning from off-policy data is impeded by the presence of KL-divergence between the updated policy and the reference policy. From the perspective of dataset distribution, we systematically summarize the inherent flaws in existing algorithms that employ DPO to address hallucination issues. To alleviate the problems, we propose On-Policy Alignment (OPA)-DPO framework, which uniquely leverages expert feedback to correct hallucinated responses and aligns both the original and expert-revised responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B: 13.26\\% on the AMBER benchmark and 5.39\\% on the Object-Hal benchmark, compared to the previous SOTA algorithm trained with 16k samples. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF",
      "index": 91,
      "title": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons",
      "authors": [
        "Andrew Szot",
        "Bogdan Mazoure",
        "Omar Attia",
        "Aleksei Timofeev",
        "Harsh Agrawal",
        "Devon Hjelm",
        "Zhe Gan",
        "Zsolt Kira",
        "Alexander Toshev"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "gea",
        "generalist",
        "embodied",
        "multimodal",
        "agents",
        "lessons",
        "diverse",
        "online",
        "mllms",
        "mllm"
      ],
      "summary": "We examine the capability of Multimodal Large Language Models (MLLMs) to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. Specifically, our focus lies in areas such as Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. GEA is trained with supervised learning on a large dataset of embodied experiences and with online RL in interactive simulators. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents. The final GEA model achieves strong generalization performance to unseen tasks across diverse benchmarks compared to other generalist models and benchmark-specific approaches.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 44,
        "kimi": 22
      },
      "raw_excerpt": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons [PDF 44 ] [Copy] [Kimi 22 ] [REL] Authors : Andrew Szot , Bogdan Mazoure , Omar Attia , Aleksei Timofeev , Harsh Agrawal , Devon Hjelm , Zhe Gan , Zsolt Kira , Alexander Toshev We examine the capability of Multimodal Large Language Models (MLLMs) to tackle diverse domains that extend beyond the traditional language and vision tasks these models are typically trained on. Specifically, our focus lies in areas such as Embodied AI, Games, UI Control, and Planning. To this end, we introduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA). GEA is a single unified model capable of grounding itself across these varied domains through a multi-embodiment action tokenizer. GEA is trained with supervised learning on a large dataset of embodied experiences and with online RL in interactive simulators. We explore the data and algorithmic choices necessary to develop such a model. Our findings reveal the importance of training with cross-domain data and online RL for building generalist agents. The final GEA model achieves strong generalization performance to unseen tasks across diverse benchmarks compared to other generalist models and benchmark-specific approaches. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF",
      "index": 92,
      "title": "DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution",
      "authors": [
        "Zhengxue Wang",
        "Zhiqiang Yan",
        "Jinshan Pan",
        "Guangwei Gao",
        "Kai Zhang",
        "Jian Yang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "degradation",
        "dornet",
        "depth",
        "rgb",
        "oriented",
        "regularized",
        "unknown",
        "resolution",
        "super",
        "real"
      ],
      "summary": "Recent RGB-guided depth super-resolution methods have achieved impressive performance under the assumption of fixed and known degradation (e.g., bicubic downsampling). However, in real-world scenarios, captured depth data often suffer from unconventional and unknown degradation due to sensor limitations and complex imaging environments (e.g., low reflective surfaces, varying illumination). Consequently, the performance of these methods significantly declines when real-world degradation deviate from their assumptions. In this paper, we propose the Degradation Oriented and Regularized Network (DORNet), a novel framework designed to adaptively address unknown degradation in real-world scenes through implicit degradation representations. Our approach begins with the development of a self-supervised degradation learning strategy, which models the degradation representations of low-resolution depth data using routing selection-based degradation regularization. To facilitate effective RGB-D fusion, we further introduce a degradation-oriented feature transformation module that selectively propagates RGB content into the depth data based on the learned degradation priors. Extensive experimental results on both real and synthetic datasets demonstrate the superiority of our DORNet in handling unknown degradations, outperforming existing methods.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 42,
        "kimi": 17
      },
      "raw_excerpt": "DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution [PDF 42 ] [Copy] [Kimi 17 ] [REL] Authors : Zhengxue Wang , Zhiqiang Yan , Jinshan Pan , Guangwei Gao , Kai Zhang , Jian Yang Recent RGB-guided depth super-resolution methods have achieved impressive performance under the assumption of fixed and known degradation (e.g., bicubic downsampling). However, in real-world scenarios, captured depth data often suffer from unconventional and unknown degradation due to sensor limitations and complex imaging environments (e.g., low reflective surfaces, varying illumination). Consequently, the performance of these methods significantly declines when real-world degradation deviate from their assumptions. In this paper, we propose the Degradation Oriented and Regularized Network (DORNet), a novel framework designed to adaptively address unknown degradation in real-world scenes through implicit degradation representations. Our approach begins with the development of a self-supervised degradation learning strategy, which models the degradation representations of low-resolution depth data using routing selection-based degradation regularization. To facilitate effective RGB-D fusion, we further introduce a degradation-oriented feature transformation module that selectively propagates RGB content into the depth data based on the learned degradation priors. Extensive experimental results on both real and synthetic datasets demonstrate the superiority of our DORNet in handling unknown degradations, outperforming existing methods. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF",
      "index": 93,
      "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
      "authors": [
        "Pascal Chang",
        "Sergio Sancho",
        "Jingwei Tang",
        "Markus Gross",
        "Vinicius Azevedo"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "warping",
        "illusions",
        "viewed",
        "generative",
        "pyramid",
        "lookingglass",
        "anamorphoses",
        "laplacian",
        "anamorphosis",
        "anagrams"
      ],
      "summary": "Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce _Laplacian Pyramid Warping_, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams [Geng et al. 2024] to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 45,
        "kimi": 24
      },
      "raw_excerpt": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping [PDF 45 ] [Copy] [Kimi 24 ] [REL] Authors : Pascal Chang , Sergio Sancho , Jingwei Tang , Markus Gross , Vinicius Azevedo Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce _Laplacian Pyramid Warping_, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams [Geng et al. 2024] to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF",
      "index": 94,
      "title": "DiffFNO: Diffusion Fourier Neural Operator",
      "authors": [
        "Xiaoyi Liu",
        "Hao Tang"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "difffno",
        "ode",
        "wfno",
        "fourier",
        "super",
        "operator",
        "resolution",
        "neural",
        "diffusion",
        "details"
      ],
      "summary": "We introduce DiffFNO, a novel framework for arbitrary-scale super-resolution that incorporates a Weighted Fourier Neural Operator (WFNO) enhanced by a diffusion process. DiffFNO's adaptive mode weighting mechanism in the Fourier domain effectively captures critical frequency components, significantly improving the reconstruction of high-frequency image details that are essential for super-resolution tasks.Additionally, we propose a Gated Fusion Mechanism to efficiently integrate features from the WFNO and an attention-based neural operator, enhancing the network's capability to capture both global and local image details. To further improve efficiency, DiffFNO employs a deterministic ODE sampling strategy called the Adaptive Time-step ODE Solver (AT-ODE), which accelerates inference by dynamically adjusting step sizes while preserving output quality.Extensive experiments demonstrate that DiffFNO achieves state-of-the-art results, outperforming existing methods across various scaling factors, including those beyond the training distribution, by a margin of 2–4 dB in PSNR. Our approach sets a new standard in super-resolution, delivering both superior accuracy and computational efficiency.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 82,
        "kimi": 35
      },
      "raw_excerpt": "DiffFNO: Diffusion Fourier Neural Operator [PDF 82 ] [Copy] [Kimi 35 ] [REL] Authors : Xiaoyi Liu , Hao Tang We introduce DiffFNO, a novel framework for arbitrary-scale super-resolution that incorporates a Weighted Fourier Neural Operator (WFNO) enhanced by a diffusion process. DiffFNO's adaptive mode weighting mechanism in the Fourier domain effectively captures critical frequency components, significantly improving the reconstruction of high-frequency image details that are essential for super-resolution tasks.Additionally, we propose a Gated Fusion Mechanism to efficiently integrate features from the WFNO and an attention-based neural operator, enhancing the network's capability to capture both global and local image details. To further improve efficiency, DiffFNO employs a deterministic ODE sampling strategy called the Adaptive Time-step ODE Solver (AT-ODE), which accelerates inference by dynamically adjusting step sizes while preserving output quality.Extensive experiments demonstrate that DiffFNO achieves state-of-the-art results, outperforming existing methods across various scaling factors, including those beyond the training distribution, by a margin of 2–4 dB in PSNR. Our approach sets a new standard in super-resolution, delivering both superior accuracy and computational efficiency. Subject : CVPR.2025 - Oral"
    },
    {
      "paper_id": "Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF",
      "index": 95,
      "title": "DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
      "authors": [
        "Jay Zhangjie Wu",
        "Yuxuan Zhang",
        "Haithem Turki",
        "Xuanchi Ren",
        "Jun Gao",
        "Mike Zheng Shou",
        "Sanja Fidler",
        "Zan Gojcic",
        "Huan Ling"
      ],
      "subjects": [
        "CVPR.2025 - Oral"
      ],
      "keywords": [
        "difix3d",
        "difix",
        "reconstruction",
        "underconstrained",
        "artifacts",
        "rendered",
        "novel",
        "step",
        "diffusion",
        "single"
      ],
      "summary": "Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation.Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2x improvement in FID score over baselines while maintaining 3D consistency.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models_CVPR_2025_paper.html"
        ],
        "venue": [
          "/venue/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models@CVPR2025@CVF",
          "/venue/CVPR.2025?group=Oral"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models_CVPR_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 85,
        "kimi": 39
      },
      "raw_excerpt": "DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models [PDF 85 ] [Copy] [Kimi 39 ] [REL] Authors : Jay Zhangjie Wu , Yuxuan Zhang , Haithem Turki , Xuanchi Ren , Jun Gao , Mike Zheng Shou , Sanja Fidler , Zan Gojcic , Huan Ling Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation.Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2x improvement in FID score over baselines while maintaining 3D consistency. Subject : CVPR.2025 - Oral"
    }
  ]
}