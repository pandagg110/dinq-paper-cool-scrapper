<html><head>
    <title>ICLR.2025 - Oral | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for ICLR.2025 - Oral, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_Typewriter; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf?V=2.7.9') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.9') format('opentype')}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">ICLR.2025 - Oral</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 207
    </p>
    <div class="papers">
        <div id="GMwRl2e9Y1@OpenReview" class="panel paper" keywords="vector,codebook,quantization,encoder,restructuring,vaes,rotation,trick,output,layer">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GMwRl2e9Y1" target="_blank" title="1/207"><span class="index notranslate">#1</span></a>
                <a id="title-GMwRl2e9Y1@OpenReview" class="title-link" href="/venue/GMwRl2e9Y1@OpenReview" target="_blank">Restructuring Vector Quantization with the Rotation Trick</a>
                <a id="pdf-GMwRl2e9Y1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GMwRl2e9Y1@OpenReview', this)" data="https://openreview.net/pdf?id=GMwRl2e9Y1">[PDF<sup id="pdf-stars-GMwRl2e9Y1@OpenReview">380</sup>]</a>
                <a id="copy-GMwRl2e9Y1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GMwRl2e9Y1@OpenReview')">[Copy]</a>
                <a id="kimi-GMwRl2e9Y1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GMwRl2e9Y1@OpenReview', this)">[Kimi<sup id="kimi-stars-GMwRl2e9Y1@OpenReview">461</sup>]</a>
                <a id="rel-GMwRl2e9Y1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GMwRl2e9Y1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GMwRl2e9Y1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Fifty" target="_blank">Christopher Fifty</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronald Junkins" target="_blank">Ronald Junkins</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dennis Duan" target="_blank">Dennis Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aniketh Iyengar" target="_blank">Aniketh Iyengar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jerry Liu" target="_blank">Jerry Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ehsan Amid" target="_blank">Ehsan Amid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Thrun" target="_blank">Sebastian Thrun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Re" target="_blank">Christopher Re</a>
            </p>
            <p id="summary-GMwRl2e9Y1@OpenReview" class="summary">Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows _around_ the vector quantization layer rather than _through_ it in a straight-through approximation.This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder.Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error.</p>
            <p id="subjects-GMwRl2e9Y1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-GMwRl2e9Y1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GMwRl2e9Y1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GMwRl2e9Y1@OpenReview" onclick="foldPdfKimi('GMwRl2e9Y1@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="GRMfXcAAFh@OpenReview" class="panel paper" keywords="linoss,oscillatory,long,space,state,stable,discretization,range,forecasting,sequences">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GRMfXcAAFh" target="_blank" title="2/207"><span class="index notranslate">#2</span></a>
                <a id="title-GRMfXcAAFh@OpenReview" class="title-link" href="/venue/GRMfXcAAFh@OpenReview" target="_blank">Oscillatory State-Space Models</a>
                <a id="pdf-GRMfXcAAFh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GRMfXcAAFh@OpenReview', this)" data="https://openreview.net/pdf?id=GRMfXcAAFh">[PDF<sup id="pdf-stars-GRMfXcAAFh@OpenReview">185</sup>]</a>
                <a id="copy-GRMfXcAAFh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GRMfXcAAFh@OpenReview')">[Copy]</a>
                <a id="kimi-GRMfXcAAFh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GRMfXcAAFh@OpenReview', this)">[Kimi<sup id="kimi-stars-GRMfXcAAFh@OpenReview">219</sup>]</a>
                <a id="rel-GRMfXcAAFh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GRMfXcAAFh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GRMfXcAAFh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=T. Konstantin Rusch" target="_blank">T. Konstantin Rusch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniela Rus" target="_blank">Daniela Rus</a>
            </p>
            <p id="summary-GRMfXcAAFh@OpenReview" class="summary">We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with sequences of length 50k.</p>
            <p id="subjects-GRMfXcAAFh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-GRMfXcAAFh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GRMfXcAAFh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GRMfXcAAFh@OpenReview" onclick="foldPdfKimi('GRMfXcAAFh@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="cmfyMV45XO@OpenReview" class="panel paper" keywords="feedback,neural,odes,latent,generalization,favors,neatly,correct,learned,dynamics">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cmfyMV45XO" target="_blank" title="3/207"><span class="index notranslate">#3</span></a>
                <a id="title-cmfyMV45XO@OpenReview" class="title-link" href="/venue/cmfyMV45XO@OpenReview" target="_blank">Feedback Favors the Generalization of Neural ODEs</a>
                <a id="pdf-cmfyMV45XO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cmfyMV45XO@OpenReview', this)" data="https://openreview.net/pdf?id=cmfyMV45XO">[PDF<sup id="pdf-stars-cmfyMV45XO@OpenReview">113</sup>]</a>
                <a id="copy-cmfyMV45XO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cmfyMV45XO@OpenReview')">[Copy]</a>
                <a id="kimi-cmfyMV45XO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cmfyMV45XO@OpenReview', this)">[Kimi<sup id="kimi-stars-cmfyMV45XO@OpenReview">161</sup>]</a>
                <a id="rel-cmfyMV45XO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cmfyMV45XO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cmfyMV45XO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jindou Jia" target="_blank">Jindou Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihan Yang" target="_blank">Zihan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Wang" target="_blank">Meng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kexin Guo" target="_blank">Kexin Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfei Yang" target="_blank">Jianfei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Yu" target="_blank">Xiang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Guo" target="_blank">Lei Guo</a>
            </p>
            <p id="summary-cmfyMV45XO@OpenReview" class="summary">The well-known generalization problem hinders the application of artificial neural networks in continuous-time prediction tasks with varying latent dynamics. In sharp contrast, biological systems can neatly adapt to evolving environments benefiting from real-time feedback mechanisms. Inspired by the feedback philosophy, we present feedback neural networks, showing that a feedback loop can flexibly correct the learned latent dynamics of neural ordinary differential equations (neural ODEs), leading to a prominent generalization improvement. The feedback neural network is a novel two-DOF neural network, which possesses robust performance in unseen scenarios with no loss of accuracy performance on previous tasks. A linear feedback form is presented to correct the learned latent dynamics firstly, with a convergence guarantee. Then, domain randomization is utilized to learn a nonlinear neural feedback form. Finally, extensive tests including trajectory prediction of a real irregular object and model predictive control of a quadrotor with various uncertainties, are implemented, indicating significant improvements over state-of-the-art model-based and learning-based methods.</p>
            <p id="subjects-cmfyMV45XO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-cmfyMV45XO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cmfyMV45XO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cmfyMV45XO@OpenReview" onclick="foldPdfKimi('cmfyMV45XO@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="k38Th3x4d9@OpenReview" class="panel paper" keywords="aerca,root,anomalies,granger,series,exogenous,causes,causal,multivariate,cause">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=k38Th3x4d9" target="_blank" title="4/207"><span class="index notranslate">#4</span></a>
                <a id="title-k38Th3x4d9@OpenReview" class="title-link" href="/venue/k38Th3x4d9@OpenReview" target="_blank">Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery</a>
                <a id="pdf-k38Th3x4d9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('k38Th3x4d9@OpenReview', this)" data="https://openreview.net/pdf?id=k38Th3x4d9">[PDF<sup id="pdf-stars-k38Th3x4d9@OpenReview">79</sup>]</a>
                <a id="copy-k38Th3x4d9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('k38Th3x4d9@OpenReview')">[Copy]</a>
                <a id="kimi-k38Th3x4d9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('k38Th3x4d9@OpenReview', this)">[Kimi<sup id="kimi-stars-k38Th3x4d9@OpenReview">111</sup>]</a>
                <a id="rel-k38Th3x4d9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('k38Th3x4d9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-k38Th3x4d9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Han" target="_blank">Xiao Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saima Absar" target="_blank">Saima Absar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Zhang" target="_blank">Lu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhan Yuan" target="_blank">Shuhan Yuan</a>
            </p>
            <p id="summary-k38Th3x4d9@OpenReview" class="summary">Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies as interventions on the exogenous variables of time series, AERCA not only learns the Granger causality among time series but also explicitly models the distributions of exogenous variables under normal conditions. AERCA then identifies the root causes of anomalies by highlighting exogenous variables that significantly deviate from their normal states. Experiments on multiple synthetic and real-world datasets demonstrate that AERCA can accurately capture the causal relationships among time series and effectively identify the root causes of anomalies.</p>
            <p id="subjects-k38Th3x4d9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-k38Th3x4d9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-k38Th3x4d9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-k38Th3x4d9@OpenReview" onclick="foldPdfKimi('k38Th3x4d9@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="rdv6yeMFpn@OpenReview" class="panel paper" keywords="gnns,homomorphism,spectral,expressivity,citet,invariant,graph,invariants,quantitative,arvind2024hierarchy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rdv6yeMFpn" target="_blank" title="5/207"><span class="index notranslate">#5</span></a>
                <a id="title-rdv6yeMFpn@OpenReview" class="title-link" href="/venue/rdv6yeMFpn@OpenReview" target="_blank">Homomorphism Expressivity of Spectral Invariant Graph Neural Networks</a>
                <a id="pdf-rdv6yeMFpn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rdv6yeMFpn@OpenReview', this)" data="https://openreview.net/pdf?id=rdv6yeMFpn">[PDF<sup id="pdf-stars-rdv6yeMFpn@OpenReview">75</sup>]</a>
                <a id="copy-rdv6yeMFpn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rdv6yeMFpn@OpenReview')">[Copy]</a>
                <a id="kimi-rdv6yeMFpn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rdv6yeMFpn@OpenReview', this)">[Kimi<sup id="kimi-stars-rdv6yeMFpn@OpenReview">97</sup>]</a>
                <a id="rel-rdv6yeMFpn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rdv6yeMFpn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rdv6yeMFpn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingchu Gai" target="_blank">Jingchu Gai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiheng Du" target="_blank">Yiheng Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bohang Zhang" target="_blank">Bohang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haggai Maron" target="_blank">Haggai Maron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Wang" target="_blank">Liwei Wang</a>
            </p>
            <p id="summary-rdv6yeMFpn@OpenReview" class="summary">Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs --- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as \emph{parallel trees}. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend \citet{arvind2024hierarchy} and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by \citet{zhang2024expressive}.</p>
            <p id="subjects-rdv6yeMFpn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-rdv6yeMFpn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rdv6yeMFpn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rdv6yeMFpn@OpenReview" onclick="foldPdfKimi('rdv6yeMFpn@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="vf5aUZT0Fz@OpenReview" class="panel paper" keywords="dept,training,pre,curse,vocabulary,billion,embeddings,decoupled,syntactical,semantical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vf5aUZT0Fz" target="_blank" title="6/207"><span class="index notranslate">#6</span></a>
                <a id="title-vf5aUZT0Fz@OpenReview" class="title-link" href="/venue/vf5aUZT0Fz@OpenReview" target="_blank">DEPT: Decoupled Embeddings for Pre-training Language Models</a>
                <a id="pdf-vf5aUZT0Fz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vf5aUZT0Fz@OpenReview', this)" data="https://openreview.net/pdf?id=vf5aUZT0Fz">[PDF<sup id="pdf-stars-vf5aUZT0Fz@OpenReview">162</sup>]</a>
                <a id="copy-vf5aUZT0Fz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vf5aUZT0Fz@OpenReview')">[Copy]</a>
                <a id="kimi-vf5aUZT0Fz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vf5aUZT0Fz@OpenReview', this)">[Kimi<sup id="kimi-stars-vf5aUZT0Fz@OpenReview">202</sup>]</a>
                <a id="rel-vf5aUZT0Fz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vf5aUZT0Fz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vf5aUZT0Fz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Iacob" target="_blank">Alex Iacob</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Sani" target="_blank">Lorenzo Sani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meghdad Kurmanji" target="_blank">Meghdad Kurmanji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William Shen" target="_blank">William Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchi Qiu" target="_blank">Xinchi Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongqi Cai" target="_blank">Dongqi Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Gao" target="_blank">Yan Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nic Lane" target="_blank">Nic Lane</a>
            </p>
            <p id="summary-vf5aUZT0Fz@OpenReview" class="summary">Past works have shown that lexical, syntactical, and semantical differences in heterogeneous data sources can cause challenges such as negative interference or the ``curse of multilinguality''. Because of this, training on such heterogeneous corpora requires extensive and costly efforts to balance data mixtures. We propose a novel pre-training framework to alleviate this curse. Our method, DEPT, decouples embeddings from the transformer body while simultaneously training the latter in multiple contexts without a shared global vocabulary. DEPT: (1) trains robustly and effectively under significant data heterogeneity, (2) reduces token embedding parameters by up to 80% and communication costs by 714x for billion-scale models, (3) enhances transformer body plasticity and generalization, improving average perplexity upward of 15.3-20% and improving performance for downstream fine-tuning in our experiments, and (4) permits training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated multilingual pre-training of a billion-scale model, reducing total parameters by 24% versus standard training.</p>
            <p id="subjects-vf5aUZT0Fz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-vf5aUZT0Fz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vf5aUZT0Fz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vf5aUZT0Fz@OpenReview" onclick="foldPdfKimi('vf5aUZT0Fz@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="wM2sfVgMDH@OpenReview" class="panel paper" keywords="planning,driving,planner,behaviors,diffusion,autonomous,guidance,flexible,nuplan,modal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wM2sfVgMDH" target="_blank" title="7/207"><span class="index notranslate">#7</span></a>
                <a id="title-wM2sfVgMDH@OpenReview" class="title-link" href="/venue/wM2sfVgMDH@OpenReview" target="_blank">Diffusion-Based Planning for Autonomous Driving with Flexible Guidance</a>
                <a id="pdf-wM2sfVgMDH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wM2sfVgMDH@OpenReview', this)" data="https://openreview.net/pdf?id=wM2sfVgMDH">[PDF<sup id="pdf-stars-wM2sfVgMDH@OpenReview">101</sup>]</a>
                <a id="copy-wM2sfVgMDH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wM2sfVgMDH@OpenReview')">[Copy]</a>
                <a id="kimi-wM2sfVgMDH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wM2sfVgMDH@OpenReview', this)">[Kimi<sup id="kimi-stars-wM2sfVgMDH@OpenReview">101</sup>]</a>
                <a id="rel-wM2sfVgMDH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wM2sfVgMDH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wM2sfVgMDH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yinan Zheng" target="_blank">Yinan Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiming Liang" target="_blank">Ruiming Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kexin ZHENG" target="_blank">Kexin ZHENG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinliang Zheng" target="_blank">Jinliang Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liyuan Mao" target="_blank">Liyuan Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianxiong Li" target="_blank">Jianxiong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weihao Gu" target="_blank">Weihao Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Ai" target="_blank">Rui Ai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengbo Li" target="_blank">Shengbo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianyuan Zhan" target="_blank">Xianyuan Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Liu" target="_blank">Jingjing Liu</a>
            </p>
            <p id="summary-wM2sfVgMDH@OpenReview" class="summary">Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles.</p>
            <p id="subjects-wM2sfVgMDH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-wM2sfVgMDH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wM2sfVgMDH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wM2sfVgMDH@OpenReview" onclick="foldPdfKimi('wM2sfVgMDH@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="CLE09ESvul@OpenReview" class="panel paper" keywords="information,local,neurons,neuron,pid,designing,goals,learning,achieving,redundantly">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CLE09ESvul" target="_blank" title="8/207"><span class="index notranslate">#8</span></a>
                <a id="title-CLE09ESvul@OpenReview" class="title-link" href="/venue/CLE09ESvul@OpenReview" target="_blank">What should a neuron aim for? Designing local objective functions based on information theory</a>
                <a id="pdf-CLE09ESvul@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CLE09ESvul@OpenReview', this)" data="https://openreview.net/pdf?id=CLE09ESvul">[PDF<sup id="pdf-stars-CLE09ESvul@OpenReview">77</sup>]</a>
                <a id="copy-CLE09ESvul@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CLE09ESvul@OpenReview')">[Copy]</a>
                <a id="kimi-CLE09ESvul@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CLE09ESvul@OpenReview', this)">[Kimi<sup id="kimi-stars-CLE09ESvul@OpenReview">119</sup>]</a>
                <a id="rel-CLE09ESvul@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CLE09ESvul@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CLE09ESvul@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Schneider" target="_blank">Andreas Schneider</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valentin Neuhaus" target="_blank">Valentin Neuhaus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Ehrlich" target="_blank">David Ehrlich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abdullah Makkeh" target="_blank">Abdullah Makkeh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander S Ecker" target="_blank">Alexander S Ecker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viola Priesemann" target="_blank">Viola Priesemann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Wibral" target="_blank">Michael Wibral</a>
            </p>
            <p id="summary-CLE09ESvul@OpenReview" class="summary">In modern deep neural networks, the learning dynamics of the individual neurons is often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. We here show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e. feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies.</p>
            <p id="subjects-CLE09ESvul@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-CLE09ESvul@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CLE09ESvul@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CLE09ESvul@OpenReview" onclick="foldPdfKimi('CLE09ESvul@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="JDm7oIcx4Y@OpenReview" class="panel paper" keywords="backpropagation,residual,gradient,highway,iterative,path,sequentiality,along,architectures,major">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=JDm7oIcx4Y" target="_blank" title="9/207"><span class="index notranslate">#9</span></a>
                <a id="title-JDm7oIcx4Y@OpenReview" class="title-link" href="/venue/JDm7oIcx4Y@OpenReview" target="_blank">Accelerated training through iterative gradient propagation along the residual path</a>
                <a id="pdf-JDm7oIcx4Y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('JDm7oIcx4Y@OpenReview', this)" data="https://openreview.net/pdf?id=JDm7oIcx4Y">[PDF<sup id="pdf-stars-JDm7oIcx4Y@OpenReview">72</sup>]</a>
                <a id="copy-JDm7oIcx4Y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('JDm7oIcx4Y@OpenReview')">[Copy]</a>
                <a id="kimi-JDm7oIcx4Y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('JDm7oIcx4Y@OpenReview', this)">[Kimi<sup id="kimi-stars-JDm7oIcx4Y@OpenReview">85</sup>]</a>
                <a id="rel-JDm7oIcx4Y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('JDm7oIcx4Y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-JDm7oIcx4Y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Erwan Fagnou" target="_blank">Erwan Fagnou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Caillon" target="_blank">Paul Caillon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Blaise Delattre" target="_blank">Blaise Delattre</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Allauzen" target="_blank">Alexandre Allauzen</a>
            </p>
            <p id="summary-JDm7oIcx4Y@OpenReview" class="summary">Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.Such models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures.However, the computational cost of backpropagation remains a major burden, accounting for most of the training time.Taking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.Through an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation.</p>
            <p id="subjects-JDm7oIcx4Y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-JDm7oIcx4Y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-JDm7oIcx4Y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-JDm7oIcx4Y@OpenReview" onclick="foldPdfKimi('JDm7oIcx4Y@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="VVixJ9QavY@OpenReview" class="panel paper" keywords="reasoning,counterfactual,language,factual,metrics,elicitation,fine,models,capabilities,underdeveloped">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VVixJ9QavY" target="_blank" title="10/207"><span class="index notranslate">#10</span></a>
                <a id="title-VVixJ9QavY@OpenReview" class="title-link" href="/venue/VVixJ9QavY@OpenReview" target="_blank">Reasoning Elicitation in Language Models via Counterfactual Feedback</a>
                <a id="pdf-VVixJ9QavY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VVixJ9QavY@OpenReview', this)" data="https://openreview.net/pdf?id=VVixJ9QavY">[PDF<sup id="pdf-stars-VVixJ9QavY@OpenReview">98</sup>]</a>
                <a id="copy-VVixJ9QavY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VVixJ9QavY@OpenReview')">[Copy]</a>
                <a id="kimi-VVixJ9QavY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VVixJ9QavY@OpenReview', this)">[Kimi<sup id="kimi-stars-VVixJ9QavY@OpenReview">144</sup>]</a>
                <a id="rel-VVixJ9QavY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VVixJ9QavY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VVixJ9QavY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alihan Hyk" target="_blank">Alihan Hyk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinnuo Xu" target="_blank">Xinnuo Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacqueline Maasch" target="_blank">Jacqueline Maasch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya Nori" target="_blank">Aditya Nori</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Javier Hernandez" target="_blank">Javier Hernandez</a>
            </p>
            <p id="summary-VVixJ9QavY@OpenReview" class="summary">Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.</p>
            <p id="subjects-VVixJ9QavY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-VVixJ9QavY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VVixJ9QavY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VVixJ9QavY@OpenReview" onclick="foldPdfKimi('VVixJ9QavY@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Ha6RTeWMd0@OpenReview" class="panel paper" keywords="sam,anything,segmentation,segment,video,model,videos,images,promptable,data">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Ha6RTeWMd0" target="_blank" title="11/207"><span class="index notranslate">#11</span></a>
                <a id="title-Ha6RTeWMd0@OpenReview" class="title-link" href="/venue/Ha6RTeWMd0@OpenReview" target="_blank">SAM 2: Segment Anything in Images and Videos</a>
                <a id="pdf-Ha6RTeWMd0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Ha6RTeWMd0@OpenReview', this)" data="https://openreview.net/pdf?id=Ha6RTeWMd0">[PDF<sup id="pdf-stars-Ha6RTeWMd0@OpenReview">91</sup>]</a>
                <a id="copy-Ha6RTeWMd0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Ha6RTeWMd0@OpenReview')">[Copy]</a>
                <a id="kimi-Ha6RTeWMd0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Ha6RTeWMd0@OpenReview', this)">[Kimi<sup id="kimi-stars-Ha6RTeWMd0@OpenReview">74</sup>]</a>
                <a id="rel-Ha6RTeWMd0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Ha6RTeWMd0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Ha6RTeWMd0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nikhila Ravi" target="_blank">Nikhila Ravi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Valentin Gabeur" target="_blank">Valentin Gabeur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan-Ting Hu" target="_blank">Yuan-Ting Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronghang Hu" target="_blank">Ronghang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaitanya Ryali" target="_blank">Chaitanya Ryali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengyu Ma" target="_blank">Tengyu Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haitham Khedr" target="_blank">Haitham Khedr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roman Rdle" target="_blank">Roman Rdle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chloe Rolland" target="_blank">Chloe Rolland</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laura Gustafson" target="_blank">Laura Gustafson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Mintun" target="_blank">Eric Mintun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junting Pan" target="_blank">Junting Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kalyan Vasudev Alwala" target="_blank">Kalyan Vasudev Alwala</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Carion" target="_blank">Nicolas Carion</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao-Yuan Wu" target="_blank">Chao-Yuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ross Girshick" target="_blank">Ross Girshick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Piotr Dollar" target="_blank">Piotr Dollar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Feichtenhofer" target="_blank">Christoph Feichtenhofer</a>
            </p>
            <p id="summary-Ha6RTeWMd0@OpenReview" class="summary">We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code.</p>
            <p id="subjects-Ha6RTeWMd0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Ha6RTeWMd0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ha6RTeWMd0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ha6RTeWMd0@OpenReview" onclick="foldPdfKimi('Ha6RTeWMd0@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="xXTkbTBmqq@OpenReview" class="panel paper" keywords="olmoe,moe,experts,open,deepseekmoe,mixture,16b,language,pretrain,trillion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xXTkbTBmqq" target="_blank" title="12/207"><span class="index notranslate">#12</span></a>
                <a id="title-xXTkbTBmqq@OpenReview" class="title-link" href="/venue/xXTkbTBmqq@OpenReview" target="_blank">OLMoE: Open Mixture-of-Experts Language Models</a>
                <a id="pdf-xXTkbTBmqq@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xXTkbTBmqq@OpenReview', this)" data="https://openreview.net/pdf?id=xXTkbTBmqq">[PDF<sup id="pdf-stars-xXTkbTBmqq@OpenReview">77</sup>]</a>
                <a id="copy-xXTkbTBmqq@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xXTkbTBmqq@OpenReview')">[Copy]</a>
                <a id="kimi-xXTkbTBmqq@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xXTkbTBmqq@OpenReview', this)">[Kimi<sup id="kimi-stars-xXTkbTBmqq@OpenReview">95</sup>]</a>
                <a id="rel-xXTkbTBmqq@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xXTkbTBmqq@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xXTkbTBmqq@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Muennighoff" target="_blank">Niklas Muennighoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Soldaini" target="_blank">Luca Soldaini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dirk Groeneveld" target="_blank">Dirk Groeneveld</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyle Lo" target="_blank">Kyle Lo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Morrison" target="_blank">Jacob Morrison</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sewon Min" target="_blank">Sewon Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijia Shi" target="_blank">Weijia Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pete Walsh" target="_blank">Pete Walsh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oyvind Tafjord" target="_blank">Oyvind Tafjord</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Lambert" target="_blank">Nathan Lambert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuling Gu" target="_blank">Yuling Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shane Arora" target="_blank">Shane Arora</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akshita Bhagia" target="_blank">Akshita Bhagia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dustin Schwenk" target="_blank">Dustin Schwenk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Wadden" target="_blank">David Wadden</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Wettig" target="_blank">Alexander Wettig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binyuan Hui" target="_blank">Binyuan Hui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Dettmers" target="_blank">Tim Dettmers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Douwe Kiela" target="_blank">Douwe Kiela</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Farhadi" target="_blank">Ali Farhadi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Smith" target="_blank">Noah Smith</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pang Wei Koh" target="_blank">Pang Wei Koh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amanpreet Singh" target="_blank">Amanpreet Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanna Hajishirzi" target="_blank">Hanna Hajishirzi</a>
            </p>
            <p id="summary-xXTkbTBmqq@OpenReview" class="summary">We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs.</p>
            <p id="subjects-xXTkbTBmqq@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-xXTkbTBmqq@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xXTkbTBmqq@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xXTkbTBmqq@OpenReview" onclick="foldPdfKimi('xXTkbTBmqq@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="8EfxjTCg2k@OpenReview" class="panel paper" keywords="modegpt,compression,decomposition,frameworkthat,a98,performancewith,modular,ona,tive,meth">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8EfxjTCg2k" target="_blank" title="13/207"><span class="index notranslate">#13</span></a>
                <a id="title-8EfxjTCg2k@OpenReview" class="title-link" href="/venue/8EfxjTCg2k@OpenReview" target="_blank">MoDeGPT: Modular Decomposition for Large Language Model Compression</a>
                <a id="pdf-8EfxjTCg2k@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8EfxjTCg2k@OpenReview', this)" data="https://openreview.net/pdf?id=8EfxjTCg2k">[PDF<sup id="pdf-stars-8EfxjTCg2k@OpenReview">64</sup>]</a>
                <a id="copy-8EfxjTCg2k@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8EfxjTCg2k@OpenReview')">[Copy]</a>
                <a id="kimi-8EfxjTCg2k@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8EfxjTCg2k@OpenReview', this)">[Kimi<sup id="kimi-stars-8EfxjTCg2k@OpenReview">84</sup>]</a>
                <a id="rel-8EfxjTCg2k@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8EfxjTCg2k@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8EfxjTCg2k@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chi-Heng Lin" target="_blank">Chi-Heng Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangqian Gao" target="_blank">Shangqian Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Smith" target="_blank">James Smith</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Patel" target="_blank">Abhishek Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shikhar Tuli" target="_blank">Shikhar Tuli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilin Shen" target="_blank">Yilin Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxia Jin" target="_blank">Hongxia Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yen-Chang Hsu" target="_blank">Yen-Chang Hsu</a>
            </p>
            <p id="summary-8EfxjTCg2k@OpenReview" class="summary">Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources.While recent compression methods based on low-rank matrices show potentialsolutions, they often suffer from significant loss of accuracy or introduce substantialoverhead in parameters and inference time. In this paper, we introduce Modular De-composition (MoDeGPT), a new, efficient, and structured compression frameworkthat overcomes these limitations. MoDeGPT jointly decomposes pairs of consecu-tive subcomponents within Transformer blocks, reduces hidden dimensions throughoutput reconstruction on a larger structural scale than conventional low-rank meth-ods, and repurposes three classical matrix decomposition algorithmsNystrmapproximation, CR decomposition, and SVDto ensure bounded errors in ournovel decomposition approach. Our experiments show that MoDeGPT, withoutrelying on backward propagation, consistently matches or surpasses the performance of prior techniques that depend on gradient information, while achieving a98% reduction in compute costs when compressing a 13B-parameter model. OnLLaMA-2/3 and OPT models, MoDeGPT retains 90-95% of zero-shot performancewith compression rates of 25-30%. The compression process can be completed ona single GPU in a few hours, boosting inference throughput by up to 46%.</p>
            <p id="subjects-8EfxjTCg2k@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-8EfxjTCg2k@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8EfxjTCg2k@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8EfxjTCg2k@OpenReview" onclick="foldPdfKimi('8EfxjTCg2k@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="UvTo3tVBk2@OpenReview" class="panel paper" keywords="lrnns,deltanet,tracking,eigenvalues,mamba,parity,rnns,matrices,state,negative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UvTo3tVBk2" target="_blank" title="14/207"><span class="index notranslate">#14</span></a>
                <a id="title-UvTo3tVBk2@OpenReview" class="title-link" href="/venue/UvTo3tVBk2@OpenReview" target="_blank">Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues</a>
                <a id="pdf-UvTo3tVBk2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UvTo3tVBk2@OpenReview', this)" data="https://openreview.net/pdf?id=UvTo3tVBk2">[PDF<sup id="pdf-stars-UvTo3tVBk2@OpenReview">34</sup>]</a>
                <a id="copy-UvTo3tVBk2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UvTo3tVBk2@OpenReview')">[Copy]</a>
                <a id="kimi-UvTo3tVBk2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UvTo3tVBk2@OpenReview', this)">[Kimi<sup id="kimi-stars-UvTo3tVBk2@OpenReview">46</sup>]</a>
                <a id="rel-UvTo3tVBk2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UvTo3tVBk2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UvTo3tVBk2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Riccardo Grazzi" target="_blank">Riccardo Grazzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julien Siems" target="_blank">Julien Siems</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jrg Franke" target="_blank">Jrg Franke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arber Zela" target="_blank">Arber Zela</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Frank Hutter" target="_blank">Frank Hutter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=massimiliano pontil" target="_blank">massimiliano pontil</a>
            </p>
            <p id="summary-UvTo3tVBk2@OpenReview" class="summary">Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.88em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-4" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-5" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-6" style="font-family: MathJax_Main; padding-left: 0.159em;">1</span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">[0, 1]</script> and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-8" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-9"><span class="mn" id="MathJax-Span-10" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-2">3</script>. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-11" style="width: 3.336em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.763em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.66em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-12"><span class="mo" id="MathJax-Span-13" style="font-family: MathJax_Main;">[</span><span class="mo" id="MathJax-Span-14" style="font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-15" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-16" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-17" style="font-family: MathJax_Main; padding-left: 0.159em;">1</span><span class="mo" id="MathJax-Span-18" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mo></mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-3">[-1, 1]</script>. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.</p>
            <p id="subjects-UvTo3tVBk2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-UvTo3tVBk2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UvTo3tVBk2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UvTo3tVBk2@OpenReview" onclick="foldPdfKimi('UvTo3tVBk2@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="kbjJ9ZOakb@OpenReview" class="panel paper" keywords="invariance,neurons,manifolds,neuron,sensory,visual,aligning,selectivity,manifold,accurately">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kbjJ9ZOakb" target="_blank" title="15/207"><span class="index notranslate">#15</span></a>
                <a id="title-kbjJ9ZOakb@OpenReview" class="title-link" href="/venue/kbjJ9ZOakb@OpenReview" target="_blank">Learning and aligning single-neuron invariance manifolds in visual cortex</a>
                <a id="pdf-kbjJ9ZOakb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kbjJ9ZOakb@OpenReview', this)" data="https://openreview.net/pdf?id=kbjJ9ZOakb">[PDF<sup id="pdf-stars-kbjJ9ZOakb@OpenReview">33</sup>]</a>
                <a id="copy-kbjJ9ZOakb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kbjJ9ZOakb@OpenReview')">[Copy]</a>
                <a id="kimi-kbjJ9ZOakb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kbjJ9ZOakb@OpenReview', this)">[Kimi<sup id="kimi-stars-kbjJ9ZOakb@OpenReview">39</sup>]</a>
                <a id="rel-kbjJ9ZOakb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kbjJ9ZOakb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kbjJ9ZOakb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Bashiri" target="_blank">Mohammad Bashiri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Baroni" target="_blank">Luca Baroni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jn Antolk" target="_blank">Jn Antolk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Sinz" target="_blank">Fabian Sinz</a>
            </p>
            <p id="summary-kbjJ9ZOakb@OpenReview" class="summary">Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. Building on recent advances in learning continuous invariance manifolds, we introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons.</p>
            <p id="subjects-kbjJ9ZOakb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-kbjJ9ZOakb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kbjJ9ZOakb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kbjJ9ZOakb@OpenReview" onclick="foldPdfKimi('kbjJ9ZOakb@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="0ctvBgKFgc@OpenReview" class="panel paper" keywords="ellipsoids,protein,proteins,protcomposer,substructure,layouts,helix,conditioning,enables,designability">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0ctvBgKFgc" target="_blank" title="16/207"><span class="index notranslate">#16</span></a>
                <a id="title-0ctvBgKFgc@OpenReview" class="title-link" href="/venue/0ctvBgKFgc@OpenReview" target="_blank">ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids</a>
                <a id="pdf-0ctvBgKFgc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0ctvBgKFgc@OpenReview', this)" data="https://openreview.net/pdf?id=0ctvBgKFgc">[PDF<sup id="pdf-stars-0ctvBgKFgc@OpenReview">31</sup>]</a>
                <a id="copy-0ctvBgKFgc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0ctvBgKFgc@OpenReview')">[Copy]</a>
                <a id="kimi-0ctvBgKFgc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0ctvBgKFgc@OpenReview', this)">[Kimi<sup id="kimi-stars-0ctvBgKFgc@OpenReview">32</sup>]</a>
                <a id="rel-0ctvBgKFgc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0ctvBgKFgc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0ctvBgKFgc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hannes Strk" target="_blank">Hannes Strk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Jing" target="_blank">Bowen Jing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Geffner" target="_blank">Tomas Geffner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Yim" target="_blank">Jason Yim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommi Jaakkola" target="_blank">Tommi Jaakkola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arash Vahdat" target="_blank">Arash Vahdat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karsten Kreis" target="_blank">Karsten Kreis</a>
            </p>
            <p id="summary-0ctvBgKFgc@OpenReview" class="summary">We develop ProtComposer to generate protein structures conditioned on spatial protein layouts that are specified via a set of 3D ellipsoids capturing substructure shapes and semantics. At inference time, we condition on ellipsoids that are hand-constructed, extracted from existing proteins, or from a statistical model, with each option unlocking new capabilities. Hand-specifying ellipsoids enables users to control the location, size, orientation, secondary structure, and approximate shape of protein substructures. Conditioning on ellipsoids of existing proteins enables redesigning their substructure's connectivity or editing substructure properties. By conditioning on novel and diverse ellipsoid layouts from a simple statistical model, we improve protein generation with expanded Pareto frontiers between designability, novelty, and diversity. Further, this enables sampling designable proteins with a helix-fraction that matches PDB proteins, unlike existing generative models that commonly oversample conceptually simple helix bundles.</p>
            <p id="subjects-0ctvBgKFgc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-0ctvBgKFgc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0ctvBgKFgc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0ctvBgKFgc@OpenReview" onclick="foldPdfKimi('0ctvBgKFgc@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="TVQLu34bdw@OpenReview" class="panel paper" keywords="protein,proteina,novo,backbone,guidance,backbones,flow,conditioning,proteins,autoguidance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TVQLu34bdw" target="_blank" title="17/207"><span class="index notranslate">#17</span></a>
                <a id="title-TVQLu34bdw@OpenReview" class="title-link" href="/venue/TVQLu34bdw@OpenReview" target="_blank">Proteina: Scaling Flow-based Protein Structure Generative Models</a>
                <a id="pdf-TVQLu34bdw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TVQLu34bdw@OpenReview', this)" data="https://openreview.net/pdf?id=TVQLu34bdw">[PDF<sup id="pdf-stars-TVQLu34bdw@OpenReview">49</sup>]</a>
                <a id="copy-TVQLu34bdw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TVQLu34bdw@OpenReview')">[Copy]</a>
                <a id="kimi-TVQLu34bdw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TVQLu34bdw@OpenReview', this)">[Kimi<sup id="kimi-stars-TVQLu34bdw@OpenReview">39</sup>]</a>
                <a id="rel-TVQLu34bdw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TVQLu34bdw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TVQLu34bdw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Geffner" target="_blank">Tomas Geffner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kieran Didi" target="_blank">Kieran Didi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zuobai Zhang" target="_blank">Zuobai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danny Reidenbach" target="_blank">Danny Reidenbach</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhonglin Cao" target="_blank">Zhonglin Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Yim" target="_blank">Jason Yim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mario Geiger" target="_blank">Mario Geiger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Dallago" target="_blank">Christian Dallago</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emine Kucukbenli" target="_blank">Emine Kucukbenli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arash Vahdat" target="_blank">Arash Vahdat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karsten Kreis" target="_blank">Karsten Kreis</a>
            </p>
            <p id="summary-TVQLu34bdw@OpenReview" class="summary">Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop Proteina, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mn" id="MathJax-Span-21" style="font-family: MathJax_Main;">5</span><span class="mo" id="MathJax-Span-22" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-4">5\times</script> as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation.</p>
            <p id="subjects-TVQLu34bdw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-TVQLu34bdw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TVQLu34bdw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TVQLu34bdw@OpenReview" onclick="foldPdfKimi('TVQLu34bdw@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="HnhNRrLPwm@OpenReview" class="panel paper" keywords="mmie,interleaved,lvlms,multimodal,evaluation,comprehension,benchmark,vision,advancements,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HnhNRrLPwm" target="_blank" title="18/207"><span class="index notranslate">#18</span></a>
                <a id="title-HnhNRrLPwm@OpenReview" class="title-link" href="/venue/HnhNRrLPwm@OpenReview" target="_blank">MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models</a>
                <a id="pdf-HnhNRrLPwm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HnhNRrLPwm@OpenReview', this)" data="https://openreview.net/pdf?id=HnhNRrLPwm">[PDF<sup id="pdf-stars-HnhNRrLPwm@OpenReview">79</sup>]</a>
                <a id="copy-HnhNRrLPwm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HnhNRrLPwm@OpenReview')">[Copy]</a>
                <a id="kimi-HnhNRrLPwm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HnhNRrLPwm@OpenReview', this)">[Kimi<sup id="kimi-stars-HnhNRrLPwm@OpenReview">65</sup>]</a>
                <a id="rel-HnhNRrLPwm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HnhNRrLPwm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HnhNRrLPwm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Xia" target="_blank">Peng Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Han" target="_blank">Siwei Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shi Qiu" target="_blank">Shi Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyang Zhou" target="_blank">Yiyang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyang Wang" target="_blank">Zhaoyang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Zheng" target="_blank">Wenhao Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaorun Chen" target="_blank">Zhaorun Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenhang Cui" target="_blank">Chenhang Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyu Ding" target="_blank">Mingyu Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linjie Li" target="_blank">Linjie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijuan Wang" target="_blank">Lijuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaxiu Yao" target="_blank">Huaxiu Yao</a>
            </p>
            <p id="summary-HnhNRrLPwm@OpenReview" class="summary">Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs.</p>
            <p id="subjects-HnhNRrLPwm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-HnhNRrLPwm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HnhNRrLPwm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HnhNRrLPwm@OpenReview" onclick="foldPdfKimi('HnhNRrLPwm@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="1pXzC30ry5@OpenReview" class="panel paper" keywords="rmp,segmentation,sam,real,purpose,tasks,time,multi,adapter,comunity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1pXzC30ry5" target="_blank" title="19/207"><span class="index notranslate">#19</span></a>
                <a id="title-1pXzC30ry5@OpenReview" class="title-link" href="/venue/1pXzC30ry5@OpenReview" target="_blank">RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything</a>
                <a id="pdf-1pXzC30ry5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1pXzC30ry5@OpenReview', this)" data="https://openreview.net/pdf?id=1pXzC30ry5">[PDF<sup id="pdf-stars-1pXzC30ry5@OpenReview">36</sup>]</a>
                <a id="copy-1pXzC30ry5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1pXzC30ry5@OpenReview')">[Copy]</a>
                <a id="kimi-1pXzC30ry5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1pXzC30ry5@OpenReview', this)">[Kimi<sup id="kimi-stars-1pXzC30ry5@OpenReview">32</sup>]</a>
                <a id="rel-1pXzC30ry5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1pXzC30ry5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1pXzC30ry5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shilin Xu" target="_blank">Shilin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haobo Yuan" target="_blank">Haobo Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyu Shi" target="_blank">Qingyu Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Qi" target="_blank">Lu Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingbo Wang" target="_blank">Jingbo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yibo Yang" target="_blank">Yibo Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yining Li" target="_blank">Yining Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Chen" target="_blank">Kai Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhai Tong" target="_blank">Yunhai Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernard Ghanem" target="_blank">Bernard Ghanem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangtai Li" target="_blank">Xiangtai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Hsuan Yang" target="_blank">Ming-Hsuan Yang</a>
            </p>
            <p id="summary-1pXzC30ry5@OpenReview" class="summary">Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks. However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios. To explore real-time segmentation, recent advancements primarily focus on semantic segmentation within specific environments, such as autonomous driving. However, they often overlook the generalization ability of these models across diverse scenarios. Therefore, to fill this gap, this work explores a novel real-time segmentation setting called real-time multi-purpose segmentation. It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. Unlike previous methods, which use a specific design for each task, we aim to use only a single end-to-end model to accomplish all these tasks in real-time. To meet real-time requirements and balance multi-task learning, we present a novel dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM). It contains an efficient encoder and an efficient decoupled adapter to perform prompt-driven decoding. Moreover, we further explore different training strategies and one new adapter design to boost co-training performance further. We benchmark several strong baselines by extending existing works to support our multi-purpose segmentation. Extensive experiments demonstrate that RMP-SAM is effective and generalizes well on proposed benchmarks and other specific semantic tasks. Our implementation of RMP-SAM achieves the optimal balance between accuracy and speed for these tasks. Code and model will be available to the comunity.</p>
            <p id="subjects-1pXzC30ry5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-1pXzC30ry5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1pXzC30ry5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1pXzC30ry5@OpenReview" onclick="foldPdfKimi('1pXzC30ry5@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="DzGe40glxs@OpenReview" class="panel paper" keywords="planning,agent,plans,guez,free,reinforcement,plan,representations,sokoban,drc">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DzGe40glxs" target="_blank" title="20/207"><span class="index notranslate">#20</span></a>
                <a id="title-DzGe40glxs@OpenReview" class="title-link" href="/venue/DzGe40glxs@OpenReview" target="_blank">Interpreting Emergent Planning in Model-Free Reinforcement Learning</a>
                <a id="pdf-DzGe40glxs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DzGe40glxs@OpenReview', this)" data="https://openreview.net/pdf?id=DzGe40glxs">[PDF<sup id="pdf-stars-DzGe40glxs@OpenReview">66</sup>]</a>
                <a id="copy-DzGe40glxs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DzGe40glxs@OpenReview')">[Copy]</a>
                <a id="kimi-DzGe40glxs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DzGe40glxs@OpenReview', this)">[Kimi<sup id="kimi-stars-DzGe40glxs@OpenReview">82</sup>]</a>
                <a id="rel-DzGe40glxs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DzGe40glxs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DzGe40glxs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Bush" target="_blank">Thomas Bush</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephen Chung" target="_blank">Stephen Chung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Usman Anwar" target="_blank">Usman Anwar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adri Garriga-Alonso" target="_blank">Adri Garriga-Alonso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Krueger" target="_blank">David Krueger</a>
            </p>
            <p id="summary-DzGe40glxs@OpenReview" class="summary">We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by [Guez et al. (2019)](https://arxiv.org/abs/1901.03559), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in agent's representations) have causal effect on agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, enabling improved diagnosis, interpretation, and control of agent planning processes.</p>
            <p id="subjects-DzGe40glxs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-DzGe40glxs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DzGe40glxs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DzGe40glxs@OpenReview" onclick="foldPdfKimi('DzGe40glxs@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="vo9t20wsmd@OpenReview" class="panel paper" keywords="speculative,cascades,decoding,deferral,rule,cascading,offs,approaches,execution,quality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vo9t20wsmd" target="_blank" title="21/207"><span class="index notranslate">#21</span></a>
                <a id="title-vo9t20wsmd@OpenReview" class="title-link" href="/venue/vo9t20wsmd@OpenReview" target="_blank">Faster Cascades via Speculative Decoding</a>
                <a id="pdf-vo9t20wsmd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vo9t20wsmd@OpenReview', this)" data="https://openreview.net/pdf?id=vo9t20wsmd">[PDF<sup id="pdf-stars-vo9t20wsmd@OpenReview">51</sup>]</a>
                <a id="copy-vo9t20wsmd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vo9t20wsmd@OpenReview')">[Copy]</a>
                <a id="kimi-vo9t20wsmd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vo9t20wsmd@OpenReview', this)">[Kimi<sup id="kimi-stars-vo9t20wsmd@OpenReview">74</sup>]</a>
                <a id="rel-vo9t20wsmd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vo9t20wsmd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vo9t20wsmd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Harikrishna Narasimhan" target="_blank">Harikrishna Narasimhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wittawat Jitkrittum" target="_blank">Wittawat Jitkrittum</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ankit Singh Rawat" target="_blank">Ankit Singh Rawat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungyeon Kim" target="_blank">Seungyeon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neha Gupta" target="_blank">Neha Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya Krishna Menon" target="_blank">Aditya Krishna Menon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjiv Kumar" target="_blank">Sanjiv Kumar</a>
            </p>
            <p id="summary-vo9t20wsmd@OpenReview" class="summary">Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for "hard" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades offer better cost-quality trade-offs, often even outperforming the large model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.</p>
            <p id="subjects-vo9t20wsmd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-vo9t20wsmd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vo9t20wsmd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vo9t20wsmd@OpenReview" onclick="foldPdfKimi('vo9t20wsmd@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="6s5uXNWGIh@OpenReview" class="panel paper" keywords="engineering,kaggle,agents,mle,bench,machine,competitions,benchmark,aide,medal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6s5uXNWGIh" target="_blank" title="22/207"><span class="index notranslate">#22</span></a>
                <a id="title-6s5uXNWGIh@OpenReview" class="title-link" href="/venue/6s5uXNWGIh@OpenReview" target="_blank">MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering</a>
                <a id="pdf-6s5uXNWGIh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6s5uXNWGIh@OpenReview', this)" data="https://openreview.net/pdf?id=6s5uXNWGIh">[PDF<sup id="pdf-stars-6s5uXNWGIh@OpenReview">34</sup>]</a>
                <a id="copy-6s5uXNWGIh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6s5uXNWGIh@OpenReview')">[Copy]</a>
                <a id="kimi-6s5uXNWGIh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6s5uXNWGIh@OpenReview', this)">[Kimi<sup id="kimi-stars-6s5uXNWGIh@OpenReview">39</sup>]</a>
                <a id="rel-6s5uXNWGIh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6s5uXNWGIh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6s5uXNWGIh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Shern Chan" target="_blank">Jun Shern Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neil Chowdhury" target="_blank">Neil Chowdhury</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Jaffe" target="_blank">Oliver Jaffe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Aung" target="_blank">James Aung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dane Sherburn" target="_blank">Dane Sherburn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Evan Mays" target="_blank">Evan Mays</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giulio Starace" target="_blank">Giulio Starace</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Liu" target="_blank">Kevin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leon Maksin" target="_blank">Leon Maksin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tejal Patwardhan" target="_blank">Tejal Patwardhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksander Madry" target="_blank">Aleksander Madry</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lilian Weng" target="_blank">Lilian Weng</a>
            </p>
            <p id="summary-6s5uXNWGIh@OpenReview" class="summary">We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 71 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup  OpenAI's o1-preview with AIDE scaffolding  achieves at least the level of a Kaggle bronze medal in 17.3\% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code to facilitate future research in understanding the ML engineering capabilities of AI agents.</p>
            <p id="subjects-6s5uXNWGIh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-6s5uXNWGIh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6s5uXNWGIh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6s5uXNWGIh@OpenReview" onclick="foldPdfKimi('6s5uXNWGIh@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="t8FG4cJuL3@OpenReview" class="panel paper" keywords="games,iterate,algorithms,last,varying,player,convergence,gradient,behaviours,perturbed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=t8FG4cJuL3" target="_blank" title="23/207"><span class="index notranslate">#23</span></a>
                <a id="title-t8FG4cJuL3@OpenReview" class="title-link" href="/venue/t8FG4cJuL3@OpenReview" target="_blank">Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games</a>
                <a id="pdf-t8FG4cJuL3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('t8FG4cJuL3@OpenReview', this)" data="https://openreview.net/pdf?id=t8FG4cJuL3">[PDF<sup id="pdf-stars-t8FG4cJuL3@OpenReview">14</sup>]</a>
                <a id="copy-t8FG4cJuL3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('t8FG4cJuL3@OpenReview')">[Copy]</a>
                <a id="kimi-t8FG4cJuL3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('t8FG4cJuL3@OpenReview', this)">[Kimi<sup id="kimi-stars-t8FG4cJuL3@OpenReview">28</sup>]</a>
                <a id="rel-t8FG4cJuL3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('t8FG4cJuL3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-t8FG4cJuL3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yanzheng Chen" target="_blank">Yanzheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Yu" target="_blank">Jun Yu</a>
            </p>
            <p id="summary-t8FG4cJuL3@OpenReview" class="summary">Last-iterate convergence behaviours of well-known algorithms are intensively investigated in various games, such as two-player bilinear zero-sum games.However, most known last-iterate convergence properties rely on strict settings where the underlying games must have time-invariant payoffs.Besides, the limited known attempts on the games with time-varying payoffs are in two-player bilinear time-varying zero-sum games and strictly monotone games. By contrast, in other time-varying games, the last-iterate behaviours of two classic algorithms, i.e., optimistic gradient (OG) and extra gradient (EG) algorithms, still lack research, especially the convergence rates in multi-player games.In this paper, we investigate the last-iterate behaviours of OG and EG algorithms for convergent perturbed games, which extend upon the usual model of time-invariant games and incorporate external factors, such as vanishing noises.Using the recently proposed notion of the tangent residual (or its modifications) as the potential function of games and the measure of proximity to the Nash equilibrium, we prove that the last-iterate convergence rates of EG and OG algorithms for perturbed games on bounded convex closed sets are <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-23" style="width: 4.898em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.065em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1003.96em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-24"><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-26" style="font-family: MathJax_Main;">(</span><span class="texatom" id="MathJax-Span-27"><span class="mrow" id="MathJax-Span-28"><span class="mn" id="MathJax-Span-29" style="font-family: MathJax_Main;">1</span></span></span><span class="texatom" id="MathJax-Span-30"><span class="mrow" id="MathJax-Span-31"><span class="mo" id="MathJax-Span-32" style="font-family: MathJax_Main;">/</span></span></span><span class="texatom" id="MathJax-Span-33"><span class="mrow" id="MathJax-Span-34"><span class="msqrt" id="MathJax-Span-35"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-36"><span class="mi" id="MathJax-Span-37" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;"><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;"><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-38" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mrow class="MJX-TeXAtom-ORD"><msqrt><mi>T</mi></msqrt></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-5">O({1}/{\sqrt{T}})</script> if such games converge to monotone games at rates fast enough and that such a result holds true for certain unconstrained perturbed games. With this result, we address an open questionasking for the last-iterate convergence rate of the extra gradient and the optimistic gradient algorithms in constrained and time-varying settings. The above convergence rates are similar to known tight results on corresponding time-invariant games.</p>
            <p id="subjects-t8FG4cJuL3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-t8FG4cJuL3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-t8FG4cJuL3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-t8FG4cJuL3@OpenReview" onclick="foldPdfKimi('t8FG4cJuL3@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="st77ShxP1K@OpenReview" class="panel paper" keywords="conformity,llms,benchform,collaborative,think,agent,influencing,systems,language,strategies">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=st77ShxP1K" target="_blank" title="24/207"><span class="index notranslate">#24</span></a>
                <a id="title-st77ShxP1K@OpenReview" class="title-link" href="/venue/st77ShxP1K@OpenReview" target="_blank">Do as We Do, Not as You Think: the Conformity of Large Language Models</a>
                <a id="pdf-st77ShxP1K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('st77ShxP1K@OpenReview', this)" data="https://openreview.net/pdf?id=st77ShxP1K">[PDF<sup id="pdf-stars-st77ShxP1K@OpenReview">111</sup>]</a>
                <a id="copy-st77ShxP1K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('st77ShxP1K@OpenReview')">[Copy]</a>
                <a id="kimi-st77ShxP1K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('st77ShxP1K@OpenReview', this)">[Kimi<sup id="kimi-stars-st77ShxP1K@OpenReview">162</sup>]</a>
                <a id="rel-st77ShxP1K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('st77ShxP1K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-st77ShxP1K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Weng" target="_blank">Zhiyuan Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guikun Chen" target="_blank">Guikun Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenguan Wang" target="_blank">Wenguan Wang</a>
            </p>
            <p id="summary-st77ShxP1K@OpenReview" class="summary">Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BENCHFORM, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs behavior in collaborative scenarios. Several representative LLMs are evaluated on BENCHFORM, using metrics such as conformity rate and independence rate to quantify conformitys impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code will be publicly available.</p>
            <p id="subjects-st77ShxP1K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-st77ShxP1K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-st77ShxP1K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-st77ShxP1K@OpenReview" onclick="foldPdfKimi('st77ShxP1K@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="xoXn62FzD0@OpenReview" class="panel paper" keywords="syntactic,smc,lew,sequential,monte,carlo,llm,constraints,inference,generation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xoXn62FzD0" target="_blank" title="25/207"><span class="index notranslate">#25</span></a>
                <a id="title-xoXn62FzD0@OpenReview" class="title-link" href="/venue/xoXn62FzD0@OpenReview" target="_blank">Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo</a>
                <a id="pdf-xoXn62FzD0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xoXn62FzD0@OpenReview', this)" data="https://openreview.net/pdf?id=xoXn62FzD0">[PDF<sup id="pdf-stars-xoXn62FzD0@OpenReview">47</sup>]</a>
                <a id="copy-xoXn62FzD0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xoXn62FzD0@OpenReview')">[Copy]</a>
                <a id="kimi-xoXn62FzD0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xoXn62FzD0@OpenReview', this)">[Kimi<sup id="kimi-stars-xoXn62FzD0@OpenReview">76</sup>]</a>
                <a id="rel-xoXn62FzD0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xoXn62FzD0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xoXn62FzD0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Joo Loula" target="_blank">Joo Loula</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin LeBrun" target="_blank">Benjamin LeBrun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Du" target="_blank">Li Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Lipkin" target="_blank">Ben Lipkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clemente Pasti" target="_blank">Clemente Pasti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Grand" target="_blank">Gabriel Grand</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Liu" target="_blank">Tianyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yahya Emara" target="_blank">Yahya Emara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marjorie Freedman" target="_blank">Marjorie Freedman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Eisner" target="_blank">Jason Eisner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Cotterell" target="_blank">Ryan Cotterell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vikash Mansinghka" target="_blank">Vikash Mansinghka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Lew" target="_blank">Alexander Lew</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Vieira" target="_blank">Tim Vieira</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Timothy O'Donnell" target="_blank">Timothy O'Donnell</a>
            </p>
            <p id="summary-xoXn62FzD0@OpenReview" class="summary">A wide range of LLM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints nontrivially alters the distribution over sequences, usually making exact sampling intractable. In this work, building on the Language Model Probabilistic Programming framework of Lew et al. (2023), we develop an approach to approximate inference for controlled LLM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computation in light of new information during the course of generation. We demonstrate that our approach improves downstream performance on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis. We compare to a number of alternative and ablated approaches, showing that our accuracy improvements are driven by better approximation to the full Bayesian posterior.</p>
            <p id="subjects-xoXn62FzD0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-xoXn62FzD0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xoXn62FzD0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xoXn62FzD0@OpenReview" onclick="foldPdfKimi('xoXn62FzD0@OpenReview', this)" class="hr hr-fold">
        </div>
    <div id="wg1PCg3CUP@OpenReview" class="panel paper" keywords="precision,laws,training,scaling,pretraining,precisions,inference,post,quantization,degradation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wg1PCg3CUP" target="_blank" title="26/207"><span class="index notranslate">#26</span></a>
                <a id="title-wg1PCg3CUP@OpenReview" class="title-link" href="/venue/wg1PCg3CUP@OpenReview" target="_blank">Scaling Laws for Precision</a>
                <a id="pdf-wg1PCg3CUP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wg1PCg3CUP@OpenReview', this)" data="https://openreview.net/pdf?id=wg1PCg3CUP">[PDF<sup id="pdf-stars-wg1PCg3CUP@OpenReview">49</sup>]</a>
                <a id="copy-wg1PCg3CUP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wg1PCg3CUP@OpenReview')">[Copy]</a>
                <a id="kimi-wg1PCg3CUP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wg1PCg3CUP@OpenReview', this)">[Kimi<sup id="kimi-stars-wg1PCg3CUP@OpenReview">81</sup>]</a>
                <a id="rel-wg1PCg3CUP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wg1PCg3CUP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wg1PCg3CUP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tanishq Kumar" target="_blank">Tanishq Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zachary Ankner" target="_blank">Zachary Ankner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Spector" target="_blank">Benjamin Spector</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Blake Bordelon" target="_blank">Blake Bordelon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Muennighoff" target="_blank">Niklas Muennighoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mansheej Paul" target="_blank">Mansheej Paul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cengiz Pehlevan" target="_blank">Cengiz Pehlevan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Re" target="_blank">Christopher Re</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditi Raghunathan" target="_blank">Aditi Raghunathan</a>
            </p>
            <p id="summary-wg1PCg3CUP@OpenReview" class="summary">Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise "precision-aware" scaling laws for both training and inference. We propose that training in lower precision reduces the model's "effective parameter count," allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.</p>
            <p id="subjects-wg1PCg3CUP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-wg1PCg3CUP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wg1PCg3CUP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wg1PCg3CUP@OpenReview" onclick="foldPdfKimi('wg1PCg3CUP@OpenReview', this)" class="hr hr-fold">
        </div><div id="tyEyYT267x@OpenReview" class="panel paper" keywords="autoregressive,sar,diffusion,language,models,denoising,generation,interpolating,discrete,variance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tyEyYT267x" target="_blank" title="27/207"><span class="index notranslate">#27</span></a>
                <a id="title-tyEyYT267x@OpenReview" class="title-link" href="/venue/tyEyYT267x@OpenReview" target="_blank">Interpolating Autoregressive and Discrete Denoising Diffusion Language Models</a>
                <a id="pdf-tyEyYT267x@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tyEyYT267x@OpenReview', this)" data="https://openreview.net/pdf?id=tyEyYT267x">[PDF<sup id="pdf-stars-tyEyYT267x@OpenReview">80</sup>]</a>
                <a id="copy-tyEyYT267x@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tyEyYT267x@OpenReview')">[Copy]</a>
                <a id="kimi-tyEyYT267x@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tyEyYT267x@OpenReview', this)">[Kimi<sup id="kimi-stars-tyEyYT267x@OpenReview">77</sup>]</a>
                <a id="rel-tyEyYT267x@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tyEyYT267x@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tyEyYT267x@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marianne Arriola" target="_blank">Marianne Arriola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Gokaslan" target="_blank">Aaron Gokaslan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Chiu" target="_blank">Justin Chiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Han" target="_blank">Jiaqi Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihan Yang" target="_blank">Zhihan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixuan Qi" target="_blank">Zhixuan Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Subham Sahoo" target="_blank">Subham Sahoo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Volodymyr Kuleshov" target="_blank">Volodymyr Kuleshov</a>
            </p>
            <p id="summary-tyEyYT267x@OpenReview" class="summary">Diffusion language models offer unique benefits over autoregressive (AR) models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of semi-autoregressive (SAR) diffusion models that interpolate between discrete denoising diffusion and autoregressive models. We propose a recipe for building effective SAR models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. SAR models overcome key limitations of diffusion language models, setting a new state-of-the-art performance on language modeling benchmarks and enabling generation of arbitrary-length sequences.</p>
            <p id="subjects-tyEyYT267x@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-tyEyYT267x@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tyEyYT267x@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tyEyYT267x@OpenReview" onclick="foldPdfKimi('tyEyYT267x@OpenReview', this)" class="hr hr-fold">
        </div><div id="tcvMzR2NrP@OpenReview" class="panel paper" keywords="probability,paths,kinetic,discrete,construction,path,space,domain,generation,colloquially">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tcvMzR2NrP" target="_blank" title="28/207"><span class="index notranslate">#28</span></a>
                <a id="title-tcvMzR2NrP@OpenReview" class="title-link" href="/venue/tcvMzR2NrP@OpenReview" target="_blank">Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective</a>
                <a id="pdf-tcvMzR2NrP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tcvMzR2NrP@OpenReview', this)" data="https://openreview.net/pdf?id=tcvMzR2NrP">[PDF<sup id="pdf-stars-tcvMzR2NrP@OpenReview">44</sup>]</a>
                <a id="copy-tcvMzR2NrP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tcvMzR2NrP@OpenReview')">[Copy]</a>
                <a id="kimi-tcvMzR2NrP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tcvMzR2NrP@OpenReview', this)">[Kimi<sup id="kimi-stars-tcvMzR2NrP@OpenReview">54</sup>]</a>
                <a id="rel-tcvMzR2NrP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tcvMzR2NrP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tcvMzR2NrP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Neta Shaul" target="_blank">Neta Shaul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Itai Gat" target="_blank">Itai Gat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marton Havasi" target="_blank">Marton Havasi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Severo" target="_blank">Daniel Severo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anuroop Sriram" target="_blank">Anuroop Sriram</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Holderrieth" target="_blank">Peter Holderrieth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Karrer" target="_blank">Brian Karrer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaron Lipman" target="_blank">Yaron Lipman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ricky T. Q. Chen" target="_blank">Ricky T. Q. Chen</a>
            </p>
            <p id="summary-tcvMzR2NrP@OpenReview" class="summary">The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.</p>
            <p id="subjects-tcvMzR2NrP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-tcvMzR2NrP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tcvMzR2NrP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tcvMzR2NrP@OpenReview" onclick="foldPdfKimi('tcvMzR2NrP@OpenReview', this)" class="hr hr-fold">
        </div><div id="tPNHOoZFl9@OpenReview" class="panel paper" keywords="finetuning,dpo,phrases,llm,responses,preference,learning,dynamics,tuning,might">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tPNHOoZFl9" target="_blank" title="29/207"><span class="index notranslate">#29</span></a>
                <a id="title-tPNHOoZFl9@OpenReview" class="title-link" href="/venue/tPNHOoZFl9@OpenReview" target="_blank">Learning Dynamics of LLM Finetuning</a>
                <a id="pdf-tPNHOoZFl9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tPNHOoZFl9@OpenReview', this)" data="https://openreview.net/pdf?id=tPNHOoZFl9">[PDF<sup id="pdf-stars-tPNHOoZFl9@OpenReview">111</sup>]</a>
                <a id="copy-tPNHOoZFl9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tPNHOoZFl9@OpenReview')">[Copy]</a>
                <a id="kimi-tPNHOoZFl9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tPNHOoZFl9@OpenReview', this)">[Kimi<sup id="kimi-stars-tPNHOoZFl9@OpenReview">131</sup>]</a>
                <a id="rel-tPNHOoZFl9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tPNHOoZFl9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tPNHOoZFl9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=YI REN" target="_blank">YI REN</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danica Sutherland" target="_blank">Danica Sutherland</a>
            </p>
            <p id="summary-tPNHOoZFl9@OpenReview" class="summary">Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique ``squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.</p>
            <p id="subjects-tPNHOoZFl9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-tPNHOoZFl9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tPNHOoZFl9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tPNHOoZFl9@OpenReview" onclick="foldPdfKimi('tPNHOoZFl9@OpenReview', this)" class="hr hr-fold">
        </div><div id="sbG8qhMjkZ@OpenReview" class="panel paper" keywords="ksd,stein,rates,wasserstein,particle,convergence,descent,shi2024finite,bilinear,variational">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=sbG8qhMjkZ" target="_blank" title="30/207"><span class="index notranslate">#30</span></a>
                <a id="title-sbG8qhMjkZ@OpenReview" class="title-link" href="/venue/sbG8qhMjkZ@OpenReview" target="_blank">Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent</a>
                <a id="pdf-sbG8qhMjkZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('sbG8qhMjkZ@OpenReview', this)" data="https://openreview.net/pdf?id=sbG8qhMjkZ">[PDF<sup id="pdf-stars-sbG8qhMjkZ@OpenReview">9</sup>]</a>
                <a id="copy-sbG8qhMjkZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('sbG8qhMjkZ@OpenReview')">[Copy]</a>
                <a id="kimi-sbG8qhMjkZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('sbG8qhMjkZ@OpenReview', this)">[Kimi<sup id="kimi-stars-sbG8qhMjkZ@OpenReview">18</sup>]</a>
                <a id="rel-sbG8qhMjkZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('sbG8qhMjkZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-sbG8qhMjkZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Krishna Balasubramanian" target="_blank">Krishna Balasubramanian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sayan Banerjee" target="_blank">Sayan Banerjee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=PROMIT GHOSAL" target="_blank">PROMIT GHOSAL</a>
            </p>
            <p id="summary-sbG8qhMjkZ@OpenReview" class="summary">We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtext mathcolor=&quot;red&quot;&gt;\KSD&lt;/mtext&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-39" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.55em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-40"><span class="mtext" id="MathJax-Span-41" style="font-family: MathJax_Main; color: red;">\KSD</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathcolor="red">\KSD</mtext></math></span></span><script type="math/tex" id="MathJax-Element-6">\KSD</script>) and Wasserstein-2 metrics. Our key insight is the observation that the time derivative of the relative entropy between the joint density of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-42" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-43"><span class="mi" id="MathJax-Span-44" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-7">N</script> particle locations and the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-45" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-46"><span class="mi" id="MathJax-Span-47" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-8">N</script>-fold product target measure, starting from a regular initial distribution, splits into a dominant negative part proportional to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-48" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-49"><span class="mi" id="MathJax-Span-50" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-9">N</script> times the expected <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mtext mathcolor=&quot;red&quot;&gt;\KSD&lt;/mtext&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-51" style="width: 3.648em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1003.02em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-52"><span class="msubsup" id="MathJax-Span-53"><span style="display: inline-block; position: relative; width: 3.023em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1002.55em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mtext" id="MathJax-Span-54" style="font-family: MathJax_Main; color: red;">\KSD</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.602em; left: 2.607em;"><span class="mn" id="MathJax-Span-55" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mtext mathcolor="red">\KSD</mtext><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-10">\KSD^2</script> and a smaller positive part. This observation leads to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtext mathcolor=&quot;red&quot;&gt;\KSD&lt;/mtext&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-56" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.55em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-57"><span class="mtext" id="MathJax-Span-58" style="font-family: MathJax_Main; color: red;">\KSD</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathcolor="red">\KSD</mtext></math></span></span><script type="math/tex" id="MathJax-Element-11">\KSD</script> rates of order <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-59" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.71em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-60"><span class="mn" id="MathJax-Span-61" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-62"><span class="mrow" id="MathJax-Span-63"><span class="mo" id="MathJax-Span-64" style="font-family: MathJax_Main;">/</span></span></span><span class="msqrt" id="MathJax-Span-65"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-66"><span class="mi" id="MathJax-Span-67" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.89em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;"><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.211em;"><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><msqrt><mi>N</mi></msqrt></math></span></span><script type="math/tex" id="MathJax-Element-12">1/\sqrt{N}</script>, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-68" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-69"><span class="mi" id="MathJax-Span-70" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></span></span><script type="math/tex" id="MathJax-Element-13">d</script>. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Matrn' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.</p>
            <p id="subjects-sbG8qhMjkZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-sbG8qhMjkZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-sbG8qhMjkZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-sbG8qhMjkZ@OpenReview" onclick="foldPdfKimi('sbG8qhMjkZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="pqOjj90Vwp@OpenReview" class="panel paper" keywords="gnn,logical,expressiveness,gnns,expressivity,homomorphism,graph,subareas,framework,lehman">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pqOjj90Vwp" target="_blank" title="31/207"><span class="index notranslate">#31</span></a>
                <a id="title-pqOjj90Vwp@OpenReview" class="title-link" href="/venue/pqOjj90Vwp@OpenReview" target="_blank">Towards a Complete Logical Framework for GNN Expressiveness</a>
                <a id="pdf-pqOjj90Vwp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pqOjj90Vwp@OpenReview', this)" data="https://openreview.net/pdf?id=pqOjj90Vwp">[PDF<sup id="pdf-stars-pqOjj90Vwp@OpenReview">39</sup>]</a>
                <a id="copy-pqOjj90Vwp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pqOjj90Vwp@OpenReview')">[Copy]</a>
                <a id="kimi-pqOjj90Vwp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pqOjj90Vwp@OpenReview', this)">[Kimi<sup id="kimi-stars-pqOjj90Vwp@OpenReview">27</sup>]</a>
                <a id="rel-pqOjj90Vwp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pqOjj90Vwp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pqOjj90Vwp@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tuo Xu" target="_blank">Tuo Xu</a>
            </p>
            <p id="summary-pqOjj90Vwp@OpenReview" class="summary">Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the specific graph patterns captured by them. Given the connection between WL tests and first-order logic, some have explored the logical expressiveness of Message Passing Neural Networks. This paper aims to establish a comprehensive and systematic relationship between GNNs and logic. We propose a framework for identifying the equivalent logical formulas for arbitrary GNN architectures, which not only explains existing models, but also provides inspiration for future research. As case studies, we analyze multiple classes of prominent GNNs within this framework, unifying different subareas of the field. Additionally, we conduct a detailed examination of homomorphism expressivity from a logical perspective and present a general method for determining the homomorphism expressivity of arbitrary GNN models, as well as addressing several open problems.</p>
            <p id="subjects-pqOjj90Vwp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-pqOjj90Vwp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pqOjj90Vwp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pqOjj90Vwp@OpenReview" onclick="foldPdfKimi('pqOjj90Vwp@OpenReview', this)" class="hr hr-fold">
        </div><div id="or8mMhmyRV@OpenReview" class="panel paper" keywords="maestromotif,skill,skills,language,feedback,llm,nethack,nle,design,natural">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=or8mMhmyRV" target="_blank" title="32/207"><span class="index notranslate">#32</span></a>
                <a id="title-or8mMhmyRV@OpenReview" class="title-link" href="/venue/or8mMhmyRV@OpenReview" target="_blank">MaestroMotif: Skill Design from Artificial Intelligence Feedback</a>
                <a id="pdf-or8mMhmyRV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('or8mMhmyRV@OpenReview', this)" data="https://openreview.net/pdf?id=or8mMhmyRV">[PDF<sup id="pdf-stars-or8mMhmyRV@OpenReview">31</sup>]</a>
                <a id="copy-or8mMhmyRV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('or8mMhmyRV@OpenReview')">[Copy]</a>
                <a id="kimi-or8mMhmyRV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('or8mMhmyRV@OpenReview', this)">[Kimi<sup id="kimi-stars-or8mMhmyRV@OpenReview">47</sup>]</a>
                <a id="rel-or8mMhmyRV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('or8mMhmyRV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-or8mMhmyRV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Klissarov" target="_blank">Martin Klissarov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mikael Henaff" target="_blank">Mikael Henaff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roberta Raileanu" target="_blank">Roberta Raileanu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shagun Sodhani" target="_blank">Shagun Sodhani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Vincent" target="_blank">Pascal Vincent</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amy Zhang" target="_blank">Amy Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre-Luc Bacon" target="_blank">Pierre-Luc Bacon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Doina Precup" target="_blank">Doina Precup</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marlos C. Machado" target="_blank">Marlos C. Machado</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierluca D'Oro" target="_blank">Pierluca D'Oro</a>
            </p>
            <p id="summary-or8mMhmyRV@OpenReview" class="summary">Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.</p>
            <p id="subjects-or8mMhmyRV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-or8mMhmyRV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-or8mMhmyRV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-or8mMhmyRV@OpenReview" onclick="foldPdfKimi('or8mMhmyRV@OpenReview', this)" class="hr hr-fold">
        </div><div id="o9kqa5K3tB@OpenReview" class="panel paper" keywords="pdes,memno,memory,dependent,benefits,markovian,fnos,modeling,past,zwanzig">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=o9kqa5K3tB" target="_blank" title="33/207"><span class="index notranslate">#33</span></a>
                <a id="title-o9kqa5K3tB@OpenReview" class="title-link" href="/venue/o9kqa5K3tB@OpenReview" target="_blank">On the Benefits of Memory for Modeling Time-Dependent PDEs</a>
                <a id="pdf-o9kqa5K3tB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('o9kqa5K3tB@OpenReview', this)" data="https://openreview.net/pdf?id=o9kqa5K3tB">[PDF<sup id="pdf-stars-o9kqa5K3tB@OpenReview">13</sup>]</a>
                <a id="copy-o9kqa5K3tB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('o9kqa5K3tB@OpenReview')">[Copy]</a>
                <a id="kimi-o9kqa5K3tB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('o9kqa5K3tB@OpenReview', this)">[Kimi<sup id="kimi-stars-o9kqa5K3tB@OpenReview">19</sup>]</a>
                <a id="rel-o9kqa5K3tB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('o9kqa5K3tB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-o9kqa5K3tB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ricardo Buitrago Ruiz" target="_blank">Ricardo Buitrago Ruiz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tanya Marwah" target="_blank">Tanya Marwah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert Gu" target="_blank">Albert Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrej Risteski" target="_blank">Andrej Risteski</a>
            </p>
            <p id="summary-o9kqa5K3tB@OpenReview" class="summary">Data-driven techniques have emerged as a promising alternative to traditional numerical methods. For time-dependent PDEs, many approaches are Markovian---the evolution of the trained system only depends on the current state, and not the past states. In this work, we investigate the benefits of using memory for modeling time-dependent PDEs: that is, when past states are explicitly used to predict the future. Motivated by the Mori-Zwanzig theory of model reduction, we theoretically exhibit examples of simple (even linear) PDEs, in which a solution that uses memory is arbitrarily better than a Markovian solution. Additionally, we introduce Memory Neural Operator (MemNO), a neural operator architecture that combines recent state space models (specifically, S4) and Fourier Neural Operators (FNOs) to effectively model memory. We empirically demonstrate that when the PDEs are supplied in low resolution or contain observation noise at train and test time, MemNO significantly outperforms the baselines without memory---with up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-71" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-72"><span class="mn" id="MathJax-Span-73" style="font-family: MathJax_Main;">6</span><span class="mo" id="MathJax-Span-74" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-14">6 \times</script> reduction in test error. Furthermore, we show that this benefit is particularly pronounced when the PDE solutions have significant high-frequency Fourier modes (e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark dataset consisting of such PDEs.</p>
            <p id="subjects-o9kqa5K3tB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-o9kqa5K3tB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-o9kqa5K3tB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-o9kqa5K3tB@OpenReview" onclick="foldPdfKimi('o9kqa5K3tB@OpenReview', this)" class="hr hr-fold">
        </div><div id="o5TsWTUSeF@OpenReview" class="panel paper" keywords="chartmoe,connector,chart,moe,diversely,mllms,json,expert,alignment,understanding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=o5TsWTUSeF" target="_blank" title="34/207"><span class="index notranslate">#34</span></a>
                <a id="title-o5TsWTUSeF@OpenReview" class="title-link" href="/venue/o5TsWTUSeF@OpenReview" target="_blank">ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding</a>
                <a id="pdf-o5TsWTUSeF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('o5TsWTUSeF@OpenReview', this)" data="https://openreview.net/pdf?id=o5TsWTUSeF">[PDF<sup id="pdf-stars-o5TsWTUSeF@OpenReview">45</sup>]</a>
                <a id="copy-o5TsWTUSeF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('o5TsWTUSeF@OpenReview')">[Copy]</a>
                <a id="kimi-o5TsWTUSeF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('o5TsWTUSeF@OpenReview', this)">[Kimi<sup id="kimi-stars-o5TsWTUSeF@OpenReview">53</sup>]</a>
                <a id="rel-o5TsWTUSeF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('o5TsWTUSeF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-o5TsWTUSeF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengzhuo Xu" target="_blank">Zhengzhuo Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Qu" target="_blank">Bowen Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyan Qi" target="_blank">Yiyan Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=SiNan Du" target="_blank">SiNan Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengjin Xu" target="_blank">Chengjin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun Yuan" target="_blank">Chun Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Guo" target="_blank">Jian Guo</a>
            </p>
            <p id="summary-o5TsWTUSeF@OpenReview" class="summary">Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, current MLLMs still struggle to provide faithful data and reliable analysis only based on charts. To address it, we propose ChartMoE, which employs the Mixture of Expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train several linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with nearly 1 million chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts diversely and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.</p>
            <p id="subjects-o5TsWTUSeF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-o5TsWTUSeF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-o5TsWTUSeF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-o5TsWTUSeF@OpenReview" onclick="foldPdfKimi('o5TsWTUSeF@OpenReview', this)" class="hr hr-fold">
        </div><div id="m2nmp8P5in@OpenReview" class="panel paper" keywords="llm,scientific,equations,discovery,equation,symbolic,domain,knowledge,programs,priors">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=m2nmp8P5in" target="_blank" title="35/207"><span class="index notranslate">#35</span></a>
                <a id="title-m2nmp8P5in@OpenReview" class="title-link" href="/venue/m2nmp8P5in@OpenReview" target="_blank">LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</a>
                <a id="pdf-m2nmp8P5in@OpenReview" class="title-pdf notranslate" onclick="togglePdf('m2nmp8P5in@OpenReview', this)" data="https://openreview.net/pdf?id=m2nmp8P5in">[PDF<sup id="pdf-stars-m2nmp8P5in@OpenReview">26</sup>]</a>
                <a id="copy-m2nmp8P5in@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('m2nmp8P5in@OpenReview')">[Copy]</a>
                <a id="kimi-m2nmp8P5in@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('m2nmp8P5in@OpenReview', this)">[Kimi<sup id="kimi-stars-m2nmp8P5in@OpenReview">41</sup>]</a>
                <a id="rel-m2nmp8P5in@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('m2nmp8P5in@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-m2nmp8P5in@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Parshin Shojaee" target="_blank">Parshin Shojaee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kazem Meidani" target="_blank">Kazem Meidani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shashank Gupta" target="_blank">Shashank Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir Barati Farimani" target="_blank">Amir Barati Farimani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chandan Reddy" target="_blank">Chandan Reddy</a>
            </p>
            <p id="summary-m2nmp8P5in@OpenReview" class="summary">Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines.</p>
            <p id="subjects-m2nmp8P5in@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-m2nmp8P5in@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-m2nmp8P5in@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-m2nmp8P5in@OpenReview" onclick="foldPdfKimi('m2nmp8P5in@OpenReview', this)" class="hr hr-fold">
        </div><div id="k3tbMMW8rH@OpenReview" class="panel paper" keywords="matching,fsbm,scalability,eot,frameworks,feedback,pairings,bridge,schrdinger,transport">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=k3tbMMW8rH" target="_blank" title="36/207"><span class="index notranslate">#36</span></a>
                <a id="title-k3tbMMW8rH@OpenReview" class="title-link" href="/venue/k3tbMMW8rH@OpenReview" target="_blank">Feedback Schrdinger Bridge Matching</a>
                <a id="pdf-k3tbMMW8rH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('k3tbMMW8rH@OpenReview', this)" data="https://openreview.net/pdf?id=k3tbMMW8rH">[PDF<sup id="pdf-stars-k3tbMMW8rH@OpenReview">28</sup>]</a>
                <a id="copy-k3tbMMW8rH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('k3tbMMW8rH@OpenReview')">[Copy]</a>
                <a id="kimi-k3tbMMW8rH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('k3tbMMW8rH@OpenReview', this)">[Kimi<sup id="kimi-stars-k3tbMMW8rH@OpenReview">24</sup>]</a>
                <a id="rel-k3tbMMW8rH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('k3tbMMW8rH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-k3tbMMW8rH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Panagiotis Theodoropoulos" target="_blank">Panagiotis Theodoropoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guan-Horng Liu" target="_blank">Guan-Horng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikolaos Komianos" target="_blank">Nikolaos Komianos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Pacelli" target="_blank">Vincent Pacelli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Evangelos Theodorou" target="_blank">Evangelos Theodorou</a>
            </p>
            <p id="summary-k3tbMMW8rH@OpenReview" class="summary">Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. Fully unsupervised methods make minimal assumptions but incur high computational costs, limiting their practicality. On the other hand, imposing full supervision of the matching process with optimal pairings improves scalability, however, it can be infeasible in most applications.To strike a balance between scalability and minimal supervision, we introduce Feedback Schrdinger Bridge Matching (FSBM), a novel semi-supervised matching framework that incorporates a small portion (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;8&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-75" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.51em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-76"><span class="mo" id="MathJax-Span-77" style="font-family: MathJax_Main;">&lt;</span><span class="mn" id="MathJax-Span-78" style="font-family: MathJax_Main; padding-left: 0.263em;">8</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo><mn>8</mn></math></span></span><script type="math/tex" id="MathJax-Element-15"><8</script>% of the entire dataset) of pre-aligned pairs as state feedback to guide the transport map of non-coupled samples, thereby significantly improving efficiency. This is achieved by formulating a static Entropic Optimal Transport (EOT) problem with an additional term capturing the semi-supervised guidance. The generalized EOT objective is then recast into a dynamic formulation to leverage the scalability of matching frameworks. Extensive experiments demonstrate that FSBM accelerates training and enhances generalization by leveraging coupled pairs' guidance, opening new avenues for training matching frameworks with partially aligned datasets.</p>
            <p id="subjects-k3tbMMW8rH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-k3tbMMW8rH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-k3tbMMW8rH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-k3tbMMW8rH@OpenReview" onclick="foldPdfKimi('k3tbMMW8rH@OpenReview', this)" class="hr hr-fold">
        </div><div id="ja4rpheN2n@OpenReview" class="panel paper" keywords="gesubnet,subtype,gene,disease,subtypes,patient,specific,databases,mismatch,genes">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ja4rpheN2n" target="_blank" title="37/207"><span class="index notranslate">#37</span></a>
                <a id="title-ja4rpheN2n@OpenReview" class="title-link" href="/venue/ja4rpheN2n@OpenReview" target="_blank">GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation</a>
                <a id="pdf-ja4rpheN2n@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ja4rpheN2n@OpenReview', this)" data="https://openreview.net/pdf?id=ja4rpheN2n">[PDF<sup id="pdf-stars-ja4rpheN2n@OpenReview">23</sup>]</a>
                <a id="copy-ja4rpheN2n@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ja4rpheN2n@OpenReview')">[Copy]</a>
                <a id="kimi-ja4rpheN2n@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ja4rpheN2n@OpenReview', this)">[Kimi<sup id="kimi-stars-ja4rpheN2n@OpenReview">22</sup>]</a>
                <a id="rel-ja4rpheN2n@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ja4rpheN2n@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ja4rpheN2n@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Yang" target="_blank">Ziwei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Chen" target="_blank">Zheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=XIN LIU" target="_blank">XIN LIU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rikuto Kotoge" target="_blank">Rikuto Kotoge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Chen" target="_blank">Peng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yasuko Matsubara" target="_blank">Yasuko Matsubara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yasushi Sakurai" target="_blank">Yasushi Sakurai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jimeng Sun" target="_blank">Jimeng Sun</a>
            </p>
            <p id="summary-ja4rpheN2n@OpenReview" class="summary">Retrieving gene functional networks from knowledge databases presents a challenge due to the mismatch between disease networks and subtype-specific variations. Current solutions, including statistical and deep learning methods, often fail to effectively integrate gene interaction knowledge from databases or explicitly learn subtype-specific interactions. To address this mismatch, we propose GeSubNet, which learns a unified representation capable of predicting gene interactions while distinguishing between different disease subtypes. Graphs generated by such representations can be considered subtype-specific networks. GeSubNet is a multi-step representation learning framework with three modules: First, a deep generative model learns distinct disease subtypes from patient gene expression profiles. Second, a graph neural network captures representations of prior gene networks from knowledge databases, ensuring accurate physical gene interactions. Finally, we integrate these two representations using an inference loss that leverages graph generation capabilities, conditioned on the patient separation loss, to refine subtype-specific information in the learned representation. GeSubNet consistently outperforms traditional methods, with average improvements of 30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged over four cancer datasets. Particularly, we conduct a biological simulation experiment to assess how the behavior of selected genes from over 11,000 candidates affects subtypes or patient distributions. The results show that the generated network has the potential to identify subtype-specific genes with an 83% likelihood of impacting patient distribution shifts. The GeSubNet resource is available: https://anonymous.4open.science/r/GeSubNet/</p>
            <p id="subjects-ja4rpheN2n@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-ja4rpheN2n@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ja4rpheN2n@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ja4rpheN2n@OpenReview" onclick="foldPdfKimi('ja4rpheN2n@OpenReview', this)" class="hr hr-fold">
        </div><div id="gc8QAQfXv6@OpenReview" class="panel paper" keywords="forgetting,catastrophic,continual,function,llms,unlocking,tasks,training,overwriting,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gc8QAQfXv6" target="_blank" title="38/207"><span class="index notranslate">#38</span></a>
                <a id="title-gc8QAQfXv6@OpenReview" class="title-link" href="/venue/gc8QAQfXv6@OpenReview" target="_blank">Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning</a>
                <a id="pdf-gc8QAQfXv6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gc8QAQfXv6@OpenReview', this)" data="https://openreview.net/pdf?id=gc8QAQfXv6">[PDF<sup id="pdf-stars-gc8QAQfXv6@OpenReview">31</sup>]</a>
                <a id="copy-gc8QAQfXv6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gc8QAQfXv6@OpenReview')">[Copy]</a>
                <a id="kimi-gc8QAQfXv6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gc8QAQfXv6@OpenReview', this)">[Kimi<sup id="kimi-stars-gc8QAQfXv6@OpenReview">50</sup>]</a>
                <a id="rel-gc8QAQfXv6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gc8QAQfXv6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gc8QAQfXv6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gangwei Jiang" target="_blank">Gangwei Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=caigao jiang" target="_blank">caigao jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyi Li" target="_blank">Zhaoyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siqiao Xue" target="_blank">Siqiao Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=JUN ZHOU" target="_blank">JUN ZHOU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linqi Song" target="_blank">Linqi Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Defu Lian" target="_blank">Defu Lian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Wei" target="_blank">Ying Wei</a>
            </p>
            <p id="summary-gc8QAQfXv6@OpenReview" class="summary">Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior.Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions.Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.</p>
            <p id="subjects-gc8QAQfXv6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-gc8QAQfXv6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gc8QAQfXv6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gc8QAQfXv6@OpenReview" onclick="foldPdfKimi('gc8QAQfXv6@OpenReview', this)" class="hr hr-fold">
        </div><div id="gHLWTzKiZV@OpenReview" class="panel paper" keywords="docking,unbalanced,flexibility,poses,energetically,flexible,relaxation,composing,protein,favorable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gHLWTzKiZV" target="_blank" title="39/207"><span class="index notranslate">#39</span></a>
                <a id="title-gHLWTzKiZV@OpenReview" class="title-link" href="/venue/gHLWTzKiZV@OpenReview" target="_blank">Composing Unbalanced Flows for Flexible Docking and Relaxation</a>
                <a id="pdf-gHLWTzKiZV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gHLWTzKiZV@OpenReview', this)" data="https://openreview.net/pdf?id=gHLWTzKiZV">[PDF<sup id="pdf-stars-gHLWTzKiZV@OpenReview">18</sup>]</a>
                <a id="copy-gHLWTzKiZV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gHLWTzKiZV@OpenReview')">[Copy]</a>
                <a id="kimi-gHLWTzKiZV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gHLWTzKiZV@OpenReview', this)">[Kimi<sup id="kimi-stars-gHLWTzKiZV@OpenReview">19</sup>]</a>
                <a id="rel-gHLWTzKiZV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gHLWTzKiZV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gHLWTzKiZV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriele Corso" target="_blank">Gabriele Corso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vignesh Ram Somnath" target="_blank">Vignesh Ram Somnath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Getz" target="_blank">Noah Getz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Regina Barzilay" target="_blank">Regina Barzilay</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommi Jaakkola" target="_blank">Tommi Jaakkola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Krause" target="_blank">Andreas Krause</a>
            </p>
            <p id="summary-gHLWTzKiZV@OpenReview" class="summary">Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack the flexibility to define effective maps between such complex distributions. To address this limitation we propose Unbalanced Flow Matching, a generalization of Flow Matching (FM) that allows trading off sample efficiency with approximation accuracy and enables more accurate transport. Empirically, we apply Unbalanced FM on flexible docking and structure relaxation, demonstrating our ability to model protein flexibility and generate energetically favorable poses. On the PDBBind docking benchmark, our method FlexDock improves the docking performance while increasing the proportion of energetically favorable poses from 30% to 73%.</p>
            <p id="subjects-gHLWTzKiZV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-gHLWTzKiZV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gHLWTzKiZV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gHLWTzKiZV@OpenReview" onclick="foldPdfKimi('gHLWTzKiZV@OpenReview', this)" class="hr hr-fold">
        </div><div id="f4gF6AIHRy@OpenReview" class="panel paper" keywords="disf,file,training,selection,collapse,submodular,combatting,590m,pre,files">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=f4gF6AIHRy" target="_blank" title="40/207"><span class="index notranslate">#40</span></a>
                <a id="title-f4gF6AIHRy@OpenReview" class="title-link" href="/venue/f4gF6AIHRy@OpenReview" target="_blank">Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection</a>
                <a id="pdf-f4gF6AIHRy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('f4gF6AIHRy@OpenReview', this)" data="https://openreview.net/pdf?id=f4gF6AIHRy">[PDF<sup id="pdf-stars-f4gF6AIHRy@OpenReview">19</sup>]</a>
                <a id="copy-f4gF6AIHRy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('f4gF6AIHRy@OpenReview')">[Copy]</a>
                <a id="kimi-f4gF6AIHRy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('f4gF6AIHRy@OpenReview', this)">[Kimi<sup id="kimi-stars-f4gF6AIHRy@OpenReview">28</sup>]</a>
                <a id="rel-f4gF6AIHRy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('f4gF6AIHRy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-f4gF6AIHRy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqing Fan" target="_blank">Ziqing Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Du" target="_blank">Siyuan Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengchao Hu" target="_blank">Shengchao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pingjie Wang" target="_blank">Pingjie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Shen" target="_blank">Li Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ya Zhang" target="_blank">Ya Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dacheng Tao" target="_blank">Dacheng Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanfeng Wang" target="_blank">Yanfeng Wang</a>
            </p>
            <p id="summary-f4gF6AIHRy@OpenReview" class="summary">Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B3;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-79" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-80"><span class="mi" id="MathJax-Span-81" style="font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-16">\gamma</script>-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency.</p>
            <p id="subjects-f4gF6AIHRy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-f4gF6AIHRy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-f4gF6AIHRy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-f4gF6AIHRy@OpenReview" onclick="foldPdfKimi('f4gF6AIHRy@OpenReview', this)" class="hr hr-fold">
        </div><div id="eHehzSDUFp@OpenReview" class="panel paper" keywords="knowledge,acquisition,entropy,pretraining,sources,memory,retention,model,decline,hinders">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eHehzSDUFp" target="_blank" title="41/207"><span class="index notranslate">#41</span></a>
                <a id="title-eHehzSDUFp@OpenReview" class="title-link" href="/venue/eHehzSDUFp@OpenReview" target="_blank">Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition</a>
                <a id="pdf-eHehzSDUFp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eHehzSDUFp@OpenReview', this)" data="https://openreview.net/pdf?id=eHehzSDUFp">[PDF<sup id="pdf-stars-eHehzSDUFp@OpenReview">35</sup>]</a>
                <a id="copy-eHehzSDUFp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eHehzSDUFp@OpenReview')">[Copy]</a>
                <a id="kimi-eHehzSDUFp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eHehzSDUFp@OpenReview', this)">[Kimi<sup id="kimi-stars-eHehzSDUFp@OpenReview">39</sup>]</a>
                <a id="rel-eHehzSDUFp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eHehzSDUFp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eHehzSDUFp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiyeon Kim" target="_blank">Jiyeon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunji Lee" target="_blank">Hyunji Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyowon Cho" target="_blank">Hyowon Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joel Jang" target="_blank">Joel Jang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyeonbin Hwang" target="_blank">Hyeonbin Hwang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungpil Won" target="_blank">Seungpil Won</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youbin Ahn" target="_blank">Youbin Ahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dohaeng Lee" target="_blank">Dohaeng Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minjoon Seo" target="_blank">Minjoon Seo</a>
            </p>
            <p id="summary-eHehzSDUFp@OpenReview" class="summary">In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.</p>
            <p id="subjects-eHehzSDUFp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-eHehzSDUFp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eHehzSDUFp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eHehzSDUFp@OpenReview" onclick="foldPdfKimi('eHehzSDUFp@OpenReview', this)" class="hr hr-fold">
        </div><div id="dhAL5fy8wS@OpenReview" class="panel paper" keywords="pds,pmp,selection,data,commmoncrawl,lms,400b,optimal,corpora,10t">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=dhAL5fy8wS" target="_blank" title="42/207"><span class="index notranslate">#42</span></a>
                <a id="title-dhAL5fy8wS@OpenReview" class="title-link" href="/venue/dhAL5fy8wS@OpenReview" target="_blank">Data Selection via Optimal Control for Language Models</a>
                <a id="pdf-dhAL5fy8wS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('dhAL5fy8wS@OpenReview', this)" data="https://openreview.net/pdf?id=dhAL5fy8wS">[PDF<sup id="pdf-stars-dhAL5fy8wS@OpenReview">35</sup>]</a>
                <a id="copy-dhAL5fy8wS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('dhAL5fy8wS@OpenReview')">[Copy]</a>
                <a id="kimi-dhAL5fy8wS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('dhAL5fy8wS@OpenReview', this)">[Kimi<sup id="kimi-stars-dhAL5fy8wS@OpenReview">53</sup>]</a>
                <a id="rel-dhAL5fy8wS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('dhAL5fy8wS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-dhAL5fy8wS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxian Gu" target="_blank">Yuxian Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Dong" target="_blank">Li Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongning Wang" target="_blank">Hongning Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaru Hao" target="_blank">Yaru Hao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingxiu Dong" target="_blank">Qingxiu Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Furu Wei" target="_blank">Furu Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minlie Huang" target="_blank">Minlie Huang</a>
            </p>
            <p id="summary-dhAL5fy8wS@OpenReview" class="summary">This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics.Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes.Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws.PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. We will open-source our code, models, and data.</p>
            <p id="subjects-dhAL5fy8wS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-dhAL5fy8wS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-dhAL5fy8wS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-dhAL5fy8wS@OpenReview" onclick="foldPdfKimi('dhAL5fy8wS@OpenReview', this)" class="hr hr-fold">
        </div><div id="cNmu0hZ4CL@OpenReview" class="panel paper" keywords="neural,responses,noisy,systems,comparing,computational,transport,biological,artificial,capabilities">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cNmu0hZ4CL" target="_blank" title="43/207"><span class="index notranslate">#43</span></a>
                <a id="title-cNmu0hZ4CL@OpenReview" class="title-link" href="/venue/cNmu0hZ4CL@OpenReview" target="_blank">Comparing noisy neural population dynamics using optimal transport distances</a>
                <a id="pdf-cNmu0hZ4CL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cNmu0hZ4CL@OpenReview', this)" data="https://openreview.net/pdf?id=cNmu0hZ4CL">[PDF<sup id="pdf-stars-cNmu0hZ4CL@OpenReview">18</sup>]</a>
                <a id="copy-cNmu0hZ4CL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cNmu0hZ4CL@OpenReview')">[Copy]</a>
                <a id="kimi-cNmu0hZ4CL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cNmu0hZ4CL@OpenReview', this)">[Kimi<sup id="kimi-stars-cNmu0hZ4CL@OpenReview">11</sup>]</a>
                <a id="rel-cNmu0hZ4CL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cNmu0hZ4CL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cNmu0hZ4CL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amin Nejatbakhsh" target="_blank">Amin Nejatbakhsh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victor Geadah" target="_blank">Victor Geadah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Williams" target="_blank">Alex Williams</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Lipshutz" target="_blank">David Lipshutz</a>
            </p>
            <p id="summary-cNmu0hZ4CL@OpenReview" class="summary">Biological and artificial neural systems form high-dimensional neural representations that underpin their computational capabilities. Methods for quantifying geometric similarity in neural representations have become a popular tool for identifying computational principles that are potentially shared across neural systems. These methods generally assume that neural responses are deterministic and static. However, responses of biological systems, and some artificial systems, are noisy and dynamically unfold over time. Furthermore, these characteristics can have substantial influence on a systems computational capabilities. Here, we demonstrate that existing metrics can fail to capture key differences between neural systems with noisy dynamic responses. We then propose a metric for comparing the geometry of noisy neural trajectories, which can be derived as an optimal transport distance between Gaussian processes. We use the metric to compare models of neural responses in different regions of the motor system and to compare the dynamics of latent diffusion models for text-to-image synthesis.</p>
            <p id="subjects-cNmu0hZ4CL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-cNmu0hZ4CL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cNmu0hZ4CL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cNmu0hZ4CL@OpenReview" onclick="foldPdfKimi('cNmu0hZ4CL@OpenReview', this)" class="hr hr-fold">
        </div><div id="cH65nS5sOz@OpenReview" class="panel paper" keywords="fedlog,overfitting,local,unseen,federated,generalization,client,data,subgraph,mutable">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=cH65nS5sOz" target="_blank" title="44/207"><span class="index notranslate">#44</span></a>
                <a id="title-cH65nS5sOz@OpenReview" class="title-link" href="/venue/cH65nS5sOz@OpenReview" target="_blank">Subgraph Federated Learning for Local Generalization</a>
                <a id="pdf-cH65nS5sOz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('cH65nS5sOz@OpenReview', this)" data="https://openreview.net/pdf?id=cH65nS5sOz">[PDF<sup id="pdf-stars-cH65nS5sOz@OpenReview">20</sup>]</a>
                <a id="copy-cH65nS5sOz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('cH65nS5sOz@OpenReview')">[Copy]</a>
                <a id="kimi-cH65nS5sOz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('cH65nS5sOz@OpenReview', this)">[Kimi<sup id="kimi-stars-cH65nS5sOz@OpenReview">12</sup>]</a>
                <a id="rel-cH65nS5sOz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('cH65nS5sOz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-cH65nS5sOz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sungwon Kim" target="_blank">Sungwon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoonho Lee" target="_blank">Yoonho Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhak Oh" target="_blank">Yunhak Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Namkyeong Lee" target="_blank">Namkyeong Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sukwon Yun" target="_blank">Sukwon Yun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junseok Lee" target="_blank">Junseok Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sein Kim" target="_blank">Sein Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carl Yang" target="_blank">Carl Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chanyoung Park" target="_blank">Chanyoung Park</a>
            </p>
            <p id="summary-cH65nS5sOz@OpenReview" class="summary">Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution. Since they focus solely on performing well on each client's local data, they are prone to overfitting to their local distributions (i.e., local overfitting), which hinders their ability to generalize to unseen data with diverse label distributions. In contrast, our proposed method, FedLoG, effectively tackles this issue by mitigating local overfitting. Our model generates global synthetic data by condensing the reliable information from each class representation and its structural information across clients. Using these synthetic data as a training set, we alleviate the local overfitting problem by adaptively generalizing the absent knowledge within each local dataset. This enhances the generalization capabilities of local models, enabling them to handle unseen data effectively. Our model outperforms baselines in our proposed experimental settings, which are designed to measure generalization power to unseen data in practical scenarios. Our code is available at https://anonymous.4open.science/r/FedLoG-89EE</p>
            <p id="subjects-cH65nS5sOz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-cH65nS5sOz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-cH65nS5sOz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-cH65nS5sOz@OpenReview" onclick="foldPdfKimi('cH65nS5sOz@OpenReview', this)" class="hr hr-fold">
        </div><div id="bnINPG5A32@OpenReview" class="panel paper" keywords="style,content,modulation,reference,personalization,training,free,controlnets,difficulties,stochastic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bnINPG5A32" target="_blank" title="45/207"><span class="index notranslate">#45</span></a>
                <a id="title-bnINPG5A32@OpenReview" class="title-link" href="/venue/bnINPG5A32@OpenReview" target="_blank">RB-Modulation: Training-Free Personalization using Stochastic Optimal Control</a>
                <a id="pdf-bnINPG5A32@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bnINPG5A32@OpenReview', this)" data="https://openreview.net/pdf?id=bnINPG5A32">[PDF<sup id="pdf-stars-bnINPG5A32@OpenReview">27</sup>]</a>
                <a id="copy-bnINPG5A32@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bnINPG5A32@OpenReview')">[Copy]</a>
                <a id="kimi-bnINPG5A32@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bnINPG5A32@OpenReview', this)">[Kimi<sup id="kimi-stars-bnINPG5A32@OpenReview">18</sup>]</a>
                <a id="rel-bnINPG5A32@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bnINPG5A32@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bnINPG5A32@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Litu Rout" target="_blank">Litu Rout</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujia Chen" target="_blank">Yujia Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nataniel Ruiz" target="_blank">Nataniel Ruiz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Kumar" target="_blank">Abhishek Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Constantine Caramanis" target="_blank">Constantine Caramanis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjay Shakkottai" target="_blank">Sanjay Shakkottai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen-Sheng Chu" target="_blank">Wen-Sheng Chu</a>
            </p>
            <p id="summary-bnINPG5A32@OpenReview" class="summary">We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models.Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image.With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of *content* and *style* in a training-free manner. Additionally, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets</p>
            <p id="subjects-bnINPG5A32@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-bnINPG5A32@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bnINPG5A32@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bnINPG5A32@OpenReview" onclick="foldPdfKimi('bnINPG5A32@OpenReview', this)" class="hr hr-fold">
        </div><div id="bVTM2QKYuA@OpenReview" class="panel paper" keywords="representation,concepts,formalization,language,contrasts,animal,natural,geometry,hypothesis,directions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=bVTM2QKYuA" target="_blank" title="46/207"><span class="index notranslate">#46</span></a>
                <a id="title-bVTM2QKYuA@OpenReview" class="title-link" href="/venue/bVTM2QKYuA@OpenReview" target="_blank">The Representation Geometry of Features and Hierarchy in Large Language Models</a>
                <a id="pdf-bVTM2QKYuA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('bVTM2QKYuA@OpenReview', this)" data="https://openreview.net/pdf?id=bVTM2QKYuA">[PDF<sup id="pdf-stars-bVTM2QKYuA@OpenReview">41</sup>]</a>
                <a id="copy-bVTM2QKYuA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('bVTM2QKYuA@OpenReview')">[Copy]</a>
                <a id="kimi-bVTM2QKYuA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('bVTM2QKYuA@OpenReview', this)">[Kimi<sup id="kimi-stars-bVTM2QKYuA@OpenReview">44</sup>]</a>
                <a id="rel-bVTM2QKYuA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('bVTM2QKYuA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-bVTM2QKYuA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kiho Park" target="_blank">Kiho Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yo Joong Choe" target="_blank">Yo Joong Choe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yibo Jiang" target="_blank">Yibo Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victor Veitch" target="_blank">Victor Veitch</a>
            </p>
            <p id="summary-bVTM2QKYuA@OpenReview" class="summary">The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as _directions_ in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as _vectors_. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet.</p>
            <p id="subjects-bVTM2QKYuA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-bVTM2QKYuA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-bVTM2QKYuA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-bVTM2QKYuA@OpenReview" onclick="foldPdfKimi('bVTM2QKYuA@OpenReview', this)" class="hr hr-fold">
        </div><div id="rFpZnn11gj@OpenReview" class="panel paper" keywords="pathgen,pathology,image,clip,wsi,pairs,quality,vlms,slide,captions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rFpZnn11gj" target="_blank" title="47/207"><span class="index notranslate">#47</span></a>
                <a id="title-rFpZnn11gj@OpenReview" class="title-link" href="/venue/rFpZnn11gj@OpenReview" target="_blank">PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration</a>
                <a id="pdf-rFpZnn11gj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rFpZnn11gj@OpenReview', this)" data="https://openreview.net/pdf?id=rFpZnn11gj">[PDF<sup id="pdf-stars-rFpZnn11gj@OpenReview">33</sup>]</a>
                <a id="copy-rFpZnn11gj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rFpZnn11gj@OpenReview')">[Copy]</a>
                <a id="kimi-rFpZnn11gj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rFpZnn11gj@OpenReview', this)">[Kimi<sup id="kimi-stars-rFpZnn11gj@OpenReview">46</sup>]</a>
                <a id="rel-rFpZnn11gj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rFpZnn11gj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rFpZnn11gj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Sun" target="_blank">Yuxuan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunlong Zhang" target="_blank">Yunlong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Si" target="_blank">Yixuan Si</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenglu Zhu" target="_blank">Chenglu Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongyi Shui" target="_blank">Zhongyi Shui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingxiong Li" target="_blank">Jingxiong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Gong" target="_blank">Xuan Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=XINHENG LYU" target="_blank">XINHENG LYU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Lin" target="_blank">Tao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Yang" target="_blank">Lin Yang</a>
            </p>
            <p id="summary-rFpZnn11gj@OpenReview" class="summary">Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model to generate captions for these images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathGen-1-6M/PathGen-1.6M.</p>
            <p id="subjects-rFpZnn11gj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-rFpZnn11gj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rFpZnn11gj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rFpZnn11gj@OpenReview" onclick="foldPdfKimi('rFpZnn11gj@OpenReview', this)" class="hr hr-fold">
        </div><div id="aWXnKanInf@OpenReview" class="panel paper" keywords="topolm,organization,language,brain,functional,spatio,clusters,topographic,human,system">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aWXnKanInf" target="_blank" title="48/207"><span class="index notranslate">#48</span></a>
                <a id="title-aWXnKanInf@OpenReview" class="title-link" href="/venue/aWXnKanInf@OpenReview" target="_blank">TopoLM: brain-like spatio-functional organization in a topographic language model</a>
                <a id="pdf-aWXnKanInf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aWXnKanInf@OpenReview', this)" data="https://openreview.net/pdf?id=aWXnKanInf">[PDF<sup id="pdf-stars-aWXnKanInf@OpenReview">17</sup>]</a>
                <a id="copy-aWXnKanInf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aWXnKanInf@OpenReview')">[Copy]</a>
                <a id="kimi-aWXnKanInf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aWXnKanInf@OpenReview', this)">[Kimi<sup id="kimi-stars-aWXnKanInf@OpenReview">23</sup>]</a>
                <a id="rel-aWXnKanInf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aWXnKanInf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aWXnKanInf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Neil Rathi" target="_blank">Neil Rathi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johannes Mehrer" target="_blank">Johannes Mehrer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Badr AlKhamissi" target="_blank">Badr AlKhamissi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taha Binhuraib" target="_blank">Taha Binhuraib</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Blauch" target="_blank">Nicholas Blauch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Schrimpf" target="_blank">Martin Schrimpf</a>
            </p>
            <p id="summary-aWXnKanInf@OpenReview" class="summary">Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.</p>
            <p id="subjects-aWXnKanInf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-aWXnKanInf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aWXnKanInf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aWXnKanInf@OpenReview" onclick="foldPdfKimi('aWXnKanInf@OpenReview', this)" class="hr hr-fold">
        </div><div id="VpWki1v2P8@OpenReview" class="panel paper" keywords="lora,rite,gemma,llm,transformation,optimizers,equilibration,hellaswag,openbookqa,optimization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VpWki1v2P8" target="_blank" title="49/207"><span class="index notranslate">#49</span></a>
                <a id="title-VpWki1v2P8@OpenReview" class="title-link" href="/venue/VpWki1v2P8@OpenReview" target="_blank">LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization</a>
                <a id="pdf-VpWki1v2P8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VpWki1v2P8@OpenReview', this)" data="https://openreview.net/pdf?id=VpWki1v2P8">[PDF<sup id="pdf-stars-VpWki1v2P8@OpenReview">41</sup>]</a>
                <a id="copy-VpWki1v2P8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VpWki1v2P8@OpenReview')">[Copy]</a>
                <a id="kimi-VpWki1v2P8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VpWki1v2P8@OpenReview', this)">[Kimi<sup id="kimi-stars-VpWki1v2P8@OpenReview">26</sup>]</a>
                <a id="rel-VpWki1v2P8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VpWki1v2P8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VpWki1v2P8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jui-Nan Yen" target="_blank">Jui-Nan Yen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Si Si" target="_blank">Si Si</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhao Meng" target="_blank">Zhao Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Yu" target="_blank">Felix Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Venkata Sai Surya Subramanyam Duvvuri" target="_blank">Venkata Sai Surya Subramanyam Duvvuri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Inderjit Dhillon" target="_blank">Inderjit Dhillon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cho-Jui Hsieh" target="_blank">Cho-Jui Hsieh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjiv Kumar" target="_blank">Sanjiv Kumar</a>
            </p>
            <p id="summary-VpWki1v2P8@OpenReview" class="summary">Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).</p>
            <p id="subjects-VpWki1v2P8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-VpWki1v2P8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VpWki1v2P8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VpWki1v2P8@OpenReview" onclick="foldPdfKimi('VpWki1v2P8@OpenReview', this)" class="hr hr-fold">
        </div><div id="YrycTjllL0@OpenReview" class="panel paper" keywords="bigcodebench,llms,calls,instructions,tasks,function,tools,diverse,task,solve">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YrycTjllL0" target="_blank" title="50/207"><span class="index notranslate">#50</span></a>
                <a id="title-YrycTjllL0@OpenReview" class="title-link" href="/venue/YrycTjllL0@OpenReview" target="_blank">BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</a>
                <a id="pdf-YrycTjllL0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YrycTjllL0@OpenReview', this)" data="https://openreview.net/pdf?id=YrycTjllL0">[PDF<sup id="pdf-stars-YrycTjllL0@OpenReview">18</sup>]</a>
                <a id="copy-YrycTjllL0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YrycTjllL0@OpenReview')">[Copy]</a>
                <a id="kimi-YrycTjllL0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YrycTjllL0@OpenReview', this)">[Kimi<sup id="kimi-stars-YrycTjllL0@OpenReview">22</sup>]</a>
                <a id="rel-YrycTjllL0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YrycTjllL0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YrycTjllL0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Terry Yue Zhuo" target="_blank">Terry Yue Zhuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minh Chien Vu" target="_blank">Minh Chien Vu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jenny Chim" target="_blank">Jenny Chim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Hu" target="_blank">Han Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Yu" target="_blank">Wenhao Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ratnadira Widyasari" target="_blank">Ratnadira Widyasari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imam Nur Bani Yusuf" target="_blank">Imam Nur Bani Yusuf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haolan Zhan" target="_blank">Haolan Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junda He" target="_blank">Junda He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Indraneil Paul" target="_blank">Indraneil Paul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Brunner" target="_blank">Simon Brunner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen GONG" target="_blank">Chen GONG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Hoang" target="_blank">James Hoang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Armel Zebaze" target="_blank">Armel Zebaze</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoheng Hong" target="_blank">Xiaoheng Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen-Ding Li" target="_blank">Wen-Ding Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean Kaddour" target="_blank">Jean Kaddour</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Xu" target="_blank">Ming Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihan Zhang" target="_blank">Zhihan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Yadav" target="_blank">Prateek Yadav</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naman Jain" target="_blank">Naman Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Gu" target="_blank">Alex Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhoujun Cheng" target="_blank">Zhoujun Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Liu" target="_blank">Jiawei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Liu" target="_blank">Qian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zijian Wang" target="_blank">Zijian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Lo" target="_blank">David Lo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binyuan Hui" target="_blank">Binyuan Hui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niklas Muennighoff" target="_blank">Niklas Muennighoff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Fried" target="_blank">Daniel Fried</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoning Du" target="_blank">Xiaoning Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harm de Vries" target="_blank">Harm de Vries</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leandro Von Werra" target="_blank">Leandro Von Werra</a>
            </p>
            <p id="summary-YrycTjllL0@OpenReview" class="summary">Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that **LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%**. The results underscore the need for further advancements in this area.</p>
            <p id="subjects-YrycTjllL0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-YrycTjllL0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YrycTjllL0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YrycTjllL0@OpenReview" onclick="foldPdfKimi('YrycTjllL0@OpenReview', this)" class="hr hr-fold">
        </div><div id="YUYJsHOf3c@OpenReview" class="panel paper" keywords="regenesis,reasoning,self,paths,ood,improvement,generalists,task,post,synthesizing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YUYJsHOf3c" target="_blank" title="51/207"><span class="index notranslate">#51</span></a>
                <a id="title-YUYJsHOf3c@OpenReview" class="title-link" href="/venue/YUYJsHOf3c@OpenReview" target="_blank">ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement</a>
                <a id="pdf-YUYJsHOf3c@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YUYJsHOf3c@OpenReview', this)" data="https://openreview.net/pdf?id=YUYJsHOf3c">[PDF<sup id="pdf-stars-YUYJsHOf3c@OpenReview">55</sup>]</a>
                <a id="copy-YUYJsHOf3c@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YUYJsHOf3c@OpenReview')">[Copy]</a>
                <a id="kimi-YUYJsHOf3c@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YUYJsHOf3c@OpenReview', this)">[Kimi<sup id="kimi-stars-YUYJsHOf3c@OpenReview">70</sup>]</a>
                <a id="rel-YUYJsHOf3c@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YUYJsHOf3c@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YUYJsHOf3c@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=XIANGYU PENG" target="_blank">XIANGYU PENG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Congying Xia" target="_blank">Congying Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyi Yang" target="_blank">Xinyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caiming Xiong" target="_blank">Caiming Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chien-Sheng Wu" target="_blank">Chien-Sheng Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Xing" target="_blank">Chen Xing</a>
            </p>
            <p id="summary-YUYJsHOf3c@OpenReview" class="summary">Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose **Reasoning Generalist via Self-Improvement (ReGenesis)**, a method to *self-synthesize reasoning paths as post-training data by progressing from abstract to concrete*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices.</p>
            <p id="subjects-YUYJsHOf3c@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-YUYJsHOf3c@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YUYJsHOf3c@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YUYJsHOf3c@OpenReview" onclick="foldPdfKimi('YUYJsHOf3c@OpenReview', this)" class="hr hr-fold">
        </div><div id="YLIsIzC74j@OpenReview" class="panel paper" keywords="metrics,placement,lamplace,stage,macro,chip,cross,predictor,surrogate,timing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YLIsIzC74j" target="_blank" title="52/207"><span class="index notranslate">#52</span></a>
                <a id="title-YLIsIzC74j@OpenReview" class="title-link" href="/venue/YLIsIzC74j@OpenReview" target="_blank">LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement</a>
                <a id="pdf-YLIsIzC74j@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YLIsIzC74j@OpenReview', this)" data="https://openreview.net/pdf?id=YLIsIzC74j">[PDF<sup id="pdf-stars-YLIsIzC74j@OpenReview">14</sup>]</a>
                <a id="copy-YLIsIzC74j@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YLIsIzC74j@OpenReview')">[Copy]</a>
                <a id="kimi-YLIsIzC74j@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YLIsIzC74j@OpenReview', this)">[Kimi<sup id="kimi-stars-YLIsIzC74j@OpenReview">15</sup>]</a>
                <a id="rel-YLIsIzC74j@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YLIsIzC74j@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YLIsIzC74j@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zijie Geng" target="_blank">Zijie Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Wang" target="_blank">Jie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyan Liu" target="_blank">Ziyan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Xu" target="_blank">Siyuan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhentao Tang" target="_blank">Zhentao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shixiong Kai" target="_blank">Shixiong Kai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingxuan Yuan" target="_blank">Mingxuan Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianye HAO" target="_blank">Jianye HAO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Wu" target="_blank">Feng Wu</a>
            </p>
            <p id="summary-YLIsIzC74j@OpenReview" class="summary">Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design.However, existing methods primarily focus on *online* optimization of *intermediate surrogate metrics* that are available at the current placement stage, rather than directly targeting the *cross-stage metrics*---such as the timing performance---that measure the final chip quality.This is mainly because of the high computational costs associated with performing post-placement stages for evaluating such metrics, making the *online* optimization impractical.Consequently, these optimizations struggle to align with actual performance improvements and can even lead to severe manufacturing issues.To bridge this gap, we propose **LaMPlace**, which **L**earns **a** **M**ask for optimizing cross-stage metrics in macro placement.Specifically, LaMPlace trains a predictor on *offline* data to estimate these *cross-stage metrics* and then leverages the predictor to quickly generate a mask, i.e., a pixel-level feature map that quantifies the impact of placing a macro in each chip grid location on the design metrics.This mask essentially acts as a fast evaluator, enabling placement decisions based on *cross-stage metrics* rather than *intermediate surrogate metrics*.Experiments on commonly used benchmarks demonstrate that LaMPlace significantly improves the chip quality across several key design metrics, achieving an average improvement of 9.6\%, notably 43.0\% and 30.4\% in terms of WNS and TNS, respectively, which are two crucial cross-stage metrics that reflect the final chip quality in terms of the timing performance.</p>
            <p id="subjects-YLIsIzC74j@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-YLIsIzC74j@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YLIsIzC74j@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YLIsIzC74j@OpenReview" onclick="foldPdfKimi('YLIsIzC74j@OpenReview', this)" class="hr hr-fold">
        </div><div id="Y6aHdDNQYD@OpenReview" class="panel paper" keywords="shifts,synergy,checkpoints,test,mos,corruptions,adaptation,lidar,cross,corruption">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Y6aHdDNQYD" target="_blank" title="53/207"><span class="index notranslate">#53</span></a>
                <a id="title-Y6aHdDNQYD@OpenReview" class="title-link" href="/venue/Y6aHdDNQYD@OpenReview" target="_blank">MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection</a>
                <a id="pdf-Y6aHdDNQYD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Y6aHdDNQYD@OpenReview', this)" data="https://openreview.net/pdf?id=Y6aHdDNQYD">[PDF<sup id="pdf-stars-Y6aHdDNQYD@OpenReview">13</sup>]</a>
                <a id="copy-Y6aHdDNQYD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Y6aHdDNQYD@OpenReview')">[Copy]</a>
                <a id="kimi-Y6aHdDNQYD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Y6aHdDNQYD@OpenReview', this)">[Kimi<sup id="kimi-stars-Y6aHdDNQYD@OpenReview">12</sup>]</a>
                <a id="rel-Y6aHdDNQYD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Y6aHdDNQYD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Y6aHdDNQYD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoxiao Chen" target="_blank">Zhuoxiao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Meng" target="_blank">Junjie Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahsa Baktashmotlagh" target="_blank">Mahsa Baktashmotlagh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonggang Zhang" target="_blank">Yonggang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zi Huang" target="_blank">Zi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yadan Luo" target="_blank">Yadan Luo</a>
            </p>
            <p id="summary-Y6aHdDNQYD@OpenReview" class="summary">LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;cross-corruption&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-82" style="width: 8.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1007.19em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-83"><span class="texatom" id="MathJax-Span-84"><span class="mrow" id="MathJax-Span-85"><span class="mtext" id="MathJax-Span-86" style="font-family: MathJax_Main-italic;">cross-corruption</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">cross-corruption</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17">\textit{cross-corruption}</script> scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: https://github.com/zhuoxiao-chen/MOS.</p>
            <p id="subjects-Y6aHdDNQYD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Y6aHdDNQYD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Y6aHdDNQYD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Y6aHdDNQYD@OpenReview" onclick="foldPdfKimi('Y6aHdDNQYD@OpenReview', this)" class="hr hr-fold">
        </div><div id="xDrFWUmCne@OpenReview" class="panel paper" keywords="ld3,nfe,dpms,sampling,diffusion,unconditional,afhqv2,discretize,generation,pre">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xDrFWUmCne" target="_blank" title="54/207"><span class="index notranslate">#54</span></a>
                <a id="title-xDrFWUmCne@OpenReview" class="title-link" href="/venue/xDrFWUmCne@OpenReview" target="_blank">Learning to Discretize Denoising Diffusion ODEs</a>
                <a id="pdf-xDrFWUmCne@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xDrFWUmCne@OpenReview', this)" data="https://openreview.net/pdf?id=xDrFWUmCne">[PDF<sup id="pdf-stars-xDrFWUmCne@OpenReview">37</sup>]</a>
                <a id="copy-xDrFWUmCne@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xDrFWUmCne@OpenReview')">[Copy]</a>
                <a id="kimi-xDrFWUmCne@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xDrFWUmCne@OpenReview', this)">[Kimi<sup id="kimi-stars-xDrFWUmCne@OpenReview">29</sup>]</a>
                <a id="rel-xDrFWUmCne@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xDrFWUmCne@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xDrFWUmCne@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vinh Tong" target="_blank">Vinh Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Trung-Dung Hoang" target="_blank">Trung-Dung Hoang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anji Liu" target="_blank">Anji Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guy Van den Broeck" target="_blank">Guy Van den Broeck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mathias Niepert" target="_blank">Mathias Niepert</a>
            </p>
            <p id="summary-xDrFWUmCne@OpenReview" class="summary">Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models.</p>
            <p id="subjects-xDrFWUmCne@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-xDrFWUmCne@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xDrFWUmCne@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xDrFWUmCne@OpenReview" onclick="foldPdfKimi('xDrFWUmCne@OpenReview', this)" class="hr hr-fold">
        </div><div id="Xo0Q1N7CGk@OpenReview" class="panel paper" keywords="conformal,hypothesis,isometry,grid,hexagon,neural,space,cells,agent,patterns">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Xo0Q1N7CGk" target="_blank" title="55/207"><span class="index notranslate">#55</span></a>
                <a id="title-Xo0Q1N7CGk@OpenReview" class="title-link" href="/venue/Xo0Q1N7CGk@OpenReview" target="_blank">An Investigation of Conformal Isometry Hypothesis for Grid Cells</a>
                <a id="pdf-Xo0Q1N7CGk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Xo0Q1N7CGk@OpenReview', this)" data="https://openreview.net/pdf?id=Xo0Q1N7CGk">[PDF<sup id="pdf-stars-Xo0Q1N7CGk@OpenReview">14</sup>]</a>
                <a id="copy-Xo0Q1N7CGk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Xo0Q1N7CGk@OpenReview')">[Copy]</a>
                <a id="kimi-Xo0Q1N7CGk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Xo0Q1N7CGk@OpenReview', this)">[Kimi<sup id="kimi-stars-Xo0Q1N7CGk@OpenReview">16</sup>]</a>
                <a id="rel-Xo0Q1N7CGk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Xo0Q1N7CGk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Xo0Q1N7CGk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dehong Xu" target="_blank">Dehong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiqi Gao" target="_blank">Ruiqi Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Zhang" target="_blank">Wenhao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xue-Xin Wei" target="_blank">Xue-Xin Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingnian Wu" target="_blank">Yingnian Wu</a>
            </p>
            <p id="summary-Xo0Q1N7CGk@OpenReview" class="summary">This paper investigates the conformal isometry hypothesis as a potential explanation for hexagonal periodic patterns in grid cell response maps. The hypothesis posits that grid cell activity forms a high-dimensional vector in neural space, encoding the agents position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis suggests that this neural manifold is a conformally isometric embedding of physical space, where local displacements in neural space are proportional to those in physical space. In this paper, we conduct numerical experiments to show that this hypothesis leads to the hexagon periodic patterns of grid cells, agnostic to the choice of transformation models. Furthermore, we present a theoretical understanding that hexagon patterns emerge by minimizing our loss function because hexagon flat torus exhibits minimal deviation from local conformal isometry. In addition, we propose a conformal modulation of the agent's input velocity, enabling the recurrent neural network of grid cells to satisfy the conformal isometry hypothesis automatically.</p>
            <p id="subjects-Xo0Q1N7CGk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Xo0Q1N7CGk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xo0Q1N7CGk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xo0Q1N7CGk@OpenReview" onclick="foldPdfKimi('Xo0Q1N7CGk@OpenReview', this)" class="hr hr-fold">
        </div><div id="XmProj9cPs@OpenReview" class="panel paper" keywords="sql,spider,enterprise,world,text,database,real,workflows,workflow,often">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XmProj9cPs" target="_blank" title="56/207"><span class="index notranslate">#56</span></a>
                <a id="title-XmProj9cPs@OpenReview" class="title-link" href="/venue/XmProj9cPs@OpenReview" target="_blank">Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows</a>
                <a id="pdf-XmProj9cPs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XmProj9cPs@OpenReview', this)" data="https://openreview.net/pdf?id=XmProj9cPs">[PDF<sup id="pdf-stars-XmProj9cPs@OpenReview">16</sup>]</a>
                <a id="copy-XmProj9cPs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XmProj9cPs@OpenReview')">[Copy]</a>
                <a id="kimi-XmProj9cPs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XmProj9cPs@OpenReview', this)">[Kimi<sup id="kimi-stars-XmProj9cPs@OpenReview">24</sup>]</a>
                <a id="rel-XmProj9cPs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XmProj9cPs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XmProj9cPs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fangyu Lei" target="_blank">Fangyu Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jixuan Chen" target="_blank">Jixuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxiao Ye" target="_blank">Yuxiao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruisheng Cao" target="_blank">Ruisheng Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongchan Shin" target="_blank">Dongchan Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongjin SU" target="_blank">Hongjin SU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoqing Suo" target="_blank">Zhaoqing Suo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongcheng Gao" target="_blank">Hongcheng Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjing Hu" target="_blank">Wenjing Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengcheng Yin" target="_blank">Pengcheng Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victor Zhong" target="_blank">Victor Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caiming Xiong" target="_blank">Caiming Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Sun" target="_blank">Ruoxi Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Liu" target="_blank">Qian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sida Wang" target="_blank">Sida Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Yu" target="_blank">Tao Yu</a>
            </p>
            <p id="summary-XmProj9cPs@OpenReview" class="summary">Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics.We introduce Spider 2.0, an evaluation framework comprising <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;595&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-87" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-88"><span class="mn" id="MathJax-Span-89" style="font-family: MathJax_Main;">595</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>595</mn></math></span></span><script type="math/tex" id="MathJax-Element-18">595</script> real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake.We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-90" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-91"><span class="mn" id="MathJax-Span-92" style="font-family: MathJax_Main;">100</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>100</mn></math></span></span><script type="math/tex" id="MathJax-Element-19">100</script> lines, which goes far beyond traditional text-to-SQL challenges.Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 15.1\% of the tasks, compared with 91.2\% on Spider 1.0 and 73.0\% on BIRD.Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation --- especially in prior text-to-SQL benchmarks --- they require significant improvement in order to achieve adequate performance for real-world enterprise usage.Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings.</p>
            <p id="subjects-XmProj9cPs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-XmProj9cPs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XmProj9cPs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XmProj9cPs@OpenReview" onclick="foldPdfKimi('XmProj9cPs@OpenReview', this)" class="hr hr-fold">
        </div><div id="XFYUwIyTxQ@OpenReview" class="panel paper" keywords="vfm,masks,perception,embodiedsam,online,embodied,frames,sam,segment,instance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=XFYUwIyTxQ" target="_blank" title="57/207"><span class="index notranslate">#57</span></a>
                <a id="title-XFYUwIyTxQ@OpenReview" class="title-link" href="/venue/XFYUwIyTxQ@OpenReview" target="_blank">EmbodiedSAM: Online Segment Any 3D Thing in Real Time</a>
                <a id="pdf-XFYUwIyTxQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('XFYUwIyTxQ@OpenReview', this)" data="https://openreview.net/pdf?id=XFYUwIyTxQ">[PDF<sup id="pdf-stars-XFYUwIyTxQ@OpenReview">22</sup>]</a>
                <a id="copy-XFYUwIyTxQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('XFYUwIyTxQ@OpenReview')">[Copy]</a>
                <a id="kimi-XFYUwIyTxQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('XFYUwIyTxQ@OpenReview', this)">[Kimi<sup id="kimi-stars-XFYUwIyTxQ@OpenReview">16</sup>]</a>
                <a id="rel-XFYUwIyTxQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('XFYUwIyTxQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-XFYUwIyTxQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiuwei Xu" target="_blank">Xiuwei Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huangxing Chen" target="_blank">Huangxing Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linqing Zhao" target="_blank">Linqing Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Wang" target="_blank">Ziwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Zhou" target="_blank">Jie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiwen Lu" target="_blank">Jiwen Lu</a>
            </p>
            <p id="summary-XFYUwIyTxQ@OpenReview" class="summary">Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the field of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either offline or too slow that cannot be applied in practical embodied tasks. In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting. This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so efficient object matching between frames is required. To address these challenges, we first propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively refined by a dual-level query decoder. In this way, the 2D masks are transferred to fine-grained shapes on 3D point clouds. Benefit from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efficient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves state-of-the-art performance among online 3D perception models, even outperforming offline VFM-assisted 3D instance segmentation methods by a large margin. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in data-efficient setting. Code and demo will be released.</p>
            <p id="subjects-XFYUwIyTxQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-XFYUwIyTxQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-XFYUwIyTxQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-XFYUwIyTxQ@OpenReview" onclick="foldPdfKimi('XFYUwIyTxQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="X1OfiRYCLn@OpenReview" class="panel paper" keywords="vlb,lvlms,bootstrapping,multimodal,evaluation,contamination,benchmarks,language,vision,seedbench">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=X1OfiRYCLn" target="_blank" title="58/207"><span class="index notranslate">#58</span></a>
                <a id="title-X1OfiRYCLn@OpenReview" class="title-link" href="/venue/X1OfiRYCLn@OpenReview" target="_blank">Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping</a>
                <a id="pdf-X1OfiRYCLn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('X1OfiRYCLn@OpenReview', this)" data="https://openreview.net/pdf?id=X1OfiRYCLn">[PDF<sup id="pdf-stars-X1OfiRYCLn@OpenReview">17</sup>]</a>
                <a id="copy-X1OfiRYCLn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('X1OfiRYCLn@OpenReview')">[Copy]</a>
                <a id="kimi-X1OfiRYCLn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('X1OfiRYCLn@OpenReview', this)">[Kimi<sup id="kimi-stars-X1OfiRYCLn@OpenReview">26</sup>]</a>
                <a id="rel-X1OfiRYCLn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('X1OfiRYCLn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-X1OfiRYCLn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Yang" target="_blank">Yue Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuibo Zhang" target="_blank">Shuibo Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Shao" target="_blank">Wenqi Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaipeng Zhang" target="_blank">Kaipeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Bin" target="_blank">Yi Bin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Wang" target="_blank">Yu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Luo" target="_blank">Ping Luo</a>
            </p>
            <p id="summary-X1OfiRYCLn@OpenReview" class="summary">Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs.</p>
            <p id="subjects-X1OfiRYCLn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-X1OfiRYCLn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-X1OfiRYCLn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-X1OfiRYCLn@OpenReview" onclick="foldPdfKimi('X1OfiRYCLn@OpenReview', this)" class="hr hr-fold">
        </div><div id="odjMSBSWRt@OpenReview" class="panel paper" keywords="darkbench,manipulative,llms,dark,patterns,companies,sneaking,sycophancy,anthropomorphism,untruthful">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=odjMSBSWRt" target="_blank" title="59/207"><span class="index notranslate">#59</span></a>
                <a id="title-odjMSBSWRt@OpenReview" class="title-link" href="/venue/odjMSBSWRt@OpenReview" target="_blank">DarkBench: Benchmarking Dark Patterns in Large Language Models</a>
                <a id="pdf-odjMSBSWRt@OpenReview" class="title-pdf notranslate" onclick="togglePdf('odjMSBSWRt@OpenReview', this)" data="https://openreview.net/pdf?id=odjMSBSWRt">[PDF<sup id="pdf-stars-odjMSBSWRt@OpenReview">15</sup>]</a>
                <a id="copy-odjMSBSWRt@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('odjMSBSWRt@OpenReview')">[Copy]</a>
                <a id="kimi-odjMSBSWRt@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('odjMSBSWRt@OpenReview', this)">[Kimi<sup id="kimi-stars-odjMSBSWRt@OpenReview">20</sup>]</a>
                <a id="rel-odjMSBSWRt@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('odjMSBSWRt@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-odjMSBSWRt@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Esben Kran" target="_blank">Esben Kran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hieu Minh Nguyen" target="_blank">Hieu Minh Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akash Kundu" target="_blank">Akash Kundu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sami Jawhar" target="_blank">Sami Jawhar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinsuk Park" target="_blank">Jinsuk Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mateusz Jurewicz" target="_blank">Mateusz Jurewicz</a>
            </p>
            <p id="summary-odjMSBSWRt@OpenReview" class="summary">We introduce DarkBench, a comprehensive benchmark for detecting dark design patternsmanipulative techniques that influence user behaviorin interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.</p>
            <p id="subjects-odjMSBSWRt@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-odjMSBSWRt@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-odjMSBSWRt@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-odjMSBSWRt@OpenReview" onclick="foldPdfKimi('odjMSBSWRt@OpenReview', this)" class="hr hr-fold">
        </div><div id="WCRQFlji2q@OpenReview" class="panel paper" keywords="entity,autoencoders,hallucinations,directions,refuse,hallucinate,chat,know,sparse,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WCRQFlji2q" target="_blank" title="60/207"><span class="index notranslate">#60</span></a>
                <a id="title-WCRQFlji2q@OpenReview" class="title-link" href="/venue/WCRQFlji2q@OpenReview" target="_blank">Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</a>
                <a id="pdf-WCRQFlji2q@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WCRQFlji2q@OpenReview', this)" data="https://openreview.net/pdf?id=WCRQFlji2q">[PDF<sup id="pdf-stars-WCRQFlji2q@OpenReview">34</sup>]</a>
                <a id="copy-WCRQFlji2q@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WCRQFlji2q@OpenReview')">[Copy]</a>
                <a id="kimi-WCRQFlji2q@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WCRQFlji2q@OpenReview', this)">[Kimi<sup id="kimi-stars-WCRQFlji2q@OpenReview">44</sup>]</a>
                <a id="rel-WCRQFlji2q@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WCRQFlji2q@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WCRQFlji2q@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Javier Ferrando" target="_blank">Javier Ferrando</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oscar Obeso" target="_blank">Oscar Obeso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Senthooran Rajamanoharan" target="_blank">Senthooran Rajamanoharan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neel Nanda" target="_blank">Neel Nanda</a>
            </p>
            <p id="summary-WCRQFlji2q@OpenReview" class="summary">Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.</p>
            <p id="subjects-WCRQFlji2q@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-WCRQFlji2q@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WCRQFlji2q@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WCRQFlji2q@OpenReview" onclick="foldPdfKimi('WCRQFlji2q@OpenReview', this)" class="hr hr-fold">
        </div><div id="r5IXBlTCGc@OpenReview" class="panel paper" keywords="forecasters,consistency,forecasting,forecaster,checks,predictions,questions,arbitrageur,truth,llm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=r5IXBlTCGc" target="_blank" title="61/207"><span class="index notranslate">#61</span></a>
                <a id="title-r5IXBlTCGc@OpenReview" class="title-link" href="/venue/r5IXBlTCGc@OpenReview" target="_blank">Consistency Checks for Language Model Forecasters</a>
                <a id="pdf-r5IXBlTCGc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('r5IXBlTCGc@OpenReview', this)" data="https://openreview.net/pdf?id=r5IXBlTCGc">[PDF<sup id="pdf-stars-r5IXBlTCGc@OpenReview">12</sup>]</a>
                <a id="copy-r5IXBlTCGc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('r5IXBlTCGc@OpenReview')">[Copy]</a>
                <a id="kimi-r5IXBlTCGc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('r5IXBlTCGc@OpenReview', this)">[Kimi<sup id="kimi-stars-r5IXBlTCGc@OpenReview">28</sup>]</a>
                <a id="rel-r5IXBlTCGc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('r5IXBlTCGc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-r5IXBlTCGc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Paleka" target="_blank">Daniel Paleka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhimanyu Pallavi Sudhir" target="_blank">Abhimanyu Pallavi Sudhir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alejandro Alvarez" target="_blank">Alejandro Alvarez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vineeth Bhat" target="_blank">Vineeth Bhat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Shen" target="_blank">Adam Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Evan Wang" target="_blank">Evan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Tramer" target="_blank">Florian Tramer</a>
            </p>
            <p id="summary-r5IXBlTCGc@OpenReview" class="summary">Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters *instantaneously*? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on *arbitrage*: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60\% probability of winning the 2024 US presidential election, an arbitrageur could trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate strongly with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.</p>
            <p id="subjects-r5IXBlTCGc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-r5IXBlTCGc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-r5IXBlTCGc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-r5IXBlTCGc@OpenReview" onclick="foldPdfKimi('r5IXBlTCGc@OpenReview', this)" class="hr hr-fold">
        </div><div id="UHPnqSTBPO@OpenReview" class="panel paper" keywords="evaluation,judges,agreement,human,escalate,selective,llm,provable,guarantees,judge">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UHPnqSTBPO" target="_blank" title="62/207"><span class="index notranslate">#62</span></a>
                <a id="title-UHPnqSTBPO@OpenReview" class="title-link" href="/venue/UHPnqSTBPO@OpenReview" target="_blank">Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</a>
                <a id="pdf-UHPnqSTBPO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UHPnqSTBPO@OpenReview', this)" data="https://openreview.net/pdf?id=UHPnqSTBPO">[PDF<sup id="pdf-stars-UHPnqSTBPO@OpenReview">10</sup>]</a>
                <a id="copy-UHPnqSTBPO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UHPnqSTBPO@OpenReview')">[Copy]</a>
                <a id="kimi-UHPnqSTBPO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UHPnqSTBPO@OpenReview', this)">[Kimi<sup id="kimi-stars-UHPnqSTBPO@OpenReview">22</sup>]</a>
                <a id="rel-UHPnqSTBPO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UHPnqSTBPO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UHPnqSTBPO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaehun Jung" target="_blank">Jaehun Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Faeze Brahman" target="_blank">Faeze Brahman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yejin Choi" target="_blank">Yejin Choi</a>
            </p>
            <p id="summary-UHPnqSTBPO@OpenReview" class="summary">We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this *selective evaluation* framework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce *Simulated Annotators*, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose *Cascaded Selective Evaluation*, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, *guarantees* over 80% human agreement with almost 80% test coverage.</p>
            <p id="subjects-UHPnqSTBPO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-UHPnqSTBPO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UHPnqSTBPO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UHPnqSTBPO@OpenReview" onclick="foldPdfKimi('UHPnqSTBPO@OpenReview', this)" class="hr hr-fold">
        </div><div id="tTPHgb0EtV@OpenReview" class="panel paper" keywords="harmful,booster,tuning,perturbation,fine,alignment,regularizer,attenuating,qi2023fine,loss">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tTPHgb0EtV" target="_blank" title="63/207"><span class="index notranslate">#63</span></a>
                <a id="title-tTPHgb0EtV@OpenReview" class="title-link" href="/venue/tTPHgb0EtV@OpenReview" target="_blank">Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation</a>
                <a id="pdf-tTPHgb0EtV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tTPHgb0EtV@OpenReview', this)" data="https://openreview.net/pdf?id=tTPHgb0EtV">[PDF<sup id="pdf-stars-tTPHgb0EtV@OpenReview">15</sup>]</a>
                <a id="copy-tTPHgb0EtV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tTPHgb0EtV@OpenReview')">[Copy]</a>
                <a id="kimi-tTPHgb0EtV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tTPHgb0EtV@OpenReview', this)">[Kimi<sup id="kimi-stars-tTPHgb0EtV@OpenReview">16</sup>]</a>
                <a id="rel-tTPHgb0EtV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tTPHgb0EtV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tTPHgb0EtV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tiansheng Huang" target="_blank">Tiansheng Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sihao Hu" target="_blank">Sihao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fatih Ilhan" target="_blank">Fatih Ilhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Selim Tekin" target="_blank">Selim Tekin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Liu" target="_blank">Ling Liu</a>
            </p>
            <p id="summary-tTPHgb0EtV@OpenReview" class="summary">Harmful fine-tuning attack \citep{qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that \textit{harmful perturbation} over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at \url{https://anonymous.4open.science/r/Booster-EF18}.</p>
            <p id="subjects-tTPHgb0EtV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-tTPHgb0EtV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tTPHgb0EtV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tTPHgb0EtV@OpenReview" onclick="foldPdfKimi('tTPHgb0EtV@OpenReview', this)" class="hr hr-fold">
        </div><div id="SBCMNc3Mq3@OpenReview" class="panel paper" keywords="ecd,electronic,precision,charge,ernzerhof,hse,dft,pbe,density,crystalline">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SBCMNc3Mq3" target="_blank" title="64/207"><span class="index notranslate">#64</span></a>
                <a id="title-SBCMNc3Mq3@OpenReview" class="title-link" href="/venue/SBCMNc3Mq3@OpenReview" target="_blank">ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials</a>
                <a id="pdf-SBCMNc3Mq3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SBCMNc3Mq3@OpenReview', this)" data="https://openreview.net/pdf?id=SBCMNc3Mq3">[PDF<sup id="pdf-stars-SBCMNc3Mq3@OpenReview">11</sup>]</a>
                <a id="copy-SBCMNc3Mq3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SBCMNc3Mq3@OpenReview')">[Copy]</a>
                <a id="kimi-SBCMNc3Mq3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SBCMNc3Mq3@OpenReview', this)">[Kimi<sup id="kimi-stars-SBCMNc3Mq3@OpenReview">13</sup>]</a>
                <a id="rel-SBCMNc3Mq3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SBCMNc3Mq3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SBCMNc3Mq3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pin Chen" target="_blank">Pin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zexin Xu" target="_blank">Zexin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Mo" target="_blank">Qing Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongjin Zhong" target="_blank">Hongjin Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengyang Xu" target="_blank">Fengyang Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yutong Lu" target="_blank">Yutong Lu</a>
            </p>
            <p id="summary-SBCMNc3Mq3@OpenReview" class="summary">Supervised machine learning techniques are increasingly being adopted to speed up electronic structure predictions, serving as alternatives to first-principles methods like Density Functional Theory (DFT). Although current DFT datasets mainly emphasize chemical properties and atomic forces, the precise prediction of electronic charge density is essential for accurately determining a system's total energy and ground state properties. In this study, we introduce a novel electronic charge density dataset named ECD, which encompasses 140,646 stable crystal geometries with medium-precision PerdewBurkeErnzerhof (PBE) functional data. Within this dataset, a subset of 7,147 geometries includes high-precision electronic charge density data calculated using the HeydScuseriaErnzerhof (HSE) functional in DFT. By designing various benchmark tasks for crystalline materials and emphasizing training with large-scale PBE data while fine-tuning with a smaller subset of high-precision HSE data, we demonstrate the efficacy of current machine learning models in predicting electronic charge densities.The ECD dataset and baseline models are open-sourced to support community efforts in developing new methodologies and accelerating materials design and applications.</p>
            <p id="subjects-SBCMNc3Mq3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-SBCMNc3Mq3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SBCMNc3Mq3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SBCMNc3Mq3@OpenReview" onclick="foldPdfKimi('SBCMNc3Mq3@OpenReview', this)" class="hr hr-fold">
        </div><div id="RuP17cJtZo@OpenReview" class="panel paper" keywords="generator,matching,generative,markov,processes,modeling,generators,jump,arbitrary,vein">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RuP17cJtZo" target="_blank" title="65/207"><span class="index notranslate">#65</span></a>
                <a id="title-RuP17cJtZo@OpenReview" class="title-link" href="/venue/RuP17cJtZo@OpenReview" target="_blank">Generator Matching: Generative modeling with arbitrary Markov processes</a>
                <a id="pdf-RuP17cJtZo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RuP17cJtZo@OpenReview', this)" data="https://openreview.net/pdf?id=RuP17cJtZo">[PDF<sup id="pdf-stars-RuP17cJtZo@OpenReview">25</sup>]</a>
                <a id="copy-RuP17cJtZo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RuP17cJtZo@OpenReview')">[Copy]</a>
                <a id="kimi-RuP17cJtZo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RuP17cJtZo@OpenReview', this)">[Kimi<sup id="kimi-stars-RuP17cJtZo@OpenReview">30</sup>]</a>
                <a id="rel-RuP17cJtZo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RuP17cJtZo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RuP17cJtZo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Holderrieth" target="_blank">Peter Holderrieth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marton Havasi" target="_blank">Marton Havasi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Yim" target="_blank">Jason Yim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neta Shaul" target="_blank">Neta Shaul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Itai Gat" target="_blank">Itai Gat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommi Jaakkola" target="_blank">Tommi Jaakkola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Karrer" target="_blank">Brian Karrer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ricky T. Q. Chen" target="_blank">Ricky T. Q. Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaron Lipman" target="_blank">Yaron Lipman</a>
            </p>
            <p id="summary-RuP17cJtZo@OpenReview" class="summary">We introduce generator matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that generator matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it provides the foundation to expand the design space to new and unexplored Markov processes such as jump processes. Finally, generator matching enables the construction of superpositions of Markov generative processes and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on protein and image structure generation, showing that superposition with a jump process improves image generation.</p>
            <p id="subjects-RuP17cJtZo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-RuP17cJtZo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RuP17cJtZo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RuP17cJtZo@OpenReview" onclick="foldPdfKimi('RuP17cJtZo@OpenReview', this)" class="hr hr-fold">
        </div><div id="RWJX5F5I9g@OpenReview" class="panel paper" keywords="bbn,exploration,brain,bandit,mab,neural,network,uncertain,controls,tasks">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=RWJX5F5I9g" target="_blank" title="66/207"><span class="index notranslate">#66</span></a>
                <a id="title-RWJX5F5I9g@OpenReview" class="title-link" href="/venue/RWJX5F5I9g@OpenReview" target="_blank">Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration</a>
                <a id="pdf-RWJX5F5I9g@OpenReview" class="title-pdf notranslate" onclick="togglePdf('RWJX5F5I9g@OpenReview', this)" data="https://openreview.net/pdf?id=RWJX5F5I9g">[PDF<sup id="pdf-stars-RWJX5F5I9g@OpenReview">17</sup>]</a>
                <a id="copy-RWJX5F5I9g@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('RWJX5F5I9g@OpenReview')">[Copy]</a>
                <a id="kimi-RWJX5F5I9g@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('RWJX5F5I9g@OpenReview', this)">[Kimi<sup id="kimi-stars-RWJX5F5I9g@OpenReview">18</sup>]</a>
                <a id="rel-RWJX5F5I9g@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('RWJX5F5I9g@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-RWJX5F5I9g@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Jiang" target="_blank">Chen Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui An" target="_blank">Jiahui An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yating Liu" target="_blank">Yating Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ni Ji" target="_blank">Ni Ji</a>
            </p>
            <p id="summary-RWJX5F5I9g@OpenReview" class="summary">How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel conditions. To understand how the brains neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this type of network (which is named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield networks. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with an uncertainty bias in a way that resembles human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical basis for efficient exploration in biological neural networks and proposes a general, brain-inspired algorithmic architecture for efficient exploration in RL.</p>
            <p id="subjects-RWJX5F5I9g@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-RWJX5F5I9g@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-RWJX5F5I9g@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-RWJX5F5I9g@OpenReview" onclick="foldPdfKimi('RWJX5F5I9g@OpenReview', this)" class="hr hr-fold">
        </div><div id="vzItLaEoDa@OpenReview" class="panel paper" keywords="imagination,world,short,term,long,open,minedojo,reinforcement,jumpy,agents">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vzItLaEoDa" target="_blank" title="67/207"><span class="index notranslate">#67</span></a>
                <a id="title-vzItLaEoDa@OpenReview" class="title-link" href="/venue/vzItLaEoDa@OpenReview" target="_blank">Open-World Reinforcement Learning over Long Short-Term Imagination</a>
                <a id="pdf-vzItLaEoDa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vzItLaEoDa@OpenReview', this)" data="https://openreview.net/pdf?id=vzItLaEoDa">[PDF<sup id="pdf-stars-vzItLaEoDa@OpenReview">28</sup>]</a>
                <a id="copy-vzItLaEoDa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vzItLaEoDa@OpenReview')">[Copy]</a>
                <a id="kimi-vzItLaEoDa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vzItLaEoDa@OpenReview', this)">[Kimi<sup id="kimi-stars-vzItLaEoDa@OpenReview">36</sup>]</a>
                <a id="rel-vzItLaEoDa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vzItLaEoDa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vzItLaEoDa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajian Li" target="_blank">Jiajian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Wang" target="_blank">Qi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunbo Wang" target="_blank">Yunbo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Jin" target="_blank">Xin Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Li" target="_blank">Yang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjun Zeng" target="_blank">Wenjun Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaokang Yang" target="_blank">Xiaokang Yang</a>
            </p>
            <p id="summary-vzItLaEoDa@OpenReview" class="summary">Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be short-sighted, as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;long short-term world model&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-93" style="width: 14.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1012.09em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-94"><span class="texatom" id="MathJax-Span-95"><span class="mrow" id="MathJax-Span-96"><span class="mtext" id="MathJax-Span-97" style="font-family: MathJax_Main-italic;">long short-term world model</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">long short-term world model</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-20">\textit{long short-term world model}</script>. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo.</p>
            <p id="subjects-vzItLaEoDa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-vzItLaEoDa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vzItLaEoDa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vzItLaEoDa@OpenReview" onclick="foldPdfKimi('vzItLaEoDa@OpenReview', this)" class="hr hr-fold">
        </div><div id="QWunLKbBGF@OpenReview" class="panel paper" keywords="prefeval,preference,llms,preferences,following,conversations,user,personalized,conversational,prompting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QWunLKbBGF" target="_blank" title="68/207"><span class="index notranslate">#68</span></a>
                <a id="title-QWunLKbBGF@OpenReview" class="title-link" href="/venue/QWunLKbBGF@OpenReview" target="_blank">Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs</a>
                <a id="pdf-QWunLKbBGF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QWunLKbBGF@OpenReview', this)" data="https://openreview.net/pdf?id=QWunLKbBGF">[PDF<sup id="pdf-stars-QWunLKbBGF@OpenReview">25</sup>]</a>
                <a id="copy-QWunLKbBGF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QWunLKbBGF@OpenReview')">[Copy]</a>
                <a id="kimi-QWunLKbBGF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QWunLKbBGF@OpenReview', this)">[Kimi<sup id="kimi-stars-QWunLKbBGF@OpenReview">51</sup>]</a>
                <a id="rel-QWunLKbBGF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QWunLKbBGF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QWunLKbBGF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Siyan Zhao" target="_blank">Siyan Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyi Hong" target="_blank">Mingyi Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Liu" target="_blank">Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Devamanyu Hazarika" target="_blank">Devamanyu Hazarika</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaixiang Lin" target="_blank">Kaixiang Lin</a>
            </p>
            <p id="summary-QWunLKbBGF@OpenReview" class="summary">Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced andproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. We also find that multiple stated preferences within a conversation improve adherence and models are not affected by conflicting preferences. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents.</p>
            <p id="subjects-QWunLKbBGF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-QWunLKbBGF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QWunLKbBGF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QWunLKbBGF@OpenReview" onclick="foldPdfKimi('QWunLKbBGF@OpenReview', this)" class="hr hr-fold">
        </div><div id="QEHrmQPBdd@OpenReview" class="panel paper" keywords="reward,bench,style,models,subtlety,language,responses,evaluate,subtle,content">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QEHrmQPBdd" target="_blank" title="69/207"><span class="index notranslate">#69</span></a>
                <a id="title-QEHrmQPBdd@OpenReview" class="title-link" href="/venue/QEHrmQPBdd@OpenReview" target="_blank">RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</a>
                <a id="pdf-QEHrmQPBdd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QEHrmQPBdd@OpenReview', this)" data="https://openreview.net/pdf?id=QEHrmQPBdd">[PDF<sup id="pdf-stars-QEHrmQPBdd@OpenReview">12</sup>]</a>
                <a id="copy-QEHrmQPBdd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QEHrmQPBdd@OpenReview')">[Copy]</a>
                <a id="kimi-QEHrmQPBdd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QEHrmQPBdd@OpenReview', this)">[Kimi<sup id="kimi-stars-QEHrmQPBdd@OpenReview">12</sup>]</a>
                <a id="rel-QEHrmQPBdd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QEHrmQPBdd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QEHrmQPBdd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yantao Liu" target="_blank">Yantao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zijun Yao" target="_blank">Zijun Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Min" target="_blank">Rui Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixin Cao" target="_blank">Yixin Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Hou" target="_blank">Lei Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juanzi Li" target="_blank">Juanzi Li</a>
            </p>
            <p id="summary-QEHrmQPBdd@OpenReview" class="summary">Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.These findings highlight the significant room for improvement in current reward models.</p>
            <p id="subjects-QEHrmQPBdd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-QEHrmQPBdd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QEHrmQPBdd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QEHrmQPBdd@OpenReview" onclick="foldPdfKimi('QEHrmQPBdd@OpenReview', this)" class="hr hr-fold">
        </div><div id="syThiTmWWm@OpenReview" class="panel paper" keywords="win,cheating,alpacaeval,promotional,benchmarks,automatic,rates,arena,bench,llm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=syThiTmWWm" target="_blank" title="70/207"><span class="index notranslate">#70</span></a>
                <a id="title-syThiTmWWm@OpenReview" class="title-link" href="/venue/syThiTmWWm@OpenReview" target="_blank">Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates</a>
                <a id="pdf-syThiTmWWm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('syThiTmWWm@OpenReview', this)" data="https://openreview.net/pdf?id=syThiTmWWm">[PDF<sup id="pdf-stars-syThiTmWWm@OpenReview">11</sup>]</a>
                <a id="copy-syThiTmWWm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('syThiTmWWm@OpenReview')">[Copy]</a>
                <a id="kimi-syThiTmWWm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('syThiTmWWm@OpenReview', this)">[Kimi<sup id="kimi-stars-syThiTmWWm@OpenReview">14</sup>]</a>
                <a id="rel-syThiTmWWm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('syThiTmWWm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-syThiTmWWm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaosen Zheng" target="_blank">Xiaosen Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Pang" target="_blank">Tianyu Pang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Du" target="_blank">Chao Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Liu" target="_blank">Qian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Jiang" target="_blank">Jing Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Lin" target="_blank">Min Lin</a>
            </p>
            <p id="summary-syThiTmWWm@OpenReview" class="summary">Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a **"null model"** that always outputs a **constant** response (*irrelevant to input instructions*) can cheat automatic benchmarks and achieve top-ranked win rates: an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax MathJax_FullWidth notranslate" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;86.5&lt;/mn&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-98" style="width: 100%; display: inline-block; min-width: 2.138em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(3.18em, 1001.72em, 5.367em, -999.997em); top: -4.008em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-99"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.18em, 1001.72em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-100" style="font-family: MathJax_Main;">86.5</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -2.81em; left: 0em;"><span class="mspace" id="MathJax-Span-101" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>86.5</mn><mspace linebreak="newline"></mspace></math></span></span><script type="math/tex" id="MathJax-Element-21">86.5\\%</script> LC win rate on AlpacaEval 2.0; an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;83.0&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-102" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.72em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-103"><span class="mn" id="MathJax-Span-104" style="font-family: MathJax_Main;">83.0</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>83.0</mn></math></span></span><script type="math/tex" id="MathJax-Element-22">83.0</script> score on Arena-Hard-Auto; and a <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;9.55&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-105" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.72em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-106"><span class="mn" id="MathJax-Span-107" style="font-family: MathJax_Main;">9.55</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9.55</mn></math></span></span><script type="math/tex" id="MathJax-Element-23">9.55</script> score on MT-Bench. Moreover, the crafted cheating outputs are **transferable** because we assume that the instructions of these benchmarks (e.g., <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;805&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-108" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-109"><span class="mn" id="MathJax-Span-110" style="font-family: MathJax_Main;">805</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>805</mn></math></span></span><script type="math/tex" id="MathJax-Element-24">805</script> samples of AlpacaEval 2.0) are *private* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks.</p>
            <p id="subjects-syThiTmWWm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-syThiTmWWm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-syThiTmWWm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-syThiTmWWm@OpenReview" onclick="foldPdfKimi('syThiTmWWm@OpenReview', this)" class="hr hr-fold">
        </div><div id="PSiijdQjNU@OpenReview" class="panel paper" keywords="protein,profilebfn,profile,family,proteins,msa,bayesian,flow,design,steering">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PSiijdQjNU" target="_blank" title="71/207"><span class="index notranslate">#71</span></a>
                <a id="title-PSiijdQjNU@OpenReview" class="title-link" href="/venue/PSiijdQjNU@OpenReview" target="_blank">Steering Protein Family Design through Profile Bayesian Flow</a>
                <a id="pdf-PSiijdQjNU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PSiijdQjNU@OpenReview', this)" data="https://openreview.net/pdf?id=PSiijdQjNU">[PDF<sup id="pdf-stars-PSiijdQjNU@OpenReview">15</sup>]</a>
                <a id="copy-PSiijdQjNU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PSiijdQjNU@OpenReview')">[Copy]</a>
                <a id="kimi-PSiijdQjNU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PSiijdQjNU@OpenReview', this)">[Kimi<sup id="kimi-stars-PSiijdQjNU@OpenReview">11</sup>]</a>
                <a id="rel-PSiijdQjNU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PSiijdQjNU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PSiijdQjNU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingjing Gong" target="_blank">Jingjing Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Pei" target="_blank">Yu Pei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Long" target="_blank">Siyu Long</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Song" target="_blank">Yuxuan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Zhang" target="_blank">Zhe Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Huang" target="_blank">Wenhao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyao Cao" target="_blank">Ziyao Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuyi Zhang" target="_blank">Shuyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhou" target="_blank">Hao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Ying Ma" target="_blank">Wei-Ying Ma</a>
            </p>
            <p id="summary-PSiijdQjNU@OpenReview" class="summary">Protein family design emerges as a promising alternative by combining the advantages of de novo protein design and mutation-based directed evolution.In this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for specifically generative modeling of protein families. ProfileBFN extends the discrete Bayesian Flow Network from an MSA profile perspective, which can be trained on single protein sequences by regarding it as a degenerate profile, thereby achieving efficient protein family design by avoiding large-scale MSA data construction and training. Empirical results show that ProfileBFN has a profound understanding of proteins. When generating diverse and novel family proteins, it can accurately capture the structural characteristics of the family. The enzyme produced by this method is more likely than the previous approach to have the corresponding function, offering better odds of generating diverse proteins with the desired functionality.</p>
            <p id="subjects-PSiijdQjNU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-PSiijdQjNU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PSiijdQjNU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PSiijdQjNU@OpenReview" onclick="foldPdfKimi('PSiijdQjNU@OpenReview', this)" class="hr hr-fold">
        </div><div id="PBjCTeDL6o@OpenReview" class="panel paper" keywords="unlearning,interpretations,baselines,static,manipulable,smooths,departing,debiased,erasing,blurring">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PBjCTeDL6o" target="_blank" title="72/207"><span class="index notranslate">#72</span></a>
                <a id="title-PBjCTeDL6o@OpenReview" class="title-link" href="/venue/PBjCTeDL6o@OpenReview" target="_blank">Unlearning-based Neural Interpretations</a>
                <a id="pdf-PBjCTeDL6o@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PBjCTeDL6o@OpenReview', this)" data="https://openreview.net/pdf?id=PBjCTeDL6o">[PDF<sup id="pdf-stars-PBjCTeDL6o@OpenReview">19</sup>]</a>
                <a id="copy-PBjCTeDL6o@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PBjCTeDL6o@OpenReview')">[Copy]</a>
                <a id="kimi-PBjCTeDL6o@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PBjCTeDL6o@OpenReview', this)">[Kimi<sup id="kimi-stars-PBjCTeDL6o@OpenReview">26</sup>]</a>
                <a id="rel-PBjCTeDL6o@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PBjCTeDL6o@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PBjCTeDL6o@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ching Lam Choi" target="_blank">Ching Lam Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Duplessis" target="_blank">Alexandre Duplessis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Serge Belongie" target="_blank">Serge Belongie</a>
            </p>
            <p id="summary-PBjCTeDL6o@OpenReview" class="summary">Gradient-based interpretations often require an anchor point of comparison to avoid saturation in computing feature importance. We show that current baselines defined using static functions constant mapping, averaging or blurring  inject harmful colour, texture or frequency assumptions that deviate from model behaviour. This leads to accumulation of irregular gradients, resulting in attribution maps that are biased, fragile and manipulable. Departing from the static approach, we propose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;UNI&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-111" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.51em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-112"><span class="texatom" id="MathJax-Span-113"><span class="mrow" id="MathJax-Span-114"><span class="mtext" id="MathJax-Span-115" style="font-family: MathJax_Typewriter;">UNI</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">UNI</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-25">\texttt{UNI}</script> to compute an (un)learnable, debiased and adaptive baseline by perturbing the input towards an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;unlearning direction&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-116" style="width: 10.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.753em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1008.8em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-117"><span class="texatom" id="MathJax-Span-118"><span class="mrow" id="MathJax-Span-119"><span class="mtext" id="MathJax-Span-120" style="font-family: MathJax_Main-italic;">unlearning direction</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">unlearning direction</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-26">\textit{unlearning direction}</script> of steepest ascent. Our method discovers reliable baselines and succeeds in erasing salient features, which in turn locally smooths the high-curvature decision boundaries. Our analyses point to unlearning as a promising avenue for generating faithful, efficient and robust interpretations.</p>
            <p id="subjects-PBjCTeDL6o@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-PBjCTeDL6o@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PBjCTeDL6o@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PBjCTeDL6o@OpenReview" onclick="foldPdfKimi('PBjCTeDL6o@OpenReview', this)" class="hr hr-fold">
        </div><div id="P4o9akekdf@OpenReview" class="panel paper" keywords="pose,unposed,gaussians,gaussian,splats,view,input,estimation,reconstruction,primitives">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=P4o9akekdf" target="_blank" title="73/207"><span class="index notranslate">#73</span></a>
                <a id="title-P4o9akekdf@OpenReview" class="title-link" href="/venue/P4o9akekdf@OpenReview" target="_blank">No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</a>
                <a id="pdf-P4o9akekdf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('P4o9akekdf@OpenReview', this)" data="https://openreview.net/pdf?id=P4o9akekdf">[PDF<sup id="pdf-stars-P4o9akekdf@OpenReview">15</sup>]</a>
                <a id="copy-P4o9akekdf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('P4o9akekdf@OpenReview')">[Copy]</a>
                <a id="kimi-P4o9akekdf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('P4o9akekdf@OpenReview', this)">[Kimi<sup id="kimi-stars-P4o9akekdf@OpenReview">11</sup>]</a>
                <a id="rel-P4o9akekdf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('P4o9akekdf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-P4o9akekdf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Botao Ye" target="_blank">Botao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sifei Liu" target="_blank">Sifei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Songyou Peng" target="_blank">Songyou Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haofei Xu" target="_blank">Haofei Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueting Li" target="_blank">Xueting Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Hsuan Yang" target="_blank">Ming-Hsuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>
            </p>
            <p id="summary-P4o9akekdf@OpenReview" class="summary">We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from unposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. The source code and trained models will be made available to the public.</p>
            <p id="subjects-P4o9akekdf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-P4o9akekdf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-P4o9akekdf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-P4o9akekdf@OpenReview" onclick="foldPdfKimi('P4o9akekdf@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZCOwwRAaEl@OpenReview" class="panel paper" keywords="optimization,lbo,normalizing,latent,bayesian,token,spaces,autoregressive,discrepancy,input">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZCOwwRAaEl" target="_blank" title="74/207"><span class="index notranslate">#74</span></a>
                <a id="title-ZCOwwRAaEl@OpenReview" class="title-link" href="/venue/ZCOwwRAaEl@OpenReview" target="_blank">Latent Bayesian Optimization via Autoregressive Normalizing Flows</a>
                <a id="pdf-ZCOwwRAaEl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZCOwwRAaEl@OpenReview', this)" data="https://openreview.net/pdf?id=ZCOwwRAaEl">[PDF<sup id="pdf-stars-ZCOwwRAaEl@OpenReview">21</sup>]</a>
                <a id="copy-ZCOwwRAaEl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZCOwwRAaEl@OpenReview')">[Copy]</a>
                <a id="kimi-ZCOwwRAaEl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZCOwwRAaEl@OpenReview', this)">[Kimi<sup id="kimi-stars-ZCOwwRAaEl@OpenReview">28</sup>]</a>
                <a id="rel-ZCOwwRAaEl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZCOwwRAaEl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZCOwwRAaEl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seunghun Lee" target="_blank">Seunghun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyoung Park" target="_blank">Jinyoung Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewon Chu" target="_blank">Jaewon Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minseo Yoon" target="_blank">Minseo Yoon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunwoo Kim" target="_blank">Hyunwoo Kim</a>
            </p>
            <p id="summary-ZCOwwRAaEl@OpenReview" class="summary">Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions.Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the complexity of high-dimensional and structured data spaces.However, existing LBO approaches often suffer from the value discrepancy problem, which arises from the reconstruction gap between latent and input spaces.This value discrepancy problem propagates errors throughout the optimization process, which induces suboptimal optimization outcomes.To address this issue, we propose a Normalizing Flow-based Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative model to establish accurate and one-to-one mappings between latent and input spaces.To deal with sequence-based inputs, we introduce SeqFlow, an autoregressive sequence-specialized normalizing flow model designed to maintain one-to-one mappings between the input and latent spaces. Moreover, we develop a token-level adaptive candidate sampling strategy that dynamically adjusts the exploration probability of each token based on the token-level importance in the optimization process.Through extensive experiments, our NF-BO method demonstrates superior performance in molecule generation tasks, significantly outperforming traditional optimization methods and existing LBO approaches.</p>
            <p id="subjects-ZCOwwRAaEl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-ZCOwwRAaEl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZCOwwRAaEl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZCOwwRAaEl@OpenReview" onclick="foldPdfKimi('ZCOwwRAaEl@OpenReview', this)" class="hr hr-fold">
        </div><div id="Ozo7qJ5vZi@OpenReview" class="panel paper" keywords="kans,mlps,arnold,kolmogorov,interpretability,accuracy,kan,alternatives,activation,weights">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Ozo7qJ5vZi" target="_blank" title="75/207"><span class="index notranslate">#75</span></a>
                <a id="title-Ozo7qJ5vZi@OpenReview" class="title-link" href="/venue/Ozo7qJ5vZi@OpenReview" target="_blank">KAN: KolmogorovArnold Networks</a>
                <a id="pdf-Ozo7qJ5vZi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Ozo7qJ5vZi@OpenReview', this)" data="https://openreview.net/pdf?id=Ozo7qJ5vZi">[PDF<sup id="pdf-stars-Ozo7qJ5vZi@OpenReview">41</sup>]</a>
                <a id="copy-Ozo7qJ5vZi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Ozo7qJ5vZi@OpenReview')">[Copy]</a>
                <a id="kimi-Ozo7qJ5vZi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Ozo7qJ5vZi@OpenReview', this)">[Kimi<sup id="kimi-stars-Ozo7qJ5vZi@OpenReview">27</sup>]</a>
                <a id="rel-Ozo7qJ5vZi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Ozo7qJ5vZi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Ozo7qJ5vZi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziming Liu" target="_blank">Ziming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Wang" target="_blank">Yixuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sachin Vaidya" target="_blank">Sachin Vaidya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Ruehle" target="_blank">Fabian Ruehle</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Halverson" target="_blank">James Halverson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marin Soljacic" target="_blank">Marin Soljacic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Hou" target="_blank">Thomas Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Tegmark" target="_blank">Max Tegmark</a>
            </p>
            <p id="summary-Ozo7qJ5vZi@OpenReview" class="summary">Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons''), KANs have learnable activation functions on edges ("weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful ``collaborators'' helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs. Despite the slow training of KANs, their improved accuracy and interpretability show the potential to improve today's deep learning models which rely heavily on MLPs. More research is necessary to make KANs' training more efficient.</p>
            <p id="subjects-Ozo7qJ5vZi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Ozo7qJ5vZi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ozo7qJ5vZi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ozo7qJ5vZi@OpenReview" onclick="foldPdfKimi('Ozo7qJ5vZi@OpenReview', this)" class="hr hr-fold">
        </div><div id="OvoCm1gGhN@OpenReview" class="panel paper" keywords="transformer,diff,attention,context,hallucination,overallocate,irrelevant,differential,canceling,distracted">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OvoCm1gGhN" target="_blank" title="76/207"><span class="index notranslate">#76</span></a>
                <a id="title-OvoCm1gGhN@OpenReview" class="title-link" href="/venue/OvoCm1gGhN@OpenReview" target="_blank">Differential Transformer</a>
                <a id="pdf-OvoCm1gGhN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OvoCm1gGhN@OpenReview', this)" data="https://openreview.net/pdf?id=OvoCm1gGhN">[PDF<sup id="pdf-stars-OvoCm1gGhN@OpenReview">69</sup>]</a>
                <a id="copy-OvoCm1gGhN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OvoCm1gGhN@OpenReview')">[Copy]</a>
                <a id="kimi-OvoCm1gGhN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OvoCm1gGhN@OpenReview', this)">[Kimi<sup id="kimi-stars-OvoCm1gGhN@OpenReview">56</sup>]</a>
                <a id="rel-OvoCm1gGhN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OvoCm1gGhN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OvoCm1gGhN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianzhu Ye" target="_blank">Tianzhu Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Dong" target="_blank">Li Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqing Xia" target="_blank">Yuqing Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yutao Sun" target="_blank">Yutao Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhu" target="_blank">Yi Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gao Huang" target="_blank">Gao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Furu Wei" target="_blank">Furu Wei</a>
            </p>
            <p id="summary-OvoCm1gGhN@OpenReview" class="summary">Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture for large language models.</p>
            <p id="subjects-OvoCm1gGhN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-OvoCm1gGhN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OvoCm1gGhN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OvoCm1gGhN@OpenReview" onclick="foldPdfKimi('OvoCm1gGhN@OpenReview', this)" class="hr hr-fold">
        </div><div id="OlzB6LnXcS@OpenReview" class="panel paper" keywords="shortcut,models,step,sampling,budgets,network,training,reflow,diffusion,multiple">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OlzB6LnXcS" target="_blank" title="77/207"><span class="index notranslate">#77</span></a>
                <a id="title-OlzB6LnXcS@OpenReview" class="title-link" href="/venue/OlzB6LnXcS@OpenReview" target="_blank">One Step Diffusion via Shortcut Models</a>
                <a id="pdf-OlzB6LnXcS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OlzB6LnXcS@OpenReview', this)" data="https://openreview.net/pdf?id=OlzB6LnXcS">[PDF<sup id="pdf-stars-OlzB6LnXcS@OpenReview">41</sup>]</a>
                <a id="copy-OlzB6LnXcS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OlzB6LnXcS@OpenReview')">[Copy]</a>
                <a id="kimi-OlzB6LnXcS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OlzB6LnXcS@OpenReview', this)">[Kimi<sup id="kimi-stars-OlzB6LnXcS@OpenReview">37</sup>]</a>
                <a id="rel-OlzB6LnXcS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OlzB6LnXcS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OlzB6LnXcS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Frans" target="_blank">Kevin Frans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Danijar Hafner" target="_blank">Danijar Hafner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Levine" target="_blank">Sergey Levine</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pieter Abbeel" target="_blank">Pieter Abbeel</a>
            </p>
            <p id="summary-OlzB6LnXcS@OpenReview" class="summary">Diffusion models and flow matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce Shortcut Models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.</p>
            <p id="subjects-OlzB6LnXcS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-OlzB6LnXcS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OlzB6LnXcS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OlzB6LnXcS@OpenReview" onclick="foldPdfKimi('OlzB6LnXcS@OpenReview', this)" class="hr hr-fold">
        </div><div id="OfjIlbelrT@OpenReview" class="panel paper" keywords="attention,sparse,flexprefill,patterns,inference,query,sequence,predefined,aware,filling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OfjIlbelrT" target="_blank" title="78/207"><span class="index notranslate">#78</span></a>
                <a id="title-OfjIlbelrT@OpenReview" class="title-link" href="/venue/OfjIlbelrT@OpenReview" target="_blank">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a>
                <a id="pdf-OfjIlbelrT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OfjIlbelrT@OpenReview', this)" data="https://openreview.net/pdf?id=OfjIlbelrT">[PDF<sup id="pdf-stars-OfjIlbelrT@OpenReview">27</sup>]</a>
                <a id="copy-OfjIlbelrT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OfjIlbelrT@OpenReview')">[Copy]</a>
                <a id="kimi-OfjIlbelrT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OfjIlbelrT@OpenReview', this)">[Kimi<sup id="kimi-stars-OfjIlbelrT@OpenReview">35</sup>]</a>
                <a id="rel-OfjIlbelrT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OfjIlbelrT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OfjIlbelrT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xunhao Lai" target="_blank">Xunhao Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianqiao Lu" target="_blank">Jianqiao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Luo" target="_blank">Yao Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyuan Ma" target="_blank">Yiyuan Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xun Zhou" target="_blank">Xun Zhou</a>
            </p>
            <p id="summary-OfjIlbelrT@OpenReview" class="summary">Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold.FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference.</p>
            <p id="subjects-OfjIlbelrT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-OfjIlbelrT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OfjIlbelrT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OfjIlbelrT@OpenReview" onclick="foldPdfKimi('OfjIlbelrT@OpenReview', this)" class="hr hr-fold">
        </div><div id="u1cQYxRI1H@OpenReview" class="panel paper" keywords="illumination,editing,wild,harmonization,diffusion,light,albedos,imposing,training,consistent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=u1cQYxRI1H" target="_blank" title="79/207"><span class="index notranslate">#79</span></a>
                <a id="title-u1cQYxRI1H@OpenReview" class="title-link" href="/venue/u1cQYxRI1H@OpenReview" target="_blank">Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport</a>
                <a id="pdf-u1cQYxRI1H@OpenReview" class="title-pdf notranslate" onclick="togglePdf('u1cQYxRI1H@OpenReview', this)" data="https://openreview.net/pdf?id=u1cQYxRI1H">[PDF<sup id="pdf-stars-u1cQYxRI1H@OpenReview">11</sup>]</a>
                <a id="copy-u1cQYxRI1H@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('u1cQYxRI1H@OpenReview')">[Copy]</a>
                <a id="kimi-u1cQYxRI1H@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('u1cQYxRI1H@OpenReview', this)">[Kimi<sup id="kimi-stars-u1cQYxRI1H@OpenReview">19</sup>]</a>
                <a id="rel-u1cQYxRI1H@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('u1cQYxRI1H@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-u1cQYxRI1H@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lvmin Zhang" target="_blank">Lvmin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anyi Rao" target="_blank">Anyi Rao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maneesh Agrawala" target="_blank">Maneesh Agrawala</a>
            </p>
            <p id="summary-u1cQYxRI1H@OpenReview" class="summary">Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (&gt; 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos.</p>
            <p id="subjects-u1cQYxRI1H@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-u1cQYxRI1H@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-u1cQYxRI1H@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-u1cQYxRI1H@OpenReview" onclick="foldPdfKimi('u1cQYxRI1H@OpenReview', this)" class="hr hr-fold">
        </div><div id="NN6QHwgRrQ@OpenReview" class="panel paper" keywords="alignment,human,values,value,palette,map,align,across,offs,multi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NN6QHwgRrQ" target="_blank" title="80/207"><span class="index notranslate">#80</span></a>
                <a id="title-NN6QHwgRrQ@OpenReview" class="title-link" href="/venue/NN6QHwgRrQ@OpenReview" target="_blank">MAP: Multi-Human-Value Alignment Palette</a>
                <a id="pdf-NN6QHwgRrQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NN6QHwgRrQ@OpenReview', this)" data="https://openreview.net/pdf?id=NN6QHwgRrQ">[PDF<sup id="pdf-stars-NN6QHwgRrQ@OpenReview">17</sup>]</a>
                <a id="copy-NN6QHwgRrQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NN6QHwgRrQ@OpenReview')">[Copy]</a>
                <a id="kimi-NN6QHwgRrQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NN6QHwgRrQ@OpenReview', this)">[Kimi<sup id="kimi-stars-NN6QHwgRrQ@OpenReview">18</sup>]</a>
                <a id="rel-NN6QHwgRrQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NN6QHwgRrQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NN6QHwgRrQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinran Wang" target="_blank">Xinran Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=qi le" target="_blank">qi le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ammar Ahmed" target="_blank">Ammar Ahmed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enmao Diao" target="_blank">Enmao Diao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhou" target="_blank">Yi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathalie Baracaldo" target="_blank">Nathalie Baracaldo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Ding" target="_blank">Jie Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Anwar" target="_blank">Ali Anwar</a>
            </p>
            <p id="summary-NN6QHwgRrQ@OpenReview" class="summary">Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.</p>
            <p id="subjects-NN6QHwgRrQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-NN6QHwgRrQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NN6QHwgRrQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NN6QHwgRrQ@OpenReview" onclick="foldPdfKimi('NN6QHwgRrQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="SnDmPkOJ0T@OpenReview" class="panel paper" keywords="reef,suspect,victim,llms,intellectual,owners,model,parties,fingerprints,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SnDmPkOJ0T" target="_blank" title="81/207"><span class="index notranslate">#81</span></a>
                <a id="title-SnDmPkOJ0T@OpenReview" class="title-link" href="/venue/SnDmPkOJ0T@OpenReview" target="_blank">REEF: Representation Encoding Fingerprints for Large Language Models</a>
                <a id="pdf-SnDmPkOJ0T@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SnDmPkOJ0T@OpenReview', this)" data="https://openreview.net/pdf?id=SnDmPkOJ0T">[PDF<sup id="pdf-stars-SnDmPkOJ0T@OpenReview">15</sup>]</a>
                <a id="copy-SnDmPkOJ0T@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SnDmPkOJ0T@OpenReview')">[Copy]</a>
                <a id="kimi-SnDmPkOJ0T@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SnDmPkOJ0T@OpenReview', this)">[Kimi<sup id="kimi-stars-SnDmPkOJ0T@OpenReview">22</sup>]</a>
                <a id="rel-SnDmPkOJ0T@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SnDmPkOJ0T@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SnDmPkOJ0T@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Zhang" target="_blank">Jie Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongrui Liu" target="_blank">Dongrui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Qian" target="_blank">Chen Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Zhang" target="_blank">Linfeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Liu" target="_blank">Yong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Qiao" target="_blank">Yu Qiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Shao" target="_blank">Jing Shao</a>
            </p>
            <p id="summary-SnDmPkOJ0T@OpenReview" class="summary">Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together.</p>
            <p id="subjects-SnDmPkOJ0T@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-SnDmPkOJ0T@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SnDmPkOJ0T@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SnDmPkOJ0T@OpenReview" onclick="foldPdfKimi('SnDmPkOJ0T@OpenReview', this)" class="hr hr-fold">
        </div><div id="Mfnh1Sqdwf@OpenReview" class="panel paper" keywords="gene,regulatory,expression,epigenomic,elements,dna,seq2exp,discover,causal,prediction">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Mfnh1Sqdwf" target="_blank" title="82/207"><span class="index notranslate">#82</span></a>
                <a id="title-Mfnh1Sqdwf@OpenReview" class="title-link" href="/venue/Mfnh1Sqdwf@OpenReview" target="_blank">Learning to Discover Regulatory Elements for Gene Expression Prediction</a>
                <a id="pdf-Mfnh1Sqdwf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Mfnh1Sqdwf@OpenReview', this)" data="https://openreview.net/pdf?id=Mfnh1Sqdwf">[PDF<sup id="pdf-stars-Mfnh1Sqdwf@OpenReview">8</sup>]</a>
                <a id="copy-Mfnh1Sqdwf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Mfnh1Sqdwf@OpenReview')">[Copy]</a>
                <a id="kimi-Mfnh1Sqdwf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Mfnh1Sqdwf@OpenReview', this)">[Kimi<sup id="kimi-stars-Mfnh1Sqdwf@OpenReview">7</sup>]</a>
                <a id="rel-Mfnh1Sqdwf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Mfnh1Sqdwf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Mfnh1Sqdwf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyu Su" target="_blank">Xingyu Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyang Yu" target="_blank">Haiyang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Degui Zhi" target="_blank">Degui Zhi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuiwang Ji" target="_blank">Shuiwang Ji</a>
            </p>
            <p id="summary-Mfnh1Sqdwf@OpenReview" class="summary">We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3.</p>
            <p id="subjects-Mfnh1Sqdwf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Mfnh1Sqdwf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mfnh1Sqdwf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mfnh1Sqdwf@OpenReview" onclick="foldPdfKimi('Mfnh1Sqdwf@OpenReview', this)" class="hr hr-fold">
        </div><div id="KSLkFYHlYg@OpenReview" class="panel paper" keywords="bioisosteric,shepherd,pharmacophores,drug,ligand,bioactive,design,chemical,scoring,electrostatics">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KSLkFYHlYg" target="_blank" title="83/207"><span class="index notranslate">#83</span></a>
                <a id="title-KSLkFYHlYg@OpenReview" class="title-link" href="/venue/KSLkFYHlYg@OpenReview" target="_blank">ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design</a>
                <a id="pdf-KSLkFYHlYg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KSLkFYHlYg@OpenReview', this)" data="https://openreview.net/pdf?id=KSLkFYHlYg">[PDF<sup id="pdf-stars-KSLkFYHlYg@OpenReview">11</sup>]</a>
                <a id="copy-KSLkFYHlYg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KSLkFYHlYg@OpenReview')">[Copy]</a>
                <a id="kimi-KSLkFYHlYg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KSLkFYHlYg@OpenReview', this)">[Kimi<sup id="kimi-stars-KSLkFYHlYg@OpenReview">8</sup>]</a>
                <a id="rel-KSLkFYHlYg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KSLkFYHlYg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KSLkFYHlYg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Keir Adams" target="_blank">Keir Adams</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kento Abeywardane" target="_blank">Kento Abeywardane</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jenna Fromer" target="_blank">Jenna Fromer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Connor Coley" target="_blank">Connor Coley</a>
            </p>
            <p id="summary-KSLkFYHlYg@OpenReview" class="summary">Engineering molecules to exhibit precise 3D intermolecular interactions with their environment forms the basis of chemical design. In ligand-based drug design, bioisosteric analogues of known bioactive hits are often identified by virtually screening chemical libraries with shape, electrostatic, and pharmacophore similarity scoring functions. We instead hypothesize that a generative model which learns the joint distribution over 3D molecular structures and their interaction profiles may facilitate 3D interaction-aware chemical design. We specifically design ShEPhERD, an SE(3)-equivariant diffusion model which jointly diffuses/denoises 3D molecular graphs and representations of their shapes, electrostatic potential surfaces, and (directional) pharmacophores to/from Gaussian noise. Inspired by traditional ligand discovery, we compose 3D similarity scoring functions to assess ShEPhERDs ability to conditionally generate novel molecules with desired interaction profiles. We demonstrate ShEPhERDs potential for impact via exemplary drug design tasks including natural product ligand hopping, protein-blind bioactive hit diversification, and bioisosteric fragment merging.</p>
            <p id="subjects-KSLkFYHlYg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-KSLkFYHlYg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KSLkFYHlYg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KSLkFYHlYg@OpenReview" onclick="foldPdfKimi('KSLkFYHlYg@OpenReview', this)" class="hr hr-fold">
        </div><div id="JWtrk7mprJ@OpenReview" class="panel paper" keywords="manifold,manifolds,gaussian,optimisation,residual,shallow,unneeded,stylised,processes,models">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=JWtrk7mprJ" target="_blank" title="84/207"><span class="index notranslate">#84</span></a>
                <a id="title-JWtrk7mprJ@OpenReview" class="title-link" href="/venue/JWtrk7mprJ@OpenReview" target="_blank">Residual Deep Gaussian Processes on Manifolds</a>
                <a id="pdf-JWtrk7mprJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('JWtrk7mprJ@OpenReview', this)" data="https://openreview.net/pdf?id=JWtrk7mprJ">[PDF<sup id="pdf-stars-JWtrk7mprJ@OpenReview">21</sup>]</a>
                <a id="copy-JWtrk7mprJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('JWtrk7mprJ@OpenReview')">[Copy]</a>
                <a id="kimi-JWtrk7mprJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('JWtrk7mprJ@OpenReview', this)">[Kimi<sup id="kimi-stars-JWtrk7mprJ@OpenReview">20</sup>]</a>
                <a id="rel-JWtrk7mprJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('JWtrk7mprJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-JWtrk7mprJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kacper Wyrwal" target="_blank">Kacper Wyrwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Krause" target="_blank">Andreas Krause</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viacheslav (Slava) Borovitskiy" target="_blank">Viacheslav (Slava) Borovitskiy</a>
            </p>
            <p id="summary-JWtrk7mprJ@OpenReview" class="summary">We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks.With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields.We target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon.For example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes.Our models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded.We further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process.Finally, we show our models to have potential for speeding up inference for non-manifold data, when, and if, it can be mapped to a proxy manifold well enough.</p>
            <p id="subjects-JWtrk7mprJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-JWtrk7mprJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-JWtrk7mprJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-JWtrk7mprJ@OpenReview" onclick="foldPdfKimi('JWtrk7mprJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="Iyrtb9EJBp@OpenReview" class="panel paper" keywords="trust,rag,align,llms,qampari,refuse,asqa,eli5,trustworthiness,llama">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Iyrtb9EJBp" target="_blank" title="85/207"><span class="index notranslate">#85</span></a>
                <a id="title-Iyrtb9EJBp@OpenReview" class="title-link" href="/venue/Iyrtb9EJBp@OpenReview" target="_blank">Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse</a>
                <a id="pdf-Iyrtb9EJBp@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Iyrtb9EJBp@OpenReview', this)" data="https://openreview.net/pdf?id=Iyrtb9EJBp">[PDF<sup id="pdf-stars-Iyrtb9EJBp@OpenReview">22</sup>]</a>
                <a id="copy-Iyrtb9EJBp@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Iyrtb9EJBp@OpenReview')">[Copy]</a>
                <a id="kimi-Iyrtb9EJBp@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Iyrtb9EJBp@OpenReview', this)">[Kimi<sup id="kimi-stars-Iyrtb9EJBp@OpenReview">34</sup>]</a>
                <a id="rel-Iyrtb9EJBp@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Iyrtb9EJBp@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Iyrtb9EJBp@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maojia Song" target="_blank">Maojia Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shang Hong Sim" target="_blank">Shang Hong Sim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Bhardwaj" target="_blank">Rishabh Bhardwaj</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai Leong Chieu" target="_blank">Hai Leong Chieu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Navonil Majumder" target="_blank">Navonil Majumder</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soujanya Poria" target="_blank">Soujanya Poria</a>
            </p>
            <p id="summary-Iyrtb9EJBp@OpenReview" class="summary">LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce TRUST-SCORE, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effec- tively adapt LLMs to the RAG task as measured by TRUST-SCORE. Consequently, we propose TRUST-ALIGN, a method to align LLMs for improved TRUST-SCORE performance. 26 out of 27 models aligned using TRUST-ALIGN substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, TRUST-ALIGN outperforms FRONT on ASQA (12.56), QAMPARI (36.04), and ELI5 (17.69). TRUST-ALIGN also significantly enhances models ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of TRUST-ALIGN across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.</p>
            <p id="subjects-Iyrtb9EJBp@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Iyrtb9EJBp@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Iyrtb9EJBp@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Iyrtb9EJBp@OpenReview" onclick="foldPdfKimi('Iyrtb9EJBp@OpenReview', this)" class="hr hr-fold">
        </div><div id="HD6bWcj87Y@OpenReview" class="panel paper" keywords="shapley,run,data,pretraining,training,contribution,retraining,foundation,model,attributing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HD6bWcj87Y" target="_blank" title="86/207"><span class="index notranslate">#86</span></a>
                <a id="title-HD6bWcj87Y@OpenReview" class="title-link" href="/venue/HD6bWcj87Y@OpenReview" target="_blank">Data Shapley in One Training Run</a>
                <a id="pdf-HD6bWcj87Y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HD6bWcj87Y@OpenReview', this)" data="https://openreview.net/pdf?id=HD6bWcj87Y">[PDF<sup id="pdf-stars-HD6bWcj87Y@OpenReview">14</sup>]</a>
                <a id="copy-HD6bWcj87Y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HD6bWcj87Y@OpenReview')">[Copy]</a>
                <a id="kimi-HD6bWcj87Y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HD6bWcj87Y@OpenReview', this)">[Kimi<sup id="kimi-stars-HD6bWcj87Y@OpenReview">16</sup>]</a>
                <a id="rel-HD6bWcj87Y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HD6bWcj87Y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HD6bWcj87Y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiachen (Tianhao) Wang" target="_blank">Jiachen (Tianhao) Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Mittal" target="_blank">Prateek Mittal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawn Song" target="_blank">Dawn Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Jia" target="_blank">Ruoxi Jia</a>
            </p>
            <p id="summary-HD6bWcj87Y@OpenReview" class="summary">Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.</p>
            <p id="subjects-HD6bWcj87Y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-HD6bWcj87Y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HD6bWcj87Y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HD6bWcj87Y@OpenReview" onclick="foldPdfKimi('HD6bWcj87Y@OpenReview', this)" class="hr hr-fold">
        </div><div id="Fur0DtynPX@OpenReview" class="panel paper" keywords="gridmix,modulation,spatial,modeling,marble,pde,fields,neural,global,domain">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Fur0DtynPX" target="_blank" title="87/207"><span class="index notranslate">#87</span></a>
                <a id="title-Fur0DtynPX@OpenReview" class="title-link" href="/venue/Fur0DtynPX@OpenReview" target="_blank">GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling</a>
                <a id="pdf-Fur0DtynPX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Fur0DtynPX@OpenReview', this)" data="https://openreview.net/pdf?id=Fur0DtynPX">[PDF<sup id="pdf-stars-Fur0DtynPX@OpenReview">12</sup>]</a>
                <a id="copy-Fur0DtynPX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Fur0DtynPX@OpenReview')">[Copy]</a>
                <a id="kimi-Fur0DtynPX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Fur0DtynPX@OpenReview', this)">[Kimi<sup id="kimi-stars-Fur0DtynPX@OpenReview">10</sup>]</a>
                <a id="rel-Fur0DtynPX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Fur0DtynPX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Fur0DtynPX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Honghui Wang" target="_blank">Honghui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiji Song" target="_blank">Shiji Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gao Huang" target="_blank">Gao Huang</a>
            </p>
            <p id="summary-Fur0DtynPX@OpenReview" class="summary">Significant advancements have been achieved in PDE modeling using neural fields. Despite their effectiveness, existing methods rely on global modulation, limiting their ability to reconstruct local details. While spatial modulation with vanilla grid-based representations offers a promising alternative, it struggles with inadequate global information modeling and over-fitting to the training spatial domain. To address these challenges, we propose GridMix, a novel approach that models spatial modulation as a mixture of grid-based representations. GridMix effectively explores global structures while preserving locality for fine-grained modulation. Furthermore, we introduce spatial domain augmentation to enhance the robustness of the modulated neural fields against spatial domain variations. With all these innovations,our comprehensive approach culminates in MARBLE, a framework that significantly advancing the capabilities of neural fields in PDE modeling. The effectiveness of MARBLE is extensivelyvalidated on diverse benchmarks encompassing dynamics modeling and geometric prediction.</p>
            <p id="subjects-Fur0DtynPX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Fur0DtynPX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fur0DtynPX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fur0DtynPX@OpenReview" onclick="foldPdfKimi('Fur0DtynPX@OpenReview', this)" class="hr hr-fold">
        </div><div id="FSjIrOm1vz@OpenReview" class="panel paper" keywords="rag,inference,scaling,computation,context,knowledge,allocated,llms,performance,strategies">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FSjIrOm1vz" target="_blank" title="88/207"><span class="index notranslate">#88</span></a>
                <a id="title-FSjIrOm1vz@OpenReview" class="title-link" href="/venue/FSjIrOm1vz@OpenReview" target="_blank">Inference Scaling for Long-Context Retrieval Augmented Generation</a>
                <a id="pdf-FSjIrOm1vz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FSjIrOm1vz@OpenReview', this)" data="https://openreview.net/pdf?id=FSjIrOm1vz">[PDF<sup id="pdf-stars-FSjIrOm1vz@OpenReview">23</sup>]</a>
                <a id="copy-FSjIrOm1vz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FSjIrOm1vz@OpenReview')">[Copy]</a>
                <a id="kimi-FSjIrOm1vz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FSjIrOm1vz@OpenReview', this)">[Kimi<sup id="kimi-stars-FSjIrOm1vz@OpenReview">44</sup>]</a>
                <a id="rel-FSjIrOm1vz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FSjIrOm1vz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FSjIrOm1vz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenrui Yue" target="_blank">Zhenrui Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Honglei Zhuang" target="_blank">Honglei Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aijun Bai" target="_blank">Aijun Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Hui" target="_blank">Kai Hui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rolf Jagerman" target="_blank">Rolf Jagerman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hansi Zeng" target="_blank">Hansi Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Qin" target="_blank">Zhen Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Wang" target="_blank">Dong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanhui Wang" target="_blank">Xuanhui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Bendersky" target="_blank">Michael Bendersky</a>
            </p>
            <p id="summary-FSjIrOm1vz@OpenReview" class="summary">The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.</p>
            <p id="subjects-FSjIrOm1vz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-FSjIrOm1vz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FSjIrOm1vz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FSjIrOm1vz@OpenReview" onclick="foldPdfKimi('FSjIrOm1vz@OpenReview', this)" class="hr hr-fold">
        </div><div id="FIj9IEPCKr@OpenReview" class="panel paper" keywords="vil,proxy,sfda,prode,adaptation,source,domain,denoising,supervision,capitalize">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FIj9IEPCKr" target="_blank" title="89/207"><span class="index notranslate">#89</span></a>
                <a id="title-FIj9IEPCKr@OpenReview" class="title-link" href="/venue/FIj9IEPCKr@OpenReview" target="_blank">Proxy Denoising for Source-Free Domain Adaptation</a>
                <a id="pdf-FIj9IEPCKr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FIj9IEPCKr@OpenReview', this)" data="https://openreview.net/pdf?id=FIj9IEPCKr">[PDF<sup id="pdf-stars-FIj9IEPCKr@OpenReview">13</sup>]</a>
                <a id="copy-FIj9IEPCKr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FIj9IEPCKr@OpenReview')">[Copy]</a>
                <a id="kimi-FIj9IEPCKr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FIj9IEPCKr@OpenReview', this)">[Kimi<sup id="kimi-stars-FIj9IEPCKr@OpenReview">18</sup>]</a>
                <a id="rel-FIj9IEPCKr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FIj9IEPCKr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FIj9IEPCKr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Song Tang" target="_blank">Song Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxin Su" target="_blank">Wenxin Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Gan" target="_blank">Yan Gan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mao Ye" target="_blank">Mao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianwei Dr. Zhang" target="_blank">Jianwei Dr. Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiatian Zhu" target="_blank">Xiatian Zhu</a>
            </p>
            <p id="summary-FIj9IEPCKr@OpenReview" class="summary">Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, we introduce a novel Proxy Denoising (__ProDe__) approach. The key idea is to leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Concretely, we design a proxy denoising mechanism to correct ViL's predictions. This is grounded on a proxy confidence theory that models the dynamic effect of proxy's divergence against the domain-invariant space during adaptation. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set, generalized SFDA, multi-target, multi-source, and test-time settings. Our code will be released.</p>
            <p id="subjects-FIj9IEPCKr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-FIj9IEPCKr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FIj9IEPCKr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FIj9IEPCKr@OpenReview" onclick="foldPdfKimi('FIj9IEPCKr@OpenReview', this)" class="hr hr-fold">
        </div><div id="FBkpCyujtS@OpenReview" class="panel paper" keywords="sampling,min,creative,llm,outputs,diversity,gpqa,text,token,quality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FBkpCyujtS" target="_blank" title="90/207"><span class="index notranslate">#90</span></a>
                <a id="title-FBkpCyujtS@OpenReview" class="title-link" href="/venue/FBkpCyujtS@OpenReview" target="_blank">Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs</a>
                <a id="pdf-FBkpCyujtS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FBkpCyujtS@OpenReview', this)" data="https://openreview.net/pdf?id=FBkpCyujtS">[PDF<sup id="pdf-stars-FBkpCyujtS@OpenReview">8</sup>]</a>
                <a id="copy-FBkpCyujtS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FBkpCyujtS@OpenReview')">[Copy]</a>
                <a id="kimi-FBkpCyujtS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FBkpCyujtS@OpenReview', this)">[Kimi<sup id="kimi-stars-FBkpCyujtS@OpenReview">23</sup>]</a>
                <a id="rel-FBkpCyujtS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FBkpCyujtS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FBkpCyujtS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minh Nguyen" target="_blank">Minh Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Baker" target="_blank">Andrew Baker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clement Neo" target="_blank">Clement Neo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Allen Roush" target="_blank">Allen Roush</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Kirsch" target="_blank">Andreas Kirsch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ravid Shwartz-Ziv" target="_blank">Ravid Shwartz-Ziv</a>
            </p>
            <p id="summary-FBkpCyujtS@OpenReview" class="summary">Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. However, popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures, leading to incoherent or repetitive outputs. To address this challenge, we propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by scaling according to the top token's probability. We conduct extensive experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing, demonstrating that min-p sampling improves both the quality and diversity of generated text, particularly at high temperatures. Moreover, human evaluations reveal a clear preference for min-p sampling in terms of both text quality and diversity. Min-p sampling has been adopted by multiple open-source LLM implementations, highlighting its practical utility and potential impact.</p>
            <p id="subjects-FBkpCyujtS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-FBkpCyujtS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FBkpCyujtS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FBkpCyujtS@OpenReview" onclick="foldPdfKimi('FBkpCyujtS@OpenReview', this)" class="hr hr-fold">
        </div><div id="EytBpUGB1Z@OpenReview" class="panel paper" keywords="heads,retrieval,context,information,hallucination,mechanistically,retrieving,activated,long,factuality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EytBpUGB1Z" target="_blank" title="91/207"><span class="index notranslate">#91</span></a>
                <a id="title-EytBpUGB1Z@OpenReview" class="title-link" href="/venue/EytBpUGB1Z@OpenReview" target="_blank">Retrieval Head Mechanistically Explains Long-Context Factuality</a>
                <a id="pdf-EytBpUGB1Z@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EytBpUGB1Z@OpenReview', this)" data="https://openreview.net/pdf?id=EytBpUGB1Z">[PDF<sup id="pdf-stars-EytBpUGB1Z@OpenReview">17</sup>]</a>
                <a id="copy-EytBpUGB1Z@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EytBpUGB1Z@OpenReview')">[Copy]</a>
                <a id="kimi-EytBpUGB1Z@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EytBpUGB1Z@OpenReview', this)">[Kimi<sup id="kimi-stars-EytBpUGB1Z@OpenReview">19</sup>]</a>
                <a id="rel-EytBpUGB1Z@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EytBpUGB1Z@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EytBpUGB1Z@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Wu" target="_blank">Wenhao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhong Wang" target="_blank">Yizhong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangxuan Xiao" target="_blank">Guangxuan Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Peng" target="_blank">Hao Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Fu" target="_blank">Yao Fu</a>
            </p>
            <p id="summary-EytBpUGB1Z@OpenReview" class="summary">Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.</p>
            <p id="subjects-EytBpUGB1Z@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-EytBpUGB1Z@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EytBpUGB1Z@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EytBpUGB1Z@OpenReview" onclick="foldPdfKimi('EytBpUGB1Z@OpenReview', this)" class="hr hr-fold">
        </div><div id="EO8xpnW7aX@OpenReview" class="panel paper" keywords="diffusion,permute,symmetricdiffusers,discrete,distribution,reverse,riffle,learning,plackett,luce">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EO8xpnW7aX" target="_blank" title="92/207"><span class="index notranslate">#92</span></a>
                <a id="title-EO8xpnW7aX@OpenReview" class="title-link" href="/venue/EO8xpnW7aX@OpenReview" target="_blank">Learning to Permute with Discrete Diffusion</a>
                <a id="pdf-EO8xpnW7aX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EO8xpnW7aX@OpenReview', this)" data="https://openreview.net/pdf?id=EO8xpnW7aX">[PDF<sup id="pdf-stars-EO8xpnW7aX@OpenReview">28</sup>]</a>
                <a id="copy-EO8xpnW7aX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EO8xpnW7aX@OpenReview')">[Copy]</a>
                <a id="kimi-EO8xpnW7aX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EO8xpnW7aX@OpenReview', this)">[Kimi<sup id="kimi-stars-EO8xpnW7aX@OpenReview">20</sup>]</a>
                <a id="rel-EO8xpnW7aX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EO8xpnW7aX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EO8xpnW7aX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxing Zhang" target="_blank">Yongxing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Donglin Yang" target="_blank">Donglin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renjie Liao" target="_blank">Renjie Liao</a>
            </p>
            <p id="summary-EO8xpnW7aX@OpenReview" class="summary">The group of permutations <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-121" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-122"><span class="msubsup" id="MathJax-Span-123"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-124" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-125" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-27">S_n</script>, also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-126" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-127"><span class="msubsup" id="MathJax-Span-128"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-129" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-130" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-28">S_n</script> poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce *SymmetricDiffusers*, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-131" style="width: 1.409em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-132"><span class="msubsup" id="MathJax-Span-133"><span style="display: inline-block; position: relative; width: 1.148em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-134" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.628em;"><span class="mi" id="MathJax-Span-135" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-29">S_n</script> by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded "denoising schedule" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.</p>
            <p id="subjects-EO8xpnW7aX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-EO8xpnW7aX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EO8xpnW7aX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EO8xpnW7aX@OpenReview" onclick="foldPdfKimi('EO8xpnW7aX@OpenReview', this)" class="hr hr-fold">
        </div><div id="E4Fk3YuG56@OpenReview" class="panel paper" keywords="cce,memory,vocabulary,cross,entropy,logits,consumption,computation,logit,cut">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=E4Fk3YuG56" target="_blank" title="93/207"><span class="index notranslate">#93</span></a>
                <a id="title-E4Fk3YuG56@OpenReview" class="title-link" href="/venue/E4Fk3YuG56@OpenReview" target="_blank">Cut Your Losses in Large-Vocabulary Language Models</a>
                <a id="pdf-E4Fk3YuG56@OpenReview" class="title-pdf notranslate" onclick="togglePdf('E4Fk3YuG56@OpenReview', this)" data="https://openreview.net/pdf?id=E4Fk3YuG56">[PDF<sup id="pdf-stars-E4Fk3YuG56@OpenReview">12</sup>]</a>
                <a id="copy-E4Fk3YuG56@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('E4Fk3YuG56@OpenReview')">[Copy]</a>
                <a id="kimi-E4Fk3YuG56@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('E4Fk3YuG56@OpenReview', this)">[Kimi<sup id="kimi-stars-E4Fk3YuG56@OpenReview">25</sup>]</a>
                <a id="rel-E4Fk3YuG56@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('E4Fk3YuG56@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-E4Fk3YuG56@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Erik Wijmans" target="_blank">Erik Wijmans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brody Huval" target="_blank">Brody Huval</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Hertzberg" target="_blank">Alexander Hertzberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vladlen Koltun" target="_blank">Vladlen Koltun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Krhenbhl" target="_blank">Philipp Krhenbhl</a>
            </p>
            <p id="summary-E4Fk3YuG56@OpenReview" class="summary">As language models grow ever larger, so do their vocabularies.This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation.Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined.We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly.We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB.To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient.Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.</p>
            <p id="subjects-E4Fk3YuG56@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-E4Fk3YuG56@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-E4Fk3YuG56@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-E4Fk3YuG56@OpenReview" onclick="foldPdfKimi('E4Fk3YuG56@OpenReview', this)" class="hr hr-fold">
        </div><div id="CxXGvKRDnL@OpenReview" class="panel paper" keywords="diffusion,compression,elbo,progressive,models,universally,likelihood,quantized,codecs,modeling">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CxXGvKRDnL" target="_blank" title="94/207"><span class="index notranslate">#94</span></a>
                <a id="title-CxXGvKRDnL@OpenReview" class="title-link" href="/venue/CxXGvKRDnL@OpenReview" target="_blank">Progressive Compression with Universally Quantized Diffusion Models</a>
                <a id="pdf-CxXGvKRDnL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CxXGvKRDnL@OpenReview', this)" data="https://openreview.net/pdf?id=CxXGvKRDnL">[PDF<sup id="pdf-stars-CxXGvKRDnL@OpenReview">18</sup>]</a>
                <a id="copy-CxXGvKRDnL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CxXGvKRDnL@OpenReview')">[Copy]</a>
                <a id="kimi-CxXGvKRDnL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CxXGvKRDnL@OpenReview', this)">[Kimi<sup id="kimi-stars-CxXGvKRDnL@OpenReview">22</sup>]</a>
                <a id="rel-CxXGvKRDnL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CxXGvKRDnL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CxXGvKRDnL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yibo Yang" target="_blank">Yibo Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Justus Will" target="_blank">Justus Will</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Mandt" target="_blank">Stephan Mandt</a>
            </p>
            <p id="summary-CxXGvKRDnL@OpenReview" class="summary">Diffusion probabilistic models have achieved mainstream success in many generative modeling tasks, from image generation to inverse problem solving. A distinct feature of these models is that they correspond to deep hierarchical latent variable models optimizing a variational evidence lower bound (ELBO) on the data likelihood.Drawing on a basic connection between likelihood modeling and compression, we explore the potential of diffusion models for progressive coding, resulting in a sequence of bits that can be incrementally transmitted and decoded with progressively improving reconstruction quality.Unlike prior work based on Gaussian diffusion or conditional diffusion models, we propose a new form of diffusion model with uniform noise in the forward process, whose negative ELBO corresponds to the end-to-end compression cost using universal quantization.We obtain promising first results on image compression, achieving competitive rate-distortion-realism results on a wide range of bit-rates with a single model, bringing neural codecs a step closer to practical deployment.</p>
            <p id="subjects-CxXGvKRDnL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-CxXGvKRDnL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CxXGvKRDnL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CxXGvKRDnL@OpenReview" onclick="foldPdfKimi('CxXGvKRDnL@OpenReview', this)" class="hr hr-fold">
        </div><div id="Cjz9Xhm7sI@OpenReview" class="panel paper" keywords="radar,nowcasting,gaussian,stc,gaumamba,weather,dynamic,prediction,forecasting,spatiotemporal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Cjz9Xhm7sI" target="_blank" title="95/207"><span class="index notranslate">#95</span></a>
                <a id="title-Cjz9Xhm7sI@OpenReview" class="title-link" href="/venue/Cjz9Xhm7sI@OpenReview" target="_blank">High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation</a>
                <a id="pdf-Cjz9Xhm7sI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Cjz9Xhm7sI@OpenReview', this)" data="https://openreview.net/pdf?id=Cjz9Xhm7sI">[PDF<sup id="pdf-stars-Cjz9Xhm7sI@OpenReview">10</sup>]</a>
                <a id="copy-Cjz9Xhm7sI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Cjz9Xhm7sI@OpenReview')">[Copy]</a>
                <a id="kimi-Cjz9Xhm7sI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Cjz9Xhm7sI@OpenReview', this)">[Kimi<sup id="kimi-stars-Cjz9Xhm7sI@OpenReview">13</sup>]</a>
                <a id="rel-Cjz9Xhm7sI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Cjz9Xhm7sI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Cjz9Xhm7sI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziye Wang" target="_blank">Ziye Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiran Qin" target="_blank">Yiran Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Zeng" target="_blank">Lin Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruimao Zhang" target="_blank">Ruimao Zhang</a>
            </p>
            <p id="summary-Cjz9Xhm7sI@OpenReview" class="summary">Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored. To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting. Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks. With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals. The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;16&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-136" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-137"><span class="mn" id="MathJax-Span-138" style="font-family: MathJax_Main;">16</span><span class="mo" id="MathJax-Span-139" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-30">16\times</script> higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions.</p>
            <p id="subjects-Cjz9Xhm7sI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Cjz9Xhm7sI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cjz9Xhm7sI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cjz9Xhm7sI@OpenReview" onclick="foldPdfKimi('Cjz9Xhm7sI@OpenReview', this)" class="hr hr-fold">
        </div><div id="Bo62NeU6VF@OpenReview" class="panel paper" keywords="backtracking,safety,unsafe,helpfulness,generation,language,happily,harmlessness,backtrack,models">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Bo62NeU6VF" target="_blank" title="96/207"><span class="index notranslate">#96</span></a>
                <a id="title-Bo62NeU6VF@OpenReview" class="title-link" href="/venue/Bo62NeU6VF@OpenReview" target="_blank">Backtracking Improves Generation Safety</a>
                <a id="pdf-Bo62NeU6VF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Bo62NeU6VF@OpenReview', this)" data="https://openreview.net/pdf?id=Bo62NeU6VF">[PDF<sup id="pdf-stars-Bo62NeU6VF@OpenReview">11</sup>]</a>
                <a id="copy-Bo62NeU6VF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Bo62NeU6VF@OpenReview')">[Copy]</a>
                <a id="kimi-Bo62NeU6VF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Bo62NeU6VF@OpenReview', this)">[Kimi<sup id="kimi-stars-Bo62NeU6VF@OpenReview">17</sup>]</a>
                <a id="rel-Bo62NeU6VF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Bo62NeU6VF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Bo62NeU6VF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Zhang" target="_blank">Yiming Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfeng Chi" target="_blank">Jianfeng Chi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hailey Nguyen" target="_blank">Hailey Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kartikeya Upasani" target="_blank">Kartikeya Upasani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Bikel" target="_blank">Daniel Bikel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason E Weston" target="_blank">Jason E Weston</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Michael Smith" target="_blank">Eric Michael Smith</a>
            </p>
            <p id="summary-Bo62NeU6VF@OpenReview" class="summary">Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic.In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text.This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety.Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to "undo" and recover from their own unsafe generation through the introduction of a special [RESET] token.Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness.We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\% <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-140" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-141"><span class="mo" id="MathJax-Span-142" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.753em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false"></mo></math></span></span><script type="math/tex" id="MathJax-Element-31">\to</script> 1.5\%) in our evaluations without regression in helpfulness.Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.</p>
            <p id="subjects-Bo62NeU6VF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Bo62NeU6VF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bo62NeU6VF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bo62NeU6VF@OpenReview" onclick="foldPdfKimi('Bo62NeU6VF@OpenReview', this)" class="hr hr-fold">
        </div><div id="AP0ndQloqR@OpenReview" class="panel paper" keywords="action,dimensionality,spaces,attainable,manifold,continuous,state,reinforcement,theoretical,induce">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AP0ndQloqR" target="_blank" title="97/207"><span class="index notranslate">#97</span></a>
                <a id="title-AP0ndQloqR@OpenReview" class="title-link" href="/venue/AP0ndQloqR@OpenReview" target="_blank">Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces</a>
                <a id="pdf-AP0ndQloqR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AP0ndQloqR@OpenReview', this)" data="https://openreview.net/pdf?id=AP0ndQloqR">[PDF<sup id="pdf-stars-AP0ndQloqR@OpenReview">12</sup>]</a>
                <a id="copy-AP0ndQloqR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AP0ndQloqR@OpenReview')">[Copy]</a>
                <a id="kimi-AP0ndQloqR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AP0ndQloqR@OpenReview', this)">[Kimi<sup id="kimi-stars-AP0ndQloqR@OpenReview">21</sup>]</a>
                <a id="rel-AP0ndQloqR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AP0ndQloqR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AP0ndQloqR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Saket Tiwari" target="_blank">Saket Tiwari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Omer Gottesman" target="_blank">Omer Gottesman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=George D Konidaris" target="_blank">George D Konidaris</a>
            </p>
            <p id="summary-AP0ndQloqR@OpenReview" class="summary">Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induce a set of attainable states in RL. We show that training dynamics of a two layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations.</p>
            <p id="subjects-AP0ndQloqR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-AP0ndQloqR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AP0ndQloqR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AP0ndQloqR@OpenReview" onclick="foldPdfKimi('AP0ndQloqR@OpenReview', this)" class="hr hr-fold">
        </div><div id="9pW2J49flQ@OpenReview" class="panel paper" keywords="ltl,deepltl,specifications,satisfy,efficiently,policies,instructions,horizon,learning,bchi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9pW2J49flQ" target="_blank" title="98/207"><span class="index notranslate">#98</span></a>
                <a id="title-9pW2J49flQ@OpenReview" class="title-link" href="/venue/9pW2J49flQ@OpenReview" target="_blank">DeepLTL: Learning to Efficiently Satisfy Complex LTL Instructions</a>
                <a id="pdf-9pW2J49flQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9pW2J49flQ@OpenReview', this)" data="https://openreview.net/pdf?id=9pW2J49flQ">[PDF<sup id="pdf-stars-9pW2J49flQ@OpenReview">7</sup>]</a>
                <a id="copy-9pW2J49flQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9pW2J49flQ@OpenReview')">[Copy]</a>
                <a id="kimi-9pW2J49flQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9pW2J49flQ@OpenReview', this)">[Kimi<sup id="kimi-stars-9pW2J49flQ@OpenReview">12</sup>]</a>
                <a id="rel-9pW2J49flQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9pW2J49flQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9pW2J49flQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mathias Jackermeier" target="_blank">Mathias Jackermeier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessandro Abate" target="_blank">Alessandro Abate</a>
            </p>
            <p id="summary-9pW2J49flQ@OpenReview" class="summary">Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to the finite-horizon fragment of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of Bchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency.</p>
            <p id="subjects-9pW2J49flQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-9pW2J49flQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9pW2J49flQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9pW2J49flQ@OpenReview" onclick="foldPdfKimi('9pW2J49flQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="9VGTk2NYjF@OpenReview" class="panel paper" keywords="polymatrix,games,team,teams,multiplayer,adversaries,nash,setting,equilibria,players">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9VGTk2NYjF" target="_blank" title="99/207"><span class="index notranslate">#99</span></a>
                <a id="title-9VGTk2NYjF@OpenReview" class="title-link" href="/venue/9VGTk2NYjF@OpenReview" target="_blank">The Complexity of Two-Team Polymatrix Games with Independent Adversaries</a>
                <a id="pdf-9VGTk2NYjF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9VGTk2NYjF@OpenReview', this)" data="https://openreview.net/pdf?id=9VGTk2NYjF">[PDF<sup id="pdf-stars-9VGTk2NYjF@OpenReview">3</sup>]</a>
                <a id="copy-9VGTk2NYjF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9VGTk2NYjF@OpenReview')">[Copy]</a>
                <a id="kimi-9VGTk2NYjF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9VGTk2NYjF@OpenReview', this)">[Kimi<sup id="kimi-stars-9VGTk2NYjF@OpenReview">10</sup>]</a>
                <a id="rel-9VGTk2NYjF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9VGTk2NYjF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9VGTk2NYjF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandros Hollender" target="_blank">Alexandros Hollender</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gilbert Maystre" target="_blank">Gilbert Maystre</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai Ganesh Nagarajan" target="_blank">Sai Ganesh Nagarajan</a>
            </p>
            <p id="summary-9VGTk2NYjF@OpenReview" class="summary">Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the computation of Nash equilibria in such games where each pair of players plays either a zero-sum or a coordination game. We are particularly interested in the setting where players can be grouped into a small number of teams of identical interest. While the three-team version of the problem is known to be PPAD-complete, the complexity for two teams has remained open. Our main contribution is to prove that the two-team version remains hard, namely it is CLS-hard. Furthermore, we show that this lower bound is tight for the setting where one of the teams consists of multiple independent adversaries. On the way to obtaining our main result, we prove hardness of finding any stationary point in the simplest type of non-convex-concave min-max constrained optimization problem, namely for a class of bilinear polynomial objective functions.</p>
            <p id="subjects-9VGTk2NYjF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-9VGTk2NYjF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9VGTk2NYjF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9VGTk2NYjF@OpenReview" onclick="foldPdfKimi('9VGTk2NYjF@OpenReview', this)" class="hr hr-fold">
        </div><div id="8enWnd6Gp3@OpenReview" class="panel paper" keywords="tetsphere,splatting,volumetric,meshes,tetrahedral,quality,manifoldness,lagrangian,mesh,underused">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8enWnd6Gp3" target="_blank" title="100/207"><span class="index notranslate">#100</span></a>
                <a id="title-8enWnd6Gp3@OpenReview" class="title-link" href="/venue/8enWnd6Gp3@OpenReview" target="_blank">TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes</a>
                <a id="pdf-8enWnd6Gp3@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8enWnd6Gp3@OpenReview', this)" data="https://openreview.net/pdf?id=8enWnd6Gp3">[PDF<sup id="pdf-stars-8enWnd6Gp3@OpenReview">10</sup>]</a>
                <a id="copy-8enWnd6Gp3@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8enWnd6Gp3@OpenReview')">[Copy]</a>
                <a id="kimi-8enWnd6Gp3@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8enWnd6Gp3@OpenReview', this)">[Kimi<sup id="kimi-stars-8enWnd6Gp3@OpenReview">7</sup>]</a>
                <a id="rel-8enWnd6Gp3@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8enWnd6Gp3@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8enWnd6Gp3@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minghao Guo" target="_blank">Minghao Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bohan Wang" target="_blank">Bohan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiming He" target="_blank">Kaiming He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wojciech Matusik" target="_blank">Wojciech Matusik</a>
            </p>
            <p id="summary-8enWnd6Gp3@OpenReview" class="summary">We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres, with geometric regularizations and constraints that effectively resolve common mesh issues such as irregular triangles, non-manifoldness, and floating artifacts. Experimental results on multi-view and single-view reconstruction highlight TetSphere splatting's superior mesh quality while maintaining competitive reconstruction accuracy compared to state-of-the-art methods. Additionally, TetSphere splatting demonstrates versatility by seamlessly integrating into generative modeling tasks, such as image-to-3D and text-to-3D generation.</p>
            <p id="subjects-8enWnd6Gp3@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-8enWnd6Gp3@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8enWnd6Gp3@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8enWnd6Gp3@OpenReview" onclick="foldPdfKimi('8enWnd6Gp3@OpenReview', this)" class="hr hr-fold">
        </div><div id="84pDoCD4lH@OpenReview" class="panel paper" keywords="vlms,ambiguities,language,spatial,conventions,vision,reference,comfort,reasoning,ambiguous">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=84pDoCD4lH" target="_blank" title="101/207"><span class="index notranslate">#101</span></a>
                <a id="title-84pDoCD4lH@OpenReview" class="title-link" href="/venue/84pDoCD4lH@OpenReview" target="_blank">Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities</a>
                <a id="pdf-84pDoCD4lH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('84pDoCD4lH@OpenReview', this)" data="https://openreview.net/pdf?id=84pDoCD4lH">[PDF<sup id="pdf-stars-84pDoCD4lH@OpenReview">13</sup>]</a>
                <a id="copy-84pDoCD4lH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('84pDoCD4lH@OpenReview')">[Copy]</a>
                <a id="kimi-84pDoCD4lH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('84pDoCD4lH@OpenReview', this)">[Kimi<sup id="kimi-stars-84pDoCD4lH@OpenReview">22</sup>]</a>
                <a id="rel-84pDoCD4lH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('84pDoCD4lH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-84pDoCD4lH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheyuan Zhang" target="_blank">Zheyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengyuan Hu" target="_blank">Fengyuan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jayjun Lee" target="_blank">Jayjun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Freda Shi" target="_blank">Freda Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Parisa Kordjamshidi" target="_blank">Parisa Kordjamshidi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joyce Chai" target="_blank">Joyce Chai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqiao Ma" target="_blank">Ziqiao Ma</a>
            </p>
            <p id="summary-84pDoCD4lH@OpenReview" class="summary">Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning.</p>
            <p id="subjects-84pDoCD4lH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-84pDoCD4lH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-84pDoCD4lH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-84pDoCD4lH@OpenReview" onclick="foldPdfKimi('84pDoCD4lH@OpenReview', this)" class="hr hr-fold">
        </div><div id="6EUtjXAvmj@OpenReview" class="panel paper" keywords="guidance,diffusion,posterior,intractable,sampling,midpoint,priors,term,inverse,score">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6EUtjXAvmj" target="_blank" title="102/207"><span class="index notranslate">#102</span></a>
                <a id="title-6EUtjXAvmj@OpenReview" class="title-link" href="/venue/6EUtjXAvmj@OpenReview" target="_blank">Variational Diffusion Posterior Sampling with Midpoint Guidance</a>
                <a id="pdf-6EUtjXAvmj@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6EUtjXAvmj@OpenReview', this)" data="https://openreview.net/pdf?id=6EUtjXAvmj">[PDF<sup id="pdf-stars-6EUtjXAvmj@OpenReview">16</sup>]</a>
                <a id="copy-6EUtjXAvmj@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6EUtjXAvmj@OpenReview')">[Copy]</a>
                <a id="kimi-6EUtjXAvmj@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6EUtjXAvmj@OpenReview', this)">[Kimi<sup id="kimi-stars-6EUtjXAvmj@OpenReview">16</sup>]</a>
                <a id="rel-6EUtjXAvmj@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6EUtjXAvmj@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6EUtjXAvmj@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Badr MOUFAD" target="_blank">Badr MOUFAD</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yazid Janati el idrissi" target="_blank">Yazid Janati el idrissi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lisa Bedin" target="_blank">Lisa Bedin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alain Oliviero Durmus" target="_blank">Alain Oliviero Durmus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=randal douc" target="_blank">randal douc</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Moulines" target="_blank">Eric Moulines</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jimmy Olsson" target="_blank">Jimmy Olsson</a>
            </p>
            <p id="summary-6EUtjXAvmj@OpenReview" class="summary">Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches formulate the problem as that of sampling from a surrogate diffusion model targeting the posterior and decompose its scores into two terms: the prior score and an intractable guidance term. While the former is replaced by the pre-trained score of the considered diffusion model, the guidance term has to be estimated. In this paper, we propose a novel approach that utilises a decomposition of the transitions which, in contrast to previous methods, allows a trade-off between the complexity of the intractable guidance term and that of the prior transitions. We validate the proposed approach through extensive experiments on linear and nonlinear inverse problems, including challenging cases with latent diffusion models as priors, and demonstrate its effectiveness in reconstructing electrocardiogram (ECG) from partial measurements for accurate cardiac diagnosis.</p>
            <p id="subjects-6EUtjXAvmj@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-6EUtjXAvmj@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6EUtjXAvmj@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6EUtjXAvmj@OpenReview" onclick="foldPdfKimi('6EUtjXAvmj@OpenReview', this)" class="hr hr-fold">
        </div><div id="5UKrnKuspb@OpenReview" class="panel paper" keywords="neuralplane,plane,primitives,reconstruction,planar,neural,observations,semantics,coplanarity,scannetv2">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5UKrnKuspb" target="_blank" title="103/207"><span class="index notranslate">#103</span></a>
                <a id="title-5UKrnKuspb@OpenReview" class="title-link" href="/venue/5UKrnKuspb@OpenReview" target="_blank">NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields</a>
                <a id="pdf-5UKrnKuspb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5UKrnKuspb@OpenReview', this)" data="https://openreview.net/pdf?id=5UKrnKuspb">[PDF<sup id="pdf-stars-5UKrnKuspb@OpenReview">10</sup>]</a>
                <a id="copy-5UKrnKuspb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5UKrnKuspb@OpenReview')">[Copy]</a>
                <a id="kimi-5UKrnKuspb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5UKrnKuspb@OpenReview', this)">[Kimi<sup id="kimi-stars-5UKrnKuspb@OpenReview">8</sup>]</a>
                <a id="rel-5UKrnKuspb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5UKrnKuspb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5UKrnKuspb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanqiao Ye" target="_blank">Hanqiao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhou Liu" target="_blank">Yuzhou Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yangdong Liu" target="_blank">Yangdong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhan Shen" target="_blank">Shuhan Shen</a>
            </p>
            <p id="summary-5UKrnKuspb@OpenReview" class="summary">3D maps assembled from planar primitives are compact and expressive in representing man-made environments, making them suitable for a spectrum of applications. In this paper, we present **NeuralPlane**, a novel approach that explores **neural** fields for multi-view 3D **plane** reconstruction. Our method is centered upon the core idea of distilling geometric and semantic cues from inconsistent 2D plane observations into a unified 3D neural representation, which unlocks the full leverage of plane attributes. This idea is accomplished by NeuralPlane via several key designs, including: 1) a monocular module that generates geometrically smooth and semantically meaningful segments as 2D plane observations, 2) a plane-guided training procedure that implicitly learns accurate plane locations from multi-view plane observations, and 3) a self-supervised feature field termed *Neural Coplanarity Field* that enables the modeling of scene semantics alongside the geometry. Without relying on plane annotations, our method achieves high-fidelity reconstruction comprising planar primitives that are not only crisp but also well-aligned with the semantic content. Comprehensive experiments on ScanNetv2 and ScanNet++ demonstrate the superiority of our results in both geometry and semantics.</p>
            <p id="subjects-5UKrnKuspb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-5UKrnKuspb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5UKrnKuspb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5UKrnKuspb@OpenReview" onclick="foldPdfKimi('5UKrnKuspb@OpenReview', this)" class="hr hr-fold">
        </div><div id="5U1rlpX68A@OpenReview" class="panel paper" keywords="lora,gating,scalable,incremental,low,rank,adaptation,foundation,learning,trained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5U1rlpX68A" target="_blank" title="104/207"><span class="index notranslate">#104</span></a>
                <a id="title-5U1rlpX68A@OpenReview" class="title-link" href="/venue/5U1rlpX68A@OpenReview" target="_blank">S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning</a>
                <a id="pdf-5U1rlpX68A@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5U1rlpX68A@OpenReview', this)" data="https://openreview.net/pdf?id=5U1rlpX68A">[PDF<sup id="pdf-stars-5U1rlpX68A@OpenReview">32</sup>]</a>
                <a id="copy-5U1rlpX68A@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5U1rlpX68A@OpenReview')">[Copy]</a>
                <a id="kimi-5U1rlpX68A@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5U1rlpX68A@OpenReview', this)">[Kimi<sup id="kimi-stars-5U1rlpX68A@OpenReview">29</sup>]</a>
                <a id="rel-5U1rlpX68A@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5U1rlpX68A@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5U1rlpX68A@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichen Wu" target="_blank">Yichen Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongming Piao" target="_blank">Hongming Piao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Long-Kai Huang" target="_blank">Long-Kai Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renzhen Wang" target="_blank">Renzhen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanhua Li" target="_blank">Wanhua Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanspeter Pfister" target="_blank">Hanspeter Pfister</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deyu Meng" target="_blank">Deyu Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kede Ma" target="_blank">Kede Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Wei" target="_blank">Ying Wei</a>
            </p>
            <p id="summary-5U1rlpX68A@OpenReview" class="summary">Continual Learning (CL) with foundation models has recently emerged as a promising approach to harnessing the power of pre-trained models for sequential tasks. Existing prompt-based methods generally use a gating mechanism to select relevant prompts aligned with the test query for further processing. However, the success of these methods largely depends on the precision of the gating mechanism, which becomes less scalable with additional computational overhead as tasks increases. To overcome these issues, we propose a Scalable Low-Rank Adaptation (S-LoRA) method for CL (in particular class incremental learning), which incrementally decouples the learning of the direction and magnitude of LoRA parameters. S-LoRA supports efficient inference by employing the last-stage trained model for direct testing without a gating process. Our theoretical and empirical analysis demonstrates that S-LoRA tends to follow a low-loss trajectory that converges to an overlapped low-loss region, resulting in an excellent stability-plasticity trade-off in CL. Furthermore, based on our findings, we develop variants of S-LoRA with further improved scalability. Extensive experiments across multiple CL benchmarks and various foundation models consistently validate the effectiveness of S-LoRA.</p>
            <p id="subjects-5U1rlpX68A@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-5U1rlpX68A@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5U1rlpX68A@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5U1rlpX68A@OpenReview" onclick="foldPdfKimi('5U1rlpX68A@OpenReview', this)" class="hr hr-fold">
        </div><div id="51WraMid8K@OpenReview" class="panel paper" keywords="unlearning,evaluations,probabilistic,llms,deterministic,alignment,estimates,output,language,comprehensive">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=51WraMid8K" target="_blank" title="105/207"><span class="index notranslate">#105</span></a>
                <a id="title-51WraMid8K@OpenReview" class="title-link" href="/venue/51WraMid8K@OpenReview" target="_blank">A Probabilistic Perspective on Unlearning and Alignment for Large Language Models</a>
                <a id="pdf-51WraMid8K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('51WraMid8K@OpenReview', this)" data="https://openreview.net/pdf?id=51WraMid8K">[PDF<sup id="pdf-stars-51WraMid8K@OpenReview">9</sup>]</a>
                <a id="copy-51WraMid8K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('51WraMid8K@OpenReview')">[Copy]</a>
                <a id="kimi-51WraMid8K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('51WraMid8K@OpenReview', this)">[Kimi<sup id="kimi-stars-51WraMid8K@OpenReview">22</sup>]</a>
                <a id="rel-51WraMid8K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('51WraMid8K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-51WraMid8K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Scholten" target="_blank">Yan Scholten</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Gnnemann" target="_blank">Stephan Gnnemann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leo Schwinn" target="_blank">Leo Schwinn</a>
            </p>
            <p id="summary-51WraMid8K@OpenReview" class="summary">Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework in LLMs. Namely, we derive novel metrics with high-probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Through a case study focused on unlearning, we reveal that deterministic evaluations falsely indicate successful unlearning, whereas our probabilistic evaluations demonstrate that most if not all of the supposedly unlearned information remains accessible in these models. Additionally, we propose a novel unlearning loss based on entropy optimization and adaptive temperature scaling, which significantly improves unlearning in probabilistic settings on recent benchmarks. Our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs.</p>
            <p id="subjects-51WraMid8K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-51WraMid8K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-51WraMid8K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-51WraMid8K@OpenReview" onclick="foldPdfKimi('51WraMid8K@OpenReview', this)" class="hr hr-fold">
        </div><div id="4OaO3GjP7k@OpenReview" class="panel paper" keywords="reward,landscapes,flatter,robustness,flat,policy,actions,parameter,reinforcement,space">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4OaO3GjP7k" target="_blank" title="106/207"><span class="index notranslate">#106</span></a>
                <a id="title-4OaO3GjP7k@OpenReview" class="title-link" href="/venue/4OaO3GjP7k@OpenReview" target="_blank">Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning</a>
                <a id="pdf-4OaO3GjP7k@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4OaO3GjP7k@OpenReview', this)" data="https://openreview.net/pdf?id=4OaO3GjP7k">[PDF<sup id="pdf-stars-4OaO3GjP7k@OpenReview">16</sup>]</a>
                <a id="copy-4OaO3GjP7k@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4OaO3GjP7k@OpenReview')">[Copy]</a>
                <a id="kimi-4OaO3GjP7k@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4OaO3GjP7k@OpenReview', this)">[Kimi<sup id="kimi-stars-4OaO3GjP7k@OpenReview">24</sup>]</a>
                <a id="rel-4OaO3GjP7k@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4OaO3GjP7k@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4OaO3GjP7k@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=HyunKyu Lee" target="_blank">HyunKyu Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sung Whan Yoon" target="_blank">Sung Whan Yoon</a>
            </p>
            <p id="summary-4OaO3GjP7k@OpenReview" class="summary">Investigating flat minima on loss surfaces in parameter space is well-documented in the supervised learning context, highlighting its advantages for model generalization. However, limited attention has been paid to the reinforcement learning (RL) context, where the impact of flatter reward landscapes in policy parameter space remains largely unexplored. Beyond merely extrapolating from supervised learning, which suggests a link between flat reward landscapes and enhanced generalization, we aim to formally connect the flatness of the reward surface to the robustness of RL models. In policy models where a deep neural network determines actions, flatter reward landscapes in response to parameter perturbations lead to consistent rewards even when actions are perturbed. Moreover, robustness to actions further contributes to robustness against other variations, such as changes in state transition probabilities and reward functions. We extensively simulate various RL environments, confirming the consistent benefits of flatter reward landscapes in enhancing the robustness of RL under diverse conditions, including action selection, transition dynamics, and reward functions.</p>
            <p id="subjects-4OaO3GjP7k@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-4OaO3GjP7k@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4OaO3GjP7k@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4OaO3GjP7k@OpenReview" onclick="foldPdfKimi('4OaO3GjP7k@OpenReview', this)" class="hr hr-fold">
        </div><div id="4FWAwZtd2n@OpenReview" class="panel paper" keywords="test,compute,scaling,time,prompt,llm,inference,llms,improve,computation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4FWAwZtd2n" target="_blank" title="107/207"><span class="index notranslate">#107</span></a>
                <a id="title-4FWAwZtd2n@OpenReview" class="title-link" href="/venue/4FWAwZtd2n@OpenReview" target="_blank">Scaling Test-Time Compute Optimally Can be More Effective than Scaling LLM Parameters</a>
                <a id="pdf-4FWAwZtd2n@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4FWAwZtd2n@OpenReview', this)" data="https://openreview.net/pdf?id=4FWAwZtd2n">[PDF<sup id="pdf-stars-4FWAwZtd2n@OpenReview">10</sup>]</a>
                <a id="copy-4FWAwZtd2n@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4FWAwZtd2n@OpenReview')">[Copy]</a>
                <a id="kimi-4FWAwZtd2n@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4FWAwZtd2n@OpenReview', this)">[Kimi<sup id="kimi-stars-4FWAwZtd2n@OpenReview">28</sup>]</a>
                <a id="rel-4FWAwZtd2n@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4FWAwZtd2n@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4FWAwZtd2n@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Charlie Snell" target="_blank">Charlie Snell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaehoon Lee" target="_blank">Jaehoon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kelvin Xu" target="_blank">Kelvin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aviral Kumar" target="_blank">Aviral Kumar</a>
            </p>
            <p id="summary-4FWAwZtd2n@OpenReview" class="summary">Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a ``compute-optimal'' scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.</p>
            <p id="subjects-4FWAwZtd2n@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-4FWAwZtd2n@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4FWAwZtd2n@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4FWAwZtd2n@OpenReview" onclick="foldPdfKimi('4FWAwZtd2n@OpenReview', this)" class="hr hr-fold">
        </div><div id="3i13Gev2hV@OpenReview" class="panel paper" keywords="hyperbolic,entailment,vision,textual,language,compositional,image,hierarchical,boxes,text">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3i13Gev2hV" target="_blank" title="108/207"><span class="index notranslate">#108</span></a>
                <a id="title-3i13Gev2hV@OpenReview" class="title-link" href="/venue/3i13Gev2hV@OpenReview" target="_blank">Compositional Entailment Learning for Hyperbolic Vision-Language Models</a>
                <a id="pdf-3i13Gev2hV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3i13Gev2hV@OpenReview', this)" data="https://openreview.net/pdf?id=3i13Gev2hV">[PDF<sup id="pdf-stars-3i13Gev2hV@OpenReview">21</sup>]</a>
                <a id="copy-3i13Gev2hV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3i13Gev2hV@OpenReview')">[Copy]</a>
                <a id="kimi-3i13Gev2hV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3i13Gev2hV@OpenReview', this)">[Kimi<sup id="kimi-stars-3i13Gev2hV@OpenReview">17</sup>]</a>
                <a id="rel-3i13Gev2hV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3i13Gev2hV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3i13Gev2hV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Avik Pal" target="_blank">Avik Pal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max van Spengler" target="_blank">Max van Spengler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guido D&amp;#x27;Amely di Melendugno" target="_blank">Guido D&amp;#x27;Amely di Melendugno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessandro Flaborea" target="_blank">Alessandro Flaborea</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabio Galasso" target="_blank">Fabio Galasso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Mettes" target="_blank">Pascal Mettes</a>
            </p>
            <p id="summary-3i13Gev2hV@OpenReview" class="summary">Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance.</p>
            <p id="subjects-3i13Gev2hV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-3i13Gev2hV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3i13Gev2hV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3i13Gev2hV@OpenReview" onclick="foldPdfKimi('3i13Gev2hV@OpenReview', this)" class="hr hr-fold">
        </div><div id="3IFRygQKGL@OpenReview" class="panel paper" keywords="optionzero,options,muzero,planning,learned,learns,superhuman,games,131,human">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3IFRygQKGL" target="_blank" title="109/207"><span class="index notranslate">#109</span></a>
                <a id="title-3IFRygQKGL@OpenReview" class="title-link" href="/venue/3IFRygQKGL@OpenReview" target="_blank">OptionZero: Planning with Learned Options</a>
                <a id="pdf-3IFRygQKGL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3IFRygQKGL@OpenReview', this)" data="https://openreview.net/pdf?id=3IFRygQKGL">[PDF<sup id="pdf-stars-3IFRygQKGL@OpenReview">13</sup>]</a>
                <a id="copy-3IFRygQKGL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3IFRygQKGL@OpenReview')">[Copy]</a>
                <a id="kimi-3IFRygQKGL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3IFRygQKGL@OpenReview', this)">[Kimi<sup id="kimi-stars-3IFRygQKGL@OpenReview">9</sup>]</a>
                <a id="rel-3IFRygQKGL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3IFRygQKGL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3IFRygQKGL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Po-Wei Huang" target="_blank">Po-Wei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pei-Chiun Peng" target="_blank">Pei-Chiun Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hung Guei" target="_blank">Hung Guei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ti-Rong Wu" target="_blank">Ti-Rong Wu</a>
            </p>
            <p id="summary-3IFRygQKGL@OpenReview" class="summary">Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data. Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named OptionZero. OptionZero incorporates an option network into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dynamics network in MuZero to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning.</p>
            <p id="subjects-3IFRygQKGL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-3IFRygQKGL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3IFRygQKGL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3IFRygQKGL@OpenReview" onclick="foldPdfKimi('3IFRygQKGL@OpenReview', this)" class="hr hr-fold">
        </div><div id="2efNHgYRvM@OpenReview" class="panel paper" keywords="causal,latent,instantaneous,textbf,identifiability,entification,instantane,sparse,temporally,atent">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2efNHgYRvM" target="_blank" title="110/207"><span class="index notranslate">#110</span></a>
                <a id="title-2efNHgYRvM@OpenReview" class="title-link" href="/venue/2efNHgYRvM@OpenReview" target="_blank">On the Identification of Temporal Causal Representation with Instantaneous Dependence</a>
                <a id="pdf-2efNHgYRvM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2efNHgYRvM@OpenReview', this)" data="https://openreview.net/pdf?id=2efNHgYRvM">[PDF<sup id="pdf-stars-2efNHgYRvM@OpenReview">7</sup>]</a>
                <a id="copy-2efNHgYRvM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2efNHgYRvM@OpenReview')">[Copy]</a>
                <a id="kimi-2efNHgYRvM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2efNHgYRvM@OpenReview', this)">[Kimi<sup id="kimi-stars-2efNHgYRvM@OpenReview">13</sup>]</a>
                <a id="rel-2efNHgYRvM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2efNHgYRvM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2efNHgYRvM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zijian Li" target="_blank">Zijian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Shen" target="_blank">Yifan Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaitao Zheng" target="_blank">Kaitao Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruichu Cai" target="_blank">Ruichu Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangchen Song" target="_blank">Xiangchen Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingming Gong" target="_blank">Mingming Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangyi Chen" target="_blank">Guangyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhang" target="_blank">Kun Zhang</a>
            </p>
            <p id="summary-2efNHgYRvM@OpenReview" class="summary">Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.</p>
            <p id="subjects-2efNHgYRvM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-2efNHgYRvM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2efNHgYRvM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2efNHgYRvM@OpenReview" onclick="foldPdfKimi('2efNHgYRvM@OpenReview', this)" class="hr hr-fold">
        </div><div id="25kAzqzTrz@OpenReview" class="panel paper" keywords="fixmatch,ssl,dnns,theoretical,supervised,sohn,freematch,softmatch,semantic,flexmatch">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=25kAzqzTrz" target="_blank" title="111/207"><span class="index notranslate">#111</span></a>
                <a id="title-25kAzqzTrz@OpenReview" class="title-link" href="/venue/25kAzqzTrz@OpenReview" target="_blank">Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning</a>
                <a id="pdf-25kAzqzTrz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('25kAzqzTrz@OpenReview', this)" data="https://openreview.net/pdf?id=25kAzqzTrz">[PDF<sup id="pdf-stars-25kAzqzTrz@OpenReview">23</sup>]</a>
                <a id="copy-25kAzqzTrz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('25kAzqzTrz@OpenReview')">[Copy]</a>
                <a id="kimi-25kAzqzTrz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('25kAzqzTrz@OpenReview', this)">[Kimi<sup id="kimi-stars-25kAzqzTrz@OpenReview">17</sup>]</a>
                <a id="rel-25kAzqzTrz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('25kAzqzTrz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-25kAzqzTrz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyang Li" target="_blank">Jingyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiachun Pan" target="_blank">Jiachun Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Tan" target="_blank">Vincent Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kim-chuan Toh" target="_blank">Kim-chuan Toh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pan Zhou" target="_blank">Pan Zhou</a>
            </p>
            <p id="summary-25kAzqzTrz@OpenReview" class="summary">Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL algorithms generalize better than SL on DNNs. In this work, we present the first theoretical justification for the enhanced test accuracy observed in FixMatch-like SSL applied to DNNs by taking convolutional neural networks (CNNs) on classification tasks as an example. Our theoretical analysis reveals that the semantic feature learning processes in FixMatch and SL are rather different. In particular, FixMatch learns all the discriminative features of each semantic class, while SL only randomly captures a subset of features due to the well-known lottery ticket hypothesis. Furthermore, we show that our analysis framework can be applied to other FixMatch-like SSL methods, e.g., FlexMatch, FreeMatch, Dash, and SoftMatch. Inspired by our theoretical analysis, we develop an improved variant of FixMatch, termed Semantic-Aware FixMatch (SA-FixMatch). Experimental results corroborate our theoretical findings and the enhanced generalization capability of SA-FixMatch.</p>
            <p id="subjects-25kAzqzTrz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-25kAzqzTrz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-25kAzqzTrz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-25kAzqzTrz@OpenReview" onclick="foldPdfKimi('25kAzqzTrz@OpenReview', this)" class="hr hr-fold">
        </div><div id="1CLzLXSFNn@OpenReview" class="panel paper" keywords="series,timemixer,time,tspm,mrti,patterns,tid,mrm,across,mcm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1CLzLXSFNn" target="_blank" title="112/207"><span class="index notranslate">#112</span></a>
                <a id="title-1CLzLXSFNn@OpenReview" class="title-link" href="/venue/1CLzLXSFNn@OpenReview" target="_blank">TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis</a>
                <a id="pdf-1CLzLXSFNn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1CLzLXSFNn@OpenReview', this)" data="https://openreview.net/pdf?id=1CLzLXSFNn">[PDF<sup id="pdf-stars-1CLzLXSFNn@OpenReview">20</sup>]</a>
                <a id="copy-1CLzLXSFNn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1CLzLXSFNn@OpenReview')">[Copy]</a>
                <a id="kimi-1CLzLXSFNn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1CLzLXSFNn@OpenReview', this)">[Kimi<sup id="kimi-stars-1CLzLXSFNn@OpenReview">14</sup>]</a>
                <a id="rel-1CLzLXSFNn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1CLzLXSFNn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1CLzLXSFNn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyu Wang" target="_blank">Shiyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei LI" target="_blank">Jiawei LI</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoming Shi" target="_blank">Xiaoming Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhou Ye" target="_blank">Zhou Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baichuan Mo" target="_blank">Baichuan Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenze Lin" target="_blank">Wenze Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengtong Ju" target="_blank">Shengtong Ju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixuan Chu" target="_blank">Zhixuan Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Jin" target="_blank">Ming Jin</a>
            </p>
            <p id="summary-1CLzLXSFNn@OpenReview" class="summary">Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce \method, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis.</p>
            <p id="subjects-1CLzLXSFNn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-1CLzLXSFNn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1CLzLXSFNn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1CLzLXSFNn@OpenReview" onclick="foldPdfKimi('1CLzLXSFNn@OpenReview', this)" class="hr hr-fold">
        </div><div id="z5uVAKwmjf@OpenReview" class="panel paper" keywords="aflow,workflows,agentic,workflow,generation,automating,code,automated,tree,dollars">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=z5uVAKwmjf" target="_blank" title="113/207"><span class="index notranslate">#113</span></a>
                <a id="title-z5uVAKwmjf@OpenReview" class="title-link" href="/venue/z5uVAKwmjf@OpenReview" target="_blank">AFlow: Automating Agentic Workflow Generation</a>
                <a id="pdf-z5uVAKwmjf@OpenReview" class="title-pdf notranslate" onclick="togglePdf('z5uVAKwmjf@OpenReview', this)" data="https://openreview.net/pdf?id=z5uVAKwmjf">[PDF<sup id="pdf-stars-z5uVAKwmjf@OpenReview">23</sup>]</a>
                <a id="copy-z5uVAKwmjf@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('z5uVAKwmjf@OpenReview')">[Copy]</a>
                <a id="kimi-z5uVAKwmjf@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('z5uVAKwmjf@OpenReview', this)">[Kimi<sup id="kimi-stars-z5uVAKwmjf@OpenReview">30</sup>]</a>
                <a id="rel-z5uVAKwmjf@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('z5uVAKwmjf@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-z5uVAKwmjf@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Zhang" target="_blank">Jiayi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyu Xiang" target="_blank">Jinyu Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyang Yu" target="_blank">Zhaoyang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengwei Teng" target="_blank">Fengwei Teng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=XiongHui Chen" target="_blank">XiongHui Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Chen" target="_blank">Jiaqi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingchen Zhuge" target="_blank">Mingchen Zhuge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Cheng" target="_blank">Xin Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sirui Hong" target="_blank">Sirui Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlin Wang" target="_blank">Jinlin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingnan Zheng" target="_blank">Bingnan Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bang Liu" target="_blank">Bang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuyu Luo" target="_blank">Yuyu Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenglin Wu" target="_blank">Chenglin Wu</a>
            </p>
            <p id="summary-z5uVAKwmjf@OpenReview" class="summary">Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce \textbf{AFlow}, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7\% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55\% of its inference cost in dollars. The code will be made available as open-source upon publication.</p>
            <p id="subjects-z5uVAKwmjf@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-z5uVAKwmjf@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-z5uVAKwmjf@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-z5uVAKwmjf@OpenReview" onclick="foldPdfKimi('z5uVAKwmjf@OpenReview', this)" class="hr hr-fold">
        </div><div id="tc90LV0yRL@OpenReview" class="panel paper" keywords="claude,cybersecurity,cybench,sonnet,agents,capabilities,ctf,tasks,agent,opus">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tc90LV0yRL" target="_blank" title="114/207"><span class="index notranslate">#114</span></a>
                <a id="title-tc90LV0yRL@OpenReview" class="title-link" href="/venue/tc90LV0yRL@OpenReview" target="_blank">Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models</a>
                <a id="pdf-tc90LV0yRL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tc90LV0yRL@OpenReview', this)" data="https://openreview.net/pdf?id=tc90LV0yRL">[PDF<sup id="pdf-stars-tc90LV0yRL@OpenReview">10</sup>]</a>
                <a id="copy-tc90LV0yRL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tc90LV0yRL@OpenReview')">[Copy]</a>
                <a id="kimi-tc90LV0yRL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tc90LV0yRL@OpenReview', this)">[Kimi<sup id="kimi-stars-tc90LV0yRL@OpenReview">16</sup>]</a>
                <a id="rel-tc90LV0yRL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tc90LV0yRL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tc90LV0yRL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andy K Zhang" target="_blank">Andy K Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neil Perry" target="_blank">Neil Perry</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Riya Dulepet" target="_blank">Riya Dulepet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joey Ji" target="_blank">Joey Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Celeste Menders" target="_blank">Celeste Menders</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Lin" target="_blank">Justin Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eliot Jones" target="_blank">Eliot Jones</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gashon Hussein" target="_blank">Gashon Hussein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samantha Liu" target="_blank">Samantha Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Donovan Jasper" target="_blank">Donovan Jasper</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pura Peetathawatchai" target="_blank">Pura Peetathawatchai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ari Glenn" target="_blank">Ari Glenn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vikram Sivashankar" target="_blank">Vikram Sivashankar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Zamoshchin" target="_blank">Daniel Zamoshchin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leo Glikbarg" target="_blank">Leo Glikbarg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Derek Askaryar" target="_blank">Derek Askaryar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoxiang Yang" target="_blank">Haoxiang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aolin Zhang" target="_blank">Aolin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishi Alluri" target="_blank">Rishi Alluri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Tran" target="_blank">Nathan Tran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rinnara Sangpisit" target="_blank">Rinnara Sangpisit</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kenny Oseleononmen" target="_blank">Kenny Oseleononmen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Boneh" target="_blank">Dan Boneh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Ho" target="_blank">Daniel Ho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Percy Liang" target="_blank">Percy Liang</a>
            </p>
            <p id="summary-tc90LV0yRL@OpenReview" class="summary">Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structured bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. Anonymized code and data are available at https://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4 and https://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy.</p>
            <p id="subjects-tc90LV0yRL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-tc90LV0yRL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tc90LV0yRL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tc90LV0yRL@OpenReview" onclick="foldPdfKimi('tc90LV0yRL@OpenReview', this)" class="hr hr-fold">
        </div><div id="eIJfOIMN9z@OpenReview" class="panel paper" keywords="language,recommendation,representations,lms,item,recommenders,advanced,representation,findings,homomorphism">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eIJfOIMN9z" target="_blank" title="115/207"><span class="index notranslate">#115</span></a>
                <a id="title-eIJfOIMN9z@OpenReview" class="title-link" href="/venue/eIJfOIMN9z@OpenReview" target="_blank">Language Representations Can be What Recommenders Need: Findings and Potentials</a>
                <a id="pdf-eIJfOIMN9z@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eIJfOIMN9z@OpenReview', this)" data="https://openreview.net/pdf?id=eIJfOIMN9z">[PDF<sup id="pdf-stars-eIJfOIMN9z@OpenReview">10</sup>]</a>
                <a id="copy-eIJfOIMN9z@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eIJfOIMN9z@OpenReview')">[Copy]</a>
                <a id="kimi-eIJfOIMN9z@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eIJfOIMN9z@OpenReview', this)">[Kimi<sup id="kimi-stars-eIJfOIMN9z@OpenReview">18</sup>]</a>
                <a id="rel-eIJfOIMN9z@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eIJfOIMN9z@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eIJfOIMN9z@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Leheng Sheng" target="_blank">Leheng Sheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=An Zhang" target="_blank">An Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhang" target="_blank">Yi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxin Chen" target="_blank">Yuxin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Wang" target="_blank">Xiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tat-Seng Chua" target="_blank">Tat-Seng Chua</a>
            </p>
            <p id="summary-eIJfOIMN9z@OpenReview" class="summary">Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space.Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.This outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Motivated by the finding of homomorphism, we explore the possibility of designing advanced collaborative filtering (CF) models purely based on language representations without ID-based embeddings.To be specific, we incorporate several crucial components (i.e., a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function) to build a simple yet effective model, with the language representations of item textual metadata (i.e., title) as the input.Empirical results show that such a simple model can outperform leading ID-based CF models on multiple datasets, which sheds light on using language representations for better recommendation.Moreover, we systematically analyze this simple model and find several key features for using advanced language representations:a good initialization for item representations, superior zero-shot recommendation abilities in new datasets, and being aware of user intention.Our findings highlight the connection between language modeling and behavior modeling, which can inspire both natural language processing and recommender system communities.</p>
            <p id="subjects-eIJfOIMN9z@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-eIJfOIMN9z@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eIJfOIMN9z@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eIJfOIMN9z@OpenReview" onclick="foldPdfKimi('eIJfOIMN9z@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZuazHmXTns@OpenReview" class="panel paper" keywords="padamfed,epsilon,federated,mathcal,stepsize,heterogeneity,communication,problem,advantages,learning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZuazHmXTns" target="_blank" title="116/207"><span class="index notranslate">#116</span></a>
                <a id="title-ZuazHmXTns@OpenReview" class="title-link" href="/venue/ZuazHmXTns@OpenReview" target="_blank">Problem-Parameter-Free Federated Learning</a>
                <a id="pdf-ZuazHmXTns@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZuazHmXTns@OpenReview', this)" data="https://openreview.net/pdf?id=ZuazHmXTns">[PDF<sup id="pdf-stars-ZuazHmXTns@OpenReview">10</sup>]</a>
                <a id="copy-ZuazHmXTns@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZuazHmXTns@OpenReview')">[Copy]</a>
                <a id="kimi-ZuazHmXTns@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZuazHmXTns@OpenReview', this)">[Kimi<sup id="kimi-stars-ZuazHmXTns@OpenReview">8</sup>]</a>
                <a id="rel-ZuazHmXTns@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZuazHmXTns@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZuazHmXTns@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjing Yan" target="_blank">Wenjing Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolu Wang" target="_blank">Xiaolu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanyu Cao" target="_blank">Xuanyu Cao</a>
            </p>
            <p id="summary-ZuazHmXTns@OpenReview" class="summary">Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned hyperparameters, particularly the learning rate or stepsize. This manual tuning process is challenging in federated settings due to data heterogeneity and limited accessibility of local datasets. Consequently, the reliance on problem-specific parameters hinders the widespread adoption of FL and potentially compromises its performance in dynamic or diverse environments. To address this issue, we introduce PAdaMFed, a novel algorithm for nonconvex FL that carefully combines adaptive stepsize and momentum techniques. PAdaMFed offers two key advantages: 1) it operates autonomously without relying on problem-specific parameters; and 2) it manages data heterogeneity and partial participation without requiring heterogeneity bounds. Despite these benefits, PAdaMFed provides several strong theoretical guarantees: 1) It achieves state-of-the-art convergence rates with a sample complexity of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-143" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.82em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-144"><span class="texatom" id="MathJax-Span-145"><span class="mrow" id="MathJax-Span-146"><span class="mi" id="MathJax-Span-147" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-148" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-149"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-150" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-151"><span class="mrow" id="MathJax-Span-152"><span class="mo" id="MathJax-Span-153" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-154" style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-155" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>4</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-32">\mathcal{O}(\epsilon^{-4})</script> and communication complexity of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-156" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.82em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-157"><span class="texatom" id="MathJax-Span-158"><span class="mrow" id="MathJax-Span-159"><span class="mi" id="MathJax-Span-160" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-161" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-162"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-163" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-164"><span class="mrow" id="MathJax-Span-165"><span class="mo" id="MathJax-Span-166" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-167" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-168" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-33">\mathcal{O}(\epsilon^{-3})</script> to obtain an accuracy of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2264;&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-169" style="width: 7.086em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1005.89em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-170"><span class="texatom" id="MathJax-Span-171"><span class="mrow" id="MathJax-Span-172"><span class="mo" id="MathJax-Span-173" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-174"><span class="mrow" id="MathJax-Span-175"><span class="mo" id="MathJax-Span-176" style="font-family: MathJax_Main;">|</span></span></span><span class="mi" id="MathJax-Span-177" style="font-family: MathJax_Main;"></span><span class="mi" id="MathJax-Span-178" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mrow" id="MathJax-Span-179" style="padding-left: 0.159em;"><span class="mo" id="MathJax-Span-180" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-181" style="font-family: MathJax_Math-bold-italic;"></span><span class="mo" id="MathJax-Span-182" style="font-family: MathJax_Main;">)</span></span><span class="texatom" id="MathJax-Span-183" style="padding-left: 0.159em;"><span class="mrow" id="MathJax-Span-184"><span class="mo" id="MathJax-Span-185" style="font-family: MathJax_Main;">|</span></span></span><span class="texatom" id="MathJax-Span-186"><span class="mrow" id="MathJax-Span-187"><span class="mo" id="MathJax-Span-188" style="font-family: MathJax_Main;">|</span></span></span><span class="mo" id="MathJax-Span-189" style="font-family: MathJax_Main; padding-left: 0.263em;"></span><span class="mi" id="MathJax-Span-190" style="font-family: MathJax_Math-italic; padding-left: 0.263em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi mathvariant="normal"></mi><mi>f</mi><mrow><mo>(</mo><mi mathvariant="bold-italic"></mi><mo>)</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo></mo><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-34">||\nabla f\left(\boldsymbol{\theta}\right)|| \leq \epsilon</script>, even using constant learning rates; 2) these complexities can be improved to the best-known <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-191" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.82em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-192"><span class="texatom" id="MathJax-Span-193"><span class="mrow" id="MathJax-Span-194"><span class="mi" id="MathJax-Span-195" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-196" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-197"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-198" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-199"><span class="mrow" id="MathJax-Span-200"><span class="mo" id="MathJax-Span-201" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-202" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-203" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-35">\mathcal{O}(\epsilon^{-3})</script> for sampling and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-204" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1002.82em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-205"><span class="texatom" id="MathJax-Span-206"><span class="mrow" id="MathJax-Span-207"><span class="mi" id="MathJax-Span-208" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-209" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-210"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-211" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-212"><span class="mrow" id="MathJax-Span-213"><span class="mo" id="MathJax-Span-214" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-215" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-216" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-36">\mathcal{O}(\epsilon^{-2})</script> for communication when incorporating variance reduction; 3) it exhibits linear speedup with respect to the number of local update steps and participating clients at each global round. These attributes make PAdaMFed highly scalable and adaptable for various real-world FL applications. Extensive empirical evidence on both image classification and sentiment analysis tasks validates the efficacy of our approaches.</p>
            <p id="subjects-ZuazHmXTns@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-ZuazHmXTns@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZuazHmXTns@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZuazHmXTns@OpenReview" onclick="foldPdfKimi('ZuazHmXTns@OpenReview', this)" class="hr hr-fold">
        </div><div id="QKBu1BOAwd@OpenReview" class="panel paper" keywords="documentation,llms,tool,tools,draft,mastery,external,comprehension,exploration,utilization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QKBu1BOAwd" target="_blank" title="117/207"><span class="index notranslate">#117</span></a>
                <a id="title-QKBu1BOAwd@OpenReview" class="title-link" href="/venue/QKBu1BOAwd@OpenReview" target="_blank">From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</a>
                <a id="pdf-QKBu1BOAwd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QKBu1BOAwd@OpenReview', this)" data="https://openreview.net/pdf?id=QKBu1BOAwd">[PDF<sup id="pdf-stars-QKBu1BOAwd@OpenReview">11</sup>]</a>
                <a id="copy-QKBu1BOAwd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QKBu1BOAwd@OpenReview')">[Copy]</a>
                <a id="kimi-QKBu1BOAwd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QKBu1BOAwd@OpenReview', this)">[Kimi<sup id="kimi-stars-QKBu1BOAwd@OpenReview">13</sup>]</a>
                <a id="rel-QKBu1BOAwd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QKBu1BOAwd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QKBu1BOAwd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Changle Qu" target="_blank">Changle Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunhao Dai" target="_blank">Sunhao Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochi Wei" target="_blank">Xiaochi Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengyi Cai" target="_blank">Hengyi Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuaiqiang Wang" target="_blank">Shuaiqiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawei Yin" target="_blank">Dawei Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Xu" target="_blank">Jun Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji-Rong Wen" target="_blank">Ji-Rong Wen</a>
            </p>
            <p id="summary-QKBu1BOAwd@OpenReview" class="summary">Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trials emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. Extensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities. Our code is available at https://anonymous.4open.science/r/DRAFT-10B3.</p>
            <p id="subjects-QKBu1BOAwd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-QKBu1BOAwd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QKBu1BOAwd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QKBu1BOAwd@OpenReview" onclick="foldPdfKimi('QKBu1BOAwd@OpenReview', this)" class="hr hr-fold">
        </div><div id="I4e82CIDxv@OpenReview" class="panel paper" keywords="circuits,sparse,discovering,feature,interpretable,polysemantic,units,editing,language,behaviors">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=I4e82CIDxv" target="_blank" title="118/207"><span class="index notranslate">#118</span></a>
                <a id="title-I4e82CIDxv@OpenReview" class="title-link" href="/venue/I4e82CIDxv@OpenReview" target="_blank">Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a>
                <a id="pdf-I4e82CIDxv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('I4e82CIDxv@OpenReview', this)" data="https://openreview.net/pdf?id=I4e82CIDxv">[PDF<sup id="pdf-stars-I4e82CIDxv@OpenReview">11</sup>]</a>
                <a id="copy-I4e82CIDxv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('I4e82CIDxv@OpenReview')">[Copy]</a>
                <a id="kimi-I4e82CIDxv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('I4e82CIDxv@OpenReview', this)">[Kimi<sup id="kimi-stars-I4e82CIDxv@OpenReview">16</sup>]</a>
                <a id="rel-I4e82CIDxv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('I4e82CIDxv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-I4e82CIDxv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Marks" target="_blank">Samuel Marks</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Can Rager" target="_blank">Can Rager</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Michaud" target="_blank">Eric Michaud</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonatan Belinkov" target="_blank">Yonatan Belinkov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Bau" target="_blank">David Bau</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Mueller" target="_blank">Aaron Mueller</a>
            </p>
            <p id="summary-I4e82CIDxv@OpenReview" class="summary">We introduce methods for discovering and applying **sparse feature circuits**. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms in neural networks. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.</p>
            <p id="subjects-I4e82CIDxv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-I4e82CIDxv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-I4e82CIDxv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-I4e82CIDxv@OpenReview" onclick="foldPdfKimi('I4e82CIDxv@OpenReview', this)" class="hr hr-fold">
        </div><div id="GGlpykXDCa@OpenReview" class="panel paper" keywords="mmqa,table,llms,multi,hop,foreign,capabilities,tabular,understanding,wikitablequestions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GGlpykXDCa" target="_blank" title="119/207"><span class="index notranslate">#119</span></a>
                <a id="title-GGlpykXDCa@OpenReview" class="title-link" href="/venue/GGlpykXDCa@OpenReview" target="_blank">MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions</a>
                <a id="pdf-GGlpykXDCa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GGlpykXDCa@OpenReview', this)" data="https://openreview.net/pdf?id=GGlpykXDCa">[PDF<sup id="pdf-stars-GGlpykXDCa@OpenReview">12</sup>]</a>
                <a id="copy-GGlpykXDCa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GGlpykXDCa@OpenReview')">[Copy]</a>
                <a id="kimi-GGlpykXDCa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GGlpykXDCa@OpenReview', this)">[Kimi<sup id="kimi-stars-GGlpykXDCa@OpenReview">14</sup>]</a>
                <a id="rel-GGlpykXDCa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GGlpykXDCa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GGlpykXDCa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Wu" target="_blank">Jian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linyi Yang" target="_blank">Linyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyuan Li" target="_blank">Dongyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuliang Ji" target="_blank">Yuliang Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manabu Okumura" target="_blank">Manabu Okumura</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Zhang" target="_blank">Yue Zhang</a>
            </p>
            <p id="summary-GGlpykXDCa@OpenReview" class="summary">While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we present a \textbf{M}ulti-table and Multi-hop Question Answering (MMQA) dataset to assess LLMs' understanding and reasoning capabilities in handling multi-table tasks. The MMQA dataset demands that models perform multiple inferences by drawing evidence from various tables, which are designed to be connected with each other and require models to identify and utilize relationships such as foreign and primary keys. Then, we introduce a comprehensive evaluation framework that tailors to assess LLMs' capabilities in several aspects including Multi-Table Retrieval, Text-to-SQL Generation, Multi-Table QA, Primary Key Selection, and Foreign Key Selection. Finally, we propose a novel multi-table retrieval method that achieves state-of-the-art (SOTA) performance on the MMQA dataset compared to several strong baselines. Our experiment results reveal that, compared with human performance, both open-source and commercial LLMs leave significant performance room for improvements in multi-table understanding and reasoning tasks. We believe that the MMQA benchmark will enhance and facilitate LLMs' multi-table capabilities in real-world scenarios.</p>
            <p id="subjects-GGlpykXDCa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-GGlpykXDCa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GGlpykXDCa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GGlpykXDCa@OpenReview" onclick="foldPdfKimi('GGlpykXDCa@OpenReview', this)" class="hr hr-fold">
        </div><div id="CRmiX0v16e@OpenReview" class="panel paper" keywords="yolo,open,instance,vocabulary,scannet200,segmentation,masks,object,mvpdist,view">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CRmiX0v16e" target="_blank" title="120/207"><span class="index notranslate">#120</span></a>
                <a id="title-CRmiX0v16e@OpenReview" class="title-link" href="/venue/CRmiX0v16e@OpenReview" target="_blank">Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation</a>
                <a id="pdf-CRmiX0v16e@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CRmiX0v16e@OpenReview', this)" data="https://openreview.net/pdf?id=CRmiX0v16e">[PDF<sup id="pdf-stars-CRmiX0v16e@OpenReview">7</sup>]</a>
                <a id="copy-CRmiX0v16e@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CRmiX0v16e@OpenReview')">[Copy]</a>
                <a id="kimi-CRmiX0v16e@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CRmiX0v16e@OpenReview', this)">[Kimi<sup id="kimi-stars-CRmiX0v16e@OpenReview">5</sup>]</a>
                <a id="rel-CRmiX0v16e@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CRmiX0v16e@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CRmiX0v16e@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mohamed el amine Boudjoghra" target="_blank">Mohamed el amine Boudjoghra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angela Dai" target="_blank">Angela Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean Lahoud" target="_blank">Jean Lahoud</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hisham Cholakkal" target="_blank">Hisham Cholakkal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rao Anwer" target="_blank">Rao Anwer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Salman Khan" target="_blank">Salman Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fahad Khan" target="_blank">Fahad Khan</a>
            </p>
            <p id="summary-CRmiX0v16e@OpenReview" class="summary">Recent works on open-vocabulary 3D instance segmentation show strong promise but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on aggregated clip features from multi-view, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP. Consequently, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a novel open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that efficiently leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. We demonstrate that our proposed Multi-View Prompt Distribution (MVPDist) method makes use of multi-view information to account for misclassification from the object detector to predict a reliable label for 3D instance masks. Furthermore, since projections of 3D object instances are already contained within the 2D bounding boxes, we show that our proposed low granularity label maps, which require only a 2D object detector to construct, are sufficient and very fast to predict prompt IDs for 3D instance masks when used with our proposed MVPDist. We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-217" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-218"><span class="mo" id="MathJax-Span-219" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-37">\sim</script>16<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-220" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-221"><span class="mo" id="MathJax-Span-222" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-38">\times</script> speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7% while operating at 22 seconds per scene. Our code will be publically available.</p>
            <p id="subjects-CRmiX0v16e@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-CRmiX0v16e@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CRmiX0v16e@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CRmiX0v16e@OpenReview" onclick="foldPdfKimi('CRmiX0v16e@OpenReview', this)" class="hr hr-fold">
        </div><div id="5Jc7r5aqHJ@OpenReview" class="panel paper" keywords="federated,backdoor,graph,energy,defense,benign,client,malicious,clients,making">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5Jc7r5aqHJ" target="_blank" title="121/207"><span class="index notranslate">#121</span></a>
                <a id="title-5Jc7r5aqHJ@OpenReview" class="title-link" href="/venue/5Jc7r5aqHJ@OpenReview" target="_blank">Energy-based Backdoor Defense Against Federated Graph Learning</a>
                <a id="pdf-5Jc7r5aqHJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5Jc7r5aqHJ@OpenReview', this)" data="https://openreview.net/pdf?id=5Jc7r5aqHJ">[PDF<sup id="pdf-stars-5Jc7r5aqHJ@OpenReview">9</sup>]</a>
                <a id="copy-5Jc7r5aqHJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5Jc7r5aqHJ@OpenReview')">[Copy]</a>
                <a id="kimi-5Jc7r5aqHJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5Jc7r5aqHJ@OpenReview', this)">[Kimi<sup id="kimi-stars-5Jc7r5aqHJ@OpenReview">9</sup>]</a>
                <a id="rel-5Jc7r5aqHJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5Jc7r5aqHJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5Jc7r5aqHJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guancheng Wan" target="_blank">Guancheng Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zitong Shi" target="_blank">Zitong Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenke Huang" target="_blank">Wenke Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guibin Zhang" target="_blank">Guibin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dacheng Tao" target="_blank">Dacheng Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mang Ye" target="_blank">Mang Ye</a>
            </p>
            <p id="summary-5Jc7r5aqHJ@OpenReview" class="summary">Federated Graph Learning is rapidly evolving as a privacy-preserving collaborative approach. However, backdoor attacks are increasingly undermining federated systems by injecting carefully designed triggers that lead to the model making incorrect predictions. Trigger structures and injection locations in Federated Graph Learning are more diverse, making traditional federated defense methods less effective. In our work, we propose an effective Federated Graph Backdoor Defense using Topological Graph Energy (FedTGE). At the local client level, it injects distribution knowledge into the local model, assigning low energy to benign samples and high energy to the constructed malicious substitutes, and selects benign clients through clustering. At the global server level, the energy elements uploaded by each client are treated as new nodes to construct a global energy graph for energy propagation, making the selected clients' energy elements more similar and further adjusting the aggregation weights. Our method can handle high data heterogeneity, does not require a validation dataset, and is effective under both small and large malicious proportions. Extensive results on various settings of federated graph scenarios under backdoor attacks validate the effectiveness of this approach.</p>
            <p id="subjects-5Jc7r5aqHJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-5Jc7r5aqHJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5Jc7r5aqHJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5Jc7r5aqHJ@OpenReview" onclick="foldPdfKimi('5Jc7r5aqHJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="ilOEOIqolQ@OpenReview" class="panel paper" keywords="creativity,index,text,web,salieri,llms,verbatim,human,snippets,linguistic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ilOEOIqolQ" target="_blank" title="122/207"><span class="index notranslate">#122</span></a>
                <a id="title-ilOEOIqolQ@OpenReview" class="title-link" href="/venue/ilOEOIqolQ@OpenReview" target="_blank">AI as Humanitys Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text</a>
                <a id="pdf-ilOEOIqolQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ilOEOIqolQ@OpenReview', this)" data="https://openreview.net/pdf?id=ilOEOIqolQ">[PDF<sup id="pdf-stars-ilOEOIqolQ@OpenReview">6</sup>]</a>
                <a id="copy-ilOEOIqolQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ilOEOIqolQ@OpenReview')">[Copy]</a>
                <a id="kimi-ilOEOIqolQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ilOEOIqolQ@OpenReview', this)">[Kimi<sup id="kimi-stars-ilOEOIqolQ@OpenReview">7</sup>]</a>
                <a id="rel-ilOEOIqolQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ilOEOIqolQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ilOEOIqolQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ximing Lu" target="_blank">Ximing Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Melanie Sclar" target="_blank">Melanie Sclar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Skyler Hallinan" target="_blank">Skyler Hallinan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niloofar Mireshghallah" target="_blank">Niloofar Mireshghallah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacheng Liu" target="_blank">Jiacheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungju Han" target="_blank">Seungju Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Allyson Ettinger" target="_blank">Allyson Ettinger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Jiang" target="_blank">Liwei Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khyathi Chandu" target="_blank">Khyathi Chandu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nouha Dziri" target="_blank">Nouha Dziri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yejin Choi" target="_blank">Yejin Choi</a>
            </p>
            <p id="summary-ilOEOIqolQ@OpenReview" class="summary">Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpasshuman creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we explore variations in the CREATIVITY INDEX among different human authors and discuss the potential factors contributing to these differences. Finally, we showcase a novel application of CREATIVITY INDEX for zero-shot machine text detection, where it proves to be surprisingly effectiveoutperforming the strong zero-shot system DetectGPT by a substantial margin of 30.2%, and even surpassing a leading supervised system, GhostBuster, in five out of six domains.</p>
            <p id="subjects-ilOEOIqolQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-ilOEOIqolQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ilOEOIqolQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ilOEOIqolQ@OpenReview" onclick="foldPdfKimi('ilOEOIqolQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="kRoWeLTpL4@OpenReview" class="panel paper" keywords="copyright,copyrighted,fuse,protective,protected,material,reproduction,fusion,regurgitation,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kRoWeLTpL4" target="_blank" title="123/207"><span class="index notranslate">#123</span></a>
                <a id="title-kRoWeLTpL4@OpenReview" class="title-link" href="/venue/kRoWeLTpL4@OpenReview" target="_blank">Copyright-Protected Language Generation via Adaptive Model Fusion</a>
                <a id="pdf-kRoWeLTpL4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kRoWeLTpL4@OpenReview', this)" data="https://openreview.net/pdf?id=kRoWeLTpL4">[PDF<sup id="pdf-stars-kRoWeLTpL4@OpenReview">3</sup>]</a>
                <a id="copy-kRoWeLTpL4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kRoWeLTpL4@OpenReview')">[Copy]</a>
                <a id="kimi-kRoWeLTpL4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kRoWeLTpL4@OpenReview', this)">[Kimi<sup id="kimi-stars-kRoWeLTpL4@OpenReview">6</sup>]</a>
                <a id="rel-kRoWeLTpL4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kRoWeLTpL4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kRoWeLTpL4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Javier Abad" target="_blank">Javier Abad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Konstantin Donhauser" target="_blank">Konstantin Donhauser</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francesco Pinto" target="_blank">Francesco Pinto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fanny Yang" target="_blank">Fanny Yang</a>
            </p>
            <p id="summary-kRoWeLTpL4@OpenReview" class="summary">The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown promise in addressing the complexities of copyright regulation. However, they often incur prohibitive computational costs or suffer from performance trade-offs. To overcome these limitations, we introduce Copyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines models trained on disjoint sets of copyrighted material during inference. In particular, CP-Fuse adaptively aggregates the model outputs to minimize the reproduction of copyrighted content, adhering to a crucial balancing property to prevent the regurgitation of memorized data. Through extensive experiments, we show that CP-Fuse significantly reduces the reproduction of protected material without compromising the quality of text and code generation. Moreover, its post-hoc nature allows seamless integration with other protective measures, further enhancing copyright safeguards. Lastly, we show that CP-Fuse is robust against common techniques for extracting training data</p>
            <p id="subjects-kRoWeLTpL4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-kRoWeLTpL4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kRoWeLTpL4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kRoWeLTpL4@OpenReview" onclick="foldPdfKimi('kRoWeLTpL4@OpenReview', this)" class="hr hr-fold">
        </div><div id="meRCKuUpmc@OpenReview" class="panel paper" keywords="robotic,pidm,end,seer,inverse,scalable,action,world,manipulation,vision">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=meRCKuUpmc" target="_blank" title="124/207"><span class="index notranslate">#124</span></a>
                <a id="title-meRCKuUpmc@OpenReview" class="title-link" href="/venue/meRCKuUpmc@OpenReview" target="_blank">Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation</a>
                <a id="pdf-meRCKuUpmc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('meRCKuUpmc@OpenReview', this)" data="https://openreview.net/pdf?id=meRCKuUpmc">[PDF<sup id="pdf-stars-meRCKuUpmc@OpenReview">9</sup>]</a>
                <a id="copy-meRCKuUpmc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('meRCKuUpmc@OpenReview')">[Copy]</a>
                <a id="kimi-meRCKuUpmc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('meRCKuUpmc@OpenReview', this)">[Kimi<sup id="kimi-stars-meRCKuUpmc@OpenReview">9</sup>]</a>
                <a id="rel-meRCKuUpmc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('meRCKuUpmc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-meRCKuUpmc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Tian" target="_blank">Yang Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sizhe Yang" target="_blank">Sizhe Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jia Zeng" target="_blank">Jia Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Wang" target="_blank">Ping Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dahua Lin" target="_blank">Dahua Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Dong" target="_blank">Hao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangmiao Pang" target="_blank">Jiangmiao Pang</a>
            </p>
            <p id="summary-meRCKuUpmc@OpenReview" class="summary">Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available.</p>
            <p id="subjects-meRCKuUpmc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-meRCKuUpmc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-meRCKuUpmc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-meRCKuUpmc@OpenReview" onclick="foldPdfKimi('meRCKuUpmc@OpenReview', this)" class="hr hr-fold">
        </div><div id="mtJSMcF3ek@OpenReview" class="panel paper" keywords="improvement,self,llm,verification,gap,reweights,mind,language,examining,training">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mtJSMcF3ek" target="_blank" title="125/207"><span class="index notranslate">#125</span></a>
                <a id="title-mtJSMcF3ek@OpenReview" class="title-link" href="/venue/mtJSMcF3ek@OpenReview" target="_blank">Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models</a>
                <a id="pdf-mtJSMcF3ek@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mtJSMcF3ek@OpenReview', this)" data="https://openreview.net/pdf?id=mtJSMcF3ek">[PDF<sup id="pdf-stars-mtJSMcF3ek@OpenReview">21</sup>]</a>
                <a id="copy-mtJSMcF3ek@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mtJSMcF3ek@OpenReview')">[Copy]</a>
                <a id="kimi-mtJSMcF3ek@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mtJSMcF3ek@OpenReview', this)">[Kimi<sup id="kimi-stars-mtJSMcF3ek@OpenReview">35</sup>]</a>
                <a id="rel-mtJSMcF3ek@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mtJSMcF3ek@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mtJSMcF3ek@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuda Song" target="_blank">Yuda Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanlin Zhang" target="_blank">Hanlin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Udaya Ghai" target="_blank">Udaya Ghai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carson Eisenach" target="_blank">Carson Eisenach</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sham Kakade" target="_blank">Sham Kakade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dean Foster" target="_blank">Dean Foster</a>
            </p>
            <p id="summary-mtJSMcF3ek@OpenReview" class="summary">Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the *generation-verification gap*. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. We believe our results have several empirical implications, and our study leaves many exciting future directions for understanding the potential and limits of LLM self-improvement.</p>
            <p id="subjects-mtJSMcF3ek@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-mtJSMcF3ek@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mtJSMcF3ek@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mtJSMcF3ek@OpenReview" onclick="foldPdfKimi('mtJSMcF3ek@OpenReview', this)" class="hr hr-fold">
        </div><div id="EjJGND0m1x@OpenReview" class="panel paper" keywords="computation,complexity,parameters,input,mind,thinking,dynamic,efficiently,inefficiently,introspection">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EjJGND0m1x" target="_blank" title="126/207"><span class="index notranslate">#126</span></a>
                <a id="title-EjJGND0m1x@OpenReview" class="title-link" href="/venue/EjJGND0m1x@OpenReview" target="_blank">MIND over Body: Adaptive Thinking using Dynamic Computation</a>
                <a id="pdf-EjJGND0m1x@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EjJGND0m1x@OpenReview', this)" data="https://openreview.net/pdf?id=EjJGND0m1x">[PDF<sup id="pdf-stars-EjJGND0m1x@OpenReview">12</sup>]</a>
                <a id="copy-EjJGND0m1x@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EjJGND0m1x@OpenReview')">[Copy]</a>
                <a id="kimi-EjJGND0m1x@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EjJGND0m1x@OpenReview', this)">[Kimi<sup id="kimi-stars-EjJGND0m1x@OpenReview">20</sup>]</a>
                <a id="rel-EjJGND0m1x@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EjJGND0m1x@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EjJGND0m1x@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mrinal Mathur" target="_blank">Mrinal Mathur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Barak Pearlmutter" target="_blank">Barak Pearlmutter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Plis" target="_blank">Sergey Plis</a>
            </p>
            <p id="summary-EjJGND0m1x@OpenReview" class="summary">While the human brain efficiently handles various computations with a limited number of neurons, traditional deep learning networks require a significant increase in parameters to improve performance. Yet, these parameters are used inefficiently as the networks employ the same amount of computation for inputs of the same size, regardless of the input's complexity. We address this inefficiency by introducing self-introspection capabilities to the network, enabling it to adjust the number of used parameters based on the internal representation of the task and adapt the computation time based on the task complexity. This enables the network to adaptively reuse parameters across tasks, dynamically adjusting the computational effort to match the complexity of the input. We demonstrate the effectiveness of this method on language modeling and computer vision tasks. Notably, our model surpasses much larger ResNet-50 and EfficientNet on ImageNet, achieving 96.62\% accuracy, and achieves a 95.8\% F1 score on the SQuAD dataset, all with just a three-layer network. These results showcase the potential for dynamic and reflective computation, contributing to the creation of intelligent systems that efficiently manage resources based on input data complexity.</p>
            <p id="subjects-EjJGND0m1x@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-EjJGND0m1x@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EjJGND0m1x@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EjJGND0m1x@OpenReview" onclick="foldPdfKimi('EjJGND0m1x@OpenReview', this)" class="hr hr-fold">
        </div><div id="kGvXIlIVLM@OpenReview" class="panel paper" keywords="guidance,cca,visual,cfg,alignment,generation,sampling,contrastive,unifying,pretraining">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kGvXIlIVLM" target="_blank" title="127/207"><span class="index notranslate">#127</span></a>
                <a id="title-kGvXIlIVLM@OpenReview" class="title-link" href="/venue/kGvXIlIVLM@OpenReview" target="_blank">Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment</a>
                <a id="pdf-kGvXIlIVLM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kGvXIlIVLM@OpenReview', this)" data="https://openreview.net/pdf?id=kGvXIlIVLM">[PDF<sup id="pdf-stars-kGvXIlIVLM@OpenReview">13</sup>]</a>
                <a id="copy-kGvXIlIVLM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kGvXIlIVLM@OpenReview')">[Copy]</a>
                <a id="kimi-kGvXIlIVLM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kGvXIlIVLM@OpenReview', this)">[Kimi<sup id="kimi-stars-kGvXIlIVLM@OpenReview">13</sup>]</a>
                <a id="rel-kGvXIlIVLM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kGvXIlIVLM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kGvXIlIVLM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huayu Chen" target="_blank">Huayu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Su" target="_blank">Hang Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peize Sun" target="_blank">Peize Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Zhu" target="_blank">Jun Zhu</a>
            </p>
            <p id="summary-kGvXIlIVLM@OpenReview" class="summary">Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (1% of pretraining epochs) on the pretraining dataset. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields.</p>
            <p id="subjects-kGvXIlIVLM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-kGvXIlIVLM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kGvXIlIVLM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kGvXIlIVLM@OpenReview" onclick="foldPdfKimi('kGvXIlIVLM@OpenReview', this)" class="hr hr-fold">
        </div><div id="nwDRD4AMoN@OpenReview" class="panel paper" keywords="kuramoto,neurons,oscillatory,neuroscience,artificial,representations,akorn,importance,hypothesized,dynamical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=nwDRD4AMoN" target="_blank" title="128/207"><span class="index notranslate">#128</span></a>
                <a id="title-nwDRD4AMoN@OpenReview" class="title-link" href="/venue/nwDRD4AMoN@OpenReview" target="_blank">Artificial Kuramoto Oscillatory Neurons</a>
                <a id="pdf-nwDRD4AMoN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('nwDRD4AMoN@OpenReview', this)" data="https://openreview.net/pdf?id=nwDRD4AMoN">[PDF<sup id="pdf-stars-nwDRD4AMoN@OpenReview">9</sup>]</a>
                <a id="copy-nwDRD4AMoN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('nwDRD4AMoN@OpenReview')">[Copy]</a>
                <a id="kimi-nwDRD4AMoN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('nwDRD4AMoN@OpenReview', this)">[Kimi<sup id="kimi-stars-nwDRD4AMoN@OpenReview">10</sup>]</a>
                <a id="rel-nwDRD4AMoN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('nwDRD4AMoN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-nwDRD4AMoN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Takeru Miyato" target="_blank">Takeru Miyato</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sindy Lwe" target="_blank">Sindy Lwe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Geiger" target="_blank">Andreas Geiger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Welling" target="_blank">Max Welling</a>
            </p>
            <p id="summary-nwDRD4AMoN@OpenReview" class="summary">It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (*AKOrN*) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations.</p>
            <p id="subjects-nwDRD4AMoN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-nwDRD4AMoN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-nwDRD4AMoN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-nwDRD4AMoN@OpenReview" onclick="foldPdfKimi('nwDRD4AMoN@OpenReview', this)" class="hr hr-fold">
        </div><div id="uKZdlihDDn@OpenReview" class="panel paper" keywords="distributions,fluid,diffusion,graph,complex,simulations,statistics,latent,physical,unsteady">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uKZdlihDDn" target="_blank" title="129/207"><span class="index notranslate">#129</span></a>
                <a id="title-uKZdlihDDn@OpenReview" class="title-link" href="/venue/uKZdlihDDn@OpenReview" target="_blank">Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks</a>
                <a id="pdf-uKZdlihDDn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uKZdlihDDn@OpenReview', this)" data="https://openreview.net/pdf?id=uKZdlihDDn">[PDF<sup id="pdf-stars-uKZdlihDDn@OpenReview">8</sup>]</a>
                <a id="copy-uKZdlihDDn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uKZdlihDDn@OpenReview')">[Copy]</a>
                <a id="kimi-uKZdlihDDn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uKZdlihDDn@OpenReview', this)">[Kimi<sup id="kimi-stars-uKZdlihDDn@OpenReview">9</sup>]</a>
                <a id="rel-uKZdlihDDn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uKZdlihDDn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uKZdlihDDn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mario Lino" target="_blank">Mario Lino</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tobias Pfaff" target="_blank">Tobias Pfaff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nils Thuerey" target="_blank">Nils Thuerey</a>
            </p>
            <p id="summary-uKZdlihDDn@OpenReview" class="summary">Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding of our work is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks.</p>
            <p id="subjects-uKZdlihDDn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-uKZdlihDDn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uKZdlihDDn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uKZdlihDDn@OpenReview" onclick="foldPdfKimi('uKZdlihDDn@OpenReview', this)" class="hr hr-fold">
        </div><div id="uHLgDEgiS5@OpenReview" class="panel paper" keywords="loo,data,influence,training,trajectory,embedding,specific,ordering,stages,emph">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uHLgDEgiS5" target="_blank" title="130/207"><span class="index notranslate">#130</span></a>
                <a id="title-uHLgDEgiS5@OpenReview" class="title-link" href="/venue/uHLgDEgiS5@OpenReview" target="_blank">Capturing the Temporal Dependence of Training Data Influence</a>
                <a id="pdf-uHLgDEgiS5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uHLgDEgiS5@OpenReview', this)" data="https://openreview.net/pdf?id=uHLgDEgiS5">[PDF<sup id="pdf-stars-uHLgDEgiS5@OpenReview">9</sup>]</a>
                <a id="copy-uHLgDEgiS5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uHLgDEgiS5@OpenReview')">[Copy]</a>
                <a id="kimi-uHLgDEgiS5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uHLgDEgiS5@OpenReview', this)">[Kimi<sup id="kimi-stars-uHLgDEgiS5@OpenReview">9</sup>]</a>
                <a id="rel-uHLgDEgiS5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uHLgDEgiS5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uHLgDEgiS5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiachen (Tianhao) Wang" target="_blank">Jiachen (Tianhao) Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawn Song" target="_blank">Dawn Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Y Zou" target="_blank">James Y Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Mittal" target="_blank">Prateek Mittal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoxi Jia" target="_blank">Ruoxi Jia</a>
            </p>
            <p id="summary-uHLgDEgiS5@OpenReview" class="summary">Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigmsespecially for foundation models using stochastic algorithms and non-convergent, multi-stage curriculaare sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research.</p>
            <p id="subjects-uHLgDEgiS5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-uHLgDEgiS5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uHLgDEgiS5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uHLgDEgiS5@OpenReview" onclick="foldPdfKimi('uHLgDEgiS5@OpenReview', this)" class="hr hr-fold">
        </div><div id="uAFHCZRmXk@OpenReview" class="panel paper" keywords="gap,modality,bias,object,vlms,contrastive,imbalance,vision,tasks,attributes">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uAFHCZRmXk" target="_blank" title="131/207"><span class="index notranslate">#131</span></a>
                <a id="title-uAFHCZRmXk@OpenReview" class="title-link" href="/venue/uAFHCZRmXk@OpenReview" target="_blank">Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models</a>
                <a id="pdf-uAFHCZRmXk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uAFHCZRmXk@OpenReview', this)" data="https://openreview.net/pdf?id=uAFHCZRmXk">[PDF<sup id="pdf-stars-uAFHCZRmXk@OpenReview">19</sup>]</a>
                <a id="copy-uAFHCZRmXk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uAFHCZRmXk@OpenReview')">[Copy]</a>
                <a id="kimi-uAFHCZRmXk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uAFHCZRmXk@OpenReview', this)">[Kimi<sup id="kimi-stars-uAFHCZRmXk@OpenReview">20</sup>]</a>
                <a id="rel-uAFHCZRmXk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uAFHCZRmXk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uAFHCZRmXk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Schrodi" target="_blank">Simon Schrodi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Hoffmann" target="_blank">David Hoffmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Argus" target="_blank">Max Argus</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Volker Fischer" target="_blank">Volker Fischer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Brox" target="_blank">Thomas Brox</a>
            </p>
            <p id="summary-uAFHCZRmXk@OpenReview" class="summary">Contrastive vision-language models (VLMs), like CLIP, have gained popularity for their versatile applicability to various downstream tasks. Despite their successes in some tasks, like zero-shot object recognition, they perform surprisingly poor on other tasks, like attribute recognition. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and to a bias towards objects over other factors, such as attributes. In this analysis paper, we investigate both phenomena thoroughly. We evaluated off-the-shelf VLMs and find that while the gap's influence on performance is typically overshadowed by other factors, we find indications that closing the gap indeed leads to improvements. Moreover, we find that, contrary to intuition, only few embedding dimensions drive the gap and that the embedding spaces are differently organized. To allow for a clean study of object bias, we introduce a definition and a corresponding measure of it. Equipped with this tool, we find that object bias does not lead to worse performance on other concepts, such as attributes per se. However, why do both phenomena, modality gap and object bias, emerge in the first place? To answer this fundamental question and uncover some of the inner workings of contrastive VLMs, we conducted experiments that allowed us to control the amount of shared information between the modalities. These experiments revealed that the driving factor behind both the modality gap and the object bias, is an information imbalance between images and captions, and unveiled an intriguing connection between the modality gap and entropy of the logits.</p>
            <p id="subjects-uAFHCZRmXk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-uAFHCZRmXk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uAFHCZRmXk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uAFHCZRmXk@OpenReview" onclick="foldPdfKimi('uAFHCZRmXk@OpenReview', this)" class="hr hr-fold">
        </div><div id="rwqShzb9li@OpenReview" class="panel paper" keywords="ideological,perspective,slant,subjective,heads,liberal,politicians,perspectives,activation,llm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rwqShzb9li" target="_blank" title="132/207"><span class="index notranslate">#132</span></a>
                <a id="title-rwqShzb9li@OpenReview" class="title-link" href="/venue/rwqShzb9li@OpenReview" target="_blank">Linear Representations of Political Perspective Emerge in Large Language Models</a>
                <a id="pdf-rwqShzb9li@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rwqShzb9li@OpenReview', this)" data="https://openreview.net/pdf?id=rwqShzb9li">[PDF<sup id="pdf-stars-rwqShzb9li@OpenReview">14</sup>]</a>
                <a id="copy-rwqShzb9li@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rwqShzb9li@OpenReview')">[Copy]</a>
                <a id="kimi-rwqShzb9li@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rwqShzb9li@OpenReview', this)">[Kimi<sup id="kimi-stars-rwqShzb9li@OpenReview">21</sup>]</a>
                <a id="rel-rwqShzb9li@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rwqShzb9li@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rwqShzb9li@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junsol Kim" target="_blank">Junsol Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Evans" target="_blank">James Evans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Schein" target="_blank">Aaron Schein</a>
            </p>
            <p id="summary-rwqShzb9li@OpenReview" class="summary">Large language models (LLMs) have demonstrated the ability to simulate responses aligned with human subjective perspectives, such as liberal or conservative ideologies in American politics. Our study reveals that LLMs achieve this by learning a ``geometry of perspective'' that linearly represents subjective perspectives in the activation space, where similar simulated perspectives are represented closer to each other. Specifically, we probe the hidden layers of open, transformer-based LLMs (\texttt{Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b}) when prompted to generate texts under the ideological perspective of distinct politicians. We find a set of attention heads that represent U.S. ideological slant, which is primarily located in the middle layers known to encode high-level concepts and tasks. The activation of these attention heads, when prompted about U.S.~politicians and media outlets, linearly correlates with existing measures of their ideological slant. We use this activation to detect the ideological slant implicitly adopted by an LLM as it is generating each token. We further show that by intervening on these attention heads, we can tune LLM output to any position along the linear dimension from a liberal to conservative ideological perspective. Our research shows that political ideology serves as a fundamental dimension of LLM representations, and present an interpretability method to identify, monitor, and control the subjective perspective used to generate text. Code: https://osf.io/us9yx/?view_only=cf0fdcdb123e4d6bb7d10a64be5c1a09</p>
            <p id="subjects-rwqShzb9li@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-rwqShzb9li@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rwqShzb9li@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rwqShzb9li@OpenReview" onclick="foldPdfKimi('rwqShzb9li@OpenReview', this)" class="hr hr-fold">
        </div><div id="rfdblE10qm@OpenReview" class="panel paper" keywords="reward,modeling,preference,model,alignment,rethinking,language,bradley,terry,response">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rfdblE10qm" target="_blank" title="133/207"><span class="index notranslate">#133</span></a>
                <a id="title-rfdblE10qm@OpenReview" class="title-link" href="/venue/rfdblE10qm@OpenReview" target="_blank">Rethinking Reward Modeling in Preference-based Large Language Model Alignment</a>
                <a id="pdf-rfdblE10qm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rfdblE10qm@OpenReview', this)" data="https://openreview.net/pdf?id=rfdblE10qm">[PDF<sup id="pdf-stars-rfdblE10qm@OpenReview">8</sup>]</a>
                <a id="copy-rfdblE10qm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rfdblE10qm@OpenReview')">[Copy]</a>
                <a id="kimi-rfdblE10qm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rfdblE10qm@OpenReview', this)">[Kimi<sup id="kimi-stars-rfdblE10qm@OpenReview">28</sup>]</a>
                <a id="rel-rfdblE10qm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rfdblE10qm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rfdblE10qm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Sun" target="_blank">Hao Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunyi Shen" target="_blank">Yunyi Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean-Francois Ton" target="_blank">Jean-Francois Ton</a>
            </p>
            <p id="summary-rfdblE10qm@OpenReview" class="summary">The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear *why* this model --- originally developed for multi-player stochastic game matching --- can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. In this paper, we first establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use.Despite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization, this is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. We highlight the critical concept of *order consistency* in reward modeling and demonstrate that the BT model possesses this property.Moreover, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. To offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-223" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-224"><span class="mn" id="MathJax-Span-225" style="font-family: MathJax_Main;">6</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn></math></span></span><script type="math/tex" id="MathJax-Element-39">6</script> base LLMs, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-226" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-227"><span class="mn" id="MathJax-Span-228" style="font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-40">2</script> datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations.</p>
            <p id="subjects-rfdblE10qm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-rfdblE10qm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rfdblE10qm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rfdblE10qm@OpenReview" onclick="foldPdfKimi('rfdblE10qm@OpenReview', this)" class="hr hr-fold">
        </div><div id="pISLZG7ktL@OpenReview" class="panel paper" keywords="demonstrations,environments,scaling,objects,data,imitation,robotic,manipulation,laws,generalization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pISLZG7ktL" target="_blank" title="134/207"><span class="index notranslate">#134</span></a>
                <a id="title-pISLZG7ktL@OpenReview" class="title-link" href="/venue/pISLZG7ktL@OpenReview" target="_blank">Data Scaling Laws in Imitation Learning for Robotic Manipulation</a>
                <a id="pdf-pISLZG7ktL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pISLZG7ktL@OpenReview', this)" data="https://openreview.net/pdf?id=pISLZG7ktL">[PDF<sup id="pdf-stars-pISLZG7ktL@OpenReview">8</sup>]</a>
                <a id="copy-pISLZG7ktL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pISLZG7ktL@OpenReview')">[Copy]</a>
                <a id="kimi-pISLZG7ktL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pISLZG7ktL@OpenReview', this)">[Kimi<sup id="kimi-stars-pISLZG7ktL@OpenReview">17</sup>]</a>
                <a id="rel-pISLZG7ktL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pISLZG7ktL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pISLZG7ktL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fanqi Lin" target="_blank">Fanqi Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingdong Hu" target="_blank">Yingdong Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pingyue Sheng" target="_blank">Pingyue Sheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuan Wen" target="_blank">Chuan Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacheng You" target="_blank">Jiacheng You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Gao" target="_blank">Yang Gao</a>
            </p>
            <p id="summary-pISLZG7ktL@OpenReview" class="summary">Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policys generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\% success rates in novel environments with unseen objects.</p>
            <p id="subjects-pISLZG7ktL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-pISLZG7ktL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pISLZG7ktL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pISLZG7ktL@OpenReview" onclick="foldPdfKimi('pISLZG7ktL@OpenReview', this)" class="hr hr-fold">
        </div><div id="mtSSFiqW6y@OpenReview" class="panel paper" keywords="judge,draft,405b,speculative,tokens,decoding,target,llama,speedup,alignment">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mtSSFiqW6y" target="_blank" title="135/207"><span class="index notranslate">#135</span></a>
                <a id="title-mtSSFiqW6y@OpenReview" class="title-link" href="/venue/mtSSFiqW6y@OpenReview" target="_blank">Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment</a>
                <a id="pdf-mtSSFiqW6y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mtSSFiqW6y@OpenReview', this)" data="https://openreview.net/pdf?id=mtSSFiqW6y">[PDF<sup id="pdf-stars-mtSSFiqW6y@OpenReview">8</sup>]</a>
                <a id="copy-mtSSFiqW6y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mtSSFiqW6y@OpenReview')">[Copy]</a>
                <a id="kimi-mtSSFiqW6y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mtSSFiqW6y@OpenReview', this)">[Kimi<sup id="kimi-stars-mtSSFiqW6y@OpenReview">17</sup>]</a>
                <a id="rel-mtSSFiqW6y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mtSSFiqW6y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mtSSFiqW6y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gregor Bachmann" target="_blank">Gregor Bachmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sotiris Anagnostidis" target="_blank">Sotiris Anagnostidis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert Pumarola" target="_blank">Albert Pumarola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Markos Georgopoulos" target="_blank">Markos Georgopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Artsiom Sanakoyeu" target="_blank">Artsiom Sanakoyeu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuming Du" target="_blank">Yuming Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edgar Schoenfeld" target="_blank">Edgar Schoenfeld</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Thabet" target="_blank">Ali Thabet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Kohler" target="_blank">Jonas Kohler</a>
            </p>
            <p id="summary-mtSSFiqW6y@OpenReview" class="summary">The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;9&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-229" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-230"><span class="mn" id="MathJax-Span-231" style="font-family: MathJax_Main;">9</span><span class="mo" id="MathJax-Span-232" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-41">9\times</script> over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-42-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;141&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-233" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-234"><span class="mn" id="MathJax-Span-235" style="font-family: MathJax_Main;">141</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>141</mn></math></span></span><script type="math/tex" id="MathJax-Element-42">141</script> tokens/s for 8B/70B-Judge and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-43-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;129&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-236" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-237"><span class="mn" id="MathJax-Span-238" style="font-family: MathJax_Main;">129</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>129</mn></math></span></span><script type="math/tex" id="MathJax-Element-43">129</script> tokens/s for 8B/405B on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-44-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-239" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-240"><span class="mn" id="MathJax-Span-241" style="font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-44">2</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-45-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;8&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-242" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-243"><span class="mn" id="MathJax-Span-244" style="font-family: MathJax_Main;">8</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>8</mn></math></span></span><script type="math/tex" id="MathJax-Element-45">8</script> H100s respectively.</p>
            <p id="subjects-mtSSFiqW6y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-mtSSFiqW6y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mtSSFiqW6y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mtSSFiqW6y@OpenReview" onclick="foldPdfKimi('mtSSFiqW6y@OpenReview', this)" class="hr hr-fold">
        </div><div id="mMPMHWOdOy@OpenReview" class="panel paper" keywords="wizardmath,evol,math,mathematical,reasoning,instruct,mistral,llms,rleif,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mMPMHWOdOy" target="_blank" title="136/207"><span class="index notranslate">#136</span></a>
                <a id="title-mMPMHWOdOy@OpenReview" class="title-link" href="/venue/mMPMHWOdOy@OpenReview" target="_blank">WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</a>
                <a id="pdf-mMPMHWOdOy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mMPMHWOdOy@OpenReview', this)" data="https://openreview.net/pdf?id=mMPMHWOdOy">[PDF<sup id="pdf-stars-mMPMHWOdOy@OpenReview">21</sup>]</a>
                <a id="copy-mMPMHWOdOy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mMPMHWOdOy@OpenReview')">[Copy]</a>
                <a id="kimi-mMPMHWOdOy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mMPMHWOdOy@OpenReview', this)">[Kimi<sup id="kimi-stars-mMPMHWOdOy@OpenReview">27</sup>]</a>
                <a id="rel-mMPMHWOdOy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mMPMHWOdOy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mMPMHWOdOy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haipeng Luo" target="_blank">Haipeng Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingfeng Sun" target="_blank">Qingfeng Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Can Xu" target="_blank">Can Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pu Zhao" target="_blank">Pu Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian-Guang Lou" target="_blank">Jian-Guang Lou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chongyang Tao" target="_blank">Chongyang Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiubo Geng" target="_blank">Xiubo Geng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingwei Lin" target="_blank">Qingwei Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shifeng Chen" target="_blank">Shifeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yansong Tang" target="_blank">Yansong Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongmei Zhang" target="_blank">Dongmei Zhang</a>
            </p>
            <p id="summary-mMPMHWOdOy@OpenReview" class="summary">Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.</p>
            <p id="subjects-mMPMHWOdOy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-mMPMHWOdOy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mMPMHWOdOy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mMPMHWOdOy@OpenReview" onclick="foldPdfKimi('mMPMHWOdOy@OpenReview', this)" class="hr hr-fold">
        </div><div id="jOmk0uS1hl@OpenReview" class="panel paper" keywords="task,test,confounds,training,evaluation,emergent,language,wrongful,malpractice,evaluations">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jOmk0uS1hl" target="_blank" title="137/207"><span class="index notranslate">#137</span></a>
                <a id="title-jOmk0uS1hl@OpenReview" class="title-link" href="/venue/jOmk0uS1hl@OpenReview" target="_blank">Training on the Test Task Confounds Evaluation and Emergence</a>
                <a id="pdf-jOmk0uS1hl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jOmk0uS1hl@OpenReview', this)" data="https://openreview.net/pdf?id=jOmk0uS1hl">[PDF<sup id="pdf-stars-jOmk0uS1hl@OpenReview">4</sup>]</a>
                <a id="copy-jOmk0uS1hl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jOmk0uS1hl@OpenReview')">[Copy]</a>
                <a id="kimi-jOmk0uS1hl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jOmk0uS1hl@OpenReview', this)">[Kimi<sup id="kimi-stars-jOmk0uS1hl@OpenReview">5</sup>]</a>
                <a id="rel-jOmk0uS1hl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jOmk0uS1hl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jOmk0uS1hl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ricardo Dominguez-Olmedo" target="_blank">Ricardo Dominguez-Olmedo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Eddie Dorner" target="_blank">Florian Eddie Dorner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Hardt" target="_blank">Moritz Hardt</a>
            </p>
            <p id="summary-jOmk0uS1hl@OpenReview" class="summary">We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of techniques to include task-relevant data in the pretraining stage of a language model. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data before evaluation. Lastly, we show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models with broad implications for benchmarking and the study of emergent capabilities.</p>
            <p id="subjects-jOmk0uS1hl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-jOmk0uS1hl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jOmk0uS1hl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jOmk0uS1hl@OpenReview" onclick="foldPdfKimi('jOmk0uS1hl@OpenReview', this)" class="hr hr-fold">
        </div><div id="hyfe5q5TD0@OpenReview" class="panel paper" keywords="bellman,computationally,linear,setting,efficient,deterministic,statistically,random,completeness,lqr">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hyfe5q5TD0" target="_blank" title="138/207"><span class="index notranslate">#138</span></a>
                <a id="title-hyfe5q5TD0@OpenReview" class="title-link" href="/venue/hyfe5q5TD0@OpenReview" target="_blank">Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics</a>
                <a id="pdf-hyfe5q5TD0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hyfe5q5TD0@OpenReview', this)" data="https://openreview.net/pdf?id=hyfe5q5TD0">[PDF<sup id="pdf-stars-hyfe5q5TD0@OpenReview">4</sup>]</a>
                <a id="copy-hyfe5q5TD0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hyfe5q5TD0@OpenReview')">[Copy]</a>
                <a id="kimi-hyfe5q5TD0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hyfe5q5TD0@OpenReview', this)">[Kimi<sup id="kimi-stars-hyfe5q5TD0@OpenReview">15</sup>]</a>
                <a id="rel-hyfe5q5TD0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hyfe5q5TD0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hyfe5q5TD0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Runzhe Wu" target="_blank">Runzhe Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayush Sekhari" target="_blank">Ayush Sekhari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Sun" target="_blank">Wen Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akshay Krishnamurthy" target="_blank">Akshay Krishnamurthy</a>
            </p>
            <p id="summary-hyfe5q5TD0@OpenReview" class="summary">We study computationally and statistically efficient Reinforcement Learning algorithms for the *linear Bellman Complete* setting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.</p>
            <p id="subjects-hyfe5q5TD0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-hyfe5q5TD0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hyfe5q5TD0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hyfe5q5TD0@OpenReview" onclick="foldPdfKimi('hyfe5q5TD0@OpenReview', this)" class="hr hr-fold">
        </div><div id="hrqNOxpItr@OpenReview" class="panel paper" keywords="supervised,linear,factors,variation,latent,dislib,disentanglement,generating,invert,representations">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=hrqNOxpItr" target="_blank" title="139/207"><span class="index notranslate">#139</span></a>
                <a id="title-hrqNOxpItr@OpenReview" class="title-link" href="/venue/hrqNOxpItr@OpenReview" target="_blank">Cross-Entropy Is All You Need To Invert the Data Generating Process</a>
                <a id="pdf-hrqNOxpItr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('hrqNOxpItr@OpenReview', this)" data="https://openreview.net/pdf?id=hrqNOxpItr">[PDF<sup id="pdf-stars-hrqNOxpItr@OpenReview">22</sup>]</a>
                <a id="copy-hrqNOxpItr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('hrqNOxpItr@OpenReview')">[Copy]</a>
                <a id="kimi-hrqNOxpItr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('hrqNOxpItr@OpenReview', this)">[Kimi<sup id="kimi-stars-hrqNOxpItr@OpenReview">35</sup>]</a>
                <a id="rel-hrqNOxpItr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('hrqNOxpItr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-hrqNOxpItr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Patrik Reizinger" target="_blank">Patrik Reizinger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alice Bizeul" target="_blank">Alice Bizeul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Attila Juhos" target="_blank">Attila Juhos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julia E Vogt" target="_blank">Julia E Vogt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Randall Balestriero" target="_blank">Randall Balestriero</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wieland Brendel" target="_blank">Wieland Brendel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Klindt" target="_blank">David Klindt</a>
            </p>
            <p id="summary-hrqNOxpItr@OpenReview" class="summary">Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation under a certain DGP. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation.Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised learning.</p>
            <p id="subjects-hrqNOxpItr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-hrqNOxpItr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-hrqNOxpItr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-hrqNOxpItr@OpenReview" onclick="foldPdfKimi('hrqNOxpItr@OpenReview', this)" class="hr hr-fold">
        </div><div id="h0Ak8A5yqw@OpenReview" class="panel paper" keywords="safety,heads,head,attention,mechanisms,ships,language,harmful,attribution,model">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=h0Ak8A5yqw" target="_blank" title="140/207"><span class="index notranslate">#140</span></a>
                <a id="title-h0Ak8A5yqw@OpenReview" class="title-link" href="/venue/h0Ak8A5yqw@OpenReview" target="_blank">On the Role of Attention Heads in Large Language Model Safety</a>
                <a id="pdf-h0Ak8A5yqw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('h0Ak8A5yqw@OpenReview', this)" data="https://openreview.net/pdf?id=h0Ak8A5yqw">[PDF<sup id="pdf-stars-h0Ak8A5yqw@OpenReview">14</sup>]</a>
                <a id="copy-h0Ak8A5yqw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('h0Ak8A5yqw@OpenReview')">[Copy]</a>
                <a id="kimi-h0Ak8A5yqw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('h0Ak8A5yqw@OpenReview', this)">[Kimi<sup id="kimi-stars-h0Ak8A5yqw@OpenReview">15</sup>]</a>
                <a id="rel-h0Ak8A5yqw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('h0Ak8A5yqw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-h0Ak8A5yqw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=zhenhong zhou" target="_blank">zhenhong zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyang Yu" target="_blank">Haiyang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinghua Zhang" target="_blank">Xinghua Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rongwu Xu" target="_blank">Rongwu Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Huang" target="_blank">Fei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wang" target="_blank">Kun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Liu" target="_blank">Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junfeng Fang" target="_blank">Junfeng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongbin Li" target="_blank">Yongbin Li</a>
            </p>
            <p id="summary-h0Ak8A5yqw@OpenReview" class="summary">Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose an novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Base on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to **16<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-46-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2191;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-245" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.57em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-246"><span class="mo" id="MathJax-Span-247" style="font-family: MathJax_Main;"></span><span class="mo" id="MathJax-Span-248" style="font-family: MathJax_Main; padding-left: 0.263em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo><mo stretchy="false"></mo></math></span></span><script type="math/tex" id="MathJax-Element-46">\times\uparrow</script>** more harmful queries, while only modifying **0.006\%** <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-47-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2193;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-249" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-250"><span class="mo" id="MathJax-Span-251" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false"></mo></math></span></span><script type="math/tex" id="MathJax-Element-47">\downarrow</script> of the parameters, in contrast to the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-48-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-252" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-253"><span class="mo" id="MathJax-Span-254" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-48">\sim</script> **5\%** modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms in large models.</p>
            <p id="subjects-h0Ak8A5yqw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-h0Ak8A5yqw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-h0Ak8A5yqw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-h0Ak8A5yqw@OpenReview" onclick="foldPdfKimi('h0Ak8A5yqw@OpenReview', this)" class="hr hr-fold">
        </div><div id="fV0t65OBUu@OpenReview" class="panel paper" keywords="covariance,diffusion,matching,covariances,probabilistic,ocm,optimal,involves,learned,regressing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fV0t65OBUu" target="_blank" title="141/207"><span class="index notranslate">#141</span></a>
                <a id="title-fV0t65OBUu@OpenReview" class="title-link" href="/venue/fV0t65OBUu@OpenReview" target="_blank">Improving Probabilistic Diffusion Models With Optimal Covariance Matching</a>
                <a id="pdf-fV0t65OBUu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fV0t65OBUu@OpenReview', this)" data="https://openreview.net/pdf?id=fV0t65OBUu">[PDF<sup id="pdf-stars-fV0t65OBUu@OpenReview">13</sup>]</a>
                <a id="copy-fV0t65OBUu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fV0t65OBUu@OpenReview')">[Copy]</a>
                <a id="kimi-fV0t65OBUu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fV0t65OBUu@OpenReview', this)">[Kimi<sup id="kimi-stars-fV0t65OBUu@OpenReview">11</sup>]</a>
                <a id="rel-fV0t65OBUu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fV0t65OBUu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fV0t65OBUu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zijing Ou" target="_blank">Zijing Ou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingtian Zhang" target="_blank">Mingtian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andi Zhang" target="_blank">Andi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Xiao" target="_blank">Tim Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingzhen Li" target="_blank">Yingzhen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Barber" target="_blank">David Barber</a>
            </p>
            <p id="summary-fV0t65OBUu@OpenReview" class="summary">The probabilistic diffusion model has become highly effective across various domains. Typically, sampling from a diffusion model involves using a denoising distribution characterized by a Gaussian with a learned mean and either fixed or learned covariances. In this paper, we leverage the recently proposed covariance moment matching technique and introduce a novel method for learning the diagonal covariances. Unlike traditional data-driven covariance approximation approaches, our method involves directly regressing the optimal analytic covariance using a new, unbiased objective named Optimal Covariance Matching (OCM). This approach can significantly reduce the approximation error in covariance prediction. We demonstrate how our method can substantially enhance the sampling efficiency, recall rate and likelihood of both diffusion models and latent diffusion models.</p>
            <p id="subjects-fV0t65OBUu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-fV0t65OBUu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fV0t65OBUu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fV0t65OBUu@OpenReview" onclick="foldPdfKimi('fV0t65OBUu@OpenReview', this)" class="hr hr-fold">
        </div><div id="fMTPkDEhLQ@OpenReview" class="panel paper" keywords="frac,left,right,sigma,hlder,oracle,bounds,uniformly,epsilon,order">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fMTPkDEhLQ" target="_blank" title="142/207"><span class="index notranslate">#142</span></a>
                <a id="title-fMTPkDEhLQ@OpenReview" class="title-link" href="/venue/fMTPkDEhLQ@OpenReview" target="_blank">Tight Lower Bounds under Asymmetric High-Order Hlder Smoothness and Uniform Convexity</a>
                <a id="pdf-fMTPkDEhLQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fMTPkDEhLQ@OpenReview', this)" data="https://openreview.net/pdf?id=fMTPkDEhLQ">[PDF<sup id="pdf-stars-fMTPkDEhLQ@OpenReview">6</sup>]</a>
                <a id="copy-fMTPkDEhLQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fMTPkDEhLQ@OpenReview')">[Copy]</a>
                <a id="kimi-fMTPkDEhLQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fMTPkDEhLQ@OpenReview', this)">[Kimi<sup id="kimi-stars-fMTPkDEhLQ@OpenReview">6</sup>]</a>
                <a id="rel-fMTPkDEhLQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fMTPkDEhLQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fMTPkDEhLQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cedar Site Bai" target="_blank">Cedar Site Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Bullins" target="_blank">Brian Bullins</a>
            </p>
            <p id="summary-fMTPkDEhLQ@OpenReview" class="summary">In this paper, we provide tight lower bounds for the oracle complexity of minimizing high-order Hlder smooth and uniformly convex functions. Specifically, for a function whose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-49-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-255" style="width: 1.513em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.25em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-256"><span class="msubsup" id="MathJax-Span-257"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-258" style="font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.523em;"><span class="texatom" id="MathJax-Span-259"><span class="mrow" id="MathJax-Span-260"><span class="mi" id="MathJax-Span-261" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mi" id="MathJax-Span-262" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-49">p^{th}</script>-order derivatives are Hlder continuous with degree <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-50-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-263" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-264"><span class="mi" id="MathJax-Span-265" style="font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-50">\nu</script> and parameter <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-51-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-266" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-267"><span class="mi" id="MathJax-Span-268" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></span></span><script type="math/tex" id="MathJax-Element-51">H</script>, and that is uniformly convex with degree <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-52-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-269" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-270"><span class="mi" id="MathJax-Span-271" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>q</mi></math></span></span><script type="math/tex" id="MathJax-Element-52">q</script> and parameter <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-53-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-272" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-273"><span class="mi" id="MathJax-Span-274" style="font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-53">\sigma</script>, we focus on two asymmetric cases: (1) <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-54-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-275" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1004.01em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-276"><span class="mi" id="MathJax-Span-277" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-278" style="font-family: MathJax_Main; padding-left: 0.263em;">&gt;</span><span class="mi" id="MathJax-Span-279" style="font-family: MathJax_Math-italic; padding-left: 0.263em;">p</span><span class="mo" id="MathJax-Span-280" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-281" style="font-family: MathJax_Math-italic; padding-left: 0.211em;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>q</mi><mo>&gt;</mo><mi>p</mi><mo>+</mo><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-54">q > p + \nu</script>, and (2) <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-55-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-282" style="width: 4.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1004.01em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-283"><span class="mi" id="MathJax-Span-284" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-285" style="font-family: MathJax_Main; padding-left: 0.263em;">&lt;</span><span class="mi" id="MathJax-Span-286" style="font-family: MathJax_Math-italic; padding-left: 0.263em;">p</span><span class="mo" id="MathJax-Span-287" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-288" style="font-family: MathJax_Math-italic; padding-left: 0.211em;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>q</mi><mo>&lt;</mo><mi>p</mi><mo>+</mo><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-55">q < p+\nu</script>. Given up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-56-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-289" style="width: 1.513em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.25em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-290"><span class="msubsup" id="MathJax-Span-291"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-292" style="font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.523em;"><span class="texatom" id="MathJax-Span-293"><span class="mrow" id="MathJax-Span-294"><span class="mi" id="MathJax-Span-295" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mi" id="MathJax-Span-296" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-56">p^{th}</script>-order oracle access, we establish worst-case oracle complexities of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-57-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mfrac&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-297" style="width: 14.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.773em, 1011.51em, 4.482em, -999.997em); top: -3.383em; left: 0em;"><span class="mrow" id="MathJax-Span-298"><span class="mi" id="MathJax-Span-299" style="font-family: MathJax_Main;"></span><span class="mrow" id="MathJax-Span-300" style="padding-left: 0.159em;"><span class="mo" id="MathJax-Span-301" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="msubsup" id="MathJax-Span-302"><span style="display: inline-block; position: relative; width: 4.482em; height: 0px;"><span style="position: absolute; clip: rect(1.201em, 1001.72em, 2.763em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-303"><span class="mo" id="MathJax-Span-304" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mfrac" id="MathJax-Span-305"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.31em;"><span class="mi" id="MathJax-Span-306" style="font-size: 70.7%; font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-307" style="font-size: 70.7%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.73em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.732em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-308" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span><span style="position: absolute; top: -4.633em; left: 1.878em;"><span class="mfrac" id="MathJax-Span-309"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.445em; left: 50%; margin-left: -0.102em;"><span class="mn" id="MathJax-Span-310" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.617em, 1002.19em, 2.398em, -999.997em); top: -1.768em; left: 50%; margin-left: -1.091em;"><span class="mrow" id="MathJax-Span-311"><span class="mn" id="MathJax-Span-312" style="font-size: 50%; font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-313" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-314" style="font-size: 50%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-315" style="font-size: 50%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-316" style="font-size: 50%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-317" style="font-size: 50%; font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-318" style="font-size: 50%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-319" style="font-size: 50%; font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1002.29em, 1.201em, -999.997em); top: -1.195em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.294em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-320"><span style="display: inline-block; position: relative; width: 4.898em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1001.51em, 2.763em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-321"><span class="mo" id="MathJax-Span-322" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mfrac" id="MathJax-Span-323"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-324" style="font-size: 70.7%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.32em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.154em;"><span class="mi" id="MathJax-Span-325" style="font-size: 70.7%; font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.52em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.523em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-326" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span><span style="position: absolute; top: -4.581em; left: 1.669em;"><span class="mfrac" id="MathJax-Span-327"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.617em, 1002.09em, 2.398em, -999.997em); top: -2.602em; left: 50%; margin-left: -1.091em;"><span class="mrow" id="MathJax-Span-328"><span class="mn" id="MathJax-Span-329" style="font-size: 50%; font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-330" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-331" style="font-size: 50%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-332" style="font-size: 50%; font-family: MathJax_Main;"></span><span class="mi" id="MathJax-Span-333" style="font-size: 50%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-334" style="font-size: 50%; font-family: MathJax_Main;"></span><span class="mi" id="MathJax-Span-335" style="font-size: 50%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-336" style="font-size: 50%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.617em, 1002.76em, 2.398em, -999.997em); top: -1.768em; left: 50%; margin-left: -1.404em;"><span class="mrow" id="MathJax-Span-337"><span class="mi" id="MathJax-Span-338" style="font-size: 50%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-339" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-340" style="font-size: 50%; font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-341" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-342" style="font-size: 50%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-343" style="font-size: 50%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-344" style="font-size: 50%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-345" style="font-size: 50%; font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-346" style="font-size: 50%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-347" style="font-size: 50%; font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-348" style="font-size: 50%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1002.92em, 1.201em, -999.997em); top: -1.195em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.919em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-349" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.388em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.184em; border-left: 0px solid; width: 0px; height: 3.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal"></mi><mrow><mo>(</mo><msup><mrow><mo>(</mo><mfrac><mi>H</mi><mi></mi></mfrac><mo>)</mo></mrow><mfrac><mn>2</mn><mrow><mn>3</mn><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi></mi><mo stretchy="false">)</mo><mo></mo><mn>2</mn></mrow></mfrac></msup><msup><mrow><mo>(</mo><mfrac><mi></mi><mi></mi></mfrac><mo>)</mo></mrow><mfrac><mrow><mn>2</mn><mo stretchy="false">(</mo><mi>q</mi><mo></mo><mi>p</mi><mo></mo><mi></mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi></mi><mo stretchy="false">)</mo><mo></mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mfrac></msup><mo>)</mo></mrow></math></span></span><script type="math/tex" id="MathJax-Element-57">\Omega\left( \left( \frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}\left( \frac{\sigma}{\epsilon}\right)^\frac{2(q-p-\nu)}{q(3(p+\nu)-2)}\right)</script> in the first case with an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-58-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x221E;&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-350" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-351"><span class="msubsup" id="MathJax-Span-352"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-353" style="font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.419em;"><span class="mi" id="MathJax-Span-354" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi></mi><mi mathvariant="normal"></mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-58">\ell_\infty</script>-ball-truncated-Gaussian smoothed hard function and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-59-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mfrac&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x03BD;&lt;/mi&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/msup&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-355" style="width: 21.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 17.763em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1017.55em, 4.482em, -999.997em); top: -3.383em; left: 0em;"><span class="mrow" id="MathJax-Span-356"><span class="mi" id="MathJax-Span-357" style="font-family: MathJax_Main;"></span><span class="mrow" id="MathJax-Span-358" style="padding-left: 0.159em;"><span class="mo" id="MathJax-Span-359" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="msubsup" id="MathJax-Span-360"><span style="display: inline-block; position: relative; width: 4.482em; height: 0px;"><span style="position: absolute; clip: rect(1.201em, 1001.72em, 2.763em, -999.997em); top: -2.237em; left: 0em;"><span class="mrow" id="MathJax-Span-361"><span class="mo" id="MathJax-Span-362" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mfrac" id="MathJax-Span-363"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.31em;"><span class="mi" id="MathJax-Span-364" style="font-size: 70.7%; font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.206em;"><span class="mi" id="MathJax-Span-365" style="font-size: 70.7%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.73em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.732em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-366" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span style="display: inline-block; width: 0px; height: 2.242em;"></span></span><span style="position: absolute; top: -4.633em; left: 1.878em;"><span class="mfrac" id="MathJax-Span-367"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.445em; left: 50%; margin-left: -0.102em;"><span class="mn" id="MathJax-Span-368" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.617em, 1002.19em, 2.398em, -999.997em); top: -1.768em; left: 50%; margin-left: -1.091em;"><span class="mrow" id="MathJax-Span-369"><span class="mn" id="MathJax-Span-370" style="font-size: 50%; font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-371" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-372" style="font-size: 50%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-373" style="font-size: 50%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-374" style="font-size: 50%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-375" style="font-size: 50%; font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-376" style="font-size: 50%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-377" style="font-size: 50%; font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1002.29em, 1.201em, -999.997em); top: -1.195em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.294em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-378" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="mi" id="MathJax-Span-379" style="font-family: MathJax_Main; padding-left: 0.211em;">log</span><span class="mo" id="MathJax-Span-380"></span><span class="mi" id="MathJax-Span-381" style="font-family: MathJax_Main; padding-left: 0.159em;">log</span><span class="mo" id="MathJax-Span-382"></span><span class="mrow" id="MathJax-Span-383"><span class="mo" id="MathJax-Span-384" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="msubsup" id="MathJax-Span-385"><span style="display: inline-block; position: relative; width: 4.846em; height: 0px;"><span style="position: absolute; clip: rect(1.93em, 1002.76em, 4.013em, -999.997em); top: -3.227em; left: 0em;"><span class="mrow" id="MathJax-Span-386"><span class="mo" id="MathJax-Span-387" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mfrac" id="MathJax-Span-388"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.284em, 1001.41em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.674em;"><span class="msubsup" id="MathJax-Span-389"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-390" style="font-size: 70.7%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.393em; left: 0.471em;"><span class="texatom" id="MathJax-Span-391"><span class="mrow" id="MathJax-Span-392"><span class="mi" id="MathJax-Span-393" style="font-size: 50%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-394" style="font-size: 50%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-395" style="font-size: 50%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.336em, 1000.94em, 4.169em, -999.997em); top: -3.591em; left: 50%; margin-left: -0.466em;"><span class="msubsup" id="MathJax-Span-396"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-397" style="font-size: 70.7%; font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.445em; left: 0.68em;"><span class="mi" id="MathJax-Span-398" style="font-size: 50%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.51em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.513em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-399" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.232em;"></span></span><span style="position: absolute; top: -4.893em; left: 2.919em;"><span class="mfrac" id="MathJax-Span-400"><span style="display: inline-block; position: relative; width: 1.617em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.669em, 1000.21em, 2.294em, -999.997em); top: -2.445em; left: 50%; margin-left: -0.102em;"><span class="mn" id="MathJax-Span-401" style="font-size: 50%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1001.51em, 2.398em, -999.997em); top: -1.872em; left: 50%; margin-left: -0.727em;"><span class="mrow" id="MathJax-Span-402"><span class="mi" id="MathJax-Span-403" style="font-size: 50%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-404" style="font-size: 50%; font-family: MathJax_Main;">+</span><span class="mi" id="MathJax-Span-405" style="font-size: 50%; font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-406" style="font-size: 50%; font-family: MathJax_Main;"></span><span class="mi" id="MathJax-Span-407" style="font-size: 50%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1001.62em, 1.201em, -999.997em); top: -1.195em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.617em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfrac" id="MathJax-Span-408"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(1.513em, 1000.32em, 2.294em, -999.997em); top: -2.549em; left: 50%; margin-left: -0.206em;"><span class="mn" id="MathJax-Span-409" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.32em, 2.294em, -999.997em); top: -1.768em; left: 50%; margin-left: -0.154em;"><span class="mi" id="MathJax-Span-410" style="font-size: 70.7%; font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span><span class="mo" id="MathJax-Span-411" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span></span><span class="mo" id="MathJax-Span-412" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.388em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.184em; border-left: 0px solid; width: 0px; height: 3.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal"></mi><mrow><mo>(</mo><msup><mrow><mo>(</mo><mfrac><mi>H</mi><mi></mi></mfrac><mo>)</mo></mrow><mfrac><mn>2</mn><mrow><mn>3</mn><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi></mi><mo stretchy="false">)</mo><mo></mo><mn>2</mn></mrow></mfrac></msup><mo>+</mo><mi>log</mi><mo></mo><mi>log</mi><mo></mo><mrow><mo>(</mo><msup><mrow><mo>(</mo><mfrac><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mi>p</mi><mo>+</mo><mi></mi></mrow></msup><msup><mi>H</mi><mi>q</mi></msup></mfrac><mo>)</mo></mrow><mfrac><mn>1</mn><mrow><mi>p</mi><mo>+</mo><mi></mi><mo></mo><mi>q</mi></mrow></mfrac></msup><mfrac><mn>1</mn><mi></mi></mfrac><mo>)</mo></mrow><mo>)</mo></mrow></math></span></span><script type="math/tex" id="MathJax-Element-59">\Omega\left(\left(\frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}+ \log\log\left(\left(\frac{\sigma^{p+\nu}}{H^q}\right)^\frac{1}{p+\nu-q}\frac{1}{\epsilon}\right)\right)</script> in the second case, for reaching an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-60-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-413" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-414"><span class="mi" id="MathJax-Span-415" style="font-family: MathJax_Math-italic;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-60">\epsilon</script>-approximate solution in terms of the optimality gap. Our analysis generalizes previous lower bounds for functions under first- and second-order smoothness as well as those for uniformly convex functions, and furthermore our results match the corresponding upper bounds in this general setting.</p>
            <p id="subjects-fMTPkDEhLQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-fMTPkDEhLQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fMTPkDEhLQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fMTPkDEhLQ@OpenReview" onclick="foldPdfKimi('fMTPkDEhLQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="fAAaT826Vv@OpenReview" class="panel paper" keywords="bird,llm,trustworthy,probabilities,abductions,estimations,bayesian,accurate,inference,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fAAaT826Vv" target="_blank" title="143/207"><span class="index notranslate">#143</span></a>
                <a id="title-fAAaT826Vv@OpenReview" class="title-link" href="/venue/fAAaT826Vv@OpenReview" target="_blank">BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models</a>
                <a id="pdf-fAAaT826Vv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fAAaT826Vv@OpenReview', this)" data="https://openreview.net/pdf?id=fAAaT826Vv">[PDF<sup id="pdf-stars-fAAaT826Vv@OpenReview">12</sup>]</a>
                <a id="copy-fAAaT826Vv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fAAaT826Vv@OpenReview')">[Copy]</a>
                <a id="kimi-fAAaT826Vv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fAAaT826Vv@OpenReview', this)">[Kimi<sup id="kimi-stars-fAAaT826Vv@OpenReview">17</sup>]</a>
                <a id="rel-fAAaT826Vv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fAAaT826Vv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fAAaT826Vv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Feng" target="_blank">Yu Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Zhou" target="_blank">Ben Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weidong Lin" target="_blank">Weidong Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Roth" target="_blank">Dan Roth</a>
            </p>
            <p id="summary-fAAaT826Vv@OpenReview" class="summary">Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision making and planning tasks. Current large language models (LLM) are insufficient for such accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates can further contribute to better and more trustworthy decision-making.</p>
            <p id="subjects-fAAaT826Vv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-fAAaT826Vv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fAAaT826Vv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fAAaT826Vv@OpenReview" onclick="foldPdfKimi('fAAaT826Vv@OpenReview', this)" class="hr hr-fold">
        </div><div id="Wr3UuEx72f@OpenReview" class="panel paper" keywords="larp,tokenization,autoregressive,video,discrete,tokens,tokenizing,generation,videos,prior">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Wr3UuEx72f" target="_blank" title="144/207"><span class="index notranslate">#144</span></a>
                <a id="title-Wr3UuEx72f@OpenReview" class="title-link" href="/venue/Wr3UuEx72f@OpenReview" target="_blank">LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior</a>
                <a id="pdf-Wr3UuEx72f@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Wr3UuEx72f@OpenReview', this)" data="https://openreview.net/pdf?id=Wr3UuEx72f">[PDF<sup id="pdf-stars-Wr3UuEx72f@OpenReview">12</sup>]</a>
                <a id="copy-Wr3UuEx72f@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Wr3UuEx72f@OpenReview')">[Copy]</a>
                <a id="kimi-Wr3UuEx72f@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Wr3UuEx72f@OpenReview', this)">[Kimi<sup id="kimi-stars-Wr3UuEx72f@OpenReview">15</sup>]</a>
                <a id="rel-Wr3UuEx72f@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Wr3UuEx72f@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Wr3UuEx72f@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyu Wang" target="_blank">Hanyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saksham Suri" target="_blank">Saksham Suri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Ren" target="_blank">Yixuan Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhinav Shrivastava" target="_blank">Abhinav Shrivastava</a>
            </p>
            <p id="summary-Wr3UuEx72f@OpenReview" class="summary">We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARPs strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs). Code and checkpoints will be released.</p>
            <p id="subjects-Wr3UuEx72f@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Wr3UuEx72f@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wr3UuEx72f@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wr3UuEx72f@OpenReview" onclick="foldPdfKimi('Wr3UuEx72f@OpenReview', this)" class="hr hr-fold">
        </div><div id="WJaUkwci9o@OpenReview" class="panel paper" keywords="improvement,self,sharpening,sft,rlhf,language,amortizing,capabilities,generating,sequences">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WJaUkwci9o" target="_blank" title="145/207"><span class="index notranslate">#145</span></a>
                <a id="title-WJaUkwci9o@OpenReview" class="title-link" href="/venue/WJaUkwci9o@OpenReview" target="_blank">Self-Improvement in Language Models: The Sharpening Mechanism</a>
                <a id="pdf-WJaUkwci9o@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WJaUkwci9o@OpenReview', this)" data="https://openreview.net/pdf?id=WJaUkwci9o">[PDF<sup id="pdf-stars-WJaUkwci9o@OpenReview">15</sup>]</a>
                <a id="copy-WJaUkwci9o@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WJaUkwci9o@OpenReview')">[Copy]</a>
                <a id="kimi-WJaUkwci9o@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WJaUkwci9o@OpenReview', this)">[Kimi<sup id="kimi-stars-WJaUkwci9o@OpenReview">27</sup>]</a>
                <a id="rel-WJaUkwci9o@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WJaUkwci9o@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WJaUkwci9o@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Audrey Huang" target="_blank">Audrey Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Block" target="_blank">Adam Block</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dylan Foster" target="_blank">Dylan Foster</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dhruv Rohatgi" target="_blank">Dhruv Rohatgi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cyril Zhang" target="_blank">Cyril Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Simchowitz" target="_blank">Max Simchowitz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jordan Ash" target="_blank">Jordan Ash</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akshay Krishnamurthy" target="_blank">Akshay Krishnamurthy</a>
            </p>
            <p id="summary-WJaUkwci9o@OpenReview" class="summary">Recent work in language modeling has raised the possibility of self-improvement, where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as sharpening. Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to sharpen the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.</p>
            <p id="subjects-WJaUkwci9o@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-WJaUkwci9o@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WJaUkwci9o@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WJaUkwci9o@OpenReview" onclick="foldPdfKimi('WJaUkwci9o@OpenReview', this)" class="hr hr-fold">
        </div><div id="V4K9h1qNxE@OpenReview" class="panel paper" keywords="hypernetwork,compositional,compositions,latent,generalization,attention,transformers,encountered,head,unseen">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=V4K9h1qNxE" target="_blank" title="146/207"><span class="index notranslate">#146</span></a>
                <a id="title-V4K9h1qNxE@OpenReview" class="title-link" href="/venue/V4K9h1qNxE@OpenReview" target="_blank">Attention as a Hypernetwork</a>
                <a id="pdf-V4K9h1qNxE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('V4K9h1qNxE@OpenReview', this)" data="https://openreview.net/pdf?id=V4K9h1qNxE">[PDF<sup id="pdf-stars-V4K9h1qNxE@OpenReview">21</sup>]</a>
                <a id="copy-V4K9h1qNxE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('V4K9h1qNxE@OpenReview')">[Copy]</a>
                <a id="kimi-V4K9h1qNxE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('V4K9h1qNxE@OpenReview', this)">[Kimi<sup id="kimi-stars-V4K9h1qNxE@OpenReview">29</sup>]</a>
                <a id="rel-V4K9h1qNxE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('V4K9h1qNxE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-V4K9h1qNxE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Schug" target="_blank">Simon Schug</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seijin Kobayashi" target="_blank">Seijin Kobayashi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yassir Akram" target="_blank">Yassir Akram</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joao Sacramento" target="_blank">Joao Sacramento</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Razvan Pascanu" target="_blank">Razvan Pascanu</a>
            </p>
            <p id="summary-V4K9h1qNxE@OpenReview" class="summary">Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training, but whose compositions have not.What mechanisms underlie this ability for compositional generalization?By reformulating multi-head attention as a hypernetwork, we reveal that a composable, low-dimensional latent code specifies key-query specific operations.We find empirically that this latent code is predictive of the subtasks the network performs on unseen task compositions, revealing that latent codes acquired during training are reused to solve unseen problem instances.To further examine the hypothesis that the intrinsic hypernetwork of multi-head attention supports compositional generalization, we ablate whether making the hypernetwork generated linear value network nonlinear strengthens compositionality.We find that this modification improves compositional generalization on abstract reasoning tasks.In particular, we introduce a symbolic version of the Raven's Progressive Matrices human intelligence test, which gives us precise control over the problem compositions encountered during training and evaluation.We demonstrate on this task how scaling model size and data enables compositional generalization in transformers and gives rise to a functionally structured latent space.</p>
            <p id="subjects-V4K9h1qNxE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-V4K9h1qNxE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-V4K9h1qNxE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-V4K9h1qNxE@OpenReview" onclick="foldPdfKimi('V4K9h1qNxE@OpenReview', this)" class="hr hr-fold">
        </div><div id="UV5p3JZMjC@OpenReview" class="panel paper" keywords="randomization,randomized,algorithms,transformers,instilled,remarkable,adversarial,deterministic,strategies,transformer">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UV5p3JZMjC" target="_blank" title="147/207"><span class="index notranslate">#147</span></a>
                <a id="title-UV5p3JZMjC@OpenReview" class="title-link" href="/venue/UV5p3JZMjC@OpenReview" target="_blank">Learning Randomized Algorithms with Transformers</a>
                <a id="pdf-UV5p3JZMjC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UV5p3JZMjC@OpenReview', this)" data="https://openreview.net/pdf?id=UV5p3JZMjC">[PDF<sup id="pdf-stars-UV5p3JZMjC@OpenReview">9</sup>]</a>
                <a id="copy-UV5p3JZMjC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UV5p3JZMjC@OpenReview')">[Copy]</a>
                <a id="kimi-UV5p3JZMjC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UV5p3JZMjC@OpenReview', this)">[Kimi<sup id="kimi-stars-UV5p3JZMjC@OpenReview">18</sup>]</a>
                <a id="rel-UV5p3JZMjC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UV5p3JZMjC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UV5p3JZMjC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Johannes von Oswald" target="_blank">Johannes von Oswald</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seijin Kobayashi" target="_blank">Seijin Kobayashi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yassir Akram" target="_blank">Yassir Akram</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angelika Steger" target="_blank">Angelika Steger</a>
            </p>
            <p id="summary-UV5p3JZMjC@OpenReview" class="summary">Randomization is a powerful tool that endows algorithms with remarkable properties. For instance, randomized algorithms excel in adversarial settings, often surpassing the worst-case performance of deterministic algorithms with large margins. Furthermore, their success probability can be amplified by simple strategies such as repetition and majority voting. In this paper, we enhance deep neural networks, in particular transformer models, with randomization. We demonstrate for the first time that randomized algorithms can be instilled in transformers through learning, in a purely data- and objective-driven manner. First, we analyze known adversarial objectives for which randomized algorithms offer a distinct advantage over deterministic ones. We then show that common optimization techniques, such as gradient descent or evolutionary strategies, can effectively learn transformer parameters that make use of the randomness provided to the model. To illustrate the broad applicability of randomization in empowering neural networks, we study three conceptual tasks: associative recall, graph coloring, and agents that explore grid worlds. In addition to demonstrating increased robustness against oblivious adversaries through learned randomization, our experiments reveal remarkable performance improvements due to the inherently random nature of the neural networks' computation and predictions.</p>
            <p id="subjects-UV5p3JZMjC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-UV5p3JZMjC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UV5p3JZMjC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UV5p3JZMjC@OpenReview" onclick="foldPdfKimi('UV5p3JZMjC@OpenReview', this)" class="hr hr-fold">
        </div><div id="TwJrTz9cRS@OpenReview" class="panel paper" keywords="hira,rank,hadamard,adaptation,lora,language,peft,tasks,parameter,efficient">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=TwJrTz9cRS" target="_blank" title="148/207"><span class="index notranslate">#148</span></a>
                <a id="title-TwJrTz9cRS@OpenReview" class="title-link" href="/venue/TwJrTz9cRS@OpenReview" target="_blank">HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models</a>
                <a id="pdf-TwJrTz9cRS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('TwJrTz9cRS@OpenReview', this)" data="https://openreview.net/pdf?id=TwJrTz9cRS">[PDF<sup id="pdf-stars-TwJrTz9cRS@OpenReview">16</sup>]</a>
                <a id="copy-TwJrTz9cRS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('TwJrTz9cRS@OpenReview')">[Copy]</a>
                <a id="kimi-TwJrTz9cRS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('TwJrTz9cRS@OpenReview', this)">[Kimi<sup id="kimi-stars-TwJrTz9cRS@OpenReview">18</sup>]</a>
                <a id="rel-TwJrTz9cRS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('TwJrTz9cRS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-TwJrTz9cRS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiushi Huang" target="_blank">Qiushi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Ko" target="_blank">Tom Ko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhan ZHUANG" target="_blank">Zhan ZHUANG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lilian Tang" target="_blank">Lilian Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Zhang" target="_blank">Yu Zhang</a>
            </p>
            <p id="summary-TwJrTz9cRS@OpenReview" class="summary">We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new tasks. HiRA addresses this by using a Hadamard product to retain high-rank update parameters, improving the model capacity. Empirically, HiRA outperforms LoRA and its variants on several tasks, with extensive ablation studies validating its effectiveness. Our code will be released.</p>
            <p id="subjects-TwJrTz9cRS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-TwJrTz9cRS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-TwJrTz9cRS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-TwJrTz9cRS@OpenReview" onclick="foldPdfKimi('TwJrTz9cRS@OpenReview', this)" class="hr hr-fold">
        </div><div id="SctfBCLmWo@OpenReview" class="panel paper" keywords="dataset,decade,torralba,battle,bias,yfcc,datacomp,efros,hopefully,rethink">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SctfBCLmWo" target="_blank" title="149/207"><span class="index notranslate">#149</span></a>
                <a id="title-SctfBCLmWo@OpenReview" class="title-link" href="/venue/SctfBCLmWo@OpenReview" target="_blank">A Decade's Battle on Dataset Bias: Are We There Yet?</a>
                <a id="pdf-SctfBCLmWo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SctfBCLmWo@OpenReview', this)" data="https://openreview.net/pdf?id=SctfBCLmWo">[PDF<sup id="pdf-stars-SctfBCLmWo@OpenReview">11</sup>]</a>
                <a id="copy-SctfBCLmWo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SctfBCLmWo@OpenReview')">[Copy]</a>
                <a id="kimi-SctfBCLmWo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SctfBCLmWo@OpenReview', this)">[Kimi<sup id="kimi-stars-SctfBCLmWo@OpenReview">18</sup>]</a>
                <a id="rel-SctfBCLmWo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SctfBCLmWo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SctfBCLmWo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuang Liu" target="_blank">Zhuang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiming He" target="_blank">Kaiming He</a>
            </p>
            <p id="summary-SctfBCLmWo@OpenReview" class="summary">We revisit the ``dataset classification'' experiment suggested by Torralba &amp; Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be explained by memorization. We hope our discovery will inspire the community to rethink issues involving dataset bias.</p>
            <p id="subjects-SctfBCLmWo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-SctfBCLmWo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SctfBCLmWo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SctfBCLmWo@OpenReview" onclick="foldPdfKimi('SctfBCLmWo@OpenReview', this)" class="hr hr-fold">
        </div><div id="SPS6HzVzyt@OpenReview" class="panel paper" keywords="context,instruction,finetuning,reliance,parametric,inversion,follow,phenomenon,instructions,tulu">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SPS6HzVzyt" target="_blank" title="150/207"><span class="index notranslate">#150</span></a>
                <a id="title-SPS6HzVzyt@OpenReview" class="title-link" href="/venue/SPS6HzVzyt@OpenReview" target="_blank">Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance</a>
                <a id="pdf-SPS6HzVzyt@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SPS6HzVzyt@OpenReview', this)" data="https://openreview.net/pdf?id=SPS6HzVzyt">[PDF<sup id="pdf-stars-SPS6HzVzyt@OpenReview">8</sup>]</a>
                <a id="copy-SPS6HzVzyt@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SPS6HzVzyt@OpenReview')">[Copy]</a>
                <a id="kimi-SPS6HzVzyt@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SPS6HzVzyt@OpenReview', this)">[Kimi<sup id="kimi-stars-SPS6HzVzyt@OpenReview">11</sup>]</a>
                <a id="rel-SPS6HzVzyt@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SPS6HzVzyt@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SPS6HzVzyt@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sachin Goyal" target="_blank">Sachin Goyal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christina Baek" target="_blank">Christina Baek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=J Kolter" target="_blank">J Kolter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditi Raghunathan" target="_blank">Aditi Raghunathan</a>
            </p>
            <p id="summary-SPS6HzVzyt@OpenReview" class="summary">Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-61-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;despite an initial expected increase&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-416" style="width: 17.919em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1014.9em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-417"><span class="texatom" id="MathJax-Span-418"><span class="mrow" id="MathJax-Span-419"><span class="mtext" id="MathJax-Span-420" style="font-family: MathJax_Main-italic;">despite an initial expected increase</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">despite an initial expected increase</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-61">\textit{despite an initial expected increase}</script>. We call this phenomenon as the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-62-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;context-parametric inversion&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-421" style="width: 17.346em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.43em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1014.43em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-422"><span class="texatom" id="MathJax-Span-423"><span class="mrow" id="MathJax-Span-424"><span class="mtext" id="MathJax-Span-425" style="font-family: MathJax_Main-bold;">context-parametric inversion</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">context-parametric inversion</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-62">\textbf{context-parametric inversion}</script>. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions. We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia. We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. We tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.</p>
            <p id="subjects-SPS6HzVzyt@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-SPS6HzVzyt@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SPS6HzVzyt@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SPS6HzVzyt@OpenReview" onclick="foldPdfKimi('SPS6HzVzyt@OpenReview', this)" class="hr hr-fold">
        </div><div id="SI2hI0frk6@OpenReview" class="panel paper" keywords="transfusion,modal,token,images,recipe,language,modality,tokens,reaping,multi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SI2hI0frk6" target="_blank" title="151/207"><span class="index notranslate">#151</span></a>
                <a id="title-SI2hI0frk6@OpenReview" class="title-link" href="/venue/SI2hI0frk6@OpenReview" target="_blank">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</a>
                <a id="pdf-SI2hI0frk6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SI2hI0frk6@OpenReview', this)" data="https://openreview.net/pdf?id=SI2hI0frk6">[PDF<sup id="pdf-stars-SI2hI0frk6@OpenReview">14</sup>]</a>
                <a id="copy-SI2hI0frk6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SI2hI0frk6@OpenReview')">[Copy]</a>
                <a id="kimi-SI2hI0frk6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SI2hI0frk6@OpenReview', this)">[Kimi<sup id="kimi-stars-SI2hI0frk6@OpenReview">17</sup>]</a>
                <a id="rel-SI2hI0frk6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SI2hI0frk6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SI2hI0frk6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chunting Zhou" target="_blank">Chunting Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lili Yu" target="_blank">Lili Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arun Babu" target="_blank">Arun Babu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kushal Tirumala" target="_blank">Kushal Tirumala</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michihiro Yasunaga" target="_blank">Michihiro Yasunaga</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonid Shamis" target="_blank">Leonid Shamis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Kahn" target="_blank">Jacob Kahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuezhe Ma" target="_blank">Xuezhe Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luke Zettlemoyer" target="_blank">Luke Zettlemoyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Omer Levy" target="_blank">Omer Levy</a>
            </p>
            <p id="summary-SI2hI0frk6@OpenReview" class="summary">We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.</p>
            <p id="subjects-SI2hI0frk6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-SI2hI0frk6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SI2hI0frk6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SI2hI0frk6@OpenReview" onclick="foldPdfKimi('SI2hI0frk6@OpenReview', this)" class="hr hr-fold">
        </div><div id="QQBPWtvtcn@OpenReview" class="panel paper" keywords="lvsm,view,synthesis,decoder,novel,inductive,tokens,encoder,latent,scene">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QQBPWtvtcn" target="_blank" title="152/207"><span class="index notranslate">#152</span></a>
                <a id="title-QQBPWtvtcn@OpenReview" class="title-link" href="/venue/QQBPWtvtcn@OpenReview" target="_blank">LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</a>
                <a id="pdf-QQBPWtvtcn@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QQBPWtvtcn@OpenReview', this)" data="https://openreview.net/pdf?id=QQBPWtvtcn">[PDF<sup id="pdf-stars-QQBPWtvtcn@OpenReview">3</sup>]</a>
                <a id="copy-QQBPWtvtcn@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QQBPWtvtcn@OpenReview')">[Copy]</a>
                <a id="kimi-QQBPWtvtcn@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QQBPWtvtcn@OpenReview', this)">[Kimi<sup id="kimi-stars-QQBPWtvtcn@OpenReview">7</sup>]</a>
                <a id="rel-QQBPWtvtcn@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QQBPWtvtcn@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QQBPWtvtcn@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haian Jin" target="_blank">Haian Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwen Jiang" target="_blank">Hanwen Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Tan" target="_blank">Hao Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhang" target="_blank">Kai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai Bi" target="_blank">Sai Bi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyuan Zhang" target="_blank">Tianyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fujun Luan" target="_blank">Fujun Luan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zexiang Xu" target="_blank">Zexiang Xu</a>
            </p>
            <p id="summary-QQBPWtvtcn@OpenReview" class="summary">We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details: https://lvsm-web.github.io/</p>
            <p id="subjects-QQBPWtvtcn@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-QQBPWtvtcn@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QQBPWtvtcn@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QQBPWtvtcn@OpenReview" onclick="foldPdfKimi('QQBPWtvtcn@OpenReview', this)" class="hr hr-fold">
        </div><div id="QFO1asgas2@OpenReview" class="panel paper" keywords="opponent,shaping,advantage,alignment,agents,algorithms,equilibria,beneficial,games,dilemmas">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QFO1asgas2" target="_blank" title="153/207"><span class="index notranslate">#153</span></a>
                <a id="title-QFO1asgas2@OpenReview" class="title-link" href="/venue/QFO1asgas2@OpenReview" target="_blank">Advantage Alignment Algorithms</a>
                <a id="pdf-QFO1asgas2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QFO1asgas2@OpenReview', this)" data="https://openreview.net/pdf?id=QFO1asgas2">[PDF<sup id="pdf-stars-QFO1asgas2@OpenReview">7</sup>]</a>
                <a id="copy-QFO1asgas2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QFO1asgas2@OpenReview')">[Copy]</a>
                <a id="kimi-QFO1asgas2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QFO1asgas2@OpenReview', this)">[Kimi<sup id="kimi-stars-QFO1asgas2@OpenReview">22</sup>]</a>
                <a id="rel-QFO1asgas2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QFO1asgas2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QFO1asgas2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Duque" target="_blank">Juan Duque</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Milad Aghajohari" target="_blank">Milad Aghajohari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Timotheus Cooijmans" target="_blank">Timotheus Cooijmans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Razvan Ciuca" target="_blank">Razvan Ciuca</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Zhang" target="_blank">Tianyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gauthier Gidel" target="_blank">Gauthier Gidel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Courville" target="_blank">Aaron Courville</a>
            </p>
            <p id="summary-QFO1asgas2@OpenReview" class="summary">Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation.</p>
            <p id="subjects-QFO1asgas2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-QFO1asgas2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QFO1asgas2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QFO1asgas2@OpenReview" onclick="foldPdfKimi('QFO1asgas2@OpenReview', this)" class="hr hr-fold">
        </div><div id="NxyfSW6mLK@OpenReview" class="panel paper" keywords="regent,generalist,retrieval,environments,agent,agents,context,pre,adapt,unseen">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NxyfSW6mLK" target="_blank" title="154/207"><span class="index notranslate">#154</span></a>
                <a id="title-NxyfSW6mLK@OpenReview" class="title-link" href="/venue/NxyfSW6mLK@OpenReview" target="_blank">REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments</a>
                <a id="pdf-NxyfSW6mLK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NxyfSW6mLK@OpenReview', this)" data="https://openreview.net/pdf?id=NxyfSW6mLK">[PDF<sup id="pdf-stars-NxyfSW6mLK@OpenReview">13</sup>]</a>
                <a id="copy-NxyfSW6mLK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NxyfSW6mLK@OpenReview')">[Copy]</a>
                <a id="kimi-NxyfSW6mLK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NxyfSW6mLK@OpenReview', this)">[Kimi<sup id="kimi-stars-NxyfSW6mLK@OpenReview">21</sup>]</a>
                <a id="rel-NxyfSW6mLK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NxyfSW6mLK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NxyfSW6mLK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kaustubh Sridhar" target="_blank">Kaustubh Sridhar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Souradeep Dutta" target="_blank">Souradeep Dutta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dinesh Jayaraman" target="_blank">Dinesh Jayaraman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Insup Lee" target="_blank">Insup Lee</a>
            </p>
            <p id="summary-NxyfSW6mLK@OpenReview" class="summary">Do generalist agents only require large models pre-trained on massive amounts of data to rapidly adapt to new environments? We propose a novel approach to pre-train relatively small models and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents.</p>
            <p id="subjects-NxyfSW6mLK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-NxyfSW6mLK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NxyfSW6mLK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NxyfSW6mLK@OpenReview" onclick="foldPdfKimi('NxyfSW6mLK@OpenReview', this)" class="hr hr-fold">
        </div><div id="NO6Tv6QcDs@OpenReview" class="panel paper" keywords="judge,debiasing,evaluation,frontier,llm,labels,preferencing,scalable,beat,costly">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NO6Tv6QcDs" target="_blank" title="155/207"><span class="index notranslate">#155</span></a>
                <a id="title-NO6Tv6QcDs@OpenReview" class="title-link" href="/venue/NO6Tv6QcDs@OpenReview" target="_blank">Limits to scalable evaluation at the frontier: LLM as judge wont beat twice the data</a>
                <a id="pdf-NO6Tv6QcDs@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NO6Tv6QcDs@OpenReview', this)" data="https://openreview.net/pdf?id=NO6Tv6QcDs">[PDF<sup id="pdf-stars-NO6Tv6QcDs@OpenReview">6</sup>]</a>
                <a id="copy-NO6Tv6QcDs@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NO6Tv6QcDs@OpenReview')">[Copy]</a>
                <a id="kimi-NO6Tv6QcDs@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NO6Tv6QcDs@OpenReview', this)">[Kimi<sup id="kimi-stars-NO6Tv6QcDs@OpenReview">15</sup>]</a>
                <a id="rel-NO6Tv6QcDs@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NO6Tv6QcDs@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NO6Tv6QcDs@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Eddie Dorner" target="_blank">Florian Eddie Dorner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vivian Y. Nastl" target="_blank">Vivian Y. Nastl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moritz Hardt" target="_blank">Moritz Hardt</a>
            </p>
            <p id="summary-NO6Tv6QcDs@OpenReview" class="summary">High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide cheap model evaluations. Unfortunately, this method of using models as judges introduces biases, such as self-preferencing, that can distort model comparisons. An emerging family of debiasing tools promises to fix these issues by using a few high quality labels to debias a large number of model judgments. In this paper, we study how far such debiasing methods, in principle, can go. Our main result shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half. Our result speaks to the severe limitations of the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to assess newly released models that are possibly better than the judge. Through an empirical evaluation, we demonstrate that the sample size savings achievable in practice are even more modest than what our theoretical limit suggests. Along the way, our work provides new observations about debiasing methods for model evaluation, and points out promising avenues for future work.</p>
            <p id="subjects-NO6Tv6QcDs@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-NO6Tv6QcDs@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NO6Tv6QcDs@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NO6Tv6QcDs@OpenReview" onclick="foldPdfKimi('NO6Tv6QcDs@OpenReview', this)" class="hr hr-fold">
        </div><div id="N8Oj1XhtYZ@OpenReview" class="panel paper" keywords="sana,text,times,4096,dit,resolution,laptop,1024,image,images">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=N8Oj1XhtYZ" target="_blank" title="156/207"><span class="index notranslate">#156</span></a>
                <a id="title-N8Oj1XhtYZ@OpenReview" class="title-link" href="/venue/N8Oj1XhtYZ@OpenReview" target="_blank">SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers</a>
                <a id="pdf-N8Oj1XhtYZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('N8Oj1XhtYZ@OpenReview', this)" data="https://openreview.net/pdf?id=N8Oj1XhtYZ">[PDF<sup id="pdf-stars-N8Oj1XhtYZ@OpenReview">13</sup>]</a>
                <a id="copy-N8Oj1XhtYZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('N8Oj1XhtYZ@OpenReview')">[Copy]</a>
                <a id="kimi-N8Oj1XhtYZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('N8Oj1XhtYZ@OpenReview', this)">[Kimi<sup id="kimi-stars-N8Oj1XhtYZ@OpenReview">11</sup>]</a>
                <a id="rel-N8Oj1XhtYZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('N8Oj1XhtYZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-N8Oj1XhtYZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Enze Xie" target="_blank">Enze Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junsong Chen" target="_blank">Junsong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyu Chen" target="_blank">Junyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Cai" target="_blank">Han Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Tang" target="_blank">Haotian Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Lin" target="_blank">Yujun Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhekai Zhang" target="_blank">Zhekai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muyang Li" target="_blank">Muyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ligeng Zhu" target="_blank">Ligeng Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Lu" target="_blank">Yao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Han" target="_blank">Song Han</a>
            </p>
            <p id="summary-N8Oj1XhtYZ@OpenReview" class="summary">We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-63-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-426" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-427"><span class="mo" id="MathJax-Span-428" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-63">\times</script>4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-64-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-429" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-430"><span class="mo" id="MathJax-Span-431" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-64">\times</script>, we trained an AE that can compress images 32<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-65-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-432" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-433"><span class="mo" id="MathJax-Span-434" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-65">\times</script>, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-66-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-435" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-436"><span class="mo" id="MathJax-Span-437" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-66">\times</script>1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.</p>
            <p id="subjects-N8Oj1XhtYZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-N8Oj1XhtYZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-N8Oj1XhtYZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-N8Oj1XhtYZ@OpenReview" onclick="foldPdfKimi('N8Oj1XhtYZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="KIgaAqEFHW@OpenReview" class="panel paper" keywords="minictx,texttt,proving,theorem,context,contexts,projects,minif2f,ntp,formal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KIgaAqEFHW" target="_blank" title="157/207"><span class="index notranslate">#157</span></a>
                <a id="title-KIgaAqEFHW@OpenReview" class="title-link" href="/venue/KIgaAqEFHW@OpenReview" target="_blank">miniCTX: Neural Theorem Proving with (Long-)Contexts</a>
                <a id="pdf-KIgaAqEFHW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KIgaAqEFHW@OpenReview', this)" data="https://openreview.net/pdf?id=KIgaAqEFHW">[PDF<sup id="pdf-stars-KIgaAqEFHW@OpenReview">2</sup>]</a>
                <a id="copy-KIgaAqEFHW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KIgaAqEFHW@OpenReview')">[Copy]</a>
                <a id="kimi-KIgaAqEFHW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KIgaAqEFHW@OpenReview', this)">[Kimi<sup id="kimi-stars-KIgaAqEFHW@OpenReview">11</sup>]</a>
                <a id="rel-KIgaAqEFHW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KIgaAqEFHW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KIgaAqEFHW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiewen Hu" target="_blank">Jiewen Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Zhu" target="_blank">Thomas Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean Welleck" target="_blank">Sean Welleck</a>
            </p>
            <p id="summary-KIgaAqEFHW@OpenReview" class="summary">Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-67-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniCTX&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-438" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-439"><span class="texatom" id="MathJax-Span-440"><span class="mrow" id="MathJax-Span-441"><span class="mtext" id="MathJax-Span-442" style="font-family: MathJax_Typewriter;">miniCTX</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniCTX</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-67">\texttt{miniCTX}</script>, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-68-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniCTX&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-443" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-444"><span class="texatom" id="MathJax-Span-445"><span class="mrow" id="MathJax-Span-446"><span class="mtext" id="MathJax-Span-447" style="font-family: MathJax_Typewriter;">miniCTX</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniCTX</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-68">\texttt{miniCTX}</script> contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-69-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniCTX&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-448" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-449"><span class="texatom" id="MathJax-Span-450"><span class="mrow" id="MathJax-Span-451"><span class="mtext" id="MathJax-Span-452" style="font-family: MathJax_Typewriter;">miniCTX</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniCTX</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-69">\texttt{miniCTX}</script>, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-70-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniF2F&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-453" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-454"><span class="texatom" id="MathJax-Span-455"><span class="mrow" id="MathJax-Span-456"><span class="mtext" id="MathJax-Span-457" style="font-family: MathJax_Typewriter;">miniF2F</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniF2F</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-70">\texttt{miniF2F}</script>. Alongside <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-71-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniCTX&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-458" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-459"><span class="texatom" id="MathJax-Span-460"><span class="mrow" id="MathJax-Span-461"><span class="mtext" id="MathJax-Span-462" style="font-family: MathJax_Typewriter;">miniCTX</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniCTX</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-71">\texttt{miniCTX}</script>, we offer <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-72-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;ntp-toolkit&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-463" style="width: 6.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-464"><span class="texatom" id="MathJax-Span-465"><span class="mrow" id="MathJax-Span-466"><span class="mtext" id="MathJax-Span-467" style="font-family: MathJax_Typewriter;">ntp-toolkit</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">ntp-toolkit</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-72">\texttt{ntp-toolkit}</script> for automatically extracting and annotating theorem proving data, making it easy to add new projects into <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-73-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniCTX&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-468" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-469"><span class="texatom" id="MathJax-Span-470"><span class="mrow" id="MathJax-Span-471"><span class="mtext" id="MathJax-Span-472" style="font-family: MathJax_Typewriter;">miniCTX</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniCTX</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-73">\texttt{miniCTX}</script> to ensure that contexts are not seen during training. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-74-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;miniCTX&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-473" style="width: 4.482em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.65em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-474"><span class="texatom" id="MathJax-Span-475"><span class="mrow" id="MathJax-Span-476"><span class="mtext" id="MathJax-Span-477" style="font-family: MathJax_Typewriter;">miniCTX</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">miniCTX</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-74">\texttt{miniCTX}</script> offers a challenging and realistic evaluation of neural theorem provers.</p>
            <p id="subjects-KIgaAqEFHW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-KIgaAqEFHW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KIgaAqEFHW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KIgaAqEFHW@OpenReview" onclick="foldPdfKimi('KIgaAqEFHW@OpenReview', this)" class="hr hr-fold">
        </div><div id="HvSytvg3Jh@OpenReview" class="panel paper" keywords="alphaedit,editing,llms,knowledge,preserved,locating,null,perturbation,language,projection">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HvSytvg3Jh" target="_blank" title="158/207"><span class="index notranslate">#158</span></a>
                <a id="title-HvSytvg3Jh@OpenReview" class="title-link" href="/venue/HvSytvg3Jh@OpenReview" target="_blank">AlphaEdit: Null-Space Constrained Model Editing for Language Models</a>
                <a id="pdf-HvSytvg3Jh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HvSytvg3Jh@OpenReview', this)" data="https://openreview.net/pdf?id=HvSytvg3Jh">[PDF<sup id="pdf-stars-HvSytvg3Jh@OpenReview">18</sup>]</a>
                <a id="copy-HvSytvg3Jh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HvSytvg3Jh@OpenReview')">[Copy]</a>
                <a id="kimi-HvSytvg3Jh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HvSytvg3Jh@OpenReview', this)">[Kimi<sup id="kimi-stars-HvSytvg3Jh@OpenReview">17</sup>]</a>
                <a id="rel-HvSytvg3Jh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HvSytvg3Jh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HvSytvg3Jh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junfeng Fang" target="_blank">Junfeng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Houcheng Jiang" target="_blank">Houcheng Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wang" target="_blank">Kun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunshan Ma" target="_blank">Yunshan Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Shi" target="_blank">Jie Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Wang" target="_blank">Xiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangnan He" target="_blank">Xiangnan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tat-Seng Chua" target="_blank">Tat-Seng Chua</a>
            </p>
            <p id="summary-HvSytvg3Jh@OpenReview" class="summary">Large language models (LLMs) often exhibit hallucinations, producing incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios.To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7% with a single line of additional code for projection solely.</p>
            <p id="subjects-HvSytvg3Jh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-HvSytvg3Jh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HvSytvg3Jh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HvSytvg3Jh@OpenReview" onclick="foldPdfKimi('HvSytvg3Jh@OpenReview', this)" class="hr hr-fold">
        </div><div id="HsHxSN23rM@OpenReview" class="panel paper" keywords="architectures,star,genomes,tailored,frontier,synthesis,transformers,quality,recombined,optimize">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=HsHxSN23rM" target="_blank" title="159/207"><span class="index notranslate">#159</span></a>
                <a id="title-HsHxSN23rM@OpenReview" class="title-link" href="/venue/HsHxSN23rM@OpenReview" target="_blank">STAR: Synthesis of Tailored Architectures</a>
                <a id="pdf-HsHxSN23rM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('HsHxSN23rM@OpenReview', this)" data="https://openreview.net/pdf?id=HsHxSN23rM">[PDF<sup id="pdf-stars-HsHxSN23rM@OpenReview">4</sup>]</a>
                <a id="copy-HsHxSN23rM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('HsHxSN23rM@OpenReview')">[Copy]</a>
                <a id="kimi-HsHxSN23rM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('HsHxSN23rM@OpenReview', this)">[Kimi<sup id="kimi-stars-HsHxSN23rM@OpenReview">6</sup>]</a>
                <a id="rel-HsHxSN23rM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('HsHxSN23rM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-HsHxSN23rM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Armin Thomas" target="_blank">Armin Thomas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rom Parnichkun" target="_blank">Rom Parnichkun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Amini" target="_blank">Alexander Amini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Massaroli" target="_blank">Stefano Massaroli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Poli" target="_blank">Michael Poli</a>
            </p>
            <p id="summary-HsHxSN23rM@OpenReview" class="summary">Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive, with a variety of automated or manual approaches that fall short, due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection patterns, improving over highly-optimized Transformers and striped hybrid models on the frontier of quality, parameter size, and inference cache for autoregressive language modeling.</p>
            <p id="subjects-HsHxSN23rM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-HsHxSN23rM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-HsHxSN23rM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-HsHxSN23rM@OpenReview" onclick="foldPdfKimi('HsHxSN23rM@OpenReview', this)" class="hr hr-fold">
        </div><div id="FpiCLJrSW8@OpenReview" class="panel paper" keywords="trustworthiness,rlhf,preference,attribution,alignment,aligned,human,preferences,verticals,llms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FpiCLJrSW8" target="_blank" title="160/207"><span class="index notranslate">#160</span></a>
                <a id="title-FpiCLJrSW8@OpenReview" class="title-link" href="/venue/FpiCLJrSW8@OpenReview" target="_blank">More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness</a>
                <a id="pdf-FpiCLJrSW8@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FpiCLJrSW8@OpenReview', this)" data="https://openreview.net/pdf?id=FpiCLJrSW8">[PDF<sup id="pdf-stars-FpiCLJrSW8@OpenReview">15</sup>]</a>
                <a id="copy-FpiCLJrSW8@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FpiCLJrSW8@OpenReview')">[Copy]</a>
                <a id="kimi-FpiCLJrSW8@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FpiCLJrSW8@OpenReview', this)">[Kimi<sup id="kimi-stars-FpiCLJrSW8@OpenReview">19</sup>]</a>
                <a id="rel-FpiCLJrSW8@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FpiCLJrSW8@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FpiCLJrSW8@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron J. Li" target="_blank">Aaron J. Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Satyapriya Krishna" target="_blank">Satyapriya Krishna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hima Lakkaraju" target="_blank">Hima Lakkaraju</a>
            </p>
            <p id="summary-FpiCLJrSW8@OpenReview" class="summary">The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely used to align LLMs with labeled human preferences, but its assumed effect on model trustworthiness hasn't been rigorously evaluated. To bridge this knowledge gap, this study investigates how models aligned with general-purpose preference data perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. Our results demonstrate that RLHF on human preferences doesn't automatically guarantee trustworthiness, and reverse effects are often observed. Furthermore, we propose to adapt efficient influence function based data attribution methods to the RLHF setting to better understand the influence of fine-tuning data on individual trustworthiness benchmarks, and show its feasibility by providing our estimated attribution scores. Together, our results underscore the need for more nuanced approaches for model alignment from both the data and framework perspectives, and we hope this research will guide the community towards developing language models that are increasingly capable without sacrificing trustworthiness.</p>
            <p id="subjects-FpiCLJrSW8@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-FpiCLJrSW8@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FpiCLJrSW8@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FpiCLJrSW8@OpenReview" onclick="foldPdfKimi('FpiCLJrSW8@OpenReview', this)" class="hr hr-fold">
        </div><div id="FVuqJt3c4L@OpenReview" class="panel paper" keywords="popt,decoding,pretrained,population,neural,transformer,data,subjects,representations,sparse">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FVuqJt3c4L" target="_blank" title="161/207"><span class="index notranslate">#161</span></a>
                <a id="title-FVuqJt3c4L@OpenReview" class="title-link" href="/venue/FVuqJt3c4L@OpenReview" target="_blank">Population Transformer: Learning Population-level Representations of Neural Activity</a>
                <a id="pdf-FVuqJt3c4L@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FVuqJt3c4L@OpenReview', this)" data="https://openreview.net/pdf?id=FVuqJt3c4L">[PDF<sup id="pdf-stars-FVuqJt3c4L@OpenReview">3</sup>]</a>
                <a id="copy-FVuqJt3c4L@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FVuqJt3c4L@OpenReview')">[Copy]</a>
                <a id="kimi-FVuqJt3c4L@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FVuqJt3c4L@OpenReview', this)">[Kimi<sup id="kimi-stars-FVuqJt3c4L@OpenReview">9</sup>]</a>
                <a id="rel-FVuqJt3c4L@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FVuqJt3c4L@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FVuqJt3c4L@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Geeling Chau" target="_blank">Geeling Chau</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher Wang" target="_blank">Christopher Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sabera Talukder" target="_blank">Sabera Talukder</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vighnesh Subramaniam" target="_blank">Vighnesh Subramaniam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saraswati Soedarmadji" target="_blank">Saraswati Soedarmadji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yisong Yue" target="_blank">Yisong Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Katz" target="_blank">Boris Katz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrei Barbu" target="_blank">Andrei Barbu</a>
            </p>
            <p id="summary-FVuqJt3c4L@OpenReview" class="summary">We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address two key challenges in scaling models with neural time-series data: sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained representations and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how they can be used to extract neuroscience insights from massive amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability.</p>
            <p id="subjects-FVuqJt3c4L@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-FVuqJt3c4L@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FVuqJt3c4L@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FVuqJt3c4L@OpenReview" onclick="foldPdfKimi('FVuqJt3c4L@OpenReview', this)" class="hr hr-fold">
        </div><div id="EzjsoomYEb@OpenReview" class="panel paper" keywords="homp,topological,smcn,expressivity,tdl,mcn,blindspots,expressive,message,passing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EzjsoomYEb" target="_blank" title="162/207"><span class="index notranslate">#162</span></a>
                <a id="title-EzjsoomYEb@OpenReview" class="title-link" href="/venue/EzjsoomYEb@OpenReview" target="_blank">Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity</a>
                <a id="pdf-EzjsoomYEb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EzjsoomYEb@OpenReview', this)" data="https://openreview.net/pdf?id=EzjsoomYEb">[PDF<sup id="pdf-stars-EzjsoomYEb@OpenReview">8</sup>]</a>
                <a id="copy-EzjsoomYEb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EzjsoomYEb@OpenReview')">[Copy]</a>
                <a id="kimi-EzjsoomYEb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EzjsoomYEb@OpenReview', this)">[Kimi<sup id="kimi-stars-EzjsoomYEb@OpenReview">13</sup>]</a>
                <a id="rel-EzjsoomYEb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EzjsoomYEb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EzjsoomYEb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yam Eitan" target="_blank">Yam Eitan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yoav Gelberg" target="_blank">Yoav Gelberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guy Bar-Shalom" target="_blank">Guy Bar-Shalom</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabrizio Frasca" target="_blank">Fabrizio Frasca</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Bronstein" target="_blank">Michael Bronstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haggai Maron" target="_blank">Haggai Maron</a>
            </p>
            <p id="summary-EzjsoomYEb@OpenReview" class="summary">Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information.</p>
            <p id="subjects-EzjsoomYEb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-EzjsoomYEb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EzjsoomYEb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EzjsoomYEb@OpenReview" onclick="foldPdfKimi('EzjsoomYEb@OpenReview', this)" class="hr hr-fold">
        </div><div id="EUSkm2sVJ6@OpenReview" class="panel paper" keywords="usage,dataset,none,data,textit,ourmethod,much,inference,machine,mistakenly">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EUSkm2sVJ6" target="_blank" title="163/207"><span class="index notranslate">#163</span></a>
                <a id="title-EUSkm2sVJ6@OpenReview" class="title-link" href="/venue/EUSkm2sVJ6@OpenReview" target="_blank">How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning</a>
                <a id="pdf-EUSkm2sVJ6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EUSkm2sVJ6@OpenReview', this)" data="https://openreview.net/pdf?id=EUSkm2sVJ6">[PDF<sup id="pdf-stars-EUSkm2sVJ6@OpenReview">8</sup>]</a>
                <a id="copy-EUSkm2sVJ6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EUSkm2sVJ6@OpenReview')">[Copy]</a>
                <a id="kimi-EUSkm2sVJ6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EUSkm2sVJ6@OpenReview', this)">[Kimi<sup id="kimi-stars-EUSkm2sVJ6@OpenReview">17</sup>]</a>
                <a id="rel-EUSkm2sVJ6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EUSkm2sVJ6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EUSkm2sVJ6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Tong" target="_blank">Yao Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayuan Ye" target="_blank">Jiayuan Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sajjad Zarifzadeh" target="_blank">Sajjad Zarifzadeh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Reza Shokri" target="_blank">Reza Shokri</a>
            </p>
            <p id="summary-EUSkm2sVJ6@OpenReview" class="summary">How much of a given dataset was used to train a machine learning model? This is a critical question for data owners assessing the risk of unauthorized data usage and protecting their right (United States Code, 1976). However, previous work mistakenly treats this as a binary probleminferring whether \textit{all or none} or \textit{any or none} of the data was usedwhich is fragile when faced with real, non-binary data usage risks. To address this, we propose a fine-grained analysis called Dataset Usage Cardinality Inference (\ourmethod{}), which estimates the exact proportion of data used. Our algorithm, leveraging debiased membership guesses, matches the performance of the optimal MLE approach (with a maximum error &lt;0.1) but with significantly lower (e.g., <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-75-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;300&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-478" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.14em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-479"><span class="mn" id="MathJax-Span-480" style="font-family: MathJax_Main;">300</span><span class="mo" id="MathJax-Span-481" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>300</mn><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-75">300 \times</script> less) computational cost.</p>
            <p id="subjects-EUSkm2sVJ6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-EUSkm2sVJ6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EUSkm2sVJ6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EUSkm2sVJ6@OpenReview" onclick="foldPdfKimi('EUSkm2sVJ6@OpenReview', this)" class="hr hr-fold">
        </div><div id="zCxGCdzreM@OpenReview" class="panel paper" keywords="kinetix,agent,environments,jax2d,open,physics,tasks,ended,training,agents">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zCxGCdzreM" target="_blank" title="164/207"><span class="index notranslate">#164</span></a>
                <a id="title-zCxGCdzreM@OpenReview" class="title-link" href="/venue/zCxGCdzreM@OpenReview" target="_blank">Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks</a>
                <a id="pdf-zCxGCdzreM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zCxGCdzreM@OpenReview', this)" data="https://openreview.net/pdf?id=zCxGCdzreM">[PDF<sup id="pdf-stars-zCxGCdzreM@OpenReview">6</sup>]</a>
                <a id="copy-zCxGCdzreM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zCxGCdzreM@OpenReview')">[Copy]</a>
                <a id="kimi-zCxGCdzreM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zCxGCdzreM@OpenReview', this)">[Kimi<sup id="kimi-stars-zCxGCdzreM@OpenReview">13</sup>]</a>
                <a id="rel-zCxGCdzreM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zCxGCdzreM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zCxGCdzreM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Matthews" target="_blank">Michael Matthews</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Beukman" target="_blank">Michael Beukman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Lu" target="_blank">Chris Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Foerster" target="_blank">Jakob Foerster</a>
            </p>
            <p id="summary-zCxGCdzreM@OpenReview" class="summary">While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.Our trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*. This includes solving some environments that standard RL training completely fails at.We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.We open-source Jax2D, Kinetix, and our final model weights.</p>
            <p id="subjects-zCxGCdzreM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-zCxGCdzreM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zCxGCdzreM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zCxGCdzreM@OpenReview" onclick="foldPdfKimi('zCxGCdzreM@OpenReview', this)" class="hr hr-fold">
        </div><div id="DJSZGGZYVi@OpenReview" class="panel paper" keywords="diffusion,representations,generation,sit,guidance,training,quality,transformers,easier,repa">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DJSZGGZYVi" target="_blank" title="165/207"><span class="index notranslate">#165</span></a>
                <a id="title-DJSZGGZYVi@OpenReview" class="title-link" href="/venue/DJSZGGZYVi@OpenReview" target="_blank">Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</a>
                <a id="pdf-DJSZGGZYVi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DJSZGGZYVi@OpenReview', this)" data="https://openreview.net/pdf?id=DJSZGGZYVi">[PDF<sup id="pdf-stars-DJSZGGZYVi@OpenReview">14</sup>]</a>
                <a id="copy-DJSZGGZYVi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DJSZGGZYVi@OpenReview')">[Copy]</a>
                <a id="kimi-DJSZGGZYVi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DJSZGGZYVi@OpenReview', this)">[Kimi<sup id="kimi-stars-DJSZGGZYVi@OpenReview">13</sup>]</a>
                <a id="rel-DJSZGGZYVi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DJSZGGZYVi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DJSZGGZYVi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sihyun Yu" target="_blank">Sihyun Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangkyung Kwak" target="_blank">Sangkyung Kwak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huiwon Jang" target="_blank">Huiwon Jang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jongheon Jeong" target="_blank">Jongheon Jeong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Huang" target="_blank">Jonathan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwoo Shin" target="_blank">Jinwoo Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saining Xie" target="_blank">Saining Xie</a>
            </p>
            <p id="summary-DJSZGGZYVi@OpenReview" class="summary">Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-76-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-482" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-483"><span class="mo" id="MathJax-Span-484" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-76">\times</script>, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.</p>
            <p id="subjects-DJSZGGZYVi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-DJSZGGZYVi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DJSZGGZYVi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DJSZGGZYVi@OpenReview" onclick="foldPdfKimi('DJSZGGZYVi@OpenReview', this)" class="hr hr-fold">
        </div><div id="CjwERcAU7w@OpenReview" class="panel paper" keywords="correction,self,score,training,sft,reinforcement,generated,traces,responses,reward">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CjwERcAU7w" target="_blank" title="166/207"><span class="index notranslate">#166</span></a>
                <a id="title-CjwERcAU7w@OpenReview" class="title-link" href="/venue/CjwERcAU7w@OpenReview" target="_blank">Training Language Models to Self-Correct via Reinforcement Learning</a>
                <a id="pdf-CjwERcAU7w@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CjwERcAU7w@OpenReview', this)" data="https://openreview.net/pdf?id=CjwERcAU7w">[PDF<sup id="pdf-stars-CjwERcAU7w@OpenReview">31</sup>]</a>
                <a id="copy-CjwERcAU7w@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CjwERcAU7w@OpenReview')">[Copy]</a>
                <a id="kimi-CjwERcAU7w@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CjwERcAU7w@OpenReview', this)">[Kimi<sup id="kimi-stars-CjwERcAU7w@OpenReview">32</sup>]</a>
                <a id="rel-CjwERcAU7w@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CjwERcAU7w@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CjwERcAU7w@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aviral Kumar" target="_blank">Aviral Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Zhuang" target="_blank">Vincent Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Agarwal" target="_blank">Rishabh Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Su" target="_blank">Yi Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Co-Reyes" target="_blank">John Co-Reyes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Avi Singh" target="_blank">Avi Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kate Baumli" target="_blank">Kate Baumli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shariq Iqbal" target="_blank">Shariq Iqbal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Colton Bishop" target="_blank">Colton Bishop</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rebecca Roelofs" target="_blank">Rebecca Roelofs</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Zhang" target="_blank">Lei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katrina McKinney" target="_blank">Katrina McKinney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Disha Shrivastava" target="_blank">Disha Shrivastava</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cosmin Paduraru" target="_blank">Cosmin Paduraru</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=George Tucker" target="_blank">George Tucker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Doina Precup" target="_blank">Doina Precup</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feryal Behbahani" target="_blank">Feryal Behbahani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aleksandra Faust" target="_blank">Aleksandra Faust</a>
            </p>
            <p id="summary-CjwERcAU7w@OpenReview" class="summary">Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.</p>
            <p id="subjects-CjwERcAU7w@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-CjwERcAU7w@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CjwERcAU7w@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CjwERcAU7w@OpenReview" onclick="foldPdfKimi('CjwERcAU7w@OpenReview', this)" class="hr hr-fold">
        </div><div id="BPgK5XW1Nb@OpenReview" class="panel paper" keywords="preference,alignment,llm,judgment,annotated,human,spread,annotation,llms,boosts">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=BPgK5XW1Nb" target="_blank" title="167/207"><span class="index notranslate">#167</span></a>
                <a id="title-BPgK5XW1Nb@OpenReview" class="title-link" href="/venue/BPgK5XW1Nb@OpenReview" target="_blank">Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment</a>
                <a id="pdf-BPgK5XW1Nb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('BPgK5XW1Nb@OpenReview', this)" data="https://openreview.net/pdf?id=BPgK5XW1Nb">[PDF<sup id="pdf-stars-BPgK5XW1Nb@OpenReview">13</sup>]</a>
                <a id="copy-BPgK5XW1Nb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('BPgK5XW1Nb@OpenReview')">[Copy]</a>
                <a id="kimi-BPgK5XW1Nb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('BPgK5XW1Nb@OpenReview', this)">[Kimi<sup id="kimi-stars-BPgK5XW1Nb@OpenReview">17</sup>]</a>
                <a id="rel-BPgK5XW1Nb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('BPgK5XW1Nb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-BPgK5XW1Nb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dongyoung Kim" target="_blank">Dongyoung Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaehyung Kim" target="_blank">Jaehyung Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kimin Lee" target="_blank">Kimin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwoo Shin" target="_blank">Jinwoo Shin</a>
            </p>
            <p id="summary-BPgK5XW1Nb@OpenReview" class="summary">Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data.Our key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data.To be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. Compared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective.In addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.Our experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs.For example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines.</p>
            <p id="subjects-BPgK5XW1Nb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-BPgK5XW1Nb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-BPgK5XW1Nb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-BPgK5XW1Nb@OpenReview" onclick="foldPdfKimi('BPgK5XW1Nb@OpenReview', this)" class="hr hr-fold">
        </div><div id="AoraWUmpLU@OpenReview" class="panel paper" keywords="odes,activation,training,neural,functions,nonlinearity,impact,properties,ntk,overparameterized">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=AoraWUmpLU" target="_blank" title="168/207"><span class="index notranslate">#168</span></a>
                <a id="title-AoraWUmpLU@OpenReview" class="title-link" href="/venue/AoraWUmpLU@OpenReview" target="_blank">Exploring the Impact of Activation Functions in Training Neural ODEs</a>
                <a id="pdf-AoraWUmpLU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('AoraWUmpLU@OpenReview', this)" data="https://openreview.net/pdf?id=AoraWUmpLU">[PDF<sup id="pdf-stars-AoraWUmpLU@OpenReview">8</sup>]</a>
                <a id="copy-AoraWUmpLU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('AoraWUmpLU@OpenReview')">[Copy]</a>
                <a id="kimi-AoraWUmpLU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('AoraWUmpLU@OpenReview', this)">[Kimi<sup id="kimi-stars-AoraWUmpLU@OpenReview">8</sup>]</a>
                <a id="rel-AoraWUmpLU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('AoraWUmpLU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-AoraWUmpLU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianxiang Gao" target="_blank">Tianxiang Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Sun" target="_blank">Siyuan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hailiang Liu" target="_blank">Hailiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyang Gao" target="_blank">Hongyang Gao</a>
            </p>
            <p id="summary-AoraWUmpLU@OpenReview" class="summary">Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and convergence analysis. In this paper, we address these challenges by investigating the impact of activation functions. We demonstrate that the properties of activation functionsspecifically smoothness and nonlinearityare critical to the training dynamics. Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs, while sufficient nonlinearity is essential for maintaining the spectral properties of the Neural Tangent Kernel (NTK) during training. Together, these properties enable us to establish the global convergence of Neural ODEs under gradient descent in overparameterized regimes. Our theoretical findings are validated by numerical experiments, which not only support our analysis but also provide practical guidelines for scaling Neural ODEs, potentially leading to faster training and improved performance in real-world applications.</p>
            <p id="subjects-AoraWUmpLU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-AoraWUmpLU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-AoraWUmpLU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-AoraWUmpLU@OpenReview" onclick="foldPdfKimi('AoraWUmpLU@OpenReview', this)" class="hr hr-fold">
        </div><div id="8zJRon6k5v@OpenReview" class="panel paper" keywords="acssm,amortized,irregular,doob,elbo,continuous,inference,latent,control,series">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=8zJRon6k5v" target="_blank" title="169/207"><span class="index notranslate">#169</span></a>
                <a id="title-8zJRon6k5v@OpenReview" class="title-link" href="/venue/8zJRon6k5v@OpenReview" target="_blank">Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series</a>
                <a id="pdf-8zJRon6k5v@OpenReview" class="title-pdf notranslate" onclick="togglePdf('8zJRon6k5v@OpenReview', this)" data="https://openreview.net/pdf?id=8zJRon6k5v">[PDF<sup id="pdf-stars-8zJRon6k5v@OpenReview">3</sup>]</a>
                <a id="copy-8zJRon6k5v@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('8zJRon6k5v@OpenReview')">[Copy]</a>
                <a id="kimi-8zJRon6k5v@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('8zJRon6k5v@OpenReview', this)">[Kimi<sup id="kimi-stars-8zJRon6k5v@OpenReview">7</sup>]</a>
                <a id="rel-8zJRon6k5v@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('8zJRon6k5v@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-8zJRon6k5v@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Byoungwoo Park" target="_blank">Byoungwoo Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyungi Lee" target="_blank">Hyungi Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juho Lee" target="_blank">Juho Lee</a>
            </p>
            <p id="summary-8zJRon6k5v@OpenReview" class="summary">Many real-world datasets, such as healthcare, climate, and economics, are often collected as irregular time series, which poses challenges for accurate modeling. In this paper, we propose the Amortized Control of continuous State Space Model (ACSSM) for continuous dynamical modeling of time series for irregular and discrete observations. We first present a multi-marginal Doob's <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-77-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-485" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-486"><span class="mi" id="MathJax-Span-487" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-77">h</script>-transform to construct a continuous dynamical system conditioned on these irregular observations. Following this, we introduce a variational inference algorithm with a tight evidence lower bound (ELBO), leveraging stochastic optimal control (SOC) theory to approximate the intractable Doob's <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-78-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-488" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-489"><span class="mi" id="MathJax-Span-490" style="font-family: MathJax_Math-italic;">h</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi></math></span></span><script type="math/tex" id="MathJax-Element-78">h</script>-transform and simulate the conditioned dynamics. To improve efficiency and scalability during both training and inference, ACSSM employs amortized inference to decouple representation learning from the latent dynamics. Additionally, it incorporates a simulation-free latent dynamics framework and a transformer-based data assimilation scheme, facilitating parallel inference of the latent states and ELBO computation. Through empirical evaluations across a variety of real-world datasets, ACSSM demonstrates superior performance in tasks such as classification, regression, interpolation, and extrapolation, while maintaining computational efficiency.</p>
            <p id="subjects-8zJRon6k5v@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-8zJRon6k5v@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-8zJRon6k5v@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-8zJRon6k5v@OpenReview" onclick="foldPdfKimi('8zJRon6k5v@OpenReview', this)" class="hr hr-fold">
        </div><div id="6Mxhg9PtDE@OpenReview" class="panel paper" keywords="alignment,safety,tokens,attacks,vulnerabilities,tuning,shallow,fine,issue,unifiedly">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6Mxhg9PtDE" target="_blank" title="170/207"><span class="index notranslate">#170</span></a>
                <a id="title-6Mxhg9PtDE@OpenReview" class="title-link" href="/venue/6Mxhg9PtDE@OpenReview" target="_blank">Safety Alignment Should be Made More Than Just a Few Tokens Deep</a>
                <a id="pdf-6Mxhg9PtDE@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6Mxhg9PtDE@OpenReview', this)" data="https://openreview.net/pdf?id=6Mxhg9PtDE">[PDF<sup id="pdf-stars-6Mxhg9PtDE@OpenReview">13</sup>]</a>
                <a id="copy-6Mxhg9PtDE@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6Mxhg9PtDE@OpenReview')">[Copy]</a>
                <a id="kimi-6Mxhg9PtDE@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6Mxhg9PtDE@OpenReview', this)">[Kimi<sup id="kimi-stars-6Mxhg9PtDE@OpenReview">17</sup>]</a>
                <a id="rel-6Mxhg9PtDE@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6Mxhg9PtDE@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6Mxhg9PtDE@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyu Qi" target="_blank">Xiangyu Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashwinee Panda" target="_blank">Ashwinee Panda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaifeng Lyu" target="_blank">Kaifeng Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Ma" target="_blank">Xiao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Subhrajit Roy" target="_blank">Subhrajit Roy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ahmad Beirami" target="_blank">Ahmad Beirami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Mittal" target="_blank">Prateek Mittal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Henderson" target="_blank">Peter Henderson</a>
            </p>
            <p id="summary-6Mxhg9PtDE@OpenReview" class="summary">The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.</p>
            <p id="subjects-6Mxhg9PtDE@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-6Mxhg9PtDE@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6Mxhg9PtDE@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6Mxhg9PtDE@OpenReview" onclick="foldPdfKimi('6Mxhg9PtDE@OpenReview', this)" class="hr hr-fold">
        </div><div id="5IkDAfabuo@OpenReview" class="panel paper" keywords="replay,prioritized,generative,online,experience,overfitting,generations,useful,guidance,relevance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5IkDAfabuo" target="_blank" title="171/207"><span class="index notranslate">#171</span></a>
                <a id="title-5IkDAfabuo@OpenReview" class="title-link" href="/venue/5IkDAfabuo@OpenReview" target="_blank">Prioritized Generative Replay</a>
                <a id="pdf-5IkDAfabuo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5IkDAfabuo@OpenReview', this)" data="https://openreview.net/pdf?id=5IkDAfabuo">[PDF<sup id="pdf-stars-5IkDAfabuo@OpenReview">11</sup>]</a>
                <a id="copy-5IkDAfabuo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5IkDAfabuo@OpenReview')">[Copy]</a>
                <a id="kimi-5IkDAfabuo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5IkDAfabuo@OpenReview', this)">[Kimi<sup id="kimi-stars-5IkDAfabuo@OpenReview">20</sup>]</a>
                <a id="rel-5IkDAfabuo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5IkDAfabuo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5IkDAfabuo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Renhao Wang" target="_blank">Renhao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Frans" target="_blank">Kevin Frans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pieter Abbeel" target="_blank">Pieter Abbeel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Levine" target="_blank">Sergey Levine</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexei Efros" target="_blank">Alexei Efros</a>
            </p>
            <p id="summary-5IkDAfabuo@OpenReview" class="summary">Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of ``relevance functions'' that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents.</p>
            <p id="subjects-5IkDAfabuo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-5IkDAfabuo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5IkDAfabuo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5IkDAfabuo@OpenReview" onclick="foldPdfKimi('5IkDAfabuo@OpenReview', this)" class="hr hr-fold">
        </div><div id="1aF2D2CPHi@OpenReview" class="panel paper" keywords="clip,dfkd,customization,distillation,knowledge,vocabulary,diversification,images,synthetic,inversing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1aF2D2CPHi" target="_blank" title="172/207"><span class="index notranslate">#172</span></a>
                <a id="title-1aF2D2CPHi@OpenReview" class="title-link" href="/venue/1aF2D2CPHi@OpenReview" target="_blank">Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation</a>
                <a id="pdf-1aF2D2CPHi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1aF2D2CPHi@OpenReview', this)" data="https://openreview.net/pdf?id=1aF2D2CPHi">[PDF<sup id="pdf-stars-1aF2D2CPHi@OpenReview">19</sup>]</a>
                <a id="copy-1aF2D2CPHi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1aF2D2CPHi@OpenReview')">[Copy]</a>
                <a id="kimi-1aF2D2CPHi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1aF2D2CPHi@OpenReview', this)">[Kimi<sup id="kimi-stars-1aF2D2CPHi@OpenReview">19</sup>]</a>
                <a id="rel-1aF2D2CPHi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1aF2D2CPHi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1aF2D2CPHi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxian Wei" target="_blank">Yongxian Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Hu" target="_blank">Zixuan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Shen" target="_blank">Li Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyi Wang" target="_blank">Zhenyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun Yuan" target="_blank">Chun Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dacheng Tao" target="_blank">Dacheng Tao</a>
            </p>
            <p id="summary-1aF2D2CPHi@OpenReview" class="summary">Vision-language models such as CLIP have demonstrated strong zero-shot performance, but their considerable size and inefficient inference limit customizable deployment for users. While knowledge distillation is a solution, it still requires the original data, which is not always available due to copyrights and privacy concerns. For many users seeking open-vocabulary customization, Data-Free Knowledge Distillation (DFKD) emerges as a promising direction. Upon rethinking DFKD, we find that existing methods fail on CLIP due to their heavy reliance on BatchNorm layers, which are unexpectedly unusable in CLIP. Based on our findings, we adopt image-text matching to achieve DFKD for CLIP, enabling customization based on arbitrary class texts. This involves (i) inversing a surrogate dataset from CLIP based on text prompts; and (ii) distilling a student model from CLIP using the surrogate dataset. Specifically, we introduce style dictionary diversification to enhance the diversity of synthetic images. To prevent uncontrollable semantics introduced by diversification, we propose a class consistency maintaining strategy to ensure the consistency of synthetic images. Based on synthetic images with various styles, we further propose meta knowledge distillation to train the student model with good generalization ability. Moreover, we introduce a simple yet effective method to enable customization based on few example images. Comprehensive experiments showcase the superiority of our approach across twelve customized tasks, achieving a 9.33\% improvement compared to existing DFKD methods.</p>
            <p id="subjects-1aF2D2CPHi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-1aF2D2CPHi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1aF2D2CPHi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1aF2D2CPHi@OpenReview" onclick="foldPdfKimi('1aF2D2CPHi@OpenReview', this)" class="hr hr-fold">
        </div><div id="1HCN4pjTb4@OpenReview" class="panel paper" keywords="collapse,balancedness,neural,ell,weight,dnns,conditioning,training,features,unconstrained">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1HCN4pjTb4" target="_blank" title="173/207"><span class="index notranslate">#173</span></a>
                <a id="title-1HCN4pjTb4@OpenReview" class="title-link" href="/venue/1HCN4pjTb4@OpenReview" target="_blank">Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse</a>
                <a id="pdf-1HCN4pjTb4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1HCN4pjTb4@OpenReview', this)" data="https://openreview.net/pdf?id=1HCN4pjTb4">[PDF<sup id="pdf-stars-1HCN4pjTb4@OpenReview">7</sup>]</a>
                <a id="copy-1HCN4pjTb4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1HCN4pjTb4@OpenReview')">[Copy]</a>
                <a id="kimi-1HCN4pjTb4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1HCN4pjTb4@OpenReview', this)">[Kimi<sup id="kimi-stars-1HCN4pjTb4@OpenReview">7</sup>]</a>
                <a id="rel-1HCN4pjTb4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1HCN4pjTb4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1HCN4pjTb4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Arthur Jacot" target="_blank">Arthur Jacot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Skenk" target="_blank">Peter Skenk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihan Wang" target="_blank">Zihan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Mondelli" target="_blank">Marco Mondelli</a>
            </p>
            <p id="summary-1HCN4pjTb4@OpenReview" class="summary">Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on the unconstrained features model. Here, the features of the penultimate layer are free variables, which makes the model data-agnostic and puts into question its ability to capture DNN training. Our work addresses the issue, moving away from unconstrained features and studying DNNs that end with at least two linear layers. We first prove generic guarantees on neural collapse that assume (i) low training error and balancedness of linear layers (for within-class variability collapse), and (ii) bounded conditioning of the features before the linear part (for orthogonality of class-means, and their alignment with weight matrices). The balancedness refers to the fact that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-79-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x22A4;&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;/msub&gt;&lt;msubsup&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;&amp;#x2113;&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x22A4;&lt;/mi&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-491" style="width: 10.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.753em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1008.75em, 2.711em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-492"><span class="msubsup" id="MathJax-Span-493"><span style="display: inline-block; position: relative; width: 2.19em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-494" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.497em; left: 1.148em;"><span class="mi" id="MathJax-Span-495" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.461em, 1001.25em, 2.346em, -999.997em); top: -1.82em; left: 0.94em;"><span class="texatom" id="MathJax-Span-496"><span class="mrow" id="MathJax-Span-497"><span class="mi" id="MathJax-Span-498" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mo" id="MathJax-Span-499" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-500" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-501"><span style="display: inline-block; position: relative; width: 2.19em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-502" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.94em;"><span class="texatom" id="MathJax-Span-503"><span class="mrow" id="MathJax-Span-504"><span class="mi" id="MathJax-Span-505" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mo" id="MathJax-Span-506" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-507" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-508" style="font-family: MathJax_Main; padding-left: 0.263em;"></span><span class="msubsup" id="MathJax-Span-509" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-510" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.94em;"><span class="mi" id="MathJax-Span-511" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-512"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-513" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.497em; left: 1.148em;"><span class="mi" id="MathJax-Span-514" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.461em, 1000.37em, 2.294em, -999.997em); top: -1.82em; left: 0.94em;"><span class="mi" id="MathJax-Span-515" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mi></mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal"></mi></msubsup><msub><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mi></mi><mo>+</mo><mn>1</mn></mrow></msub><mo></mo><msub><mi>W</mi><mi></mi></msub><msubsup><mi>W</mi><mi></mi><mi mathvariant="normal"></mi></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-79">W_{\ell+1}^\top W_{\ell+1}\approx W_\ell W_\ell ^\top</script> for any pair ofconsecutive weight matrices of the linear part, and the bounded conditioning requires a well-behaved ratio between largest and smallest non-zero singular values of the features. We then show that such assumptions hold for gradient descent training with weight decay: (i) for networks with a wide first layer, we prove low training error and balancedness, and (ii) for solutions that are either nearly optimal or stable under large learning rates, we additionally prove the bounded conditioning. Taken together, our results are the first to show neural collapse in the end-to-end training of DNNs.</p>
            <p id="subjects-1HCN4pjTb4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-1HCN4pjTb4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1HCN4pjTb4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1HCN4pjTb4@OpenReview" onclick="foldPdfKimi('1HCN4pjTb4@OpenReview', this)" class="hr hr-fold">
        </div><div id="zl0HLZOJC9@OpenReview" class="panel paper" keywords="l2d,experts,defer,human,workload,probabilistic,expert,annotations,cooperation,missing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zl0HLZOJC9" target="_blank" title="174/207"><span class="index notranslate">#174</span></a>
                <a id="title-zl0HLZOJC9@OpenReview" class="title-link" href="/venue/zl0HLZOJC9@OpenReview" target="_blank">Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution</a>
                <a id="pdf-zl0HLZOJC9@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zl0HLZOJC9@OpenReview', this)" data="https://openreview.net/pdf?id=zl0HLZOJC9">[PDF<sup id="pdf-stars-zl0HLZOJC9@OpenReview">3</sup>]</a>
                <a id="copy-zl0HLZOJC9@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zl0HLZOJC9@OpenReview')">[Copy]</a>
                <a id="kimi-zl0HLZOJC9@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zl0HLZOJC9@OpenReview', this)">[Kimi<sup id="kimi-stars-zl0HLZOJC9@OpenReview">4</sup>]</a>
                <a id="rel-zl0HLZOJC9@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zl0HLZOJC9@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zl0HLZOJC9@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cuong Nguyen" target="_blank">Cuong Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thanh-Toan Do" target="_blank">Thanh-Toan Do</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gustavo Carneiro" target="_blank">Gustavo Carneiro</a>
            </p>
            <p id="summary-zl0HLZOJC9@OpenReview" class="summary">Recent progress in machine learning research is gradually shifting its focus towards *human-AI cooperation* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is *learning to defer* (L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a time-consuming and expensive annotation process that can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier, which is critical to optimise resource allocation. We, therefore, propose a new probabilistic modelling approach inspired by the mixture-of-experts, where the Expectation - Maximisation algorithm is leverage to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets shows that our proposed probabilistic approach performs competitively, or surpasses previously proposed methods assessed on the same benchmarks.</p>
            <p id="subjects-zl0HLZOJC9@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-zl0HLZOJC9@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zl0HLZOJC9@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zl0HLZOJC9@OpenReview" onclick="foldPdfKimi('zl0HLZOJC9@OpenReview', this)" class="hr hr-fold">
        </div><div id="zBbZ2vdLzH@OpenReview" class="panel paper" keywords="jdr,graph,rewiring,node,denoising,gnns,joint,rewire,feature,improves">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zBbZ2vdLzH" target="_blank" title="175/207"><span class="index notranslate">#175</span></a>
                <a id="title-zBbZ2vdLzH@OpenReview" class="title-link" href="/venue/zBbZ2vdLzH@OpenReview" target="_blank">Joint Graph Rewiring and Feature Denoising via Spectral Resonance</a>
                <a id="pdf-zBbZ2vdLzH@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zBbZ2vdLzH@OpenReview', this)" data="https://openreview.net/pdf?id=zBbZ2vdLzH">[PDF<sup id="pdf-stars-zBbZ2vdLzH@OpenReview">9</sup>]</a>
                <a id="copy-zBbZ2vdLzH@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zBbZ2vdLzH@OpenReview')">[Copy]</a>
                <a id="kimi-zBbZ2vdLzH@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zBbZ2vdLzH@OpenReview', this)">[Kimi<sup id="kimi-stars-zBbZ2vdLzH@OpenReview">9</sup>]</a>
                <a id="rel-zBbZ2vdLzH@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zBbZ2vdLzH@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zBbZ2vdLzH@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Linkerhgner" target="_blank">Jonas Linkerhgner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Shi" target="_blank">Cheng Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivan Dokmani" target="_blank">Ivan Dokmani</a>
            </p>
            <p id="summary-zBbZ2vdLzH@OpenReview" class="summary">In graph learning the graph and the node features both contain noisy information about the node labels. In this paper we propose joint denoising and rewiring (JDR)an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification graph neural nets (GNNs). JDR improves the alignment between the leading eigenspaces of graph and feature matrices. To approximately solve the associated non-convex optimization problem we propose a heuristic that efficiently handles real-world graph datasets with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and verify the effectiveness of our approach through extensive experiments on synthetic and real-world graph datasets. The results show that JDR consistently outperforms existing rewiring methods on node classification using GNNs as downstream models.</p>
            <p id="subjects-zBbZ2vdLzH@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-zBbZ2vdLzH@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zBbZ2vdLzH@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zBbZ2vdLzH@OpenReview" onclick="foldPdfKimi('zBbZ2vdLzH@OpenReview', this)" class="hr hr-fold">
        </div><div id="wPMRwmytZe@OpenReview" class="panel paper" keywords="curriculum,distillation,progressive,student,teacher,implicit,checkpoints,intermediate,parity,empirical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wPMRwmytZe" target="_blank" title="176/207"><span class="index notranslate">#176</span></a>
                <a id="title-wPMRwmytZe@OpenReview" class="title-link" href="/venue/wPMRwmytZe@OpenReview" target="_blank">Progressive distillation induces an implicit curriculum</a>
                <a id="pdf-wPMRwmytZe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wPMRwmytZe@OpenReview', this)" data="https://openreview.net/pdf?id=wPMRwmytZe">[PDF<sup id="pdf-stars-wPMRwmytZe@OpenReview">6</sup>]</a>
                <a id="copy-wPMRwmytZe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wPMRwmytZe@OpenReview')">[Copy]</a>
                <a id="kimi-wPMRwmytZe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wPMRwmytZe@OpenReview', this)">[Kimi<sup id="kimi-stars-wPMRwmytZe@OpenReview">16</sup>]</a>
                <a id="rel-wPMRwmytZe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wPMRwmytZe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wPMRwmytZe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Abhishek Panigrahi" target="_blank">Abhishek Panigrahi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingbin Liu" target="_blank">Bingbin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sadhika Malladi" target="_blank">Sadhika Malladi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrej Risteski" target="_blank">Andrej Risteski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Surbhi Goel" target="_blank">Surbhi Goel</a>
            </p>
            <p id="summary-wPMRwmytZe@OpenReview" class="summary">Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several intermediate teachers. One empirically validated variant of this principle is progressive distillation, where the student learns from successive intermediate checkpoints of the teacher. Using sparse parity as a sandbox, we identify an implicit curriculum as one mechanism through which progressive distillation accelerates the students learning. This curriculum is available only through the intermediate checkpoints but not the final converged one, and imparts both empirical acceleration and a provable sample complexity benefit to the student. We then extend our investigation to Transformers trained on probabilistic context-free grammars (PCFGs) and real-world pre-training datasets (Wikipedia and Books). Through probing the teacher model, we identify an analogous implicit curriculum where the model progressively learns features that capture longer context. Our theoretical and empirical findings on sparse parity, complemented by empirical observations on more complex tasks, highlight the benefit of progressive distillation via implicit curriculum across setups.</p>
            <p id="subjects-wPMRwmytZe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-wPMRwmytZe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wPMRwmytZe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wPMRwmytZe@OpenReview" onclick="foldPdfKimi('wPMRwmytZe@OpenReview', this)" class="hr hr-fold">
        </div><div id="xoIeVdFO7U@OpenReview" class="panel paper" keywords="misl,metra,skill,mutual,successor,ingredients,learning,contrastive,fly,information">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xoIeVdFO7U" target="_blank" title="177/207"><span class="index notranslate">#177</span></a>
                <a id="title-xoIeVdFO7U@OpenReview" class="title-link" href="/venue/xoIeVdFO7U@OpenReview" target="_blank">Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning</a>
                <a id="pdf-xoIeVdFO7U@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xoIeVdFO7U@OpenReview', this)" data="https://openreview.net/pdf?id=xoIeVdFO7U">[PDF<sup id="pdf-stars-xoIeVdFO7U@OpenReview">9</sup>]</a>
                <a id="copy-xoIeVdFO7U@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xoIeVdFO7U@OpenReview')">[Copy]</a>
                <a id="kimi-xoIeVdFO7U@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xoIeVdFO7U@OpenReview', this)">[Kimi<sup id="kimi-stars-xoIeVdFO7U@OpenReview">9</sup>]</a>
                <a id="rel-xoIeVdFO7U@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xoIeVdFO7U@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xoIeVdFO7U@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chongyi Zheng" target="_blank">Chongyi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jens Tuyls" target="_blank">Jens Tuyls</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joanne Peng" target="_blank">Joanne Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Eysenbach" target="_blank">Benjamin Eysenbach</a>
            </p>
            <p id="summary-xoIeVdFO7U@OpenReview" class="summary">Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL).Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.</p>
            <p id="subjects-xoIeVdFO7U@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-xoIeVdFO7U@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xoIeVdFO7U@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xoIeVdFO7U@OpenReview" onclick="foldPdfKimi('xoIeVdFO7U@OpenReview', this)" class="hr hr-fold">
        </div><div id="trKNi4IUiP@OpenReview" class="panel paper" keywords="backdoor,poisoned,graph,attacks,defending,dropping,nodes,gnns,triggers,clean">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=trKNi4IUiP" target="_blank" title="178/207"><span class="index notranslate">#178</span></a>
                <a id="title-trKNi4IUiP@OpenReview" class="title-link" href="/venue/trKNi4IUiP@OpenReview" target="_blank">Robustness Inspired Graph Backdoor Defense</a>
                <a id="pdf-trKNi4IUiP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('trKNi4IUiP@OpenReview', this)" data="https://openreview.net/pdf?id=trKNi4IUiP">[PDF<sup id="pdf-stars-trKNi4IUiP@OpenReview">7</sup>]</a>
                <a id="copy-trKNi4IUiP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('trKNi4IUiP@OpenReview')">[Copy]</a>
                <a id="kimi-trKNi4IUiP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('trKNi4IUiP@OpenReview', this)">[Kimi<sup id="kimi-stars-trKNi4IUiP@OpenReview">5</sup>]</a>
                <a id="rel-trKNi4IUiP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('trKNi4IUiP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-trKNi4IUiP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwei Zhang" target="_blank">Zhiwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minhua Lin" target="_blank">Minhua Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Xu" target="_blank">Junjie Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongyu Wu" target="_blank">Zongyu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enyan Dai" target="_blank">Enyan Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suhang Wang" target="_blank">Suhang Wang</a>
            </p>
            <p id="summary-trKNi4IUiP@OpenReview" class="summary">Graph Neural Networks (GNNs) have achieved promising results in tasks such as node classification and graph classification. However, recent studies reveal that GNNs are vulnerable to backdoor attacks, posing a significant threat to their real-world adoption. Despite initial efforts to defend against specific graph backdoor attacks, there is no work on defending against various types of backdoor attacks where generated triggers have different properties. Hence, we first empirically verify that prediction variance under edge dropping is a crucial indicator for identifying poisoned nodes. With this observation, we propose using random edge dropping to detect backdoors and theoretically show that it can efficiently distinguish poisoned nodes from clean ones. Furthermore, we introduce a novel robust training strategy to efficiently counteract the impact of the triggers. Extensive experiments on real-world datasets show that our framework can effectively identify poisoned nodes, significantly degrade the attack success rate, and maintain clean accuracy when defending against various types of graph backdoor attacks with different properties. Our code is available at: https://anonymous.4open.science/r/RIGBD-A670.</p>
            <p id="subjects-trKNi4IUiP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-trKNi4IUiP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-trKNi4IUiP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-trKNi4IUiP@OpenReview" onclick="foldPdfKimi('trKNi4IUiP@OpenReview', this)" class="hr hr-fold">
        </div><div id="kX8h23UG6v@OpenReview" class="panel paper" keywords="optimization,matrn,standard,kernel,initialization,dimensional,bayesian,belief,kernels,gaussian">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kX8h23UG6v" target="_blank" title="179/207"><span class="index notranslate">#179</span></a>
                <a id="title-kX8h23UG6v@OpenReview" class="title-link" href="/venue/kX8h23UG6v@OpenReview" target="_blank">Standard Gaussian Process Can Be Excellent for High-Dimensional Bayesian Optimization</a>
                <a id="pdf-kX8h23UG6v@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kX8h23UG6v@OpenReview', this)" data="https://openreview.net/pdf?id=kX8h23UG6v">[PDF<sup id="pdf-stars-kX8h23UG6v@OpenReview">8</sup>]</a>
                <a id="copy-kX8h23UG6v@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kX8h23UG6v@OpenReview')">[Copy]</a>
                <a id="kimi-kX8h23UG6v@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kX8h23UG6v@OpenReview', this)">[Kimi<sup id="kimi-stars-kX8h23UG6v@OpenReview">4</sup>]</a>
                <a id="rel-kX8h23UG6v@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kX8h23UG6v@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kX8h23UG6v@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhitong Xu" target="_blank">Zhitong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haitao Wang" target="_blank">Haitao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeff Phillips" target="_blank">Jeff Phillips</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shandian Zhe" target="_blank">Shandian Zhe</a>
            </p>
            <p id="summary-kX8h23UG6v@OpenReview" class="summary">A long-standing belief holds that Bayesian Optimization (BO) with standard Gaussian processes (GP) --- referred to as standard BO --- underperforms in high-dimensional optimization problems. While this belief seems plausible, it lacks both robust empirical evidence and theoretical justification. To address this gap, we present a systematic investigation. First, through a comprehensive evaluation across eleven widely used benchmarks, we found that while the popular Square Exponential (SE) kernel often leads to poor performance, using Matrn kernels enables standard BO to consistently achieve top-tier results, frequently surpassing methods specifically designed for high-dimensional optimization. Second, our theoretical analysis reveals that the SE kernels failure primarily stems from improper initialization of the length-scale parameters, which are commonly used in practice but can cause gradient vanishing in training. We provide a probabilistic bound to characterize this issue, showing that Matrn kernels are less susceptible and can robustly handle much higher dimensions. Third, we propose a simple robust initialization strategy that dramatically improves the performance of the SE kernel, bringing it close to state-of-the-art methods, without requiring any additional priors or regularization. We prove another probabilistic bound that demonstrates how the gradient vanishing issue can be effectively mitigated with our method. Our findings advocate for a re-evaluation of standard BOs potential in high-dimensional settings.</p>
            <p id="subjects-kX8h23UG6v@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-kX8h23UG6v@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kX8h23UG6v@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kX8h23UG6v@OpenReview" onclick="foldPdfKimi('kX8h23UG6v@OpenReview', this)" class="hr hr-fold">
        </div><div id="esYrEndGsr@OpenReview" class="panel paper" keywords="attribution,influence,diffusion,data,functions,models,modelling,predicting,fac,lds">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=esYrEndGsr" target="_blank" title="180/207"><span class="index notranslate">#180</span></a>
                <a id="title-esYrEndGsr@OpenReview" class="title-link" href="/venue/esYrEndGsr@OpenReview" target="_blank">Influence Functions for Scalable Data Attribution in Diffusion Models</a>
                <a id="pdf-esYrEndGsr@OpenReview" class="title-pdf notranslate" onclick="togglePdf('esYrEndGsr@OpenReview', this)" data="https://openreview.net/pdf?id=esYrEndGsr">[PDF<sup id="pdf-stars-esYrEndGsr@OpenReview">11</sup>]</a>
                <a id="copy-esYrEndGsr@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('esYrEndGsr@OpenReview')">[Copy]</a>
                <a id="kimi-esYrEndGsr@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('esYrEndGsr@OpenReview', this)">[Kimi<sup id="kimi-stars-esYrEndGsr@OpenReview">9</sup>]</a>
                <a id="rel-esYrEndGsr@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('esYrEndGsr@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-esYrEndGsr@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bruno Mlodozeniec" target="_blank">Bruno Mlodozeniec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runa Eschenhagen" target="_blank">Runa Eschenhagen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juhan Bae" target="_blank">Juhan Bae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Immer" target="_blank">Alexander Immer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Krueger" target="_blank">David Krueger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard E Turner" target="_blank">Richard E Turner</a>
            </p>
            <p id="summary-esYrEndGsr@OpenReview" class="summary">Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by extending influence functions. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we use a K-FAC approximation based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We show that our recommended method outperforms previously proposed data attribution methods on common data attribution evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning.</p>
            <p id="subjects-esYrEndGsr@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-esYrEndGsr@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-esYrEndGsr@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-esYrEndGsr@OpenReview" onclick="foldPdfKimi('esYrEndGsr@OpenReview', this)" class="hr hr-fold">
        </div><div id="Pujt3ADZgI@OpenReview" class="panel paper" keywords="inpo,rlhf,nash,preferences,policy,aligning,win,regret,policyoptimization,winrate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Pujt3ADZgI" target="_blank" title="181/207"><span class="index notranslate">#181</span></a>
                <a id="title-Pujt3ADZgI@OpenReview" class="title-link" href="/venue/Pujt3ADZgI@OpenReview" target="_blank">Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning</a>
                <a id="pdf-Pujt3ADZgI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Pujt3ADZgI@OpenReview', this)" data="https://openreview.net/pdf?id=Pujt3ADZgI">[PDF<sup id="pdf-stars-Pujt3ADZgI@OpenReview">8</sup>]</a>
                <a id="copy-Pujt3ADZgI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Pujt3ADZgI@OpenReview')">[Copy]</a>
                <a id="kimi-Pujt3ADZgI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Pujt3ADZgI@OpenReview', this)">[Kimi<sup id="kimi-stars-Pujt3ADZgI@OpenReview">14</sup>]</a>
                <a id="rel-Pujt3ADZgI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Pujt3ADZgI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Pujt3ADZgI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuheng Zhang" target="_blank">Yuheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dian Yu" target="_blank">Dian Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baolin Peng" target="_blank">Baolin Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Song" target="_blank">Linfeng Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Tian" target="_blank">Ye Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyue Huo" target="_blank">Mingyue Huo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Jiang" target="_blank">Nan Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haitao Mi" target="_blank">Haitao Mi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Yu" target="_blank">Dong Yu</a>
            </p>
            <p id="summary-Pujt3ADZgI@OpenReview" class="summary">Reinforcement Learning with Human Feedback (RLHF) has achieved great successin aligning large language models (LLMs) with human preferences. PrevalentRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. Inthis paper, we explore RLHF under a general preference framework and approachit from a game-theoretic perspective. Specifically, we formulate the problem asa two-player game and propose a novel online algorithm, iterative Nash policyoptimization (INPO). The key idea is to let the policy play against itself via no-regret learning, thereby approximating the Nash policy. Unlike previous methods,INPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,we introduce a new loss objective that is directly minimized over a preferencedataset. We provide theoretical analysis for our approach and demonstrate itseffectiveness through experiments on various representative benchmarks. With anLLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled winrate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantialimprovement over the state-of-the-art online RLHF algorithms.</p>
            <p id="subjects-Pujt3ADZgI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-Pujt3ADZgI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pujt3ADZgI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pujt3ADZgI@OpenReview" onclick="foldPdfKimi('Pujt3ADZgI@OpenReview', this)" class="hr hr-fold">
        </div><div id="eBS3dQQ8GV@OpenReview" class="panel paper" keywords="meta,stable,mean,geshkovski,field,emergence,clustering,transformer,tokens,pde">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eBS3dQQ8GV" target="_blank" title="182/207"><span class="index notranslate">#182</span></a>
                <a id="title-eBS3dQQ8GV@OpenReview" class="title-link" href="/venue/eBS3dQQ8GV@OpenReview" target="_blank">Emergence of meta-stable clustering in mean-field transformer models</a>
                <a id="pdf-eBS3dQQ8GV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eBS3dQQ8GV@OpenReview', this)" data="https://openreview.net/pdf?id=eBS3dQQ8GV">[PDF<sup id="pdf-stars-eBS3dQQ8GV@OpenReview">4</sup>]</a>
                <a id="copy-eBS3dQQ8GV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eBS3dQQ8GV@OpenReview')">[Copy]</a>
                <a id="kimi-eBS3dQQ8GV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eBS3dQQ8GV@OpenReview', this)">[Kimi<sup id="kimi-stars-eBS3dQQ8GV@OpenReview">9</sup>]</a>
                <a id="rel-eBS3dQQ8GV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eBS3dQQ8GV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eBS3dQQ8GV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Giuseppe Bruno" target="_blank">Giuseppe Bruno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Federico Pasqualotto" target="_blank">Federico Pasqualotto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Agazzi" target="_blank">Andrea Agazzi</a>
            </p>
            <p id="summary-eBS3dQQ8GV@OpenReview" class="summary">We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in Geshkovski et al. (2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as a function of the inverse temperature parameter of the model, by the index maximizing a certain rescaling of Gegenbauer polynomials.</p>
            <p id="subjects-eBS3dQQ8GV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-eBS3dQQ8GV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eBS3dQQ8GV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eBS3dQQ8GV@OpenReview" onclick="foldPdfKimi('eBS3dQQ8GV@OpenReview', this)" class="hr hr-fold">
        </div><div id="WOzffPgVjF@OpenReview" class="panel paper" keywords="stvg,target,cues,queries,video,temporal,object,aware,transformer,multimodal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WOzffPgVjF" target="_blank" title="183/207"><span class="index notranslate">#183</span></a>
                <a id="title-WOzffPgVjF@OpenReview" class="title-link" href="/venue/WOzffPgVjF@OpenReview" target="_blank">Knowing Your Target : Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding</a>
                <a id="pdf-WOzffPgVjF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WOzffPgVjF@OpenReview', this)" data="https://openreview.net/pdf?id=WOzffPgVjF">[PDF<sup id="pdf-stars-WOzffPgVjF@OpenReview">12</sup>]</a>
                <a id="copy-WOzffPgVjF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WOzffPgVjF@OpenReview')">[Copy]</a>
                <a id="kimi-WOzffPgVjF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WOzffPgVjF@OpenReview', this)">[Kimi<sup id="kimi-stars-WOzffPgVjF@OpenReview">14</sup>]</a>
                <a id="rel-WOzffPgVjF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WOzffPgVjF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WOzffPgVjF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Gu" target="_blank">Xin Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaojie Shen" target="_blank">Yaojie Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxi Luo" target="_blank">Chenxi Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejian Luo" target="_blank">Tiejian Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Huang" target="_blank">Yan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=YUEWEI LIN" target="_blank">YUEWEI LIN</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Fan" target="_blank">Heng Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Libo Zhang" target="_blank">Libo Zhang</a>
            </p>
            <p id="summary-WOzffPgVjF@OpenReview" class="summary">Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in the complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and largely outperforms the baseline, validating its efficacy. Code will be released.</p>
            <p id="subjects-WOzffPgVjF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-WOzffPgVjF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WOzffPgVjF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WOzffPgVjF@OpenReview" onclick="foldPdfKimi('WOzffPgVjF@OpenReview', this)" class="hr hr-fold">
        </div><div id="4xWQS2z77v@OpenReview" class="panel paper" keywords="convex,neural,networks,landscape,regularized,loss,connectivity,problem,layer,stationary">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4xWQS2z77v" target="_blank" title="184/207"><span class="index notranslate">#184</span></a>
                <a id="title-4xWQS2z77v@OpenReview" class="title-link" href="/venue/4xWQS2z77v@OpenReview" target="_blank">Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality</a>
                <a id="pdf-4xWQS2z77v@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4xWQS2z77v@OpenReview', this)" data="https://openreview.net/pdf?id=4xWQS2z77v">[PDF<sup id="pdf-stars-4xWQS2z77v@OpenReview">6</sup>]</a>
                <a id="copy-4xWQS2z77v@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4xWQS2z77v@OpenReview')">[Copy]</a>
                <a id="kimi-4xWQS2z77v@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4xWQS2z77v@OpenReview', this)">[Kimi<sup id="kimi-stars-4xWQS2z77v@OpenReview">9</sup>]</a>
                <a id="rel-4xWQS2z77v@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4xWQS2z77v@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4xWQS2z77v@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sungyoon Kim" target="_blank">Sungyoon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Mishkin" target="_blank">Aaron Mishkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mert Pilanci" target="_blank">Mert Pilanci</a>
            </p>
            <p id="summary-4xWQS2z77v@OpenReview" class="summary">We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with non-increasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two layer vector-valued neural networks and parallel three-layer neural networks.</p>
            <p id="subjects-4xWQS2z77v@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-4xWQS2z77v@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4xWQS2z77v@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4xWQS2z77v@OpenReview" onclick="foldPdfKimi('4xWQS2z77v@OpenReview', this)" class="hr hr-fold">
        </div><div id="stUKwWBuBm@OpenReview" class="panel paper" keywords="rqe,economics,tractable,games,equilibria,aversion,agent,reinforcement,behavioral,rationality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=stUKwWBuBm" target="_blank" title="185/207"><span class="index notranslate">#185</span></a>
                <a id="title-stUKwWBuBm@OpenReview" class="title-link" href="/venue/stUKwWBuBm@OpenReview" target="_blank">Tractable Multi-Agent Reinforcement Learning through Behavioral Economics</a>
                <a id="pdf-stUKwWBuBm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('stUKwWBuBm@OpenReview', this)" data="https://openreview.net/pdf?id=stUKwWBuBm">[PDF<sup id="pdf-stars-stUKwWBuBm@OpenReview">7</sup>]</a>
                <a id="copy-stUKwWBuBm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('stUKwWBuBm@OpenReview')">[Copy]</a>
                <a id="kimi-stUKwWBuBm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('stUKwWBuBm@OpenReview', this)">[Kimi<sup id="kimi-stars-stUKwWBuBm@OpenReview">21</sup>]</a>
                <a id="rel-stUKwWBuBm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('stUKwWBuBm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-stUKwWBuBm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Mazumdar" target="_blank">Eric Mazumdar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kishan Panaganti" target="_blank">Kishan Panaganti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laixi Shi" target="_blank">Laixi Shi</a>
            </p>
            <p id="summary-stUKwWBuBm@OpenReview" class="summary">A significant roadblock to the development of principled multi-agent reinforcement learning is the fact that desired solution concepts like Nash equilibria may be intractable to compute. To overcome this obstacle, we take inspiration from behavioral economics and show that---by imbuing agents with important features of human decision-making like risk aversion and bounded rationality---a class of risk-averse quantal response equilibria (RQE) become tractable to compute in all <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-80-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-516" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-517"><span class="mi" id="MathJax-Span-518" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-80">n</script>-player matrix and finite-horizon Markov games. In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degree of risk-aversion and bounded rationality. To validate the richness of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model and validate our findings on a simple multi-agent reinforcement learning benchmark.</p>
            <p id="subjects-stUKwWBuBm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-stUKwWBuBm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-stUKwWBuBm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-stUKwWBuBm@OpenReview" onclick="foldPdfKimi('stUKwWBuBm@OpenReview', this)" class="hr hr-fold">
        </div><div id="is4nCVkSFA@OpenReview" class="panel paper" keywords="star,lor,tradeoff,index,polylogarithmic,achieve,statistical,tilde,neural,gradient">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=is4nCVkSFA" target="_blank" title="186/207"><span class="index notranslate">#186</span></a>
                <a id="title-is4nCVkSFA@OpenReview" class="title-link" href="/venue/is4nCVkSFA@OpenReview" target="_blank">Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model</a>
                <a id="pdf-is4nCVkSFA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('is4nCVkSFA@OpenReview', this)" data="https://openreview.net/pdf?id=is4nCVkSFA">[PDF<sup id="pdf-stars-is4nCVkSFA@OpenReview">4</sup>]</a>
                <a id="copy-is4nCVkSFA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('is4nCVkSFA@OpenReview')">[Copy]</a>
                <a id="kimi-is4nCVkSFA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('is4nCVkSFA@OpenReview', this)">[Kimi<sup id="kimi-stars-is4nCVkSFA@OpenReview">8</sup>]</a>
                <a id="rel-is4nCVkSFA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('is4nCVkSFA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-is4nCVkSFA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Chen" target="_blank">Siyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Beining Wu" target="_blank">Beining Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miao Lu" target="_blank">Miao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoran Yang" target="_blank">Zhuoran Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianhao Wang" target="_blank">Tianhao Wang</a>
            </p>
            <p id="summary-is4nCVkSFA@OpenReview" class="summary">In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? Prior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-81-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2228;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-519" style="width: 6.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.107em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1005em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-520"><span class="mi" id="MathJax-Span-521" style="font-family: MathJax_Main;"></span><span class="mo" id="MathJax-Span-522" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-523"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-524" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-525"><span class="mrow" id="MathJax-Span-526"><span class="msubsup" id="MathJax-Span-527"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-528" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.393em; left: 0.315em;"><span class="mo" id="MathJax-Span-529" style="font-size: 50%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="texatom" id="MathJax-Span-530"><span class="mrow" id="MathJax-Span-531"><span class="mo" id="MathJax-Span-532" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-533" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-534" style="font-family: MathJax_Main; padding-left: 0.211em;"></span><span class="mi" id="MathJax-Span-535" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-536" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal"></mi><mo stretchy="false">(</mo><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo></mo></msup><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo></mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-81">\Omega(d^{s^\star/2}\lor d)</script> samples, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-82-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-537" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-538"><span class="msubsup" id="MathJax-Span-539"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-540" style="font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="mo" id="MathJax-Span-541" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>s</mi><mo></mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-82">s^\star</script> is the generative exponent representing the intrinsic difficulty of learning the underlying model.However, it remains unknown whether neural networks can achieve this sample complexity. Inspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.Our method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.We show that our algorithm learns a feature representation that strongly aligns with the unknown signal <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-83-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-542" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-543"><span class="msubsup" id="MathJax-Span-544"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-545" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="mo" id="MathJax-Span-546" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mo></mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-83">\theta^\star</script>, with sample complexity <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-84-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2228;&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-547" style="width: 6.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1005.05em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-548"><span class="texatom" id="MathJax-Span-549"><span class="mrow" id="MathJax-Span-550"><span class="munderover" id="MathJax-Span-551"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-552" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.211em;"><span class="mo" id="MathJax-Span-553" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-554" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-555"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-556" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-557"><span class="mrow" id="MathJax-Span-558"><span class="msubsup" id="MathJax-Span-559"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-560" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.393em; left: 0.315em;"><span class="mo" id="MathJax-Span-561" style="font-size: 50%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="texatom" id="MathJax-Span-562"><span class="mrow" id="MathJax-Span-563"><span class="mo" id="MathJax-Span-564" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-565" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-566" style="font-family: MathJax_Main; padding-left: 0.211em;"></span><span class="mi" id="MathJax-Span-567" style="font-family: MathJax_Math-italic; padding-left: 0.211em;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-568" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo></mo></msup><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo></mo><mi>d</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-84">\tilde O (d^{s^\star/2} \lor d)</script>, matching the SQ lower bound up to a polylogarithmic factor for all generative exponents <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-85-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-569" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.66em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-570"><span class="msubsup" id="MathJax-Span-571"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-572" style="font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="mo" id="MathJax-Span-573" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-574" style="font-family: MathJax_Main; padding-left: 0.263em;"></span><span class="mn" id="MathJax-Span-575" style="font-family: MathJax_Main; padding-left: 0.263em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>s</mi><mo></mo></msup><mo></mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-85">s^\star\geq 1</script>.Furthermore, we extend our approach to the setting where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-86-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-576" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-577"><span class="msubsup" id="MathJax-Span-578"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-579" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="mo" id="MathJax-Span-580" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mo></mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-86">\theta^\star</script> is <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-87-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-581" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-582"><span class="mi" id="MathJax-Span-583" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-87">k</script>-sparse for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-88-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-584" style="width: 5.315em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.43em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1004.33em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-585"><span class="mi" id="MathJax-Span-586" style="font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-587" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mi" id="MathJax-Span-588" style="font-family: MathJax_Math-italic; padding-left: 0.263em;">o</span><span class="mo" id="MathJax-Span-589" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-590"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-591"><span class="mi" id="MathJax-Span-592" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.52em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;"><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.154em;"><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-593" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.441em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>=</mo><mi>o</mi><mo stretchy="false">(</mo><msqrt><mi>d</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-88">k = o(\sqrt{d})</script> by introducing a novel weight perturbation technique that leverages the sparsity structure. We derive a corresponding SQ lower bound of order <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-89-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x03A9;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msup&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x22C6;&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-594" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1002.61em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-595"><span class="texatom" id="MathJax-Span-596"><span class="mrow" id="MathJax-Span-597"><span class="munderover" id="MathJax-Span-598"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-599" style="font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.107em;"><span class="mo" id="MathJax-Span-600" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-601" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-602"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-603" style="font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -4.372em; left: 0.523em;"><span class="texatom" id="MathJax-Span-604"><span class="mrow" id="MathJax-Span-605"><span class="msubsup" id="MathJax-Span-606"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000.26em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-607" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.393em; left: 0.315em;"><span class="mo" id="MathJax-Span-608" style="font-size: 50%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-609" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi mathvariant="normal"></mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><msup><mi>k</mi><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo></mo></msup></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-89">\tilde\Omega(k^{s^\star})</script>, matched by our method up to a polylogarithmic factor.Our framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA.</p>
            <p id="subjects-is4nCVkSFA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-is4nCVkSFA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-is4nCVkSFA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-is4nCVkSFA@OpenReview" onclick="foldPdfKimi('is4nCVkSFA@OpenReview', this)" class="hr hr-fold">
        </div><div id="7BLXhmWvwF@OpenReview" class="panel paper" keywords="objects,hepi,deformable,cloth,equivariant,tasks,actuators,manipulation,heterogeneous,insertion">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7BLXhmWvwF" target="_blank" title="187/207"><span class="index notranslate">#187</span></a>
                <a id="title-7BLXhmWvwF@OpenReview" class="title-link" href="/venue/7BLXhmWvwF@OpenReview" target="_blank">Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects</a>
                <a id="pdf-7BLXhmWvwF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7BLXhmWvwF@OpenReview', this)" data="https://openreview.net/pdf?id=7BLXhmWvwF">[PDF<sup id="pdf-stars-7BLXhmWvwF@OpenReview">8</sup>]</a>
                <a id="copy-7BLXhmWvwF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7BLXhmWvwF@OpenReview')">[Copy]</a>
                <a id="kimi-7BLXhmWvwF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7BLXhmWvwF@OpenReview', this)">[Kimi<sup id="kimi-stars-7BLXhmWvwF@OpenReview">14</sup>]</a>
                <a id="rel-7BLXhmWvwF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7BLXhmWvwF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7BLXhmWvwF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tai Hoang" target="_blank">Tai Hoang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huy Le" target="_blank">Huy Le</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Becker" target="_blank">Philipp Becker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vien A Ngo" target="_blank">Vien A Ngo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gerhard Neumann" target="_blank">Gerhard Neumann</a>
            </p>
            <p id="summary-7BLXhmWvwF@OpenReview" class="summary">Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-90-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-610" style="width: 3.284em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.61em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-611"><span class="mi" id="MathJax-Span-612" style="font-family: MathJax_Math-italic;">S<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span class="mi" id="MathJax-Span-613" style="font-family: MathJax_Math-italic;">E<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-614" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-615" style="font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-616" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mi>E</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-90">SE(3)</script> equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects.</p>
            <p id="subjects-7BLXhmWvwF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-7BLXhmWvwF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7BLXhmWvwF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7BLXhmWvwF@OpenReview" onclick="foldPdfKimi('7BLXhmWvwF@OpenReview', this)" class="hr hr-fold">
        </div><div id="g3xuCtrG6H@OpenReview" class="panel paper" keywords="color,vision,nerve,optic,dimensionality,cortex,eye,signals,brain,human">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=g3xuCtrG6H" target="_blank" title="188/207"><span class="index notranslate">#188</span></a>
                <a id="title-g3xuCtrG6H@OpenReview" class="title-link" href="/venue/g3xuCtrG6H@OpenReview" target="_blank">A Computational Framework for Modeling Emergence of Color Vision in the Human Brain</a>
                <a id="pdf-g3xuCtrG6H@OpenReview" class="title-pdf notranslate" onclick="togglePdf('g3xuCtrG6H@OpenReview', this)" data="https://openreview.net/pdf?id=g3xuCtrG6H">[PDF<sup id="pdf-stars-g3xuCtrG6H@OpenReview">12</sup>]</a>
                <a id="copy-g3xuCtrG6H@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('g3xuCtrG6H@OpenReview')">[Copy]</a>
                <a id="kimi-g3xuCtrG6H@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('g3xuCtrG6H@OpenReview', this)">[Kimi<sup id="kimi-stars-g3xuCtrG6H@OpenReview">11</sup>]</a>
                <a id="rel-g3xuCtrG6H@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('g3xuCtrG6H@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-g3xuCtrG6H@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Atsunobu Kotani" target="_blank">Atsunobu Kotani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi-Ren Ng" target="_blank">Yi-Ren Ng</a>
            </p>
            <p id="summary-g3xuCtrG6H@OpenReview" class="summary">It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a bio-plausible model of cortical learning based on self-supervised prediction of optic nerve signal fluctuations under natural eye motions. We show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-91-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-617" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-618"><span class="mi" id="MathJax-Span-619" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-91">N</script> types of color photoreceptors, our simulation shows that <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-92-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-620" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-621"><span class="mi" id="MathJax-Span-622" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-92">N</script>-dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D.</p>
            <p id="subjects-g3xuCtrG6H@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-g3xuCtrG6H@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-g3xuCtrG6H@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-g3xuCtrG6H@OpenReview" onclick="foldPdfKimi('g3xuCtrG6H@OpenReview', this)" class="hr hr-fold">
        </div><div id="j7cyANIAxV@OpenReview" class="panel paper" keywords="similarity,drug,evaluation,split,affinity,target,aware,rethinking,set,samples">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=j7cyANIAxV" target="_blank" title="189/207"><span class="index notranslate">#189</span></a>
                <a id="title-j7cyANIAxV@OpenReview" class="title-link" href="/venue/j7cyANIAxV@OpenReview" target="_blank">Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation</a>
                <a id="pdf-j7cyANIAxV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('j7cyANIAxV@OpenReview', this)" data="https://openreview.net/pdf?id=j7cyANIAxV">[PDF<sup id="pdf-stars-j7cyANIAxV@OpenReview">6</sup>]</a>
                <a id="copy-j7cyANIAxV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('j7cyANIAxV@OpenReview')">[Copy]</a>
                <a id="kimi-j7cyANIAxV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('j7cyANIAxV@OpenReview', this)">[Kimi<sup id="kimi-stars-j7cyANIAxV@OpenReview">8</sup>]</a>
                <a id="rel-j7cyANIAxV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('j7cyANIAxV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-j7cyANIAxV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenbin Zhang" target="_blank">Chenbin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqiang Hu" target="_blank">Zhiqiang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiang Chuchu" target="_blank">Jiang Chuchu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Chen" target="_blank">Wen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=JIE XU" target="_blank">JIE XU</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaoting Zhang" target="_blank">Shaoting Zhang</a>
            </p>
            <p id="summary-j7cyANIAxV@OpenReview" class="summary">Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models.</p>
            <p id="subjects-j7cyANIAxV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-j7cyANIAxV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-j7cyANIAxV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-j7cyANIAxV@OpenReview" onclick="foldPdfKimi('j7cyANIAxV@OpenReview', this)" class="hr hr-fold">
        </div><div id="xByvdb3DCm@OpenReview" class="panel paper" keywords="interventions,causal,selection,interventional,discovery,bias,world,meets,intervention,complexities">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xByvdb3DCm" target="_blank" title="190/207"><span class="index notranslate">#190</span></a>
                <a id="title-xByvdb3DCm@OpenReview" class="title-link" href="/venue/xByvdb3DCm@OpenReview" target="_blank">When Selection meets Intervention: Additional Complexities in Causal Discovery</a>
                <a id="pdf-xByvdb3DCm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xByvdb3DCm@OpenReview', this)" data="https://openreview.net/pdf?id=xByvdb3DCm">[PDF<sup id="pdf-stars-xByvdb3DCm@OpenReview">8</sup>]</a>
                <a id="copy-xByvdb3DCm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xByvdb3DCm@OpenReview')">[Copy]</a>
                <a id="kimi-xByvdb3DCm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xByvdb3DCm@OpenReview', this)">[Kimi<sup id="kimi-stars-xByvdb3DCm@OpenReview">13</sup>]</a>
                <a id="rel-xByvdb3DCm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xByvdb3DCm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xByvdb3DCm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyue Dai" target="_blank">Haoyue Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ignavier Ng" target="_blank">Ignavier Ng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianle Sun" target="_blank">Jianle Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Tang" target="_blank">Zeyu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gongxu Luo" target="_blank">Gongxu Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinshuai Dong" target="_blank">Xinshuai Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Spirtes" target="_blank">Peter Spirtes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhang" target="_blank">Kun Zhang</a>
            </p>
            <p id="summary-xByvdb3DCm@OpenReview" class="summary">We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias.</p>
            <p id="subjects-xByvdb3DCm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-xByvdb3DCm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xByvdb3DCm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xByvdb3DCm@OpenReview" onclick="foldPdfKimi('xByvdb3DCm@OpenReview', this)" class="hr hr-fold">
        </div><div id="07yvxWDSla@OpenReview" class="panel paper" keywords="entigraph,pretraining,continued,corpus,synthetic,documents,knowledge,augmentation,entities,source">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=07yvxWDSla" target="_blank" title="191/207"><span class="index notranslate">#191</span></a>
                <a id="title-07yvxWDSla@OpenReview" class="title-link" href="/venue/07yvxWDSla@OpenReview" target="_blank">Synthetic continued pretraining</a>
                <a id="pdf-07yvxWDSla@OpenReview" class="title-pdf notranslate" onclick="togglePdf('07yvxWDSla@OpenReview', this)" data="https://openreview.net/pdf?id=07yvxWDSla">[PDF<sup id="pdf-stars-07yvxWDSla@OpenReview">19</sup>]</a>
                <a id="copy-07yvxWDSla@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('07yvxWDSla@OpenReview')">[Copy]</a>
                <a id="kimi-07yvxWDSla@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('07yvxWDSla@OpenReview', this)">[Kimi<sup id="kimi-stars-07yvxWDSla@OpenReview">23</sup>]</a>
                <a id="rel-07yvxWDSla@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('07yvxWDSla@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-07yvxWDSla@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zitong Yang" target="_blank">Zitong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neil Band" target="_blank">Neil Band</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuangping Li" target="_blank">Shuangping Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emmanuel Candes" target="_blank">Emmanuel Candes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tatsunori Hashimoto" target="_blank">Tatsunori Hashimoto</a>
            </p>
            <p id="summary-07yvxWDSla@OpenReview" class="summary">Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge.However, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it.This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once.We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus.We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source corpus and then generates diverse text by drawing connections between those entities.Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them.If the source documents are instead available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation.To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can "rearrange" knowledge to enable more data-efficient learning.</p>
            <p id="subjects-07yvxWDSla@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-07yvxWDSla@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-07yvxWDSla@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-07yvxWDSla@OpenReview" onclick="foldPdfKimi('07yvxWDSla@OpenReview', this)" class="hr hr-fold">
        </div><div id="pQqeQpMkE7@OpenReview" class="panel paper" keywords="grendel,3dgs,gaussians,splatting,scaling,gpu,psnr,gaussian,gpus,rendering">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pQqeQpMkE7" target="_blank" title="192/207"><span class="index notranslate">#192</span></a>
                <a id="title-pQqeQpMkE7@OpenReview" class="title-link" href="/venue/pQqeQpMkE7@OpenReview" target="_blank">On Scaling Up 3D Gaussian Splatting Training</a>
                <a id="pdf-pQqeQpMkE7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pQqeQpMkE7@OpenReview', this)" data="https://openreview.net/pdf?id=pQqeQpMkE7">[PDF<sup id="pdf-stars-pQqeQpMkE7@OpenReview">6</sup>]</a>
                <a id="copy-pQqeQpMkE7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pQqeQpMkE7@OpenReview')">[Copy]</a>
                <a id="kimi-pQqeQpMkE7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pQqeQpMkE7@OpenReview', this)">[Kimi<sup id="kimi-stars-pQqeQpMkE7@OpenReview">7</sup>]</a>
                <a id="rel-pQqeQpMkE7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pQqeQpMkE7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pQqeQpMkE7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hexu Zhao" target="_blank">Hexu Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyang Weng" target="_blank">Haoyang Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daohan Lu" target="_blank">Daohan Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ang Li" target="_blank">Ang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyang Li" target="_blank">Jinyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aurojit Panda" target="_blank">Aurojit Panda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saining Xie" target="_blank">Saining Xie</a>
            </p>
            <p id="summary-pQqeQpMkE7@OpenReview" class="summary">3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K ``Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU.</p>
            <p id="subjects-pQqeQpMkE7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-pQqeQpMkE7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pQqeQpMkE7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pQqeQpMkE7@OpenReview" onclick="foldPdfKimi('pQqeQpMkE7@OpenReview', this)" class="hr hr-fold">
        </div><div id="vRvVVb0NAz@OpenReview" class="panel paper" keywords="task,editing,generalization,conceptual,tasks,theoretical,unlearning,transformers,irrelevant,arithmetic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vRvVVb0NAz" target="_blank" title="193/207"><span class="index notranslate">#193</span></a>
                <a id="title-vRvVVb0NAz@OpenReview" class="title-link" href="/venue/vRvVVb0NAz@OpenReview" target="_blank">When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</a>
                <a id="pdf-vRvVVb0NAz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vRvVVb0NAz@OpenReview', this)" data="https://openreview.net/pdf?id=vRvVVb0NAz">[PDF<sup id="pdf-stars-vRvVVb0NAz@OpenReview">10</sup>]</a>
                <a id="copy-vRvVVb0NAz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vRvVVb0NAz@OpenReview')">[Copy]</a>
                <a id="kimi-vRvVVb0NAz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vRvVVb0NAz@OpenReview', this)">[Kimi<sup id="kimi-stars-vRvVVb0NAz@OpenReview">10</sup>]</a>
                <a id="rel-vRvVVb0NAz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vRvVVb0NAz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vRvVVb0NAz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongkang Li" target="_blank">Hongkang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yihua Zhang" target="_blank">Yihua Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=shuai ZHANG" target="_blank">shuai ZHANG</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Wang" target="_blank">Meng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sijia Liu" target="_blank">Sijia Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pin-Yu Chen" target="_blank">Pin-Yu Chen</a>
            </p>
            <p id="summary-vRvVVb0NAz@OpenReview" class="summary">Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).</p>
            <p id="subjects-vRvVVb0NAz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-vRvVVb0NAz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vRvVVb0NAz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vRvVVb0NAz@OpenReview" onclick="foldPdfKimi('vRvVVb0NAz@OpenReview', this)" class="hr hr-fold">
        </div><div id="t7P5BUKcYv@OpenReview" class="panel paper" keywords="moe,experts,ffn,expert,zero,computation,vanilla,tokens,skip,mixture">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=t7P5BUKcYv" target="_blank" title="194/207"><span class="index notranslate">#194</span></a>
                <a id="title-t7P5BUKcYv@OpenReview" class="title-link" href="/venue/t7P5BUKcYv@OpenReview" target="_blank">MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts</a>
                <a id="pdf-t7P5BUKcYv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('t7P5BUKcYv@OpenReview', this)" data="https://openreview.net/pdf?id=t7P5BUKcYv">[PDF<sup id="pdf-stars-t7P5BUKcYv@OpenReview">19</sup>]</a>
                <a id="copy-t7P5BUKcYv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('t7P5BUKcYv@OpenReview')">[Copy]</a>
                <a id="kimi-t7P5BUKcYv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('t7P5BUKcYv@OpenReview', this)">[Kimi<sup id="kimi-stars-t7P5BUKcYv@OpenReview">22</sup>]</a>
                <a id="rel-t7P5BUKcYv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('t7P5BUKcYv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-t7P5BUKcYv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Jin" target="_blank">Peng Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zhu" target="_blank">Bo Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Li" target="_blank">Yuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuicheng YAN" target="_blank">Shuicheng YAN</a>
            </p>
            <p id="summary-t7P5BUKcYv@OpenReview" class="summary">In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) **Low Computing Overhead**: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) **High Performance**: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) **Deployment Friendly**: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-93-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-623" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-624"><span class="mo" id="MathJax-Span-625" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-93">\sim</script>2.1<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-94-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-626" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-627"><span class="mo" id="MathJax-Span-628" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-94">\times</script> expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.</p>
            <p id="subjects-t7P5BUKcYv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-t7P5BUKcYv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-t7P5BUKcYv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-t7P5BUKcYv@OpenReview" onclick="foldPdfKimi('t7P5BUKcYv@OpenReview', this)" class="hr hr-fold">
        </div><div id="tcsZt9ZNKD@OpenReview" class="panel paper" keywords="autoencoders,sparsity,autoencoder,sparse,latents,dead,scaling,activations,makhzani,evaluating">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tcsZt9ZNKD" target="_blank" title="195/207"><span class="index notranslate">#195</span></a>
                <a id="title-tcsZt9ZNKD@OpenReview" class="title-link" href="/venue/tcsZt9ZNKD@OpenReview" target="_blank">Scaling and evaluating sparse autoencoders</a>
                <a id="pdf-tcsZt9ZNKD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tcsZt9ZNKD@OpenReview', this)" data="https://openreview.net/pdf?id=tcsZt9ZNKD">[PDF<sup id="pdf-stars-tcsZt9ZNKD@OpenReview">19</sup>]</a>
                <a id="copy-tcsZt9ZNKD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tcsZt9ZNKD@OpenReview')">[Copy]</a>
                <a id="kimi-tcsZt9ZNKD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tcsZt9ZNKD@OpenReview', this)">[Kimi<sup id="kimi-stars-tcsZt9ZNKD@OpenReview">18</sup>]</a>
                <a id="rel-tcsZt9ZNKD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tcsZt9ZNKD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tcsZt9ZNKD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Leo Gao" target="_blank">Leo Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Dupre la Tour" target="_blank">Tom Dupre la Tour</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henk Tillman" target="_blank">Henk Tillman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Goh" target="_blank">Gabriel Goh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rajan Troll" target="_blank">Rajan Troll</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alec Radford" target="_blank">Alec Radford</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilya Sutskever" target="_blank">Ilya Sutskever</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Leike" target="_blank">Jan Leike</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeffrey Wu" target="_blank">Jeffrey Wu</a>
            </p>
            <p id="summary-tcsZt9ZNKD@OpenReview" class="summary">Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.</p>
            <p id="subjects-tcsZt9ZNKD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-tcsZt9ZNKD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tcsZt9ZNKD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tcsZt9ZNKD@OpenReview" onclick="foldPdfKimi('tcsZt9ZNKD@OpenReview', this)" class="hr hr-fold">
        </div><div id="ijbA5swmoK@OpenReview" class="panel paper" keywords="complexity,epsilon,mathcal,hessians,concave,minimax,thecomputational,doikov,oracle,tilde">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ijbA5swmoK" target="_blank" title="196/207"><span class="index notranslate">#196</span></a>
                <a id="title-ijbA5swmoK@OpenReview" class="title-link" href="/venue/ijbA5swmoK@OpenReview" target="_blank">Second-Order Min-Max Optimization with Lazy Hessians</a>
                <a id="pdf-ijbA5swmoK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ijbA5swmoK@OpenReview', this)" data="https://openreview.net/pdf?id=ijbA5swmoK">[PDF<sup id="pdf-stars-ijbA5swmoK@OpenReview">11</sup>]</a>
                <a id="copy-ijbA5swmoK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ijbA5swmoK@OpenReview')">[Copy]</a>
                <a id="kimi-ijbA5swmoK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ijbA5swmoK@OpenReview', this)">[Kimi<sup id="kimi-stars-ijbA5swmoK@OpenReview">7</sup>]</a>
                <a id="rel-ijbA5swmoK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ijbA5swmoK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ijbA5swmoK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lesi Chen" target="_blank">Lesi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengchang Liu" target="_blank">Chengchang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingzhao Zhang" target="_blank">Jingzhao Zhang</a>
            </p>
            <p id="summary-ijbA5swmoK@OpenReview" class="summary">This paper studies second-order methods for convex-concave minimax optimization. Monteiro &amp; Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-95-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-629" style="width: 4.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.54em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-630"><span class="texatom" id="MathJax-Span-631"><span class="mrow" id="MathJax-Span-632"><span class="mi" id="MathJax-Span-633" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-634" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-635"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-636" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-637"><span class="mrow" id="MathJax-Span-638"><span class="mo" id="MathJax-Span-639" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-640" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span class="texatom" id="MathJax-Span-641"><span class="mrow" id="MathJax-Span-642"><span class="mo" id="MathJax-Span-643" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-644" style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-645" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>3</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-95">\mathcal{O}(\epsilon^{-3/2})</script> to find an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-96-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-646" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-647"><span class="mi" id="MathJax-Span-648" style="font-family: MathJax_Math-italic;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-96">\epsilon</script>-saddle point. However, it is unclear whether thecomputational complexity, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-97-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-649" style="width: 9.69em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.076em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1007.97em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-650"><span class="texatom" id="MathJax-Span-651"><span class="mrow" id="MathJax-Span-652"><span class="mi" id="MathJax-Span-653" style="font-family: MathJax_Caligraphic;">O</span></span></span><span class="mo" id="MathJax-Span-654" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-655" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-656" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-657" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-658" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-659" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="mn" id="MathJax-Span-660" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-661" style="font-family: MathJax_Main;">)</span><span class="mi" id="MathJax-Span-662" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msubsup" id="MathJax-Span-663"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-664" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-665"><span class="mrow" id="MathJax-Span-666"><span class="mo" id="MathJax-Span-667" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-668" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-669"><span class="mrow" id="MathJax-Span-670"><span class="mo" id="MathJax-Span-671" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-672" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-673" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mi>d</mi><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-97">\mathcal{O}((N+ d^2) d \epsilon^{-2/3})</script>, can be improved. In the above, we follow Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-98-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-674" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-675"><span class="mi" id="MathJax-Span-676" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-98">N</script> and the complexity of obtaining a second-order oracle as <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-99-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-677" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.46em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-678"><span class="mi" id="MathJax-Span-679" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-680" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mi>N</mi></math></span></span><script type="math/tex" id="MathJax-Element-99">dN</script>. In this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-100-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-681" style="width: 14.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1011.62em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-682"><span class="texatom" id="MathJax-Span-683"><span class="mrow" id="MathJax-Span-684"><span class="munderover" id="MathJax-Span-685"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="texatom" id="MathJax-Span-686"><span class="mrow" id="MathJax-Span-687"><span class="mi" id="MathJax-Span-688" style="font-family: MathJax_Caligraphic;">O</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.263em;"><span class="mo" id="MathJax-Span-689" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-690" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-691" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-692" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-693" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-694" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-695" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="mn" id="MathJax-Span-696" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-697" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-698" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-699" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-700" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-701" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-702" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-703"><span class="mrow" id="MathJax-Span-704"><span class="mn" id="MathJax-Span-705" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-706"><span class="mrow" id="MathJax-Span-707"><span class="mo" id="MathJax-Span-708" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-709" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-710"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-711" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.419em;"><span class="texatom" id="MathJax-Span-712"><span class="mrow" id="MathJax-Span-713"><span class="mo" id="MathJax-Span-714" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span class="mn" id="MathJax-Span-715" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-716"><span class="mrow" id="MathJax-Span-717"><span class="mo" id="MathJax-Span-718" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-719" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-720" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-721" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo></mo><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-100">\tilde{\mathcal{O}}( (N+d^2)(d+ d^{2/3}\epsilon^{-2/3}))</script>, which improves those of previous methods by a factor of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-101-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-722" style="width: 2.034em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1001.67em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-723"><span class="msubsup" id="MathJax-Span-724"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-725" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-726"><span class="mrow" id="MathJax-Span-727"><span class="mn" id="MathJax-Span-728" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-729"><span class="mrow" id="MathJax-Span-730"><span class="mo" id="MathJax-Span-731" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-732" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-101">d^{1/3}</script>. Furthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-102-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;O&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03BA;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-733" style="width: 13.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1011.2em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-734"><span class="texatom" id="MathJax-Span-735"><span class="mrow" id="MathJax-Span-736"><span class="munderover" id="MathJax-Span-737"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="texatom" id="MathJax-Span-738"><span class="mrow" id="MathJax-Span-739"><span class="mi" id="MathJax-Span-740" style="font-family: MathJax_Caligraphic;">O</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.086em, -999.997em); top: -2.758em; left: 0.263em;"><span class="mo" id="MathJax-Span-741" style="font-family: MathJax_Main;">~</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-742" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-743" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-744" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span class="mo" id="MathJax-Span-745" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-746" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-747" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="mn" id="MathJax-Span-748" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-749" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-750" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-751" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-752" style="font-family: MathJax_Main; padding-left: 0.211em;">+</span><span class="msubsup" id="MathJax-Span-753" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-754" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-755"><span class="mrow" id="MathJax-Span-756"><span class="mn" id="MathJax-Span-757" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-758"><span class="mrow" id="MathJax-Span-759"><span class="mo" id="MathJax-Span-760" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-761" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-762"><span style="display: inline-block; position: relative; width: 1.669em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-763" style="font-family: MathJax_Math-italic;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.576em;"><span class="texatom" id="MathJax-Span-764"><span class="mrow" id="MathJax-Span-765"><span class="mn" id="MathJax-Span-766" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-767"><span class="mrow" id="MathJax-Span-768"><span class="mo" id="MathJax-Span-769" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-770" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-771" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-772" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><msup><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-102">\tilde{\mathcal{O}}((N+d^2) (d + d^{2/3} \kappa^{2/3}) )</script> when the condition number of the problem is <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-103-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03BA;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-773" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-774"><span class="mi" id="MathJax-Span-775" style="font-family: MathJax_Math-italic;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-103">\kappa</script>, enjoying a similar speedup upon the state-of-the-art method. Numerical experiments on both real and synthetic datasets also verify the efficiency of our method.</p>
            <p id="subjects-ijbA5swmoK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-ijbA5swmoK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ijbA5swmoK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ijbA5swmoK@OpenReview" onclick="foldPdfKimi('ijbA5swmoK@OpenReview', this)" class="hr hr-fold">
        </div><div id="gQlxd3Mtru@OpenReview" class="panel paper" keywords="ruot,snapshots,unbalanced,regularized,dynamics,waddington,stochastic,transport,seq,sparsely">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gQlxd3Mtru" target="_blank" title="197/207"><span class="index notranslate">#197</span></a>
                <a id="title-gQlxd3Mtru@OpenReview" class="title-link" href="/venue/gQlxd3Mtru@OpenReview" target="_blank">Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport</a>
                <a id="pdf-gQlxd3Mtru@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gQlxd3Mtru@OpenReview', this)" data="https://openreview.net/pdf?id=gQlxd3Mtru">[PDF<sup id="pdf-stars-gQlxd3Mtru@OpenReview">7</sup>]</a>
                <a id="copy-gQlxd3Mtru@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gQlxd3Mtru@OpenReview')">[Copy]</a>
                <a id="kimi-gQlxd3Mtru@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gQlxd3Mtru@OpenReview', this)">[Kimi<sup id="kimi-stars-gQlxd3Mtru@OpenReview">8</sup>]</a>
                <a id="rel-gQlxd3Mtru@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gQlxd3Mtru@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gQlxd3Mtru@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyi Zhang" target="_blank">Zhenyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejun Li" target="_blank">Tiejun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peijie Zhou" target="_blank">Peijie Zhou</a>
            </p>
            <p id="summary-gQlxd3Mtru@OpenReview" class="summary">Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learnt directly from data. Theoretically, we explore the connections between the RUOT and Schrdinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape.</p>
            <p id="subjects-gQlxd3Mtru@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-gQlxd3Mtru@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gQlxd3Mtru@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gQlxd3Mtru@OpenReview" onclick="foldPdfKimi('gQlxd3Mtru@OpenReview', this)" class="hr hr-fold">
        </div><div id="ny8T8OuNHe@OpenReview" class="panel paper" keywords="controlnets,ctrl,video,adapter,control,diverse,diffusion,backbones,versatile,controls">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ny8T8OuNHe" target="_blank" title="198/207"><span class="index notranslate">#198</span></a>
                <a id="title-ny8T8OuNHe@OpenReview" class="title-link" href="/venue/ny8T8OuNHe@OpenReview" target="_blank">Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model</a>
                <a id="pdf-ny8T8OuNHe@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ny8T8OuNHe@OpenReview', this)" data="https://openreview.net/pdf?id=ny8T8OuNHe">[PDF<sup id="pdf-stars-ny8T8OuNHe@OpenReview">24</sup>]</a>
                <a id="copy-ny8T8OuNHe@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ny8T8OuNHe@OpenReview')">[Copy]</a>
                <a id="kimi-ny8T8OuNHe@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ny8T8OuNHe@OpenReview', this)">[Kimi<sup id="kimi-stars-ny8T8OuNHe@OpenReview">16</sup>]</a>
                <a id="rel-ny8T8OuNHe@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ny8T8OuNHe@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ny8T8OuNHe@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Han Lin" target="_blank">Han Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaemin Cho" target="_blank">Jaemin Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhay Zala" target="_blank">Abhay Zala</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohit Bansal" target="_blank">Mohit Bansal</a>
            </p>
            <p id="summary-ny8T8OuNHe@OpenReview" class="summary">ControlNets are widely used for adding spatial control to text-to-image diffusion models. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames can not effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control, zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (&lt; 10 GPU hours). We provide video examples in https://ctrladapterexamples.github.io and code in the supplementary material.</p>
            <p id="subjects-ny8T8OuNHe@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-ny8T8OuNHe@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ny8T8OuNHe@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ny8T8OuNHe@OpenReview" onclick="foldPdfKimi('ny8T8OuNHe@OpenReview', this)" class="hr hr-fold">
        </div><div id="OIvg3MqWX2@OpenReview" class="panel paper" keywords="graph,molecules,gnns,graphs,sparsity,connectivity,rigidity,guarantees,sparse,principled">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OIvg3MqWX2" target="_blank" title="199/207"><span class="index notranslate">#199</span></a>
                <a id="title-OIvg3MqWX2@OpenReview" class="title-link" href="/venue/OIvg3MqWX2@OpenReview" target="_blank">A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules</a>
                <a id="pdf-OIvg3MqWX2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OIvg3MqWX2@OpenReview', this)" data="https://openreview.net/pdf?id=OIvg3MqWX2">[PDF<sup id="pdf-stars-OIvg3MqWX2@OpenReview">15</sup>]</a>
                <a id="copy-OIvg3MqWX2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OIvg3MqWX2@OpenReview')">[Copy]</a>
                <a id="kimi-OIvg3MqWX2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OIvg3MqWX2@OpenReview', this)">[Kimi<sup id="kimi-stars-OIvg3MqWX2@OpenReview">7</sup>]</a>
                <a id="rel-OIvg3MqWX2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OIvg3MqWX2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OIvg3MqWX2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shih-Hsin Wang" target="_blank">Shih-Hsin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhao Huang" target="_blank">Yuhao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Baker" target="_blank">Justin Baker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan-En Sun" target="_blank">Yuan-En Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Tang" target="_blank">Qi Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bao Wang" target="_blank">Bao Wang</a>
            </p>
            <p id="summary-OIvg3MqWX2@OpenReview" class="summary">Graph neural networks (GNNs) -- learn graph representations by exploiting graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical guarantees for achieving connectivity and sparsity simultaneously, which are essential for the performance and scalability of GNNs. Furthermore, existing widely used graph construction methods for molecules lack rigidity, limiting GNNs' ability to exploit graph nodes' spatial arrangement. In this paper, we introduce a new hyperparameter-free graph construction of molecules and beyond with sparsity, connectivity, and rigidity guarantees. Remarkably, our method consistently generates connected and sparse graphs with the edge-to-node ratio being bounded above by 3. Our graphs' rigidity guarantees that edge distances and dihedral angles are sufficient to uniquely determine general spatial arrangements of atoms. We substantiate the effectiveness and efficiency of our proposed graphs in various molecular modeling benchmarks.</p>
            <p id="subjects-OIvg3MqWX2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-OIvg3MqWX2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OIvg3MqWX2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OIvg3MqWX2@OpenReview" onclick="foldPdfKimi('OIvg3MqWX2@OpenReview', this)" class="hr hr-fold">
        </div><div id="o2Igqm95SJ@OpenReview" class="panel paper" keywords="cax,cellular,automata,jax,accelerated,library,research,accelerate,life,modular">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=o2Igqm95SJ" target="_blank" title="200/207"><span class="index notranslate">#200</span></a>
                <a id="title-o2Igqm95SJ@OpenReview" class="title-link" href="/venue/o2Igqm95SJ@OpenReview" target="_blank">CAX: Cellular Automata Accelerated in JAX</a>
                <a id="pdf-o2Igqm95SJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('o2Igqm95SJ@OpenReview', this)" data="https://openreview.net/pdf?id=o2Igqm95SJ">[PDF<sup id="pdf-stars-o2Igqm95SJ@OpenReview">8</sup>]</a>
                <a id="copy-o2Igqm95SJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('o2Igqm95SJ@OpenReview')">[Copy]</a>
                <a id="kimi-o2Igqm95SJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('o2Igqm95SJ@OpenReview', this)">[Kimi<sup id="kimi-stars-o2Igqm95SJ@OpenReview">9</sup>]</a>
                <a id="rel-o2Igqm95SJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('o2Igqm95SJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-o2Igqm95SJ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maxence Faldor" target="_blank">Maxence Faldor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antoine Cully" target="_blank">Antoine Cully</a>
            </p>
            <p id="summary-o2Igqm95SJ@OpenReview" class="summary">Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines, spanning neuroscience, artificial life, and theoretical physics. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX offers cutting-edge performance and a modular design through a user-friendly interface, and can support both discrete and continuous cellular automata with any number of dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.</p>
            <p id="subjects-o2Igqm95SJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-o2Igqm95SJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-o2Igqm95SJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-o2Igqm95SJ@OpenReview" onclick="foldPdfKimi('o2Igqm95SJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="A3YUPeJTNR@OpenReview" class="panel paper" keywords="allocations,predictions,individuals,planner,wait,observations,improve,allocation,ranking,waiting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=A3YUPeJTNR" target="_blank" title="201/207"><span class="index notranslate">#201</span></a>
                <a id="title-A3YUPeJTNR@OpenReview" class="title-link" href="/venue/A3YUPeJTNR@OpenReview" target="_blank">The Hidden Cost of Waiting for Accurate Predictions</a>
                <a id="pdf-A3YUPeJTNR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('A3YUPeJTNR@OpenReview', this)" data="https://openreview.net/pdf?id=A3YUPeJTNR">[PDF<sup id="pdf-stars-A3YUPeJTNR@OpenReview">3</sup>]</a>
                <a id="copy-A3YUPeJTNR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('A3YUPeJTNR@OpenReview')">[Copy]</a>
                <a id="kimi-A3YUPeJTNR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('A3YUPeJTNR@OpenReview', this)">[Kimi<sup id="kimi-stars-A3YUPeJTNR@OpenReview">6</sup>]</a>
                <a id="rel-A3YUPeJTNR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('A3YUPeJTNR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-A3YUPeJTNR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Shirali" target="_blank">Ali Shirali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ariel Procaccia" target="_blank">Ariel Procaccia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rediet Abebe" target="_blank">Rediet Abebe</a>
            </p>
            <p id="summary-A3YUPeJTNR@OpenReview" class="summary">Algorithmic predictions are increasingly informing societal resource allocations by identifying individuals for targeting. Policymakers often build these systems with the assumption that by gathering more observations on individuals, they can improve predictive accuracy and, consequently, allocation efficiency. An overlooked yet consequential aspect of prediction-driven allocations is that of timing. The planner has to trade off relying on earlier and potentially noisier predictions to intervene before individuals experience undesirable outcomes, or they may wait to gather more observations to make more precise allocations. We examine this tension using a simple mathematical model, where the planner collects observations on individuals to improve predictions over time. We analyze both the ranking induced by these predictions and optimal resource allocation. We show that though individual prediction accuracy improves over time, counter-intuitively, the average ranking loss can worsen. As a result, the planner's ability to improve social welfare can decline. We identify inequality as a driving factor behind this phenomenon. Our findings provide a nuanced perspective and challenge the conventional wisdom that it is preferable to wait for more accurate predictions to ensure the most efficient allocations.</p>
            <p id="subjects-A3YUPeJTNR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-A3YUPeJTNR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-A3YUPeJTNR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-A3YUPeJTNR@OpenReview" onclick="foldPdfKimi('A3YUPeJTNR@OpenReview', this)" class="hr hr-fold">
        </div><div id="n2NidsYDop@OpenReview" class="panel paper" keywords="parity,cot,transformers,intermediate,reasoning,solve,chain,thought,teacher,efficiently">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=n2NidsYDop" target="_blank" title="202/207"><span class="index notranslate">#202</span></a>
                <a id="title-n2NidsYDop@OpenReview" class="title-link" href="/venue/n2NidsYDop@OpenReview" target="_blank">Transformers Provably Solve Parity Efficiently with Chain of Thought</a>
                <a id="pdf-n2NidsYDop@OpenReview" class="title-pdf notranslate" onclick="togglePdf('n2NidsYDop@OpenReview', this)" data="https://openreview.net/pdf?id=n2NidsYDop">[PDF<sup id="pdf-stars-n2NidsYDop@OpenReview">12</sup>]</a>
                <a id="copy-n2NidsYDop@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('n2NidsYDop@OpenReview')">[Copy]</a>
                <a id="kimi-n2NidsYDop@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('n2NidsYDop@OpenReview', this)">[Kimi<sup id="kimi-stars-n2NidsYDop@OpenReview">19</sup>]</a>
                <a id="rel-n2NidsYDop@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('n2NidsYDop@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-n2NidsYDop@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Juno Kim" target="_blank">Juno Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiji Suzuki" target="_blank">Taiji Suzuki</a>
            </p>
            <p id="summary-n2NidsYDop@OpenReview" class="summary">This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-104-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-776" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-777"><span class="mi" id="MathJax-Span-778" style="font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-104">k</script>-parity problem, extending the work on RNNs by \citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. Our findings, supported by numerical experiments, show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve multi-step reasoning ability, aligning with empirical studies of CoT.</p>
            <p id="subjects-n2NidsYDop@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-n2NidsYDop@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-n2NidsYDop@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-n2NidsYDop@OpenReview" onclick="foldPdfKimi('n2NidsYDop@OpenReview', this)" class="hr hr-fold">
        </div><div id="v593OaNePQ@OpenReview" class="panel paper" keywords="tsn,search,sequences,tree,world,demonstrations,scenarios,learning,procgen,planning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=v593OaNePQ" target="_blank" title="203/207"><span class="index notranslate">#203</span></a>
                <a id="title-v593OaNePQ@OpenReview" class="title-link" href="/venue/v593OaNePQ@OpenReview" target="_blank">Learning to Search from Demonstration Sequences</a>
                <a id="pdf-v593OaNePQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('v593OaNePQ@OpenReview', this)" data="https://openreview.net/pdf?id=v593OaNePQ">[PDF<sup id="pdf-stars-v593OaNePQ@OpenReview">7</sup>]</a>
                <a id="copy-v593OaNePQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('v593OaNePQ@OpenReview')">[Copy]</a>
                <a id="kimi-v593OaNePQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('v593OaNePQ@OpenReview', this)">[Kimi<sup id="kimi-stars-v593OaNePQ@OpenReview">12</sup>]</a>
                <a id="rel-v593OaNePQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('v593OaNePQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-v593OaNePQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dixant Mittal" target="_blank">Dixant Mittal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Kang" target="_blank">Liwei Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wee Sun Lee" target="_blank">Wee Sun Lee</a>
            </p>
            <p id="summary-v593OaNePQ@OpenReview" class="summary">Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned.</p>
            <p id="subjects-v593OaNePQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-v593OaNePQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-v593OaNePQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-v593OaNePQ@OpenReview" onclick="foldPdfKimi('v593OaNePQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="je3GZissZc@OpenReview" class="panel paper" keywords="icil,instant,demonstrations,policy,imitation,context,graph,tasks,diffusion,learning">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=je3GZissZc" target="_blank" title="204/207"><span class="index notranslate">#204</span></a>
                <a id="title-je3GZissZc@OpenReview" class="title-link" href="/venue/je3GZissZc@OpenReview" target="_blank">Instant Policy: In-Context Imitation Learning via Graph Diffusion</a>
                <a id="pdf-je3GZissZc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('je3GZissZc@OpenReview', this)" data="https://openreview.net/pdf?id=je3GZissZc">[PDF<sup id="pdf-stars-je3GZissZc@OpenReview">14</sup>]</a>
                <a id="copy-je3GZissZc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('je3GZissZc@OpenReview')">[Copy]</a>
                <a id="kimi-je3GZissZc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('je3GZissZc@OpenReview', this)">[Kimi<sup id="kimi-stars-je3GZissZc@OpenReview">12</sup>]</a>
                <a id="rel-je3GZissZc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('je3GZissZc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-je3GZissZc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vitalis Vosylius" target="_blank">Vitalis Vosylius</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edward Johns" target="_blank">Edward Johns</a>
            </p>
            <p id="summary-je3GZissZc@OpenReview" class="summary">Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations  arbitrary trajectories generated in simulation  as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks.</p>
            <p id="subjects-je3GZissZc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-je3GZissZc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-je3GZissZc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-je3GZissZc@OpenReview" onclick="foldPdfKimi('je3GZissZc@OpenReview', this)" class="hr hr-fold">
        </div><div id="eFGQ97z5Cd@OpenReview" class="panel paper" keywords="embedding,moee,moe,finetuning,llms,experts,llm,secretly,mixture,tasks">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=eFGQ97z5Cd" target="_blank" title="205/207"><span class="index notranslate">#205</span></a>
                <a id="title-eFGQ97z5Cd@OpenReview" class="title-link" href="/venue/eFGQ97z5Cd@OpenReview" target="_blank">Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free</a>
                <a id="pdf-eFGQ97z5Cd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('eFGQ97z5Cd@OpenReview', this)" data="https://openreview.net/pdf?id=eFGQ97z5Cd">[PDF<sup id="pdf-stars-eFGQ97z5Cd@OpenReview">19</sup>]</a>
                <a id="copy-eFGQ97z5Cd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('eFGQ97z5Cd@OpenReview')">[Copy]</a>
                <a id="kimi-eFGQ97z5Cd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('eFGQ97z5Cd@OpenReview', this)">[Kimi<sup id="kimi-stars-eFGQ97z5Cd@OpenReview">33</sup>]</a>
                <a id="rel-eFGQ97z5Cd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('eFGQ97z5Cd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-eFGQ97z5Cd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyue Li" target="_blank">Ziyue Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Zhou" target="_blank">Tianyi Zhou</a>
            </p>
            <p id="summary-eFGQ97z5Cd@OpenReview" class="summary">While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.</p>
            <p id="subjects-eFGQ97z5Cd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-eFGQ97z5Cd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-eFGQ97z5Cd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-eFGQ97z5Cd@OpenReview" onclick="foldPdfKimi('eFGQ97z5Cd@OpenReview', this)" class="hr hr-fold">
        </div><div id="P7KIGdgW8S@OpenReview" class="panel paper" keywords="mpnns,separation,hlder,multiset,expectation,quality,graph,maximally,analysis,lipschitz">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=P7KIGdgW8S" target="_blank" title="206/207"><span class="index notranslate">#206</span></a>
                <a id="title-P7KIGdgW8S@OpenReview" class="title-link" href="/venue/P7KIGdgW8S@OpenReview" target="_blank">On the Hlder Stability of Multiset and Graph Neural Networks</a>
                <a id="pdf-P7KIGdgW8S@OpenReview" class="title-pdf notranslate" onclick="togglePdf('P7KIGdgW8S@OpenReview', this)" data="https://openreview.net/pdf?id=P7KIGdgW8S">[PDF<sup id="pdf-stars-P7KIGdgW8S@OpenReview">7</sup>]</a>
                <a id="copy-P7KIGdgW8S@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('P7KIGdgW8S@OpenReview')">[Copy]</a>
                <a id="kimi-P7KIGdgW8S@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('P7KIGdgW8S@OpenReview', this)">[Kimi<sup id="kimi-stars-P7KIGdgW8S@OpenReview">11</sup>]</a>
                <a id="rel-P7KIGdgW8S@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('P7KIGdgW8S@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-P7KIGdgW8S@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yair Davidson" target="_blank">Yair Davidson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nadav Dym" target="_blank">Nadav Dym</a>
            </p>
            <p id="summary-P7KIGdgW8S@OpenReview" class="summary">Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of "separable" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough.In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and Hlder stability to parametric functions. The proposed framework, which we name Hlder in expectation, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-Hlder in expectation, with an exponent that decays rapidly with the network's depth . Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.</p>
            <p id="subjects-P7KIGdgW8S@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-P7KIGdgW8S@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-P7KIGdgW8S@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-P7KIGdgW8S@OpenReview" onclick="foldPdfKimi('P7KIGdgW8S@OpenReview', this)" class="hr hr-fold">
        </div><div id="LyJi5ugyJx@OpenReview" class="panel paper" keywords="512,cms,imagenet,diffusion,fid,continuous,consistency,scores,models,simplifying">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LyJi5ugyJx" target="_blank" title="207/207"><span class="index notranslate">#207</span></a>
                <a id="title-LyJi5ugyJx@OpenReview" class="title-link" href="/venue/LyJi5ugyJx@OpenReview" target="_blank">Simplifying, Stabilizing and Scaling Continuous-time Consistency Models</a>
                <a id="pdf-LyJi5ugyJx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LyJi5ugyJx@OpenReview', this)" data="https://openreview.net/pdf?id=LyJi5ugyJx">[PDF<sup id="pdf-stars-LyJi5ugyJx@OpenReview">21</sup>]</a>
                <a id="copy-LyJi5ugyJx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LyJi5ugyJx@OpenReview')">[Copy]</a>
                <a id="kimi-LyJi5ugyJx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LyJi5ugyJx@OpenReview', this)">[Kimi<sup id="kimi-stars-LyJi5ugyJx@OpenReview">22</sup>]</a>
                <a id="rel-LyJi5ugyJx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LyJi5ugyJx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LyJi5ugyJx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Lu" target="_blank">Cheng Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Song" target="_blank">Yang Song</a>
            </p>
            <p id="summary-LyJi5ugyJx@OpenReview" class="summary">Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 6464, and 1.88 on ImageNet 512512, narrowing the gap in FID scores with the best existing diffusion models to within 10\%.</p>
            <p id="subjects-LyJi5ugyJx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICLR.2025?group=Oral" target="_blank">ICLR.2025 - Oral</a>
            </p>
            <div id="pdf-container-LyJi5ugyJx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LyJi5ugyJx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LyJi5ugyJx@OpenReview" onclick="foldPdfKimi('LyJi5ugyJx@OpenReview', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-GMwRl2e9Y1@OpenReview" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#GMwRl2e9Y1@OpenReview">Restructuring Vector Quantization with the Rotation Trick</a>
                    <a class="i-star" onclick="toggleAppStar('GMwRl2e9Y1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GMwRl2e9Y1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-GRMfXcAAFh@OpenReview" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#GRMfXcAAFh@OpenReview">Oscillatory State-Space Models</a>
                    <a class="i-star" onclick="toggleAppStar('GRMfXcAAFh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GRMfXcAAFh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-cmfyMV45XO@OpenReview" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#cmfyMV45XO@OpenReview">Feedback Favors the Generalization of Neural ODEs</a>
                    <a class="i-star" onclick="toggleAppStar('cmfyMV45XO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cmfyMV45XO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-k38Th3x4d9@OpenReview" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#k38Th3x4d9@OpenReview">Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery</a>
                    <a class="i-star" onclick="toggleAppStar('k38Th3x4d9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('k38Th3x4d9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-rdv6yeMFpn@OpenReview" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#rdv6yeMFpn@OpenReview">Homomorphism Expressivity of Spectral Invariant Graph Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('rdv6yeMFpn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rdv6yeMFpn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-vf5aUZT0Fz@OpenReview" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#vf5aUZT0Fz@OpenReview">DEPT: Decoupled Embeddings for Pre-training Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('vf5aUZT0Fz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vf5aUZT0Fz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-wM2sfVgMDH@OpenReview" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#wM2sfVgMDH@OpenReview">Diffusion-Based Planning for Autonomous Driving with Flexible Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('wM2sfVgMDH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wM2sfVgMDH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-CLE09ESvul@OpenReview" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#CLE09ESvul@OpenReview">What should a neuron aim for? Designing local objective functions based on information theory</a>
                    <a class="i-star" onclick="toggleAppStar('CLE09ESvul@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CLE09ESvul@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-JDm7oIcx4Y@OpenReview" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#JDm7oIcx4Y@OpenReview">Accelerated training through iterative gradient propagation along the residual path</a>
                    <a class="i-star" onclick="toggleAppStar('JDm7oIcx4Y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('JDm7oIcx4Y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-VVixJ9QavY@OpenReview" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#VVixJ9QavY@OpenReview">Reasoning Elicitation in Language Models via Counterfactual Feedback</a>
                    <a class="i-star" onclick="toggleAppStar('VVixJ9QavY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VVixJ9QavY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ha6RTeWMd0@OpenReview" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#Ha6RTeWMd0@OpenReview">SAM 2: Segment Anything in Images and Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Ha6RTeWMd0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ha6RTeWMd0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-xXTkbTBmqq@OpenReview" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#xXTkbTBmqq@OpenReview">OLMoE: Open Mixture-of-Experts Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('xXTkbTBmqq@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xXTkbTBmqq@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-8EfxjTCg2k@OpenReview" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#8EfxjTCg2k@OpenReview">MoDeGPT: Modular Decomposition for Large Language Model Compression</a>
                    <a class="i-star" onclick="toggleAppStar('8EfxjTCg2k@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8EfxjTCg2k@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-UvTo3tVBk2@OpenReview" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#UvTo3tVBk2@OpenReview">Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues</a>
                    <a class="i-star" onclick="toggleAppStar('UvTo3tVBk2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UvTo3tVBk2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-kbjJ9ZOakb@OpenReview" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#kbjJ9ZOakb@OpenReview">Learning and aligning single-neuron invariance manifolds in visual cortex</a>
                    <a class="i-star" onclick="toggleAppStar('kbjJ9ZOakb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kbjJ9ZOakb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-0ctvBgKFgc@OpenReview" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#0ctvBgKFgc@OpenReview">ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids</a>
                    <a class="i-star" onclick="toggleAppStar('0ctvBgKFgc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0ctvBgKFgc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-TVQLu34bdw@OpenReview" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#TVQLu34bdw@OpenReview">Proteina: Scaling Flow-based Protein Structure Generative Models</a>
                    <a class="i-star" onclick="toggleAppStar('TVQLu34bdw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TVQLu34bdw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-HnhNRrLPwm@OpenReview" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#HnhNRrLPwm@OpenReview">MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('HnhNRrLPwm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HnhNRrLPwm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-1pXzC30ry5@OpenReview" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#1pXzC30ry5@OpenReview">RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything</a>
                    <a class="i-star" onclick="toggleAppStar('1pXzC30ry5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1pXzC30ry5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-DzGe40glxs@OpenReview" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#DzGe40glxs@OpenReview">Interpreting Emergent Planning in Model-Free Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('DzGe40glxs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DzGe40glxs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-vo9t20wsmd@OpenReview" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#vo9t20wsmd@OpenReview">Faster Cascades via Speculative Decoding</a>
                    <a class="i-star" onclick="toggleAppStar('vo9t20wsmd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vo9t20wsmd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-6s5uXNWGIh@OpenReview" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#6s5uXNWGIh@OpenReview">MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering</a>
                    <a class="i-star" onclick="toggleAppStar('6s5uXNWGIh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6s5uXNWGIh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-t8FG4cJuL3@OpenReview" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#t8FG4cJuL3@OpenReview">Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games</a>
                    <a class="i-star" onclick="toggleAppStar('t8FG4cJuL3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('t8FG4cJuL3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-st77ShxP1K@OpenReview" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#st77ShxP1K@OpenReview">Do as We Do, Not as You Think: the Conformity of Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('st77ShxP1K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('st77ShxP1K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-xoXn62FzD0@OpenReview" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#xoXn62FzD0@OpenReview">Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo</a>
                    <a class="i-star" onclick="toggleAppStar('xoXn62FzD0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xoXn62FzD0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-wg1PCg3CUP@OpenReview" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#wg1PCg3CUP@OpenReview">Scaling Laws for Precision</a>
                    <a class="i-star" onclick="toggleAppStar('wg1PCg3CUP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wg1PCg3CUP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tyEyYT267x@OpenReview" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#tyEyYT267x@OpenReview">Interpolating Autoregressive and Discrete Denoising Diffusion Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('tyEyYT267x@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tyEyYT267x@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tcvMzR2NrP@OpenReview" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#tcvMzR2NrP@OpenReview">Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('tcvMzR2NrP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tcvMzR2NrP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tPNHOoZFl9@OpenReview" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#tPNHOoZFl9@OpenReview">Learning Dynamics of LLM Finetuning</a>
                    <a class="i-star" onclick="toggleAppStar('tPNHOoZFl9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tPNHOoZFl9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-sbG8qhMjkZ@OpenReview" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#sbG8qhMjkZ@OpenReview">Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent</a>
                    <a class="i-star" onclick="toggleAppStar('sbG8qhMjkZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('sbG8qhMjkZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pqOjj90Vwp@OpenReview" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#pqOjj90Vwp@OpenReview">Towards a Complete Logical Framework for GNN Expressiveness</a>
                    <a class="i-star" onclick="toggleAppStar('pqOjj90Vwp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pqOjj90Vwp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-or8mMhmyRV@OpenReview" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#or8mMhmyRV@OpenReview">MaestroMotif: Skill Design from Artificial Intelligence Feedback</a>
                    <a class="i-star" onclick="toggleAppStar('or8mMhmyRV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('or8mMhmyRV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-o9kqa5K3tB@OpenReview" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#o9kqa5K3tB@OpenReview">On the Benefits of Memory for Modeling Time-Dependent PDEs</a>
                    <a class="i-star" onclick="toggleAppStar('o9kqa5K3tB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('o9kqa5K3tB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-o5TsWTUSeF@OpenReview" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#o5TsWTUSeF@OpenReview">ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('o5TsWTUSeF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('o5TsWTUSeF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-m2nmp8P5in@OpenReview" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#m2nmp8P5in@OpenReview">LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('m2nmp8P5in@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('m2nmp8P5in@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-k3tbMMW8rH@OpenReview" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#k3tbMMW8rH@OpenReview">Feedback Schrdinger Bridge Matching</a>
                    <a class="i-star" onclick="toggleAppStar('k3tbMMW8rH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('k3tbMMW8rH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ja4rpheN2n@OpenReview" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#ja4rpheN2n@OpenReview">GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation</a>
                    <a class="i-star" onclick="toggleAppStar('ja4rpheN2n@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ja4rpheN2n@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gc8QAQfXv6@OpenReview" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#gc8QAQfXv6@OpenReview">Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning</a>
                    <a class="i-star" onclick="toggleAppStar('gc8QAQfXv6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gc8QAQfXv6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gHLWTzKiZV@OpenReview" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#gHLWTzKiZV@OpenReview">Composing Unbalanced Flows for Flexible Docking and Relaxation</a>
                    <a class="i-star" onclick="toggleAppStar('gHLWTzKiZV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gHLWTzKiZV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-f4gF6AIHRy@OpenReview" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#f4gF6AIHRy@OpenReview">Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection</a>
                    <a class="i-star" onclick="toggleAppStar('f4gF6AIHRy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('f4gF6AIHRy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eHehzSDUFp@OpenReview" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#eHehzSDUFp@OpenReview">Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition</a>
                    <a class="i-star" onclick="toggleAppStar('eHehzSDUFp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eHehzSDUFp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-dhAL5fy8wS@OpenReview" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#dhAL5fy8wS@OpenReview">Data Selection via Optimal Control for Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('dhAL5fy8wS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('dhAL5fy8wS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cNmu0hZ4CL@OpenReview" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#cNmu0hZ4CL@OpenReview">Comparing noisy neural population dynamics using optimal transport distances</a>
                    <a class="i-star" onclick="toggleAppStar('cNmu0hZ4CL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cNmu0hZ4CL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-cH65nS5sOz@OpenReview" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#cH65nS5sOz@OpenReview">Subgraph Federated Learning for Local Generalization</a>
                    <a class="i-star" onclick="toggleAppStar('cH65nS5sOz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('cH65nS5sOz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bnINPG5A32@OpenReview" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#bnINPG5A32@OpenReview">RB-Modulation: Training-Free Personalization using Stochastic Optimal Control</a>
                    <a class="i-star" onclick="toggleAppStar('bnINPG5A32@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bnINPG5A32@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-bVTM2QKYuA@OpenReview" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#bVTM2QKYuA@OpenReview">The Representation Geometry of Features and Hierarchy in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('bVTM2QKYuA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('bVTM2QKYuA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rFpZnn11gj@OpenReview" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#rFpZnn11gj@OpenReview">PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration</a>
                    <a class="i-star" onclick="toggleAppStar('rFpZnn11gj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rFpZnn11gj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aWXnKanInf@OpenReview" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#aWXnKanInf@OpenReview">TopoLM: brain-like spatio-functional organization in a topographic language model</a>
                    <a class="i-star" onclick="toggleAppStar('aWXnKanInf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aWXnKanInf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-VpWki1v2P8@OpenReview" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#VpWki1v2P8@OpenReview">LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('VpWki1v2P8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VpWki1v2P8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YrycTjllL0@OpenReview" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#YrycTjllL0@OpenReview">BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</a>
                    <a class="i-star" onclick="toggleAppStar('YrycTjllL0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YrycTjllL0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YUYJsHOf3c@OpenReview" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#YUYJsHOf3c@OpenReview">ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement</a>
                    <a class="i-star" onclick="toggleAppStar('YUYJsHOf3c@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YUYJsHOf3c@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YLIsIzC74j@OpenReview" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#YLIsIzC74j@OpenReview">LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement</a>
                    <a class="i-star" onclick="toggleAppStar('YLIsIzC74j@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YLIsIzC74j@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Y6aHdDNQYD@OpenReview" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#Y6aHdDNQYD@OpenReview">MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Y6aHdDNQYD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Y6aHdDNQYD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xDrFWUmCne@OpenReview" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#xDrFWUmCne@OpenReview">Learning to Discretize Denoising Diffusion ODEs</a>
                    <a class="i-star" onclick="toggleAppStar('xDrFWUmCne@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xDrFWUmCne@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xo0Q1N7CGk@OpenReview" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#Xo0Q1N7CGk@OpenReview">An Investigation of Conformal Isometry Hypothesis for Grid Cells</a>
                    <a class="i-star" onclick="toggleAppStar('Xo0Q1N7CGk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xo0Q1N7CGk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XmProj9cPs@OpenReview" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#XmProj9cPs@OpenReview">Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows</a>
                    <a class="i-star" onclick="toggleAppStar('XmProj9cPs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XmProj9cPs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-XFYUwIyTxQ@OpenReview" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#XFYUwIyTxQ@OpenReview">EmbodiedSAM: Online Segment Any 3D Thing in Real Time</a>
                    <a class="i-star" onclick="toggleAppStar('XFYUwIyTxQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('XFYUwIyTxQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-X1OfiRYCLn@OpenReview" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#X1OfiRYCLn@OpenReview">Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping</a>
                    <a class="i-star" onclick="toggleAppStar('X1OfiRYCLn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('X1OfiRYCLn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-odjMSBSWRt@OpenReview" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#odjMSBSWRt@OpenReview">DarkBench: Benchmarking Dark Patterns in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('odjMSBSWRt@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('odjMSBSWRt@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WCRQFlji2q@OpenReview" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#WCRQFlji2q@OpenReview">Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('WCRQFlji2q@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WCRQFlji2q@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-r5IXBlTCGc@OpenReview" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#r5IXBlTCGc@OpenReview">Consistency Checks for Language Model Forecasters</a>
                    <a class="i-star" onclick="toggleAppStar('r5IXBlTCGc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('r5IXBlTCGc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UHPnqSTBPO@OpenReview" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#UHPnqSTBPO@OpenReview">Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</a>
                    <a class="i-star" onclick="toggleAppStar('UHPnqSTBPO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UHPnqSTBPO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tTPHgb0EtV@OpenReview" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#tTPHgb0EtV@OpenReview">Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation</a>
                    <a class="i-star" onclick="toggleAppStar('tTPHgb0EtV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tTPHgb0EtV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SBCMNc3Mq3@OpenReview" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#SBCMNc3Mq3@OpenReview">ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials</a>
                    <a class="i-star" onclick="toggleAppStar('SBCMNc3Mq3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SBCMNc3Mq3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-RuP17cJtZo@OpenReview" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#RuP17cJtZo@OpenReview">Generator Matching: Generative modeling with arbitrary Markov processes</a>
                    <a class="i-star" onclick="toggleAppStar('RuP17cJtZo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RuP17cJtZo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-RWJX5F5I9g@OpenReview" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#RWJX5F5I9g@OpenReview">Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration</a>
                    <a class="i-star" onclick="toggleAppStar('RWJX5F5I9g@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('RWJX5F5I9g@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vzItLaEoDa@OpenReview" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#vzItLaEoDa@OpenReview">Open-World Reinforcement Learning over Long Short-Term Imagination</a>
                    <a class="i-star" onclick="toggleAppStar('vzItLaEoDa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vzItLaEoDa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QWunLKbBGF@OpenReview" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#QWunLKbBGF@OpenReview">Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('QWunLKbBGF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QWunLKbBGF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QEHrmQPBdd@OpenReview" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#QEHrmQPBdd@OpenReview">RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</a>
                    <a class="i-star" onclick="toggleAppStar('QEHrmQPBdd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QEHrmQPBdd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-syThiTmWWm@OpenReview" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#syThiTmWWm@OpenReview">Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates</a>
                    <a class="i-star" onclick="toggleAppStar('syThiTmWWm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('syThiTmWWm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PSiijdQjNU@OpenReview" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#PSiijdQjNU@OpenReview">Steering Protein Family Design through Profile Bayesian Flow</a>
                    <a class="i-star" onclick="toggleAppStar('PSiijdQjNU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PSiijdQjNU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PBjCTeDL6o@OpenReview" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#PBjCTeDL6o@OpenReview">Unlearning-based Neural Interpretations</a>
                    <a class="i-star" onclick="toggleAppStar('PBjCTeDL6o@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PBjCTeDL6o@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-P4o9akekdf@OpenReview" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#P4o9akekdf@OpenReview">No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</a>
                    <a class="i-star" onclick="toggleAppStar('P4o9akekdf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('P4o9akekdf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZCOwwRAaEl@OpenReview" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#ZCOwwRAaEl@OpenReview">Latent Bayesian Optimization via Autoregressive Normalizing Flows</a>
                    <a class="i-star" onclick="toggleAppStar('ZCOwwRAaEl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZCOwwRAaEl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ozo7qJ5vZi@OpenReview" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#Ozo7qJ5vZi@OpenReview">KAN: KolmogorovArnold Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Ozo7qJ5vZi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ozo7qJ5vZi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OvoCm1gGhN@OpenReview" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#OvoCm1gGhN@OpenReview">Differential Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('OvoCm1gGhN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OvoCm1gGhN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OlzB6LnXcS@OpenReview" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#OlzB6LnXcS@OpenReview">One Step Diffusion via Shortcut Models</a>
                    <a class="i-star" onclick="toggleAppStar('OlzB6LnXcS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OlzB6LnXcS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OfjIlbelrT@OpenReview" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#OfjIlbelrT@OpenReview">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a>
                    <a class="i-star" onclick="toggleAppStar('OfjIlbelrT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OfjIlbelrT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-u1cQYxRI1H@OpenReview" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#u1cQYxRI1H@OpenReview">Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport</a>
                    <a class="i-star" onclick="toggleAppStar('u1cQYxRI1H@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('u1cQYxRI1H@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NN6QHwgRrQ@OpenReview" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#NN6QHwgRrQ@OpenReview">MAP: Multi-Human-Value Alignment Palette</a>
                    <a class="i-star" onclick="toggleAppStar('NN6QHwgRrQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NN6QHwgRrQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SnDmPkOJ0T@OpenReview" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#SnDmPkOJ0T@OpenReview">REEF: Representation Encoding Fingerprints for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('SnDmPkOJ0T@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SnDmPkOJ0T@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mfnh1Sqdwf@OpenReview" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#Mfnh1Sqdwf@OpenReview">Learning to Discover Regulatory Elements for Gene Expression Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('Mfnh1Sqdwf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mfnh1Sqdwf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KSLkFYHlYg@OpenReview" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#KSLkFYHlYg@OpenReview">ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design</a>
                    <a class="i-star" onclick="toggleAppStar('KSLkFYHlYg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KSLkFYHlYg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-JWtrk7mprJ@OpenReview" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#JWtrk7mprJ@OpenReview">Residual Deep Gaussian Processes on Manifolds</a>
                    <a class="i-star" onclick="toggleAppStar('JWtrk7mprJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('JWtrk7mprJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Iyrtb9EJBp@OpenReview" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#Iyrtb9EJBp@OpenReview">Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse</a>
                    <a class="i-star" onclick="toggleAppStar('Iyrtb9EJBp@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Iyrtb9EJBp@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HD6bWcj87Y@OpenReview" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#HD6bWcj87Y@OpenReview">Data Shapley in One Training Run</a>
                    <a class="i-star" onclick="toggleAppStar('HD6bWcj87Y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HD6bWcj87Y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fur0DtynPX@OpenReview" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#Fur0DtynPX@OpenReview">GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Fur0DtynPX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fur0DtynPX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FSjIrOm1vz@OpenReview" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#FSjIrOm1vz@OpenReview">Inference Scaling for Long-Context Retrieval Augmented Generation</a>
                    <a class="i-star" onclick="toggleAppStar('FSjIrOm1vz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FSjIrOm1vz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FIj9IEPCKr@OpenReview" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#FIj9IEPCKr@OpenReview">Proxy Denoising for Source-Free Domain Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('FIj9IEPCKr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FIj9IEPCKr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FBkpCyujtS@OpenReview" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#FBkpCyujtS@OpenReview">Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs</a>
                    <a class="i-star" onclick="toggleAppStar('FBkpCyujtS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FBkpCyujtS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EytBpUGB1Z@OpenReview" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#EytBpUGB1Z@OpenReview">Retrieval Head Mechanistically Explains Long-Context Factuality</a>
                    <a class="i-star" onclick="toggleAppStar('EytBpUGB1Z@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EytBpUGB1Z@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EO8xpnW7aX@OpenReview" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#EO8xpnW7aX@OpenReview">Learning to Permute with Discrete Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('EO8xpnW7aX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EO8xpnW7aX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-E4Fk3YuG56@OpenReview" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#E4Fk3YuG56@OpenReview">Cut Your Losses in Large-Vocabulary Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('E4Fk3YuG56@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('E4Fk3YuG56@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-CxXGvKRDnL@OpenReview" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#CxXGvKRDnL@OpenReview">Progressive Compression with Universally Quantized Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('CxXGvKRDnL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CxXGvKRDnL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cjz9Xhm7sI@OpenReview" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#Cjz9Xhm7sI@OpenReview">High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation</a>
                    <a class="i-star" onclick="toggleAppStar('Cjz9Xhm7sI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cjz9Xhm7sI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bo62NeU6VF@OpenReview" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#Bo62NeU6VF@OpenReview">Backtracking Improves Generation Safety</a>
                    <a class="i-star" onclick="toggleAppStar('Bo62NeU6VF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bo62NeU6VF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AP0ndQloqR@OpenReview" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#AP0ndQloqR@OpenReview">Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces</a>
                    <a class="i-star" onclick="toggleAppStar('AP0ndQloqR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AP0ndQloqR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9pW2J49flQ@OpenReview" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#9pW2J49flQ@OpenReview">DeepLTL: Learning to Efficiently Satisfy Complex LTL Instructions</a>
                    <a class="i-star" onclick="toggleAppStar('9pW2J49flQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9pW2J49flQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9VGTk2NYjF@OpenReview" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#9VGTk2NYjF@OpenReview">The Complexity of Two-Team Polymatrix Games with Independent Adversaries</a>
                    <a class="i-star" onclick="toggleAppStar('9VGTk2NYjF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9VGTk2NYjF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8enWnd6Gp3@OpenReview" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#8enWnd6Gp3@OpenReview">TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes</a>
                    <a class="i-star" onclick="toggleAppStar('8enWnd6Gp3@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8enWnd6Gp3@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-84pDoCD4lH@OpenReview" style="display:none">
                    <span class="i-index">#101</span>
                    <a class="i-title" href="#84pDoCD4lH@OpenReview">Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities</a>
                    <a class="i-star" onclick="toggleAppStar('84pDoCD4lH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('84pDoCD4lH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6EUtjXAvmj@OpenReview" style="display:none">
                    <span class="i-index">#102</span>
                    <a class="i-title" href="#6EUtjXAvmj@OpenReview">Variational Diffusion Posterior Sampling with Midpoint Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('6EUtjXAvmj@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6EUtjXAvmj@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5UKrnKuspb@OpenReview" style="display:none">
                    <span class="i-index">#103</span>
                    <a class="i-title" href="#5UKrnKuspb@OpenReview">NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields</a>
                    <a class="i-star" onclick="toggleAppStar('5UKrnKuspb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5UKrnKuspb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5U1rlpX68A@OpenReview" style="display:none">
                    <span class="i-index">#104</span>
                    <a class="i-title" href="#5U1rlpX68A@OpenReview">S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar('5U1rlpX68A@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5U1rlpX68A@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-51WraMid8K@OpenReview" style="display:none">
                    <span class="i-index">#105</span>
                    <a class="i-title" href="#51WraMid8K@OpenReview">A Probabilistic Perspective on Unlearning and Alignment for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('51WraMid8K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('51WraMid8K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4OaO3GjP7k@OpenReview" style="display:none">
                    <span class="i-index">#106</span>
                    <a class="i-title" href="#4OaO3GjP7k@OpenReview">Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('4OaO3GjP7k@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4OaO3GjP7k@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4FWAwZtd2n@OpenReview" style="display:none">
                    <span class="i-index">#107</span>
                    <a class="i-title" href="#4FWAwZtd2n@OpenReview">Scaling Test-Time Compute Optimally Can be More Effective than Scaling LLM Parameters</a>
                    <a class="i-star" onclick="toggleAppStar('4FWAwZtd2n@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4FWAwZtd2n@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3i13Gev2hV@OpenReview" style="display:none">
                    <span class="i-index">#108</span>
                    <a class="i-title" href="#3i13Gev2hV@OpenReview">Compositional Entailment Learning for Hyperbolic Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('3i13Gev2hV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3i13Gev2hV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-3IFRygQKGL@OpenReview" style="display:none">
                    <span class="i-index">#109</span>
                    <a class="i-title" href="#3IFRygQKGL@OpenReview">OptionZero: Planning with Learned Options</a>
                    <a class="i-star" onclick="toggleAppStar('3IFRygQKGL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3IFRygQKGL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2efNHgYRvM@OpenReview" style="display:none">
                    <span class="i-index">#110</span>
                    <a class="i-title" href="#2efNHgYRvM@OpenReview">On the Identification of Temporal Causal Representation with Instantaneous Dependence</a>
                    <a class="i-star" onclick="toggleAppStar('2efNHgYRvM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2efNHgYRvM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-25kAzqzTrz@OpenReview" style="display:none">
                    <span class="i-index">#111</span>
                    <a class="i-title" href="#25kAzqzTrz@OpenReview">Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning</a>
                    <a class="i-star" onclick="toggleAppStar('25kAzqzTrz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('25kAzqzTrz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1CLzLXSFNn@OpenReview" style="display:none">
                    <span class="i-index">#112</span>
                    <a class="i-title" href="#1CLzLXSFNn@OpenReview">TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('1CLzLXSFNn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1CLzLXSFNn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-z5uVAKwmjf@OpenReview" style="display:none">
                    <span class="i-index">#113</span>
                    <a class="i-title" href="#z5uVAKwmjf@OpenReview">AFlow: Automating Agentic Workflow Generation</a>
                    <a class="i-star" onclick="toggleAppStar('z5uVAKwmjf@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('z5uVAKwmjf@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tc90LV0yRL@OpenReview" style="display:none">
                    <span class="i-index">#114</span>
                    <a class="i-title" href="#tc90LV0yRL@OpenReview">Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('tc90LV0yRL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tc90LV0yRL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eIJfOIMN9z@OpenReview" style="display:none">
                    <span class="i-index">#115</span>
                    <a class="i-title" href="#eIJfOIMN9z@OpenReview">Language Representations Can be What Recommenders Need: Findings and Potentials</a>
                    <a class="i-star" onclick="toggleAppStar('eIJfOIMN9z@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eIJfOIMN9z@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZuazHmXTns@OpenReview" style="display:none">
                    <span class="i-index">#116</span>
                    <a class="i-title" href="#ZuazHmXTns@OpenReview">Problem-Parameter-Free Federated Learning</a>
                    <a class="i-star" onclick="toggleAppStar('ZuazHmXTns@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZuazHmXTns@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QKBu1BOAwd@OpenReview" style="display:none">
                    <span class="i-index">#117</span>
                    <a class="i-title" href="#QKBu1BOAwd@OpenReview">From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</a>
                    <a class="i-star" onclick="toggleAppStar('QKBu1BOAwd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QKBu1BOAwd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-I4e82CIDxv@OpenReview" style="display:none">
                    <span class="i-index">#118</span>
                    <a class="i-title" href="#I4e82CIDxv@OpenReview">Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('I4e82CIDxv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('I4e82CIDxv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GGlpykXDCa@OpenReview" style="display:none">
                    <span class="i-index">#119</span>
                    <a class="i-title" href="#GGlpykXDCa@OpenReview">MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions</a>
                    <a class="i-star" onclick="toggleAppStar('GGlpykXDCa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GGlpykXDCa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-CRmiX0v16e@OpenReview" style="display:none">
                    <span class="i-index">#120</span>
                    <a class="i-title" href="#CRmiX0v16e@OpenReview">Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('CRmiX0v16e@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CRmiX0v16e@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5Jc7r5aqHJ@OpenReview" style="display:none">
                    <span class="i-index">#121</span>
                    <a class="i-title" href="#5Jc7r5aqHJ@OpenReview">Energy-based Backdoor Defense Against Federated Graph Learning</a>
                    <a class="i-star" onclick="toggleAppStar('5Jc7r5aqHJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5Jc7r5aqHJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ilOEOIqolQ@OpenReview" style="display:none">
                    <span class="i-index">#122</span>
                    <a class="i-title" href="#ilOEOIqolQ@OpenReview">AI as Humanitys Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text</a>
                    <a class="i-star" onclick="toggleAppStar('ilOEOIqolQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ilOEOIqolQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kRoWeLTpL4@OpenReview" style="display:none">
                    <span class="i-index">#123</span>
                    <a class="i-title" href="#kRoWeLTpL4@OpenReview">Copyright-Protected Language Generation via Adaptive Model Fusion</a>
                    <a class="i-star" onclick="toggleAppStar('kRoWeLTpL4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kRoWeLTpL4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-meRCKuUpmc@OpenReview" style="display:none">
                    <span class="i-index">#124</span>
                    <a class="i-title" href="#meRCKuUpmc@OpenReview">Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation</a>
                    <a class="i-star" onclick="toggleAppStar('meRCKuUpmc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('meRCKuUpmc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mtJSMcF3ek@OpenReview" style="display:none">
                    <span class="i-index">#125</span>
                    <a class="i-title" href="#mtJSMcF3ek@OpenReview">Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('mtJSMcF3ek@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mtJSMcF3ek@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EjJGND0m1x@OpenReview" style="display:none">
                    <span class="i-index">#126</span>
                    <a class="i-title" href="#EjJGND0m1x@OpenReview">MIND over Body: Adaptive Thinking using Dynamic Computation</a>
                    <a class="i-star" onclick="toggleAppStar('EjJGND0m1x@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EjJGND0m1x@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kGvXIlIVLM@OpenReview" style="display:none">
                    <span class="i-index">#127</span>
                    <a class="i-title" href="#kGvXIlIVLM@OpenReview">Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('kGvXIlIVLM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kGvXIlIVLM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-nwDRD4AMoN@OpenReview" style="display:none">
                    <span class="i-index">#128</span>
                    <a class="i-title" href="#nwDRD4AMoN@OpenReview">Artificial Kuramoto Oscillatory Neurons</a>
                    <a class="i-star" onclick="toggleAppStar('nwDRD4AMoN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('nwDRD4AMoN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uKZdlihDDn@OpenReview" style="display:none">
                    <span class="i-index">#129</span>
                    <a class="i-title" href="#uKZdlihDDn@OpenReview">Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks</a>
                    <a class="i-star" onclick="toggleAppStar('uKZdlihDDn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uKZdlihDDn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uHLgDEgiS5@OpenReview" style="display:none">
                    <span class="i-index">#130</span>
                    <a class="i-title" href="#uHLgDEgiS5@OpenReview">Capturing the Temporal Dependence of Training Data Influence</a>
                    <a class="i-star" onclick="toggleAppStar('uHLgDEgiS5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uHLgDEgiS5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uAFHCZRmXk@OpenReview" style="display:none">
                    <span class="i-index">#131</span>
                    <a class="i-title" href="#uAFHCZRmXk@OpenReview">Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('uAFHCZRmXk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uAFHCZRmXk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rwqShzb9li@OpenReview" style="display:none">
                    <span class="i-index">#132</span>
                    <a class="i-title" href="#rwqShzb9li@OpenReview">Linear Representations of Political Perspective Emerge in Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('rwqShzb9li@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rwqShzb9li@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rfdblE10qm@OpenReview" style="display:none">
                    <span class="i-index">#133</span>
                    <a class="i-title" href="#rfdblE10qm@OpenReview">Rethinking Reward Modeling in Preference-based Large Language Model Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('rfdblE10qm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rfdblE10qm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pISLZG7ktL@OpenReview" style="display:none">
                    <span class="i-index">#134</span>
                    <a class="i-title" href="#pISLZG7ktL@OpenReview">Data Scaling Laws in Imitation Learning for Robotic Manipulation</a>
                    <a class="i-star" onclick="toggleAppStar('pISLZG7ktL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pISLZG7ktL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mtSSFiqW6y@OpenReview" style="display:none">
                    <span class="i-index">#135</span>
                    <a class="i-title" href="#mtSSFiqW6y@OpenReview">Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('mtSSFiqW6y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mtSSFiqW6y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mMPMHWOdOy@OpenReview" style="display:none">
                    <span class="i-index">#136</span>
                    <a class="i-title" href="#mMPMHWOdOy@OpenReview">WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</a>
                    <a class="i-star" onclick="toggleAppStar('mMPMHWOdOy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mMPMHWOdOy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jOmk0uS1hl@OpenReview" style="display:none">
                    <span class="i-index">#137</span>
                    <a class="i-title" href="#jOmk0uS1hl@OpenReview">Training on the Test Task Confounds Evaluation and Emergence</a>
                    <a class="i-star" onclick="toggleAppStar('jOmk0uS1hl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jOmk0uS1hl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hyfe5q5TD0@OpenReview" style="display:none">
                    <span class="i-index">#138</span>
                    <a class="i-title" href="#hyfe5q5TD0@OpenReview">Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('hyfe5q5TD0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hyfe5q5TD0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-hrqNOxpItr@OpenReview" style="display:none">
                    <span class="i-index">#139</span>
                    <a class="i-title" href="#hrqNOxpItr@OpenReview">Cross-Entropy Is All You Need To Invert the Data Generating Process</a>
                    <a class="i-star" onclick="toggleAppStar('hrqNOxpItr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('hrqNOxpItr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-h0Ak8A5yqw@OpenReview" style="display:none">
                    <span class="i-index">#140</span>
                    <a class="i-title" href="#h0Ak8A5yqw@OpenReview">On the Role of Attention Heads in Large Language Model Safety</a>
                    <a class="i-star" onclick="toggleAppStar('h0Ak8A5yqw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('h0Ak8A5yqw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fV0t65OBUu@OpenReview" style="display:none">
                    <span class="i-index">#141</span>
                    <a class="i-title" href="#fV0t65OBUu@OpenReview">Improving Probabilistic Diffusion Models With Optimal Covariance Matching</a>
                    <a class="i-star" onclick="toggleAppStar('fV0t65OBUu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fV0t65OBUu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fMTPkDEhLQ@OpenReview" style="display:none">
                    <span class="i-index">#142</span>
                    <a class="i-title" href="#fMTPkDEhLQ@OpenReview">Tight Lower Bounds under Asymmetric High-Order Hlder Smoothness and Uniform Convexity</a>
                    <a class="i-star" onclick="toggleAppStar('fMTPkDEhLQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fMTPkDEhLQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fAAaT826Vv@OpenReview" style="display:none">
                    <span class="i-index">#143</span>
                    <a class="i-title" href="#fAAaT826Vv@OpenReview">BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('fAAaT826Vv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fAAaT826Vv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wr3UuEx72f@OpenReview" style="display:none">
                    <span class="i-index">#144</span>
                    <a class="i-title" href="#Wr3UuEx72f@OpenReview">LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Wr3UuEx72f@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wr3UuEx72f@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WJaUkwci9o@OpenReview" style="display:none">
                    <span class="i-index">#145</span>
                    <a class="i-title" href="#WJaUkwci9o@OpenReview">Self-Improvement in Language Models: The Sharpening Mechanism</a>
                    <a class="i-star" onclick="toggleAppStar('WJaUkwci9o@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WJaUkwci9o@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-V4K9h1qNxE@OpenReview" style="display:none">
                    <span class="i-index">#146</span>
                    <a class="i-title" href="#V4K9h1qNxE@OpenReview">Attention as a Hypernetwork</a>
                    <a class="i-star" onclick="toggleAppStar('V4K9h1qNxE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('V4K9h1qNxE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UV5p3JZMjC@OpenReview" style="display:none">
                    <span class="i-index">#147</span>
                    <a class="i-title" href="#UV5p3JZMjC@OpenReview">Learning Randomized Algorithms with Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('UV5p3JZMjC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UV5p3JZMjC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-TwJrTz9cRS@OpenReview" style="display:none">
                    <span class="i-index">#148</span>
                    <a class="i-title" href="#TwJrTz9cRS@OpenReview">HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('TwJrTz9cRS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('TwJrTz9cRS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SctfBCLmWo@OpenReview" style="display:none">
                    <span class="i-index">#149</span>
                    <a class="i-title" href="#SctfBCLmWo@OpenReview">A Decade's Battle on Dataset Bias: Are We There Yet?</a>
                    <a class="i-star" onclick="toggleAppStar('SctfBCLmWo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SctfBCLmWo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SPS6HzVzyt@OpenReview" style="display:none">
                    <span class="i-index">#150</span>
                    <a class="i-title" href="#SPS6HzVzyt@OpenReview">Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance</a>
                    <a class="i-star" onclick="toggleAppStar('SPS6HzVzyt@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SPS6HzVzyt@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SI2hI0frk6@OpenReview" style="display:none">
                    <span class="i-index">#151</span>
                    <a class="i-title" href="#SI2hI0frk6@OpenReview">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</a>
                    <a class="i-star" onclick="toggleAppStar('SI2hI0frk6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SI2hI0frk6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QQBPWtvtcn@OpenReview" style="display:none">
                    <span class="i-index">#152</span>
                    <a class="i-title" href="#QQBPWtvtcn@OpenReview">LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</a>
                    <a class="i-star" onclick="toggleAppStar('QQBPWtvtcn@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QQBPWtvtcn@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QFO1asgas2@OpenReview" style="display:none">
                    <span class="i-index">#153</span>
                    <a class="i-title" href="#QFO1asgas2@OpenReview">Advantage Alignment Algorithms</a>
                    <a class="i-star" onclick="toggleAppStar('QFO1asgas2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QFO1asgas2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NxyfSW6mLK@OpenReview" style="display:none">
                    <span class="i-index">#154</span>
                    <a class="i-title" href="#NxyfSW6mLK@OpenReview">REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments</a>
                    <a class="i-star" onclick="toggleAppStar('NxyfSW6mLK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NxyfSW6mLK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NO6Tv6QcDs@OpenReview" style="display:none">
                    <span class="i-index">#155</span>
                    <a class="i-title" href="#NO6Tv6QcDs@OpenReview">Limits to scalable evaluation at the frontier: LLM as judge wont beat twice the data</a>
                    <a class="i-star" onclick="toggleAppStar('NO6Tv6QcDs@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NO6Tv6QcDs@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-N8Oj1XhtYZ@OpenReview" style="display:none">
                    <span class="i-index">#156</span>
                    <a class="i-title" href="#N8Oj1XhtYZ@OpenReview">SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('N8Oj1XhtYZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('N8Oj1XhtYZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KIgaAqEFHW@OpenReview" style="display:none">
                    <span class="i-index">#157</span>
                    <a class="i-title" href="#KIgaAqEFHW@OpenReview">miniCTX: Neural Theorem Proving with (Long-)Contexts</a>
                    <a class="i-star" onclick="toggleAppStar('KIgaAqEFHW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KIgaAqEFHW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HvSytvg3Jh@OpenReview" style="display:none">
                    <span class="i-index">#158</span>
                    <a class="i-title" href="#HvSytvg3Jh@OpenReview">AlphaEdit: Null-Space Constrained Model Editing for Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('HvSytvg3Jh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HvSytvg3Jh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-HsHxSN23rM@OpenReview" style="display:none">
                    <span class="i-index">#159</span>
                    <a class="i-title" href="#HsHxSN23rM@OpenReview">STAR: Synthesis of Tailored Architectures</a>
                    <a class="i-star" onclick="toggleAppStar('HsHxSN23rM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('HsHxSN23rM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FpiCLJrSW8@OpenReview" style="display:none">
                    <span class="i-index">#160</span>
                    <a class="i-title" href="#FpiCLJrSW8@OpenReview">More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness</a>
                    <a class="i-star" onclick="toggleAppStar('FpiCLJrSW8@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FpiCLJrSW8@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FVuqJt3c4L@OpenReview" style="display:none">
                    <span class="i-index">#161</span>
                    <a class="i-title" href="#FVuqJt3c4L@OpenReview">Population Transformer: Learning Population-level Representations of Neural Activity</a>
                    <a class="i-star" onclick="toggleAppStar('FVuqJt3c4L@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FVuqJt3c4L@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EzjsoomYEb@OpenReview" style="display:none">
                    <span class="i-index">#162</span>
                    <a class="i-title" href="#EzjsoomYEb@OpenReview">Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity</a>
                    <a class="i-star" onclick="toggleAppStar('EzjsoomYEb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EzjsoomYEb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EUSkm2sVJ6@OpenReview" style="display:none">
                    <span class="i-index">#163</span>
                    <a class="i-title" href="#EUSkm2sVJ6@OpenReview">How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning</a>
                    <a class="i-star" onclick="toggleAppStar('EUSkm2sVJ6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EUSkm2sVJ6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zCxGCdzreM@OpenReview" style="display:none">
                    <span class="i-index">#164</span>
                    <a class="i-title" href="#zCxGCdzreM@OpenReview">Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('zCxGCdzreM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zCxGCdzreM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-DJSZGGZYVi@OpenReview" style="display:none">
                    <span class="i-index">#165</span>
                    <a class="i-title" href="#DJSZGGZYVi@OpenReview">Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</a>
                    <a class="i-star" onclick="toggleAppStar('DJSZGGZYVi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DJSZGGZYVi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-CjwERcAU7w@OpenReview" style="display:none">
                    <span class="i-index">#166</span>
                    <a class="i-title" href="#CjwERcAU7w@OpenReview">Training Language Models to Self-Correct via Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('CjwERcAU7w@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CjwERcAU7w@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-BPgK5XW1Nb@OpenReview" style="display:none">
                    <span class="i-index">#167</span>
                    <a class="i-title" href="#BPgK5XW1Nb@OpenReview">Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('BPgK5XW1Nb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('BPgK5XW1Nb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-AoraWUmpLU@OpenReview" style="display:none">
                    <span class="i-index">#168</span>
                    <a class="i-title" href="#AoraWUmpLU@OpenReview">Exploring the Impact of Activation Functions in Training Neural ODEs</a>
                    <a class="i-star" onclick="toggleAppStar('AoraWUmpLU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('AoraWUmpLU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-8zJRon6k5v@OpenReview" style="display:none">
                    <span class="i-index">#169</span>
                    <a class="i-title" href="#8zJRon6k5v@OpenReview">Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series</a>
                    <a class="i-star" onclick="toggleAppStar('8zJRon6k5v@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('8zJRon6k5v@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6Mxhg9PtDE@OpenReview" style="display:none">
                    <span class="i-index">#170</span>
                    <a class="i-title" href="#6Mxhg9PtDE@OpenReview">Safety Alignment Should be Made More Than Just a Few Tokens Deep</a>
                    <a class="i-star" onclick="toggleAppStar('6Mxhg9PtDE@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6Mxhg9PtDE@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-5IkDAfabuo@OpenReview" style="display:none">
                    <span class="i-index">#171</span>
                    <a class="i-title" href="#5IkDAfabuo@OpenReview">Prioritized Generative Replay</a>
                    <a class="i-star" onclick="toggleAppStar('5IkDAfabuo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5IkDAfabuo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1aF2D2CPHi@OpenReview" style="display:none">
                    <span class="i-index">#172</span>
                    <a class="i-title" href="#1aF2D2CPHi@OpenReview">Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation</a>
                    <a class="i-star" onclick="toggleAppStar('1aF2D2CPHi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1aF2D2CPHi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1HCN4pjTb4@OpenReview" style="display:none">
                    <span class="i-index">#173</span>
                    <a class="i-title" href="#1HCN4pjTb4@OpenReview">Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse</a>
                    <a class="i-star" onclick="toggleAppStar('1HCN4pjTb4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1HCN4pjTb4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zl0HLZOJC9@OpenReview" style="display:none">
                    <span class="i-index">#174</span>
                    <a class="i-title" href="#zl0HLZOJC9@OpenReview">Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution</a>
                    <a class="i-star" onclick="toggleAppStar('zl0HLZOJC9@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zl0HLZOJC9@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zBbZ2vdLzH@OpenReview" style="display:none">
                    <span class="i-index">#175</span>
                    <a class="i-title" href="#zBbZ2vdLzH@OpenReview">Joint Graph Rewiring and Feature Denoising via Spectral Resonance</a>
                    <a class="i-star" onclick="toggleAppStar('zBbZ2vdLzH@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zBbZ2vdLzH@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wPMRwmytZe@OpenReview" style="display:none">
                    <span class="i-index">#176</span>
                    <a class="i-title" href="#wPMRwmytZe@OpenReview">Progressive distillation induces an implicit curriculum</a>
                    <a class="i-star" onclick="toggleAppStar('wPMRwmytZe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wPMRwmytZe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xoIeVdFO7U@OpenReview" style="display:none">
                    <span class="i-index">#177</span>
                    <a class="i-title" href="#xoIeVdFO7U@OpenReview">Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning</a>
                    <a class="i-star" onclick="toggleAppStar('xoIeVdFO7U@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xoIeVdFO7U@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-trKNi4IUiP@OpenReview" style="display:none">
                    <span class="i-index">#178</span>
                    <a class="i-title" href="#trKNi4IUiP@OpenReview">Robustness Inspired Graph Backdoor Defense</a>
                    <a class="i-star" onclick="toggleAppStar('trKNi4IUiP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('trKNi4IUiP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kX8h23UG6v@OpenReview" style="display:none">
                    <span class="i-index">#179</span>
                    <a class="i-title" href="#kX8h23UG6v@OpenReview">Standard Gaussian Process Can Be Excellent for High-Dimensional Bayesian Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('kX8h23UG6v@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kX8h23UG6v@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-esYrEndGsr@OpenReview" style="display:none">
                    <span class="i-index">#180</span>
                    <a class="i-title" href="#esYrEndGsr@OpenReview">Influence Functions for Scalable Data Attribution in Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('esYrEndGsr@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('esYrEndGsr@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pujt3ADZgI@OpenReview" style="display:none">
                    <span class="i-index">#181</span>
                    <a class="i-title" href="#Pujt3ADZgI@OpenReview">Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Pujt3ADZgI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pujt3ADZgI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eBS3dQQ8GV@OpenReview" style="display:none">
                    <span class="i-index">#182</span>
                    <a class="i-title" href="#eBS3dQQ8GV@OpenReview">Emergence of meta-stable clustering in mean-field transformer models</a>
                    <a class="i-star" onclick="toggleAppStar('eBS3dQQ8GV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eBS3dQQ8GV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-WOzffPgVjF@OpenReview" style="display:none">
                    <span class="i-index">#183</span>
                    <a class="i-title" href="#WOzffPgVjF@OpenReview">Knowing Your Target : Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding</a>
                    <a class="i-star" onclick="toggleAppStar('WOzffPgVjF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WOzffPgVjF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4xWQS2z77v@OpenReview" style="display:none">
                    <span class="i-index">#184</span>
                    <a class="i-title" href="#4xWQS2z77v@OpenReview">Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality</a>
                    <a class="i-star" onclick="toggleAppStar('4xWQS2z77v@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4xWQS2z77v@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-stUKwWBuBm@OpenReview" style="display:none">
                    <span class="i-index">#185</span>
                    <a class="i-title" href="#stUKwWBuBm@OpenReview">Tractable Multi-Agent Reinforcement Learning through Behavioral Economics</a>
                    <a class="i-star" onclick="toggleAppStar('stUKwWBuBm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('stUKwWBuBm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-is4nCVkSFA@OpenReview" style="display:none">
                    <span class="i-index">#186</span>
                    <a class="i-title" href="#is4nCVkSFA@OpenReview">Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model</a>
                    <a class="i-star" onclick="toggleAppStar('is4nCVkSFA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('is4nCVkSFA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7BLXhmWvwF@OpenReview" style="display:none">
                    <span class="i-index">#187</span>
                    <a class="i-title" href="#7BLXhmWvwF@OpenReview">Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects</a>
                    <a class="i-star" onclick="toggleAppStar('7BLXhmWvwF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7BLXhmWvwF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-g3xuCtrG6H@OpenReview" style="display:none">
                    <span class="i-index">#188</span>
                    <a class="i-title" href="#g3xuCtrG6H@OpenReview">A Computational Framework for Modeling Emergence of Color Vision in the Human Brain</a>
                    <a class="i-star" onclick="toggleAppStar('g3xuCtrG6H@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('g3xuCtrG6H@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-j7cyANIAxV@OpenReview" style="display:none">
                    <span class="i-index">#189</span>
                    <a class="i-title" href="#j7cyANIAxV@OpenReview">Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation</a>
                    <a class="i-star" onclick="toggleAppStar('j7cyANIAxV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('j7cyANIAxV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xByvdb3DCm@OpenReview" style="display:none">
                    <span class="i-index">#190</span>
                    <a class="i-title" href="#xByvdb3DCm@OpenReview">When Selection meets Intervention: Additional Complexities in Causal Discovery</a>
                    <a class="i-star" onclick="toggleAppStar('xByvdb3DCm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xByvdb3DCm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-07yvxWDSla@OpenReview" style="display:none">
                    <span class="i-index">#191</span>
                    <a class="i-title" href="#07yvxWDSla@OpenReview">Synthetic continued pretraining</a>
                    <a class="i-star" onclick="toggleAppStar('07yvxWDSla@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('07yvxWDSla@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pQqeQpMkE7@OpenReview" style="display:none">
                    <span class="i-index">#192</span>
                    <a class="i-title" href="#pQqeQpMkE7@OpenReview">On Scaling Up 3D Gaussian Splatting Training</a>
                    <a class="i-star" onclick="toggleAppStar('pQqeQpMkE7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pQqeQpMkE7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vRvVVb0NAz@OpenReview" style="display:none">
                    <span class="i-index">#193</span>
                    <a class="i-title" href="#vRvVVb0NAz@OpenReview">When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('vRvVVb0NAz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vRvVVb0NAz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-t7P5BUKcYv@OpenReview" style="display:none">
                    <span class="i-index">#194</span>
                    <a class="i-title" href="#t7P5BUKcYv@OpenReview">MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts</a>
                    <a class="i-star" onclick="toggleAppStar('t7P5BUKcYv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('t7P5BUKcYv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tcsZt9ZNKD@OpenReview" style="display:none">
                    <span class="i-index">#195</span>
                    <a class="i-title" href="#tcsZt9ZNKD@OpenReview">Scaling and evaluating sparse autoencoders</a>
                    <a class="i-star" onclick="toggleAppStar('tcsZt9ZNKD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tcsZt9ZNKD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ijbA5swmoK@OpenReview" style="display:none">
                    <span class="i-index">#196</span>
                    <a class="i-title" href="#ijbA5swmoK@OpenReview">Second-Order Min-Max Optimization with Lazy Hessians</a>
                    <a class="i-star" onclick="toggleAppStar('ijbA5swmoK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ijbA5swmoK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gQlxd3Mtru@OpenReview" style="display:none">
                    <span class="i-index">#197</span>
                    <a class="i-title" href="#gQlxd3Mtru@OpenReview">Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport</a>
                    <a class="i-star" onclick="toggleAppStar('gQlxd3Mtru@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gQlxd3Mtru@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ny8T8OuNHe@OpenReview" style="display:none">
                    <span class="i-index">#198</span>
                    <a class="i-title" href="#ny8T8OuNHe@OpenReview">Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('ny8T8OuNHe@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ny8T8OuNHe@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OIvg3MqWX2@OpenReview" style="display:none">
                    <span class="i-index">#199</span>
                    <a class="i-title" href="#OIvg3MqWX2@OpenReview">A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules</a>
                    <a class="i-star" onclick="toggleAppStar('OIvg3MqWX2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OIvg3MqWX2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-o2Igqm95SJ@OpenReview" style="display:none">
                    <span class="i-index">#200</span>
                    <a class="i-title" href="#o2Igqm95SJ@OpenReview">CAX: Cellular Automata Accelerated in JAX</a>
                    <a class="i-star" onclick="toggleAppStar('o2Igqm95SJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('o2Igqm95SJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-A3YUPeJTNR@OpenReview" style="display:none">
                    <span class="i-index">#201</span>
                    <a class="i-title" href="#A3YUPeJTNR@OpenReview">The Hidden Cost of Waiting for Accurate Predictions</a>
                    <a class="i-star" onclick="toggleAppStar('A3YUPeJTNR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('A3YUPeJTNR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-n2NidsYDop@OpenReview" style="display:none">
                    <span class="i-index">#202</span>
                    <a class="i-title" href="#n2NidsYDop@OpenReview">Transformers Provably Solve Parity Efficiently with Chain of Thought</a>
                    <a class="i-star" onclick="toggleAppStar('n2NidsYDop@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('n2NidsYDop@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-v593OaNePQ@OpenReview" style="display:none">
                    <span class="i-index">#203</span>
                    <a class="i-title" href="#v593OaNePQ@OpenReview">Learning to Search from Demonstration Sequences</a>
                    <a class="i-star" onclick="toggleAppStar('v593OaNePQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('v593OaNePQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-je3GZissZc@OpenReview" style="display:none">
                    <span class="i-index">#204</span>
                    <a class="i-title" href="#je3GZissZc@OpenReview">Instant Policy: In-Context Imitation Learning via Graph Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('je3GZissZc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('je3GZissZc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-eFGQ97z5Cd@OpenReview" style="display:none">
                    <span class="i-index">#205</span>
                    <a class="i-title" href="#eFGQ97z5Cd@OpenReview">Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free</a>
                    <a class="i-star" onclick="toggleAppStar('eFGQ97z5Cd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('eFGQ97z5Cd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-P7KIGdgW8S@OpenReview" style="display:none">
                    <span class="i-index">#206</span>
                    <a class="i-title" href="#P7KIGdgW8S@OpenReview">On the Hlder Stability of Multiset and Graph Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('P7KIGdgW8S@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('P7KIGdgW8S@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LyJi5ugyJx@OpenReview" style="display:none">
                    <span class="i-index">#207</span>
                    <a class="i-title" href="#LyJi5ugyJx@OpenReview">Simplifying, Stabilizing and Scaling Continuous-time Consistency Models</a>
                    <a class="i-star" onclick="toggleAppStar('LyJi5ugyJx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LyJi5ugyJx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Main-bold, sans-serif;"></div></div></body></html>