<html><head>
    <title>ICML.2025 - Oral | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for ICML.2025 - Oral, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.9') format('opentype')}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">ICML.2025 - Oral</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 120
    </p>
    <div class="papers">
        <div id="QqVZ28qems@OpenReview" class="panel paper" keywords="scaling,success,language,multimodal,attempts,aggregate,monkeys,power,law,problem">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QqVZ28qems" target="_blank" title="1/120"><span class="index notranslate">#1</span></a>
                <a id="title-QqVZ28qems@OpenReview" class="title-link" href="/venue/QqVZ28qems@OpenReview" target="_blank">How Do Large Language Monkeys Get Their Power (Laws)?</a>
                <a id="pdf-QqVZ28qems@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QqVZ28qems@OpenReview', this)" data="https://openreview.net/pdf?id=QqVZ28qems">[PDF<sup id="pdf-stars-QqVZ28qems@OpenReview">160</sup>]</a>
                <a id="copy-QqVZ28qems@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QqVZ28qems@OpenReview')">[Copy]</a>
                <a id="kimi-QqVZ28qems@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QqVZ28qems@OpenReview', this)">[Kimi<sup id="kimi-stars-QqVZ28qems@OpenReview">203</sup>]</a>
                <a id="rel-QqVZ28qems@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QqVZ28qems@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QqVZ28qems@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rylan Schaeffer" target="_blank">Rylan Schaeffer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua Kazdan" target="_blank">Joshua Kazdan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Hughes" target="_blank">John Hughes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jordan Juravsky" target="_blank">Jordan Juravsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Price" target="_blank">Sara Price</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aengus Lynch" target="_blank">Aengus Lynch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erik Jones" target="_blank">Erik Jones</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Kirk" target="_blank">Robert Kirk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azalia Mirhoseini" target="_blank">Azalia Mirhoseini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanmi Koyejo" target="_blank">Sanmi Koyejo</a>
            </p>
            <p id="summary-QqVZ28qems@OpenReview" class="summary">Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts.In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts.We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge?We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own.We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 3.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.92em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="texatom" id="MathJax-Span-3"><span class="mrow" id="MathJax-Span-4"><span class="mo" id="MathJax-Span-5" style="font-family: MathJax_Main;">∼</span></span></span><span class="mn" id="MathJax-Span-6" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="mn" id="MathJax-Span-8" style="font-family: MathJax_Main; padding-left: 0.211em;">4</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo>∼</mo></mrow><mn>2</mn><mo>−</mo><mn>4</mn></math></span></span><script type="math/tex" id="MathJax-Element-1">{\sim}2-4</script> orders of magnitude less inference compute.Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.</p>
            <p id="subjects-QqVZ28qems@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-QqVZ28qems@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QqVZ28qems@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QqVZ28qems@OpenReview" onclick="foldPdfKimi('QqVZ28qems@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="WGXb7UdvTX@OpenReview" class="panel paper" keywords="layer,layers,representations,language,hidden,uncovering,mid,final,intermediate,embeddings">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=WGXb7UdvTX" target="_blank" title="2/120"><span class="index notranslate">#2</span></a>
                <a id="title-WGXb7UdvTX@OpenReview" class="title-link" href="/venue/WGXb7UdvTX@OpenReview" target="_blank">Layer by Layer: Uncovering Hidden Representations in Language Models</a>
                <a id="pdf-WGXb7UdvTX@OpenReview" class="title-pdf notranslate" onclick="togglePdf('WGXb7UdvTX@OpenReview', this)" data="https://openreview.net/pdf?id=WGXb7UdvTX">[PDF<sup id="pdf-stars-WGXb7UdvTX@OpenReview">153</sup>]</a>
                <a id="copy-WGXb7UdvTX@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('WGXb7UdvTX@OpenReview')">[Copy]</a>
                <a id="kimi-WGXb7UdvTX@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('WGXb7UdvTX@OpenReview', this)">[Kimi<sup id="kimi-stars-WGXb7UdvTX@OpenReview">159</sup>]</a>
                <a id="rel-WGXb7UdvTX@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('WGXb7UdvTX@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-WGXb7UdvTX@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Oscar Skean" target="_blank">Oscar Skean</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Md Rifat Arefin" target="_blank">Md Rifat Arefin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Zhao" target="_blank">Dan Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niket Patel" target="_blank">Niket Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jalal Naghiyev" target="_blank">Jalal Naghiyev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yann LeCun" target="_blank">Yann LeCun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ravid Shwartz-Ziv" target="_blank">Ravid Shwartz-Ziv</a>
            </p>
            <p id="summary-WGXb7UdvTX@OpenReview" class="summary">From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer’s performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.</p>
            <p id="subjects-WGXb7UdvTX@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-WGXb7UdvTX@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-WGXb7UdvTX@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-WGXb7UdvTX@OpenReview" onclick="foldPdfKimi('WGXb7UdvTX@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="QmIzUuspWo@OpenReview" class="panel paper" keywords="algorithm,distributions,convex,adaptive,sampling,stochastic,nonsmooth,online,varying,subsequentially">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QmIzUuspWo" target="_blank" title="3/120"><span class="index notranslate">#3</span></a>
                <a id="title-QmIzUuspWo@OpenReview" class="title-link" href="/venue/QmIzUuspWo@OpenReview" target="_blank">An Online Adaptive Sampling Algorithm for Stochastic Difference-of-convex Optimization with Time-varying Distributions</a>
                <a id="pdf-QmIzUuspWo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QmIzUuspWo@OpenReview', this)" data="https://openreview.net/pdf?id=QmIzUuspWo">[PDF<sup id="pdf-stars-QmIzUuspWo@OpenReview">43</sup>]</a>
                <a id="copy-QmIzUuspWo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QmIzUuspWo@OpenReview')">[Copy]</a>
                <a id="kimi-QmIzUuspWo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QmIzUuspWo@OpenReview', this)">[Kimi<sup id="kimi-stars-QmIzUuspWo@OpenReview">58</sup>]</a>
                <a id="rel-QmIzUuspWo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QmIzUuspWo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QmIzUuspWo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Ye" target="_blank">Yuhan Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Cui" target="_blank">Ying Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Wang" target="_blank">Jingyi Wang</a>
            </p>
            <p id="summary-QmIzUuspWo@OpenReview" class="summary">We propose an online adaptive sampling algorithm for solving stochastic nonsmooth difference-of-convex (DC) problems under time-varying distributions. At each iteration, the algorithm relies solely on data generated from the current distribution and employs distinct adaptive sampling rates for the convex and concave components of the DC function, a novel design guided by our theoretical analysis. We show that, under proper conditions on the convergence of distributions, the algorithm converges subsequentially to DC critical points almost surely. Furthermore, the sample size requirement of our proposed algorithm matches the results achieved in the smooth case or when a measurable subgradient selector is available, both under static distributions. A key element of this analysis is the derivation of a novel <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-9" style="width: 5.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1004.07em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-10"><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-12" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-13"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1001.62em, 2.555em, -999.997em); top: -2.133em; left: 0.992em;"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-family: MathJax_Math-italic;">p</span><span class="texatom" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mo" id="MathJax-Span-18" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-19" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1001.57em, 3.961em, -999.997em); top: -4.581em; left: 0.992em;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.888em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.419em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.023em, 1000.99em, 4.534em, -999.997em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-20" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msqrt><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-2">O(\sqrt{p/n})</script> pointwise convergence rate (modulo logarithmic factors) for the sample average approximation of subdifferential mappings, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-21" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23" style="font-family: MathJax_Math-italic;">p</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">p</script> is the dimension of the variable and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-24" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-25"><span class="mi" id="MathJax-Span-26" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-4">n</script> is the sample size -- a result of independent interest. Numerical experiments confirm that the proposed algorithm is both efficient and effective for addressing stochastic nonsmooth problems.</p>
            <p id="subjects-QmIzUuspWo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-QmIzUuspWo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QmIzUuspWo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QmIzUuspWo@OpenReview" onclick="foldPdfKimi('QmIzUuspWo@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="VsJ1K2HV3k@OpenReview" class="panel paper" keywords="multimodal,mllms,generalists,generalist,capabilities,modalities,mllm,general,bench,agi">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=VsJ1K2HV3k" target="_blank" title="4/120"><span class="index notranslate">#4</span></a>
                <a id="title-VsJ1K2HV3k@OpenReview" class="title-link" href="/venue/VsJ1K2HV3k@OpenReview" target="_blank">On Path to Multimodal Generalist: General-Level and General-Bench</a>
                <a id="pdf-VsJ1K2HV3k@OpenReview" class="title-pdf notranslate" onclick="togglePdf('VsJ1K2HV3k@OpenReview', this)" data="https://openreview.net/pdf?id=VsJ1K2HV3k">[PDF<sup id="pdf-stars-VsJ1K2HV3k@OpenReview">38</sup>]</a>
                <a id="copy-VsJ1K2HV3k@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('VsJ1K2HV3k@OpenReview')">[Copy]</a>
                <a id="kimi-VsJ1K2HV3k@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('VsJ1K2HV3k@OpenReview', this)">[Kimi<sup id="kimi-stars-VsJ1K2HV3k@OpenReview">65</sup>]</a>
                <a id="rel-VsJ1K2HV3k@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('VsJ1K2HV3k@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-VsJ1K2HV3k@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Fei" target="_blank">Hao Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Zhou" target="_blank">Yuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juncheng Li" target="_blank">Juncheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangtai Li" target="_blank">Xiangtai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingshan Xu" target="_blank">Qingshan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bobo Li" target="_blank">Bobo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengqiong Wu" target="_blank">Shengqiong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaoting Wang" target="_blank">Yaoting Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junbao Zhou" target="_blank">Junbao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Meng" target="_blank">Jiahao Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyu Shi" target="_blank">Qingyu Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Zhou" target="_blank">Zhiyuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liangtao Shi" target="_blank">Liangtao Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minghe Gao" target="_blank">Minghe Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daoan Zhang" target="_blank">Daoan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqi Ge" target="_blank">Zhiqi Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Tang" target="_blank">Siliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaihang Pan" target="_blank">Kaihang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaobo Ye" target="_blank">Yaobo Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haobo Yuan" target="_blank">Haobo Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Zhang" target="_blank">Tao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weiming Wu" target="_blank">Weiming Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianjie Ju" target="_blank">Tianjie Ju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixiang Meng" target="_blank">Zixiang Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shilin Xu" target="_blank">Shilin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liyu Jia" target="_blank">Liyu Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Hu" target="_blank">Wentao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Luo" target="_blank">Meng Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiebo Luo" target="_blank">Jiebo Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tat-Seng Chua" target="_blank">Tat-Seng Chua</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuicheng YAN" target="_blank">Shuicheng YAN</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>
            </p>
            <p id="summary-VsJ1K2HV3k@OpenReview" class="summary">The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of language-based LLMs. Unlike their specialist predecessors, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting singular modalities to accommodating a wide array of or even arbitrary modalities. To assess the capabilities of various MLLMs, a diverse array of benchmark test sets has been proposed. This leads to a critical question: *Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI?*We argue that the answer is not as straightforward as it seems. In this project, we introduce an evaluation framework to delineate the capabilities and behaviors of current multimodal generalists. This framework, named **General-Level**, establishes 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI (Artificial General Intelligence). Central to our framework is the use of **Synergy** as the evaluative criterion, categorizing capabilities based on whether MLLMs preserve synergy across comprehension and generation, as well as across multimodal interactions.To evaluate the comprehensive abilities of various generalists, we present a massive multimodal benchmark, **General-Bench**, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI.Project Page: https://generalist.top/,Leaderboard: https://generalist.top/leaderboard/,Benchmark: https://huggingface.co/General-Level/.</p>
            <p id="subjects-VsJ1K2HV3k@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-VsJ1K2HV3k@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-VsJ1K2HV3k@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-VsJ1K2HV3k@OpenReview" onclick="foldPdfKimi('VsJ1K2HV3k@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="qR7YsQdFxV@OpenReview" class="panel paper" keywords="valiant,mean,lee,estimator,dang,robustness,settings,estimation,estimators,optimality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qR7YsQdFxV" target="_blank" title="5/120"><span class="index notranslate">#5</span></a>
                <a id="title-qR7YsQdFxV@OpenReview" class="title-link" href="/venue/qR7YsQdFxV@OpenReview" target="_blank">All-Purpose Mean Estimation over R: Optimal Sub-Gaussianity with Outlier Robustness and Low Moments Performance</a>
                <a id="pdf-qR7YsQdFxV@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qR7YsQdFxV@OpenReview', this)" data="https://openreview.net/pdf?id=qR7YsQdFxV">[PDF<sup id="pdf-stars-qR7YsQdFxV@OpenReview">21</sup>]</a>
                <a id="copy-qR7YsQdFxV@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qR7YsQdFxV@OpenReview')">[Copy]</a>
                <a id="kimi-qR7YsQdFxV@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qR7YsQdFxV@OpenReview', this)">[Kimi<sup id="kimi-stars-qR7YsQdFxV@OpenReview">46</sup>]</a>
                <a id="rel-qR7YsQdFxV@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qR7YsQdFxV@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qR7YsQdFxV@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jasper Lee" target="_blank">Jasper Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Walter McKelvie" target="_blank">Walter McKelvie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maoyuan Song" target="_blank">Maoyuan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Valiant" target="_blank">Paul Valiant</a>
            </p>
            <p id="summary-qR7YsQdFxV@OpenReview" class="summary">We consider the basic statistical challenge of designing an "all-purpose" mean estimation algorithm that is recommendable across a variety of settings and models.Recent work by [Lee and Valiant 2022] introduced the first 1-d mean estimator whose error in the standard finite-variance+i.i.d. setting is optimal even in its constant factors; experimental demonstration of its good performance was shown by [Gobet et al. 2022].Yet, unlike for classic (but not necessarily practical) estimators such as median-of-means and trimmed mean, this new algorithm lacked proven robustness guarantees in other settings, including the settings of adversarial data corruption and heavy-tailed distributions with infinite variance.Such robustness is important for practical use cases.This raises a research question: is it possible to have a mean estimator that is robust, *without* sacrificing provably optimal performance in the standard i.i.d. setting?In this work, we show that Lee and Valiant's estimator is in fact an "all-purpose" mean estimator by proving:(A) It is robust to an <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B7;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-27" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-28"><span class="mi" id="MathJax-Span-29" style="font-family: MathJax_Math-italic;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>η</mi></math></span></span><script type="math/tex" id="MathJax-Element-5">\eta</script>-fraction of data corruption, even in the strong contamination model; it has optimal estimation error <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;msqrt&gt;&lt;mi&gt;&amp;#x03B7;&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-30" style="width: 4.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.39em, 2.659em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-31"><span class="mi" id="MathJax-Span-32" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-33" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-34" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msqrt" id="MathJax-Span-35"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-36"><span class="mi" id="MathJax-Span-37" style="font-family: MathJax_Math-italic;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.52em, 3.961em, -999.997em); top: -4.372em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.154em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -3.852em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-38" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>σ</mi><msqrt><mi>η</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-6">O(\sigma\sqrt{\eta})</script> for distributions with variance <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-39" style="width: 1.253em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-40"><span class="msubsup" id="MathJax-Span-41"><span style="display: inline-block; position: relative; width: 1.044em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-42" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="mn" id="MathJax-Span-43" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>σ</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-7">\sigma^2</script>.(B) For distributions with finite <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mtext&gt;th&lt;/mtext&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-44" style="width: 1.513em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.25em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-45"><span class="msubsup" id="MathJax-Span-46"><span style="display: inline-block; position: relative; width: 1.253em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-47" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.523em;"><span class="mtext" id="MathJax-Span-48" style="font-size: 70.7%; font-family: MathJax_Main;">th</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>z</mi><mtext>th</mtext></msup></math></span></span><script type="math/tex" id="MathJax-Element-8">z^\text{th}</script> moment, for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-49" style="width: 4.638em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.75em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-50"><span class="mi" id="MathJax-Span-51" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-52" style="font-family: MathJax_Main; padding-left: 0.263em;">∈</span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main; padding-left: 0.263em;">(</span><span class="mn" id="MathJax-Span-54" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-55" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-56" style="font-family: MathJax_Main; padding-left: 0.159em;">2</span><span class="mo" id="MathJax-Span-57" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi><mo>∈</mo><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-9">z \in (1,2)</script>, it has optimal estimation error, matching the lower bounds of [Devroye et al. 2016] up to constants.We further show (C) that outlier robustness for 1-d mean estimators in fact implies neighborhood optimality, a notion of beyond worst-case and distribution-dependent optimality recently introduced by [Dang et al. 2023].Previously, such an optimality guarantee was only known for median-of-means, but now it holds also for all estimators that are simultaneously *robust* and *sub-Gaussian*, including Lee and Valiant's, resolving a question raised by Dang et al.Lastly, we show (D) the asymptotic normality and efficiency of Lee and Valiant's estimator, as further evidence for its performance across many settings.</p>
            <p id="subjects-qR7YsQdFxV@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-qR7YsQdFxV@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qR7YsQdFxV@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qR7YsQdFxV@OpenReview" onclick="foldPdfKimi('qR7YsQdFxV@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="l8QemUZaIA@OpenReview" class="panel paper" keywords="review,reviewer,peer,authors,system,reviewers,conference,rewards,submissions,quality">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=l8QemUZaIA" target="_blank" title="6/120"><span class="index notranslate">#6</span></a>
                <a id="title-l8QemUZaIA@OpenReview" class="title-link" href="/venue/l8QemUZaIA@OpenReview" target="_blank">Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards</a>
                <a id="pdf-l8QemUZaIA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('l8QemUZaIA@OpenReview', this)" data="https://openreview.net/pdf?id=l8QemUZaIA">[PDF<sup id="pdf-stars-l8QemUZaIA@OpenReview">34</sup>]</a>
                <a id="copy-l8QemUZaIA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('l8QemUZaIA@OpenReview')">[Copy]</a>
                <a id="kimi-l8QemUZaIA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('l8QemUZaIA@OpenReview', this)">[Kimi<sup id="kimi-stars-l8QemUZaIA@OpenReview">61</sup>]</a>
                <a id="rel-l8QemUZaIA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('l8QemUZaIA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-l8QemUZaIA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaeho Kim" target="_blank">Jaeho Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunseok Lee" target="_blank">Yunseok Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seulki Lee" target="_blank">Seulki Lee</a>
            </p>
            <p id="summary-l8QemUZaIA@OpenReview" class="summary">The peer review process in major artificial intelligence (AI) conferences faces unprecedented challenges with the surge of paper submissions (exceeding 10,000 submissions per venue), accompanied by growing concerns over review quality and reviewer responsibility. This position paper argues for **the need to transform the traditional one-way review system into a bi-directional feedback loop where authors evaluate review quality and reviewers earn formal accreditation, creating an accountability framework that promotes a sustainable, high-quality peer review system.** The current review system can be viewed as an interaction between three parties: the authors, reviewers, and system (i.e., conference), where we posit that all three parties share responsibility for the current problems. However, issues with authors can only be addressed through policy enforcement and detection tools, and ethical concerns can only be corrected through self-reflection. As such, this paper focuses on reforming reviewer accountability with systematic rewards through two key mechanisms: (1) a two-stage bi-directional review system that allows authors to evaluate reviews while minimizing retaliatory behavior, (2) a systematic reviewer reward system that incentivizes quality reviewing. We ask for the community's strong interest in these problems and the reforms that are needed to enhance the peer review process.</p>
            <p id="subjects-l8QemUZaIA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-l8QemUZaIA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-l8QemUZaIA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-l8QemUZaIA@OpenReview" onclick="foldPdfKimi('l8QemUZaIA@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="0LZRtvK871@OpenReview" class="panel paper" keywords="deliberate,synthetic,fewer,scaling,samples,informative,iterations,practice,improving,laws">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0LZRtvK871" target="_blank" title="7/120"><span class="index notranslate">#7</span></a>
                <a id="title-0LZRtvK871@OpenReview" class="title-link" href="/venue/0LZRtvK871@OpenReview" target="_blank">Improving the Scaling Laws of Synthetic Data with Deliberate Practice</a>
                <a id="pdf-0LZRtvK871@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0LZRtvK871@OpenReview', this)" data="https://openreview.net/pdf?id=0LZRtvK871">[PDF<sup id="pdf-stars-0LZRtvK871@OpenReview">34</sup>]</a>
                <a id="copy-0LZRtvK871@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0LZRtvK871@OpenReview')">[Copy]</a>
                <a id="kimi-0LZRtvK871@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0LZRtvK871@OpenReview', this)">[Kimi<sup id="kimi-stars-0LZRtvK871@OpenReview">67</sup>]</a>
                <a id="rel-0LZRtvK871@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0LZRtvK871@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0LZRtvK871@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Reyhane Askari Hemmat" target="_blank">Reyhane Askari Hemmat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Pezeshki" target="_blank">Mohammad Pezeshki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elvis Dohmatob" target="_blank">Elvis Dohmatob</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Bordes" target="_blank">Florian Bordes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pietro Astolfi" target="_blank">Pietro Astolfi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Melissa Hall" target="_blank">Melissa Hall</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Verbeek" target="_blank">Jakob Verbeek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Drozdzal" target="_blank">Michal Drozdzal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adriana Romero-Soriano" target="_blank">Adriana Romero-Soriano</a>
            </p>
            <p id="summary-0LZRtvK871@OpenReview" class="summary">Inspired by the principle of deliberate practice in human learning, we propose Deliberate Practice for Synthetic Data Generation (DP), a novel framework that improves sample efficiency through dynamic synthetic data generation. Prior work has shown that scaling synthetic data is inherently challenging, as naively adding new data leads to diminishing returns. To address this, pruning has been identified as a key mechanism for improving scaling, enabling models to focus on the most informative synthetic samples. Rather than generating a large dataset and pruning it afterward, DP efficiently approximates the direct generation of informative samples. We theoretically show how training on challenging, informative examples improves scaling laws and empirically validate that DP achieves better scaling performance with significantly fewer training samples and iterations. On ImageNet-100, DP generates 3.4x fewer samples and requires six times fewer iterations, while on ImageNet-1k, it generates 8x fewer samples with a 30% reduction in iterations, all while achieving superior performance compared to prior work.</p>
            <p id="subjects-0LZRtvK871@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-0LZRtvK871@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0LZRtvK871@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0LZRtvK871@OpenReview" onclick="foldPdfKimi('0LZRtvK871@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="26JsumCG0z@OpenReview" class="panel paper" keywords="worst,prediction,bureaucratic,levers,unemployment,surfacing,residents,prioritizing,equity,driven">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=26JsumCG0z" target="_blank" title="8/120"><span class="index notranslate">#8</span></a>
                <a id="title-26JsumCG0z@OpenReview" class="title-link" href="/venue/26JsumCG0z@OpenReview" target="_blank">The Value of Prediction in Identifying the Worst-Off</a>
                <a id="pdf-26JsumCG0z@OpenReview" class="title-pdf notranslate" onclick="togglePdf('26JsumCG0z@OpenReview', this)" data="https://openreview.net/pdf?id=26JsumCG0z">[PDF<sup id="pdf-stars-26JsumCG0z@OpenReview">13</sup>]</a>
                <a id="copy-26JsumCG0z@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('26JsumCG0z@OpenReview')">[Copy]</a>
                <a id="kimi-26JsumCG0z@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('26JsumCG0z@OpenReview', this)">[Kimi<sup id="kimi-stars-26JsumCG0z@OpenReview">39</sup>]</a>
                <a id="rel-26JsumCG0z@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('26JsumCG0z@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-26JsumCG0z@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Unai Fischer Abaigar" target="_blank">Unai Fischer Abaigar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Kern" target="_blank">Christoph Kern</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juan Perdomo" target="_blank">Juan Perdomo</a>
            </p>
            <p id="summary-26JsumCG0z@OpenReview" class="summary">Machine learning is increasingly used in government programs to identify and support the most vulnerable individuals, prioritizing assistance for those at greatest risk over optimizing aggregate outcomes. This paper examines the welfare impacts of prediction in equity-driven contexts, and how they compare to other policy levers, such as expanding bureaucratic capacity. Through mathematical models and a real-world case study on long-term unemployment amongst German residents, we develop a comprehensive understanding of the relative effectiveness of prediction in surfacing the worst-off. Our findings provide clear analytical frameworks and practical, data-driven tools that empower policymakers to make principled decisions when designing these systems.</p>
            <p id="subjects-26JsumCG0z@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-26JsumCG0z@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-26JsumCG0z@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-26JsumCG0z@OpenReview" onclick="foldPdfKimi('26JsumCG0z@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="2uheUFcFsM@OpenReview" class="panel paper" keywords="tarflow,nfs,normalizing,flows,mafs,capable,generative,autoregressive,likelihood,transformer">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2uheUFcFsM" target="_blank" title="9/120"><span class="index notranslate">#9</span></a>
                <a id="title-2uheUFcFsM@OpenReview" class="title-link" href="/venue/2uheUFcFsM@OpenReview" target="_blank">Normalizing Flows are Capable Generative Models</a>
                <a id="pdf-2uheUFcFsM@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2uheUFcFsM@OpenReview', this)" data="https://openreview.net/pdf?id=2uheUFcFsM">[PDF<sup id="pdf-stars-2uheUFcFsM@OpenReview">49</sup>]</a>
                <a id="copy-2uheUFcFsM@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2uheUFcFsM@OpenReview')">[Copy]</a>
                <a id="kimi-2uheUFcFsM@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2uheUFcFsM@OpenReview', this)">[Kimi<sup id="kimi-stars-2uheUFcFsM@OpenReview">64</sup>]</a>
                <a id="rel-2uheUFcFsM@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2uheUFcFsM@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2uheUFcFsM@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuangfei Zhai" target="_blank">Shuangfei Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruixiang Zhang" target="_blank">Ruixiang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Preetum Nakkiran" target="_blank">Preetum Nakkiran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Berthelot" target="_blank">David Berthelot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiatao Gu" target="_blank">Jiatao Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huangjie Zheng" target="_blank">Huangjie Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianrong Chen" target="_blank">Tianrong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miguel Angel Bautista Martin" target="_blank">Miguel Angel Bautista Martin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Navdeep Jaitly" target="_blank">Navdeep Jaitly</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua M Susskind" target="_blank">Joshua M Susskind</a>
            </p>
            <p id="summary-2uheUFcFsM@OpenReview" class="summary">Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at https://github.com/apple/ml-tarflow.</p>
            <p id="subjects-2uheUFcFsM@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-2uheUFcFsM@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2uheUFcFsM@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2uheUFcFsM@OpenReview" onclick="foldPdfKimi('2uheUFcFsM@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="36hVB7DEB0@OpenReview" class="panel paper" keywords="rfm,grokking,arithmetic,modular,agop,circulant,neural,gradient,emergence,test">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=36hVB7DEB0" target="_blank" title="10/120"><span class="index notranslate">#10</span></a>
                <a id="title-36hVB7DEB0@OpenReview" class="title-link" href="/venue/36hVB7DEB0@OpenReview" target="_blank">Emergence in non-neural models: grokking modular arithmetic via average gradient outer product</a>
                <a id="pdf-36hVB7DEB0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('36hVB7DEB0@OpenReview', this)" data="https://openreview.net/pdf?id=36hVB7DEB0">[PDF<sup id="pdf-stars-36hVB7DEB0@OpenReview">12</sup>]</a>
                <a id="copy-36hVB7DEB0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('36hVB7DEB0@OpenReview')">[Copy]</a>
                <a id="kimi-36hVB7DEB0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('36hVB7DEB0@OpenReview', this)">[Kimi<sup id="kimi-stars-36hVB7DEB0@OpenReview">41</sup>]</a>
                <a id="rel-36hVB7DEB0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('36hVB7DEB0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-36hVB7DEB0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Neil Mallinar" target="_blank">Neil Mallinar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Beaglehole" target="_blank">Daniel Beaglehole</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Libin Zhu" target="_blank">Libin Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adityanarayanan Radhakrishnan" target="_blank">Adityanarayanan Radhakrishnan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Parthe Pandit" target="_blank">Parthe Pandit</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Misha Belkin" target="_blank">Misha Belkin</a>
            </p>
            <p id="summary-36hVB7DEB0@OpenReview" class="summary">Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where the test accuracy starts improving long after the model achieves 100% training accuracy in the training process. It is often taken as an example of "emergence", where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that this phenomenon occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general machine learning models. When used in conjunction with kernel machines, iterating RFM results in a fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot be predicted from the training loss, which is identically zero, nor from the test loss, which remains constant in initial iterations. Instead, as we show, the transition is completely determined by feature learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks. Our results demonstrate that emergence can result purely from learning task-relevant features and is not specific to neural architectures nor gradient descent-based optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism for feature learning in neural networks.</p>
            <p id="subjects-36hVB7DEB0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-36hVB7DEB0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-36hVB7DEB0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-36hVB7DEB0@OpenReview" onclick="foldPdfKimi('36hVB7DEB0@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="3go0lhfxd0@OpenReview" class="panel paper" keywords="task,streaming,parity,neural,networks,representational,generalization,algorithm,development,generalize">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=3go0lhfxd0" target="_blank" title="11/120"><span class="index notranslate">#11</span></a>
                <a id="title-3go0lhfxd0@OpenReview" class="title-link" href="/venue/3go0lhfxd0@OpenReview" target="_blank">Algorithm Development in Neural Networks: Insights from the Streaming Parity Task</a>
                <a id="pdf-3go0lhfxd0@OpenReview" class="title-pdf notranslate" onclick="togglePdf('3go0lhfxd0@OpenReview', this)" data="https://openreview.net/pdf?id=3go0lhfxd0">[PDF<sup id="pdf-stars-3go0lhfxd0@OpenReview">10</sup>]</a>
                <a id="copy-3go0lhfxd0@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('3go0lhfxd0@OpenReview')">[Copy]</a>
                <a id="kimi-3go0lhfxd0@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('3go0lhfxd0@OpenReview', this)">[Kimi<sup id="kimi-stars-3go0lhfxd0@OpenReview">33</sup>]</a>
                <a id="rel-3go0lhfxd0@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('3go0lhfxd0@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-3go0lhfxd0@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Loek van Rossem" target="_blank">Loek van Rossem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Saxe" target="_blank">Andrew Saxe</a>
            </p>
            <p id="summary-3go0lhfxd0@OpenReview" class="summary">Even when massively overparameterized, deep neural networks show a remarkable ability to generalize. Research on this phenomenon has focused on generalization within distribution, via smooth interpolation. Yet in some settings neural networks also learn to extrapolate to data far beyond the bounds of the original training set, sometimes even allowing for infinite generalization, implying that an algorithm capable of solving the task has been learned. Here we undertake a case study of the learning dynamics of recurrent neural networks trained on the streaming parity task in order to develop an effective theory of algorithm development. The streaming parity task is a simple but nonlinear task defined on sequences up to arbitrary length. We show that, with sufficient finite training experience, RNNs exhibit a phase transition to perfect infinite generalization. Using an effective theory for the representational dynamics, we find an implicit representational merger effect which can be interpreted as the construction of a finite automaton that reproduces the task. Overall, our results disclose one mechanism by which neural networks can generalize infinitely from finite training experience.</p>
            <p id="subjects-3go0lhfxd0@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-3go0lhfxd0@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-3go0lhfxd0@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-3go0lhfxd0@OpenReview" onclick="foldPdfKimi('3go0lhfxd0@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="4tFSKOY2mT@OpenReview" class="panel paper" keywords="omnibench,virtual,capabilities,graph,multidimensional,subtask,across,agent,agents,evaluation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4tFSKOY2mT" target="_blank" title="12/120"><span class="index notranslate">#12</span></a>
                <a id="title-4tFSKOY2mT@OpenReview" class="title-link" href="/venue/4tFSKOY2mT@OpenReview" target="_blank">What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</a>
                <a id="pdf-4tFSKOY2mT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4tFSKOY2mT@OpenReview', this)" data="https://openreview.net/pdf?id=4tFSKOY2mT">[PDF<sup id="pdf-stars-4tFSKOY2mT@OpenReview">19</sup>]</a>
                <a id="copy-4tFSKOY2mT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4tFSKOY2mT@OpenReview')">[Copy]</a>
                <a id="kimi-4tFSKOY2mT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4tFSKOY2mT@OpenReview', this)">[Kimi<sup id="kimi-stars-4tFSKOY2mT@OpenReview">41</sup>]</a>
                <a id="rel-4tFSKOY2mT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4tFSKOY2mT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4tFSKOY2mT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wendong Bu" target="_blank">Wendong Bu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Wu" target="_blank">Yang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifan Yu" target="_blank">Qifan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minghe Gao" target="_blank">Minghe Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingchen Miao" target="_blank">Bingchen Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenkui Zhang" target="_blank">Zhenkui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaihang Pan" target="_blank">Kaihang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=liyunfei" target="_blank">liyunfei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengze Li" target="_blank">Mengze Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Ji" target="_blank">Wei Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juncheng Li" target="_blank">Juncheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siliang Tang" target="_blank">Siliang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueting Zhuang" target="_blank">Yueting Zhuang</a>
            </p>
            <p id="summary-4tFSKOY2mT@OpenReview" class="summary">As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it improves generalization across environments. We conduct multidimensional evaluations for virtual agents, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io.</p>
            <p id="subjects-4tFSKOY2mT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-4tFSKOY2mT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4tFSKOY2mT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4tFSKOY2mT@OpenReview" onclick="foldPdfKimi('4tFSKOY2mT@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="5zwF1GizFa@OpenReview" class="panel paper" keywords="math,rstar,slm,slms,reasoning,mcts,thinking,ppm,evolved,policy">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=5zwF1GizFa" target="_blank" title="13/120"><span class="index notranslate">#13</span></a>
                <a id="title-5zwF1GizFa@OpenReview" class="title-link" href="/venue/5zwF1GizFa@OpenReview" target="_blank">rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</a>
                <a id="pdf-5zwF1GizFa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('5zwF1GizFa@OpenReview', this)" data="https://openreview.net/pdf?id=5zwF1GizFa">[PDF<sup id="pdf-stars-5zwF1GizFa@OpenReview">62</sup>]</a>
                <a id="copy-5zwF1GizFa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('5zwF1GizFa@OpenReview')">[Copy]</a>
                <a id="kimi-5zwF1GizFa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('5zwF1GizFa@OpenReview', this)">[Kimi<sup id="kimi-stars-5zwF1GizFa@OpenReview">66</sup>]</a>
                <a id="rel-5zwF1GizFa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('5zwF1GizFa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-5zwF1GizFa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Guan" target="_blank">Xinyu Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Lyna Zhang" target="_blank">Li Lyna Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Liu" target="_blank">Yifei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Shang" target="_blank">Ning Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youran Sun" target="_blank">Youran Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhu" target="_blank">Yi Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Yang" target="_blank">Fan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mao Yang" target="_blank">Mao Yang</a>
            </p>
            <p id="summary-5zwF1GizFa@OpenReview" class="summary">We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising ``deep thinking'' through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0%, surpassing o1-preview by +4.5%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% of the brightest high school math students. Code and data are available at https://github.com/microsoft/rStar.</p>
            <p id="subjects-5zwF1GizFa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-5zwF1GizFa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-5zwF1GizFa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-5zwF1GizFa@OpenReview" onclick="foldPdfKimi('5zwF1GizFa@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Cf0N07E1vu@OpenReview" class="panel paper" keywords="ensembles,overparameterized,underparameterized,regressors,overparameterization,single,classic,generalization,ridgeless,modern">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Cf0N07E1vu" target="_blank" title="14/120"><span class="index notranslate">#14</span></a>
                <a id="title-Cf0N07E1vu@OpenReview" class="title-link" href="/venue/Cf0N07E1vu@OpenReview" target="_blank">Theoretical Limitations of Ensembles in the Age of Overparameterization</a>
                <a id="pdf-Cf0N07E1vu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Cf0N07E1vu@OpenReview', this)" data="https://openreview.net/pdf?id=Cf0N07E1vu">[PDF<sup id="pdf-stars-Cf0N07E1vu@OpenReview">17</sup>]</a>
                <a id="copy-Cf0N07E1vu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Cf0N07E1vu@OpenReview')">[Copy]</a>
                <a id="kimi-Cf0N07E1vu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Cf0N07E1vu@OpenReview', this)">[Kimi<sup id="kimi-stars-Cf0N07E1vu@OpenReview">27</sup>]</a>
                <a id="rel-Cf0N07E1vu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Cf0N07E1vu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Cf0N07E1vu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Niclas Dern" target="_blank">Niclas Dern</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Cunningham" target="_blank">John Cunningham</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Geoff Pleiss" target="_blank">Geoff Pleiss</a>
            </p>
            <p id="summary-Cf0N07E1vu@OpenReview" class="summary">Classic ensembles generalize better than any single component model. In contrast, recent empirical studies find that modern ensembles of (overparameterized) neural networks may not provide any inherent generalization advantage over single but larger neural networks. This paper clarifies how modern overparameterized ensembles differ from their classic underparameterized counterparts, using ensembles of random feature (RF) regressors as a basis for developing theory. In contrast to the underparameterized regime, where ensembling typically induces regularization and increases generalization, we prove with minimal assumptions that infinite ensembles of overparameterized RF regressors become pointwise equivalent to (single) infinite-width RF regressors, and finite width ensembles rapidly converge to single models with the same parameter budget. These results, which are exact for ridgeless models and approximate for small ridge penalties, imply that overparameterized ensembles and single large models exhibit nearly identical generalization. We further characterize the predictive variance amongst ensemble members, demonstrating that it quantifies the expected effects of increasing capacity rather than capturing any conventional notion of uncertainty. Our results challenge common assumptions about the advantages of ensembles in overparameterized settings, prompting a reconsideration of how well intuitions from underparameterized ensembles transfer to deep ensembles and the overparameterized regime.</p>
            <p id="subjects-Cf0N07E1vu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Cf0N07E1vu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cf0N07E1vu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cf0N07E1vu@OpenReview" onclick="foldPdfKimi('Cf0N07E1vu@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="DgGF2LEBPS@OpenReview" class="panel paper" keywords="embodiedbench,embodied,agents,mllms,tasks,mllm,language,comprehensive,modal,vision">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DgGF2LEBPS" target="_blank" title="15/120"><span class="index notranslate">#15</span></a>
                <a id="title-DgGF2LEBPS@OpenReview" class="title-link" href="/venue/DgGF2LEBPS@OpenReview" target="_blank">EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</a>
                <a id="pdf-DgGF2LEBPS@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DgGF2LEBPS@OpenReview', this)" data="https://openreview.net/pdf?id=DgGF2LEBPS">[PDF<sup id="pdf-stars-DgGF2LEBPS@OpenReview">25</sup>]</a>
                <a id="copy-DgGF2LEBPS@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DgGF2LEBPS@OpenReview')">[Copy]</a>
                <a id="kimi-DgGF2LEBPS@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DgGF2LEBPS@OpenReview', this)">[Kimi<sup id="kimi-stars-DgGF2LEBPS@OpenReview">28</sup>]</a>
                <a id="rel-DgGF2LEBPS@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DgGF2LEBPS@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DgGF2LEBPS@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Yang" target="_blank">Rui Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyang(Jeremy) Chen" target="_blank">Hanyang(Jeremy) Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyu Zhang" target="_blank">Junyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Zhao" target="_blank">Mark Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Qian" target="_blank">Cheng Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangrui Wang" target="_blank">Kangrui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qineng Wang" target="_blank">Qineng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Teja Koripella" target="_blank">Teja Koripella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marziyeh Movahedi" target="_blank">Marziyeh Movahedi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manling Li" target="_blank">Manling Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heng Ji" target="_blank">Heng Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Zhang" target="_blank">Huan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Zhang" target="_blank">Tong Zhang</a>
            </p>
            <p id="summary-DgGF2LEBPS@OpenReview" class="summary">Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents.EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning.Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;28.9&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-58" style="width: 3.128em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.55em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-59"><span class="mn" id="MathJax-Span-60" style="font-family: MathJax_Main;">28.9</span><span class="mi" id="MathJax-Span-61" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>28.9</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-10">28.9\%</script> on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at [https://embodiedbench.github.io](https://embodiedbench.github.io).</p>
            <p id="subjects-DgGF2LEBPS@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-DgGF2LEBPS@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DgGF2LEBPS@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DgGF2LEBPS@OpenReview" onclick="foldPdfKimi('DgGF2LEBPS@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="DjJmre5IkP@OpenReview" class="panel paper" keywords="mdms,masked,inference,subproblems,token,arms,train,diffusions,autoregressive,front">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DjJmre5IkP" target="_blank" title="16/120"><span class="index notranslate">#16</span></a>
                <a id="title-DjJmre5IkP@OpenReview" class="title-link" href="/venue/DjJmre5IkP@OpenReview" target="_blank">Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions</a>
                <a id="pdf-DjJmre5IkP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DjJmre5IkP@OpenReview', this)" data="https://openreview.net/pdf?id=DjJmre5IkP">[PDF<sup id="pdf-stars-DjJmre5IkP@OpenReview">34</sup>]</a>
                <a id="copy-DjJmre5IkP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DjJmre5IkP@OpenReview')">[Copy]</a>
                <a id="kimi-DjJmre5IkP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DjJmre5IkP@OpenReview', this)">[Kimi<sup id="kimi-stars-DjJmre5IkP@OpenReview">35</sup>]</a>
                <a id="rel-DjJmre5IkP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DjJmre5IkP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DjJmre5IkP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaeyeon Kim" target="_blank">Jaeyeon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kulin Shah" target="_blank">Kulin Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vasilis Kontonis" target="_blank">Vasilis Kontonis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sham Kakade" target="_blank">Sham Kakade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sitan Chen" target="_blank">Sitan Chen</a>
            </p>
            <p id="summary-DjJmre5IkP@OpenReview" class="summary">In recent years, masked diffusion models (MDMs) have emerged as a promising alternative approach for generative modeling over discrete domains. Compared to autoregressive models (ARMs), MDMs trade off complexity at training time with flexibility at inference time. At training time, they must learn to solve an exponentially large number of infilling problems, but at inference time, they can decode tokens in essentially arbitrary order. In this work we closely examine these two competing effects. On the training front, we theoretically and empirically demonstrate that MDMs indeed train on computationally intractable subproblems compared to their autoregressive counterparts. On the inference front, we show that a suitable strategy for adaptively choosing the token decoding order significantly enhances the capabilities of MDMs, allowing them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that adaptive inference can boost solving accuracy in pretrained MDMs from <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;7&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-62" style="width: 1.878em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.57em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-63"><span class="mo" id="MathJax-Span-64" style="font-family: MathJax_Main;">&lt;</span><span class="mn" id="MathJax-Span-65" style="font-family: MathJax_Main; padding-left: 0.263em;">7</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo><mn>7</mn></math></span></span><script type="math/tex" id="MathJax-Element-11"><7</script>\% to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x2248;&lt;/mo&gt;&lt;mn&gt;90&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-66" style="width: 2.451em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.98em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-67"><span class="mo" id="MathJax-Span-68" style="font-family: MathJax_Main;">≈</span><span class="mn" id="MathJax-Span-69" style="font-family: MathJax_Main; padding-left: 0.263em;">90</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>≈</mo><mn>90</mn></math></span></span><script type="math/tex" id="MathJax-Element-12">\approx 90</script>\%, even outperforming ARMs that were explicitly trained via teacher forcing to learn the right order of decoding.</p>
            <p id="subjects-DjJmre5IkP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-DjJmre5IkP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DjJmre5IkP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DjJmre5IkP@OpenReview" onclick="foldPdfKimi('DjJmre5IkP@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="DmH4HHVb3y@OpenReview" class="panel paper" keywords="collabllm,multiturn,user,rewards,judges,responders,requests,collaborators,llm,responsesusing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=DmH4HHVb3y" target="_blank" title="17/120"><span class="index notranslate">#17</span></a>
                <a id="title-DmH4HHVb3y@OpenReview" class="title-link" href="/venue/DmH4HHVb3y@OpenReview" target="_blank">CollabLLM: From Passive Responders to Active Collaborators</a>
                <a id="pdf-DmH4HHVb3y@OpenReview" class="title-pdf notranslate" onclick="togglePdf('DmH4HHVb3y@OpenReview', this)" data="https://openreview.net/pdf?id=DmH4HHVb3y">[PDF<sup id="pdf-stars-DmH4HHVb3y@OpenReview">24</sup>]</a>
                <a id="copy-DmH4HHVb3y@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('DmH4HHVb3y@OpenReview')">[Copy]</a>
                <a id="kimi-DmH4HHVb3y@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('DmH4HHVb3y@OpenReview', this)">[Kimi<sup id="kimi-stars-DmH4HHVb3y@OpenReview">46</sup>]</a>
                <a id="rel-DmH4HHVb3y@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('DmH4HHVb3y@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-DmH4HHVb3y@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shirley Wu" target="_blank">Shirley Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michel Galley" target="_blank">Michel Galley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baolin Peng" target="_blank">Baolin Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Cheng" target="_blank">Hao Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gavin Li" target="_blank">Gavin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Dou" target="_blank">Yao Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weixin Cai" target="_blank">Weixin Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Zou" target="_blank">James Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jure Leskovec" target="_blank">Jure Leskovec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfeng Gao" target="_blank">Jianfeng Gao</a>
            </p>
            <p id="summary-DmH4HHVb3y@OpenReview" class="summary">Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responsesusing Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions—a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.</p>
            <p id="subjects-DmH4HHVb3y@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-DmH4HHVb3y@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-DmH4HHVb3y@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-DmH4HHVb3y@OpenReview" onclick="foldPdfKimi('DmH4HHVb3y@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="E1E6T7KHlR@OpenReview" class="panel paper" keywords="slate,queries,social,choice,statements,opinions,generative,slates,democratic,task">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=E1E6T7KHlR" target="_blank" title="18/120"><span class="index notranslate">#18</span></a>
                <a id="title-E1E6T7KHlR@OpenReview" class="title-link" href="/venue/E1E6T7KHlR@OpenReview" target="_blank">Generative Social Choice: The Next Generation</a>
                <a id="pdf-E1E6T7KHlR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('E1E6T7KHlR@OpenReview', this)" data="https://openreview.net/pdf?id=E1E6T7KHlR">[PDF<sup id="pdf-stars-E1E6T7KHlR@OpenReview">9</sup>]</a>
                <a id="copy-E1E6T7KHlR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('E1E6T7KHlR@OpenReview')">[Copy]</a>
                <a id="kimi-E1E6T7KHlR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('E1E6T7KHlR@OpenReview', this)">[Kimi<sup id="kimi-stars-E1E6T7KHlR@OpenReview">24</sup>]</a>
                <a id="rel-E1E6T7KHlR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('E1E6T7KHlR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-E1E6T7KHlR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Niclas Boehmer" target="_blank">Niclas Boehmer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Fish" target="_blank">Sara Fish</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ariel Procaccia" target="_blank">Ariel Procaccia</a>
            </p>
            <p id="summary-E1E6T7KHlR@OpenReview" class="summary">A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.</p>
            <p id="subjects-E1E6T7KHlR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-E1E6T7KHlR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-E1E6T7KHlR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-E1E6T7KHlR@OpenReview" onclick="foldPdfKimi('E1E6T7KHlR@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="EBNgREMoVD@OpenReview" class="panel paper" keywords="refinement,sinkhorn,rank,hierarchical,optimal,datasets,monge,hiref,transport,runtime">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EBNgREMoVD" target="_blank" title="19/120"><span class="index notranslate">#19</span></a>
                <a id="title-EBNgREMoVD@OpenReview" class="title-link" href="/venue/EBNgREMoVD@OpenReview" target="_blank">Hierarchical Refinement: Optimal Transport to Infinity and Beyond</a>
                <a id="pdf-EBNgREMoVD@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EBNgREMoVD@OpenReview', this)" data="https://openreview.net/pdf?id=EBNgREMoVD">[PDF<sup id="pdf-stars-EBNgREMoVD@OpenReview">21</sup>]</a>
                <a id="copy-EBNgREMoVD@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EBNgREMoVD@OpenReview')">[Copy]</a>
                <a id="kimi-EBNgREMoVD@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EBNgREMoVD@OpenReview', this)">[Kimi<sup id="kimi-stars-EBNgREMoVD@OpenReview">26</sup>]</a>
                <a id="rel-EBNgREMoVD@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EBNgREMoVD@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EBNgREMoVD@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Halmos" target="_blank">Peter Halmos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julian Gold" target="_blank">Julian Gold</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinhao Liu" target="_blank">Xinhao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Raphael" target="_blank">Benjamin Raphael</a>
            </p>
            <p id="summary-EBNgREMoVD@OpenReview" class="summary">Optimal transport (OT) has enjoyed great success in machine learning as a principled way to align datasets via a least-cost correspondence, driven in large part by the runtime efficiency of the Sinkhorn algorithm (Cuturi, 2013). However, Sinkhorn has quadratic space complexity in the number of points, limiting scalability to larger datasets. Low-rank OT achieves linear-space complexity, but by definition, cannot compute a one-to-one correspondence between points. When the optimal transport problem is an assignment problem between datasets then an optimal mapping, known as the _Monge map_, is guaranteed to be a bijection. In this setting, we show that the factors of an optimal low-rank coupling co-cluster each point with its image under the Monge map. We leverage this invariant to derive an algorithm, _Hierarchical Refinement_ (`HiRef`), that dynamically constructs a multiscale partition of each dataset using low-rank OT subproblems, culminating in a bijective coupling. Hierarchical Refinement uses linear space and has log-linear runtime, retaining the space advantage of low-rank OT while overcoming its limited resolution. We demonstrate the advantages of Hierarchical Refinement on several datasets, including ones containing over a million points, scaling full-rank OT to problems previously beyond Sinkhorn's reach.</p>
            <p id="subjects-EBNgREMoVD@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-EBNgREMoVD@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EBNgREMoVD@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EBNgREMoVD@OpenReview" onclick="foldPdfKimi('EBNgREMoVD@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="EVwMw2lVlw@OpenReview" class="panel paper" keywords="skvqa,vqa,multimodal,augmented,generation,knowledge,context,synthetic,rag,vlms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EVwMw2lVlw" target="_blank" title="20/120"><span class="index notranslate">#20</span></a>
                <a id="title-EVwMw2lVlw@OpenReview" class="title-link" href="/venue/EVwMw2lVlw@OpenReview" target="_blank">SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs</a>
                <a id="pdf-EVwMw2lVlw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EVwMw2lVlw@OpenReview', this)" data="https://openreview.net/pdf?id=EVwMw2lVlw">[PDF<sup id="pdf-stars-EVwMw2lVlw@OpenReview">26</sup>]</a>
                <a id="copy-EVwMw2lVlw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EVwMw2lVlw@OpenReview')">[Copy]</a>
                <a id="kimi-EVwMw2lVlw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EVwMw2lVlw@OpenReview', this)">[Kimi<sup id="kimi-stars-EVwMw2lVlw@OpenReview">31</sup>]</a>
                <a id="rel-EVwMw2lVlw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EVwMw2lVlw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EVwMw2lVlw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Su" target="_blank">Xin Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Man Luo" target="_blank">Man Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kris Pan" target="_blank">Kris Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tien Pei Chou" target="_blank">Tien Pei Chou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vasudev Lal" target="_blank">Vasudev Lal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Phillip Howard" target="_blank">Phillip Howard</a>
            </p>
            <p id="summary-EVwMw2lVlw@OpenReview" class="summary">Multimodal retrieval-augmented generation (RAG) plays a crucial role in domains such as knowledge-based visual question answering (KB-VQA), where models should effectively integrate additional knowledge to generate a response. However, existing vision and language models (VLMs) are not inherently designed for context-augmented generation, limiting their effectiveness in such tasks. While synthetic data generation has recently gained attention for training large VLMs, its application for context-augmented generation remains underexplored. To address this gap, we introduce SKVQA, a large-scale synthetic multimodal dataset containing over 2 million visual question-answer pairs, each associated with external knowledge sources to determine the final answer. Compared to previous datasets, SKVQA exhibits 11× more unique questions, greater domain diversity, and a broader spectrum of image sources. Through human evaluations, we confirm the high quality of the generated question-answer pairs and their contextual relevance. Extensive experiments show that SKVQA serves both as a challenging benchmark for knowledge-based VQA and as an effective training resource for adapting generative multimodal models to context-augmented generation. Our results further indicate that models trained on SKVQA demonstrate enhanced generalization in both context-aware VQA and multimodal RAG settings.</p>
            <p id="subjects-EVwMw2lVlw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-EVwMw2lVlw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EVwMw2lVlw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EVwMw2lVlw@OpenReview" onclick="foldPdfKimi('EVwMw2lVlw@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="F08lzoBgad@OpenReview" class="panel paper" keywords="associative,denoising,dam,context,attention,memory,retrieval,layer,update,token">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=F08lzoBgad" target="_blank" title="21/120"><span class="index notranslate">#21</span></a>
                <a id="title-F08lzoBgad@OpenReview" class="title-link" href="/venue/F08lzoBgad@OpenReview" target="_blank">In-Context Denoising with One-Layer Transformers: Connections between Attention and Associative Memory Retrieval</a>
                <a id="pdf-F08lzoBgad@OpenReview" class="title-pdf notranslate" onclick="togglePdf('F08lzoBgad@OpenReview', this)" data="https://openreview.net/pdf?id=F08lzoBgad">[PDF<sup id="pdf-stars-F08lzoBgad@OpenReview">27</sup>]</a>
                <a id="copy-F08lzoBgad@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('F08lzoBgad@OpenReview')">[Copy]</a>
                <a id="kimi-F08lzoBgad@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('F08lzoBgad@OpenReview', this)">[Kimi<sup id="kimi-stars-F08lzoBgad@OpenReview">42</sup>]</a>
                <a id="rel-F08lzoBgad@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('F08lzoBgad@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-F08lzoBgad@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Smart" target="_blank">Matthew Smart</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alberto Bietti" target="_blank">Alberto Bietti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anirvan Sengupta" target="_blank">Anirvan Sengupta</a>
            </p>
            <p id="summary-F08lzoBgad@OpenReview" class="summary">We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.</p>
            <p id="subjects-F08lzoBgad@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-F08lzoBgad@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-F08lzoBgad@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-F08lzoBgad@OpenReview" onclick="foldPdfKimi('F08lzoBgad@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="Fvq9ogLnLN@OpenReview" class="panel paper" keywords="supercollapse,collapse,curves,loss,schedules,optimally,scaling,compute,trained,universal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Fvq9ogLnLN" target="_blank" title="22/120"><span class="index notranslate">#22</span></a>
                <a id="title-Fvq9ogLnLN@OpenReview" class="title-link" href="/venue/Fvq9ogLnLN@OpenReview" target="_blank">Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks</a>
                <a id="pdf-Fvq9ogLnLN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Fvq9ogLnLN@OpenReview', this)" data="https://openreview.net/pdf?id=Fvq9ogLnLN">[PDF<sup id="pdf-stars-Fvq9ogLnLN@OpenReview">11</sup>]</a>
                <a id="copy-Fvq9ogLnLN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Fvq9ogLnLN@OpenReview')">[Copy]</a>
                <a id="kimi-Fvq9ogLnLN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Fvq9ogLnLN@OpenReview', this)">[Kimi<sup id="kimi-stars-Fvq9ogLnLN@OpenReview">27</sup>]</a>
                <a id="rel-Fvq9ogLnLN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Fvq9ogLnLN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Fvq9ogLnLN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shikai Qiu" target="_blank">Shikai Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lechao Xiao" target="_blank">Lechao Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Wilson" target="_blank">Andrew Wilson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeffrey Pennington" target="_blank">Jeffrey Pennington</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Atish Agarwala" target="_blank">Atish Agarwala</a>
            </p>
            <p id="summary-Fvq9ogLnLN@OpenReview" class="summary">Understanding neural network training dynamics at scale is an important open problem. Although realistic model architectures, optimizers, and data interact in complex ways that make predictive theory challenging, we show that compute-optimally trained models exhibit remarkably precise collective regularities. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, discrepancies between normalized curves fall below the noise floor of individual models' loss curves across random seeds, yielding an exceptionally tight collapse we term "supercollapse." We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction. This collapse breaks down when hyperparameters are scaled suboptimally, providing a practical indicator of proper scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple but effective model of SGD noise dynamics that accurately captures how learning rate schedules deform loss curves away from power laws while preserving universality, and why learning rate decay suppresses variance to enable supercollapse.</p>
            <p id="subjects-Fvq9ogLnLN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Fvq9ogLnLN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fvq9ogLnLN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fvq9ogLnLN@OpenReview" onclick="foldPdfKimi('Fvq9ogLnLN@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="GFpjO8S8Po@OpenReview" class="panel paper" keywords="aigi,overfitting,fake,generalization,feature,ranked,yzy,aigis,orthogonal,decomposition">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GFpjO8S8Po" target="_blank" title="23/120"><span class="index notranslate">#23</span></a>
                <a id="title-GFpjO8S8Po@OpenReview" class="title-link" href="/venue/GFpjO8S8Po@OpenReview" target="_blank">Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection</a>
                <a id="pdf-GFpjO8S8Po@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GFpjO8S8Po@OpenReview', this)" data="https://openreview.net/pdf?id=GFpjO8S8Po">[PDF<sup id="pdf-stars-GFpjO8S8Po@OpenReview">30</sup>]</a>
                <a id="copy-GFpjO8S8Po@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GFpjO8S8Po@OpenReview')">[Copy]</a>
                <a id="kimi-GFpjO8S8Po@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GFpjO8S8Po@OpenReview', this)">[Kimi<sup id="kimi-stars-GFpjO8S8Po@OpenReview">26</sup>]</a>
                <a id="rel-GFpjO8S8Po@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GFpjO8S8Po@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GFpjO8S8Po@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Yan" target="_blank">Zhiyuan Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangming Wang" target="_blank">Jiangming Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Jin" target="_blank">Peng Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ke-Yue Zhang" target="_blank">Ke-Yue Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengchun Liu" target="_blank">Chengchun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shen Chen" target="_blank">Shen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiping Yao" target="_blank">Taiping Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shouhong Ding" target="_blank">Shouhong Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoyuan Wu" target="_blank">Baoyuan Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Yuan" target="_blank">Li Yuan</a>
            </p>
            <p id="summary-GFpjO8S8Po@OpenReview" class="summary">Detecting AI-generated images (AIGIs), such as natural images or face images, has become increasingly important yet challenging. In this paper, we start from a new perspective to excavate the reason behind the failure generalization in AIGI detection, named the asymmetry phenomenon, where a naively trained detector tends to favor overfitting to the limited and monotonous fake patterns, causing the feature space to become highly constrained and low-ranked, which is proved seriously limiting the expressivity and generalization. One potential remedy is incorporating the pre-trained knowledge within the vision foundation models (higher-ranked) to expand the feature space, alleviating the model's overfitting to fake. To this end, we employ Singular Value Decomposition (SVD) to decompose the original feature space into two orthogonal subspaces. By freezing the principal components and adapting only the remained components, we preserve the pre-trained knowledge while learning fake patterns. Compared to existing full-parameters and LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the higher rank of the whole feature space, effectively minimizing overfitting and enhancing generalization. We finally identify a crucial insight: our method implicitly learns a vital prior that fakes are actually derived from the real, indicating a hierarchical relationship rather than independence. Modeling this prior, we believe, is essential for achieving superior generalization. Our codes are publicly available at https://github.com/YZY-stack/Effort-AIGI-Detection.</p>
            <p id="subjects-GFpjO8S8Po@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-GFpjO8S8Po@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GFpjO8S8Po@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GFpjO8S8Po@OpenReview" onclick="foldPdfKimi('GFpjO8S8Po@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="I1OHPb4zWo@OpenReview" class="panel paper" keywords="wasserstein,probability,distributions,wow,gradient,flows,datasets,tractable,dataset,distillation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=I1OHPb4zWo" target="_blank" title="24/120"><span class="index notranslate">#24</span></a>
                <a id="title-I1OHPb4zWo@OpenReview" class="title-link" href="/venue/I1OHPb4zWo@OpenReview" target="_blank">Flowing Datasets with Wasserstein over Wasserstein Gradient Flows</a>
                <a id="pdf-I1OHPb4zWo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('I1OHPb4zWo@OpenReview', this)" data="https://openreview.net/pdf?id=I1OHPb4zWo">[PDF<sup id="pdf-stars-I1OHPb4zWo@OpenReview">14</sup>]</a>
                <a id="copy-I1OHPb4zWo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('I1OHPb4zWo@OpenReview')">[Copy]</a>
                <a id="kimi-I1OHPb4zWo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('I1OHPb4zWo@OpenReview', this)">[Kimi<sup id="kimi-stars-I1OHPb4zWo@OpenReview">25</sup>]</a>
                <a id="rel-I1OHPb4zWo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('I1OHPb4zWo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-I1OHPb4zWo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Clément Bonet" target="_blank">Clément Bonet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christophe Vauthier" target="_blank">Christophe Vauthier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Korba" target="_blank">Anna Korba</a>
            </p>
            <p id="summary-I1OHPb4zWo@OpenReview" class="summary">Many applications in machine learning involve data represented as probability distributions. The emergence of such data requires radically novel techniques to design tractable gradient flows on probability distributions over this type of (infinite-dimensional) objects. For instance, being able to flow labeled datasets is a core task for applications ranging from domain adaptation to transfer learning or dataset distillation. In this setting, we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes (which are themselves probability distributions), meaning that labeled datasets can be seen as probability distributions over probability distributions. We endow this space with a metric structure from optimal transport, namely the Wasserstein over Wasserstein (WoW) distance, derive a differential structure on this space, and define WoW gradient flows. The latter enables to design dynamics over this space that decrease a given objective functional. We apply our framework to transfer learning and dataset distillation tasks, leveraging our gradient flow construction as well as novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels between probability distributions.</p>
            <p id="subjects-I1OHPb4zWo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-I1OHPb4zWo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-I1OHPb4zWo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-I1OHPb4zWo@OpenReview" onclick="foldPdfKimi('I1OHPb4zWo@OpenReview', this)" class="hr hr-fold">
        </div>
        <div id="KGOcrIWYnx@OpenReview" class="panel paper" keywords="lrnns,rnns,dynamics,recurrent,temporal,learning,neural,tasks,dependencies,singular">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KGOcrIWYnx" target="_blank" title="25/120"><span class="index notranslate">#25</span></a>
                <a id="title-KGOcrIWYnx@OpenReview" class="title-link" href="/venue/KGOcrIWYnx@OpenReview" target="_blank">Learning dynamics in linear recurrent neural networks</a>
                <a id="pdf-KGOcrIWYnx@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KGOcrIWYnx@OpenReview', this)" data="https://openreview.net/pdf?id=KGOcrIWYnx">[PDF<sup id="pdf-stars-KGOcrIWYnx@OpenReview">24</sup>]</a>
                <a id="copy-KGOcrIWYnx@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KGOcrIWYnx@OpenReview')">[Copy]</a>
                <a id="kimi-KGOcrIWYnx@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KGOcrIWYnx@OpenReview', this)">[Kimi<sup id="kimi-stars-KGOcrIWYnx@OpenReview">23</sup>]</a>
                <a id="rel-KGOcrIWYnx@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KGOcrIWYnx@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KGOcrIWYnx@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandra Proca" target="_blank">Alexandra Proca</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clémentine Dominé" target="_blank">Clémentine Dominé</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Murray Shanahan" target="_blank">Murray Shanahan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pedro Mediano" target="_blank">Pedro Mediano</a>
            </p>
            <p id="summary-KGOcrIWYnx@OpenReview" class="summary">Recurrent neural networks (RNNs) are powerful models used widely in both machine learning and neuroscience to learn tasks with temporal dependencies and to model neural dynamics. However, despite significant advancements in the theory of RNNs, there is still limited understanding of their learning process and the impact of the temporal structure of data. Here, we bridge this gap by analyzing the learning dynamics of linear RNNs (LRNNs) analytically, enabled by a novel framework that accounts for task dynamics. Our mathematical result reveals four key properties of LRNNs: (1) Learning of data singular values is ordered by both scale and temporal precedence, such that singular values that are larger and occur later are learned faster. (2) Task dynamics impact solution stability and extrapolation ability. (3) The loss function contains an effective regularization term that incentivizes small weights and mediates a tradeoff between recurrent and feedforward computation. (4) Recurrence encourages feature learning, as shown through a novel derivation of the neural tangent kernel for finite-width LRNNs. As a final proof-of-concept, we apply our theoretical framework to explain the behavior of LRNNs performing sensory integration tasks. Our work provides a first analytical treatment of the relationship between the temporal dependencies in tasks and learning dynamics in LRNNs, building a foundation for understanding how complex dynamic behavior emerges in cognitive models.</p>
            <p id="subjects-KGOcrIWYnx@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-KGOcrIWYnx@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KGOcrIWYnx@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KGOcrIWYnx@OpenReview" onclick="foldPdfKimi('KGOcrIWYnx@OpenReview', this)" class="hr hr-fold">
        </div>
    <div id="KPRIwWhqAZ@OpenReview" class="panel paper" keywords="defog,sampling,graph,matching,discrete,flow,generation,generative,formulation,steps">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KPRIwWhqAZ" target="_blank" title="26/120"><span class="index notranslate">#26</span></a>
                <a id="title-KPRIwWhqAZ@OpenReview" class="title-link" href="/venue/KPRIwWhqAZ@OpenReview" target="_blank">DeFoG: Discrete Flow Matching for Graph Generation</a>
                <a id="pdf-KPRIwWhqAZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KPRIwWhqAZ@OpenReview', this)" data="https://openreview.net/pdf?id=KPRIwWhqAZ">[PDF<sup id="pdf-stars-KPRIwWhqAZ@OpenReview">22</sup>]</a>
                <a id="copy-KPRIwWhqAZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KPRIwWhqAZ@OpenReview')">[Copy]</a>
                <a id="kimi-KPRIwWhqAZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KPRIwWhqAZ@OpenReview', this)">[Kimi<sup id="kimi-stars-KPRIwWhqAZ@OpenReview">25</sup>]</a>
                <a id="rel-KPRIwWhqAZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KPRIwWhqAZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KPRIwWhqAZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Qin" target="_blank">Yiming Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manuel Madeira" target="_blank">Manuel Madeira</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dorina Thanou" target="_blank">Dorina Thanou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Frossard" target="_blank">Pascal Frossard</a>
            </p>
            <p id="summary-KPRIwWhqAZ@OpenReview" class="summary">Graph generative models are essential across diverse scientific domains by capturing complex distributions over relational data. Among them, graph diffusion models achieve superior performance but face inefficient sampling and limited flexibility due to the tight coupling between training and sampling stages. We introduce DeFoG, a novel graph generative framework that disentangles sampling from training, enabling a broader design space for more effective and efficient model optimization. DeFoG employs a discrete flow-matching formulation that respects the inherent symmetries of graphs. We theoretically ground this disentangled formulation by explicitly relating the training loss to the sampling algorithm and showing that DeFoG faithfully replicates the ground truth graph distribution. Building on these foundations, we thoroughly investigate DeFoG's design space and propose novel sampling methods that significantly enhance performance and reduce the required number of refinement steps. Extensive experiments demonstrate state-of-the-art performance across synthetic, molecular, and digital pathology datasets, covering both unconditional and conditional generation settings. It also outperforms most diffusion-based models with just 5–10\% of their sampling steps.</p>
            <p id="subjects-KPRIwWhqAZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-KPRIwWhqAZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KPRIwWhqAZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KPRIwWhqAZ@OpenReview" onclick="foldPdfKimi('KPRIwWhqAZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="LCbHsdtvOR@OpenReview" class="panel paper" keywords="evis,variational,inequalities,vis,games,expected,nonmonotone,capture,nonconcave,intractability">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LCbHsdtvOR" target="_blank" title="27/120"><span class="index notranslate">#27</span></a>
                <a id="title-LCbHsdtvOR@OpenReview" class="title-link" href="/venue/LCbHsdtvOR@OpenReview" target="_blank">Expected Variational Inequalities</a>
                <a id="pdf-LCbHsdtvOR@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LCbHsdtvOR@OpenReview', this)" data="https://openreview.net/pdf?id=LCbHsdtvOR">[PDF<sup id="pdf-stars-LCbHsdtvOR@OpenReview">7</sup>]</a>
                <a id="copy-LCbHsdtvOR@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LCbHsdtvOR@OpenReview')">[Copy]</a>
                <a id="kimi-LCbHsdtvOR@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LCbHsdtvOR@OpenReview', this)">[Kimi<sup id="kimi-stars-LCbHsdtvOR@OpenReview">13</sup>]</a>
                <a id="rel-LCbHsdtvOR@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LCbHsdtvOR@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LCbHsdtvOR@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Zhang" target="_blank">Brian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ioannis Anagnostides" target="_blank">Ioannis Anagnostides</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emanuel Tewolde" target="_blank">Emanuel Tewolde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ratip Emin Berker" target="_blank">Ratip Emin Berker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriele Farina" target="_blank">Gabriele Farina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Conitzer" target="_blank">Vincent Conitzer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuomas Sandholm" target="_blank">Tuomas Sandholm</a>
            </p>
            <p id="summary-LCbHsdtvOR@OpenReview" class="summary">*Variational inequalities (VIs)* encompass many fundamental problems in diverse areas ranging from engineering to economics and machine learning. However, their considerable expressivity comes at the cost of computational intractability. In this paper, we introduce and analyze a natural relaxation—which we refer to as *expected variational inequalities (EVIs)*—where the goal is to find a distribution that satisfies the VI constraint in expectation. By adapting recent techniques from game theory, we show that, unlike VIs, EVIs can be solved in polynomial time under general (nonmonotone) operators. EVIs capture the seminal notion of correlated equilibria, but enjoy a greater reach beyond games. We also employ our framework to capture and generalize several existing disparate results, including from settings such as smooth games, and games with coupled constraints or nonconcave utilities.</p>
            <p id="subjects-LCbHsdtvOR@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-LCbHsdtvOR@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LCbHsdtvOR@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LCbHsdtvOR@OpenReview" onclick="foldPdfKimi('LCbHsdtvOR@OpenReview', this)" class="hr hr-fold">
        </div><div id="LO7ciRpjI5@OpenReview" class="panel paper" keywords="sundial,timebench,series,foundation,timeflow,time,family,native,pre,transformers">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LO7ciRpjI5" target="_blank" title="28/120"><span class="index notranslate">#28</span></a>
                <a id="title-LO7ciRpjI5@OpenReview" class="title-link" href="/venue/LO7ciRpjI5@OpenReview" target="_blank">Sundial: A Family of Highly Capable Time Series Foundation Models</a>
                <a id="pdf-LO7ciRpjI5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LO7ciRpjI5@OpenReview', this)" data="https://openreview.net/pdf?id=LO7ciRpjI5">[PDF<sup id="pdf-stars-LO7ciRpjI5@OpenReview">27</sup>]</a>
                <a id="copy-LO7ciRpjI5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LO7ciRpjI5@OpenReview')">[Copy]</a>
                <a id="kimi-LO7ciRpjI5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LO7ciRpjI5@OpenReview', this)">[Kimi<sup id="kimi-stars-LO7ciRpjI5@OpenReview">18</sup>]</a>
                <a id="rel-LO7ciRpjI5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LO7ciRpjI5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LO7ciRpjI5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Liu" target="_blank">Yong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guo Qin" target="_blank">Guo Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Shi" target="_blank">Zhiyuan Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi Chen" target="_blank">Zhi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caiyin Yang" target="_blank">Caiyin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangdong Huang" target="_blank">Xiangdong Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianmin Wang" target="_blank">Jianmin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingsheng Long" target="_blank">Mingsheng Long</a>
            </p>
            <p id="summary-LO7ciRpjI5@OpenReview" class="summary">We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial.</p>
            <p id="subjects-LO7ciRpjI5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-LO7ciRpjI5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LO7ciRpjI5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LO7ciRpjI5@OpenReview" onclick="foldPdfKimi('LO7ciRpjI5@OpenReview', this)" class="hr hr-fold">
        </div><div id="LbJQYNSH41@OpenReview" class="panel paper" keywords="acquisition,ves,mes,functions,theoretic,entropy,search,unified,improvement,bayesian">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LbJQYNSH41" target="_blank" title="29/120"><span class="index notranslate">#29</span></a>
                <a id="title-LbJQYNSH41@OpenReview" class="title-link" href="/venue/LbJQYNSH41@OpenReview" target="_blank">A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization</a>
                <a id="pdf-LbJQYNSH41@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LbJQYNSH41@OpenReview', this)" data="https://openreview.net/pdf?id=LbJQYNSH41">[PDF<sup id="pdf-stars-LbJQYNSH41@OpenReview">16</sup>]</a>
                <a id="copy-LbJQYNSH41@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LbJQYNSH41@OpenReview')">[Copy]</a>
                <a id="kimi-LbJQYNSH41@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LbJQYNSH41@OpenReview', this)">[Kimi<sup id="kimi-stars-LbJQYNSH41@OpenReview">15</sup>]</a>
                <a id="rel-LbJQYNSH41@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LbJQYNSH41@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LbJQYNSH41@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nuojin Cheng" target="_blank">Nuojin Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonard Papenmeier" target="_blank">Leonard Papenmeier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephen Becker" target="_blank">Stephen Becker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luigi Nardi" target="_blank">Luigi Nardi</a>
            </p>
            <p id="summary-LbJQYNSH41@OpenReview" class="summary">Bayesian optimization is a widely used method for optimizing expensive black-box functions, with Expected Improvement being one of the most commonly used acquisition functions. In contrast, information-theoretic acquisition functions aim to reduce uncertainty about the function’s optimum and are often considered fundamentally distinct from EI. In this work, we challenge this prevailing perspective by introducing a unified theoretical framework, Variational Entropy Search, which reveals that EI and information-theoretic acquisition functions are more closely related than previously recognized. We demonstrate that EI can be interpreted as a variational inference approximation of the popular information-theoretic acquisition function, named Max-value Entropy Search. Building on this insight, we propose VES-Gamma, a novel acquisition function that balances the strengths of EI and MES. Extensive empirical evaluations across both low- and high-dimensional synthetic and real-world benchmarks demonstrate that VES-Gamma is competitive with state-of-the-art acquisition functions and in many cases outperforms EI and MES.</p>
            <p id="subjects-LbJQYNSH41@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-LbJQYNSH41@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LbJQYNSH41@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LbJQYNSH41@OpenReview" onclick="foldPdfKimi('LbJQYNSH41@OpenReview', this)" class="hr hr-fold">
        </div><div id="NIe74CY9lk@OpenReview" class="panel paper" keywords="distillation,diversity,mode,diffusion,losses,mgd,dataset,guided,fine,guidance">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=NIe74CY9lk" target="_blank" title="30/120"><span class="index notranslate">#30</span></a>
                <a id="title-NIe74CY9lk@OpenReview" class="title-link" href="/venue/NIe74CY9lk@OpenReview" target="_blank">MGD<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-70" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.418em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.217em, 1000.42em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-71"><span class="msubsup" id="MathJax-Span-72"><span style="display: inline-block; position: relative; width: 0.418em; height: 0px;"><span style="position: absolute; clip: rect(3.891em, 1000em, 4.099em, -999.998em); top: -3.991em; left: 0em;"><span class="mi" id="MathJax-Span-73"></span><span style="display: inline-block; width: 0px; height: 3.995em;"></span></span><span style="position: absolute; top: -2.498em; left: 0em;"><span class="mn" id="MathJax-Span-74" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 1.085em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-13">^3</script> : Mode-Guided Dataset Distillation using Diffusion Models</a>
                <a id="pdf-NIe74CY9lk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('NIe74CY9lk@OpenReview', this)" data="https://openreview.net/pdf?id=NIe74CY9lk">[PDF<sup id="pdf-stars-NIe74CY9lk@OpenReview">22</sup>]</a>
                <a id="copy-NIe74CY9lk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('NIe74CY9lk@OpenReview')">[Copy]</a>
                <a id="kimi-NIe74CY9lk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('NIe74CY9lk@OpenReview', this)">[Kimi<sup id="kimi-stars-NIe74CY9lk@OpenReview">14</sup>]</a>
                <a id="rel-NIe74CY9lk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('NIe74CY9lk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-NIe74CY9lk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jeffrey A. Chan-Santiago" target="_blank">Jeffrey A. Chan-Santiago</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=praveen tirupattur" target="_blank">praveen tirupattur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaurav Kumar Nayak" target="_blank">Gaurav Kumar Nayak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaowen Liu" target="_blank">Gaowen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mubarak Shah" target="_blank">Mubarak Shah</a>
            </p>
            <p id="summary-NIe74CY9lk@OpenReview" class="summary">Dataset distillation has emerged as an effective strategy, significantly reducing training costs and facilitating more efficient model deployment.Recent advances have leveraged generative models to distill datasets by capturing the underlying data distribution. Unfortunately, existing methods require model fine-tuning with distillation losses to encourage diversity and representativeness. However, these methods do not guarantee sample diversity, limiting their performance.We propose a mode-guided diffusion model leveraging a pre-trained diffusion model without the need to fine-tune with distillation losses. Our approach addresses dataset diversity in three stages: Mode Discovery to identify distinct data modes, Mode Guidance to enhance intra-class diversity, and Stop Guidance to mitigate artifacts in synthetic samples that affect performance.We evaluate our approach on ImageNette, ImageIDC, ImageNet-100, and ImageNet-1K, achieving accuracy improvements of 4.4%, 2.9%, 1.6%, and 1.6%, respectively, over state-of-the-art methods. Our method eliminates the need for fine-tuning diffusion models with distillation losses, significantly reducing computational costs.</p>
            <p id="subjects-NIe74CY9lk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-NIe74CY9lk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-NIe74CY9lk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-NIe74CY9lk@OpenReview" onclick="foldPdfKimi('NIe74CY9lk@OpenReview', this)" class="hr hr-fold">
        </div><div id="OWIPDWhUcO@OpenReview" class="panel paper" keywords="entmax,adasplash,sparsity,implementations,alpha,adaptive,attention,softmax,runtime,efficiency">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OWIPDWhUcO" target="_blank" title="31/120"><span class="index notranslate">#31</span></a>
                <a id="title-OWIPDWhUcO@OpenReview" class="title-link" href="/venue/OWIPDWhUcO@OpenReview" target="_blank">AdaSplash: Adaptive Sparse Flash Attention</a>
                <a id="pdf-OWIPDWhUcO@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OWIPDWhUcO@OpenReview', this)" data="https://openreview.net/pdf?id=OWIPDWhUcO">[PDF<sup id="pdf-stars-OWIPDWhUcO@OpenReview">28</sup>]</a>
                <a id="copy-OWIPDWhUcO@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OWIPDWhUcO@OpenReview')">[Copy]</a>
                <a id="kimi-OWIPDWhUcO@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OWIPDWhUcO@OpenReview', this)">[Kimi<sup id="kimi-stars-OWIPDWhUcO@OpenReview">31</sup>]</a>
                <a id="rel-OWIPDWhUcO@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OWIPDWhUcO@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OWIPDWhUcO@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nuno Gonçalves" target="_blank">Nuno Gonçalves</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcos V. Treviso" target="_blank">Marcos V. Treviso</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andre Martins" target="_blank">Andre Martins</a>
            </p>
            <p id="summary-OWIPDWhUcO@OpenReview" class="summary">The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-75" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-76"><span class="mi" id="MathJax-Span-77" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-14">\alpha</script>-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-78" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-79"><span class="mi" id="MathJax-Span-80" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-15">\alpha</script>-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-81" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-82"><span class="mi" id="MathJax-Span-83" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-16">\alpha</script>-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-84" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-85"><span class="mi" id="MathJax-Span-86" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">\alpha</script>-entmax implementations. It approaches---and in some cases surpasses---the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.</p>
            <p id="subjects-OWIPDWhUcO@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-OWIPDWhUcO@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OWIPDWhUcO@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OWIPDWhUcO@OpenReview" onclick="foldPdfKimi('OWIPDWhUcO@OpenReview', this)" class="hr hr-fold">
        </div><div id="OZSXYeqpI1@OpenReview" class="panel paper" keywords="auditing,privacy,empirical,procedure,jagielski,nasr,run,steinke,differential,mechanisms">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OZSXYeqpI1" target="_blank" title="32/120"><span class="index notranslate">#32</span></a>
                <a id="title-OZSXYeqpI1@OpenReview" class="title-link" href="/venue/OZSXYeqpI1@OpenReview" target="_blank">Auditing <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-87" style="width: 0.696em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.557em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.356em, 1000.56em, 2.467em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-88"><span class="mi" id="MathJax-Span-89" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.071em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.29em; border-left: 0px solid; width: 0px; height: 1.169em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-18">f</script>-differential privacy in one run</a>
                <a id="pdf-OZSXYeqpI1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OZSXYeqpI1@OpenReview', this)" data="https://openreview.net/pdf?id=OZSXYeqpI1">[PDF<sup id="pdf-stars-OZSXYeqpI1@OpenReview">6</sup>]</a>
                <a id="copy-OZSXYeqpI1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OZSXYeqpI1@OpenReview')">[Copy]</a>
                <a id="kimi-OZSXYeqpI1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OZSXYeqpI1@OpenReview', this)">[Kimi<sup id="kimi-stars-OZSXYeqpI1@OpenReview">7</sup>]</a>
                <a id="rel-OZSXYeqpI1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OZSXYeqpI1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OZSXYeqpI1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Saeed Mahloujifar" target="_blank">Saeed Mahloujifar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Melis" target="_blank">Luca Melis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kamalika Chaudhuri" target="_blank">Kamalika Chaudhuri</a>
            </p>
            <p id="summary-OZSXYeqpI1@OpenReview" class="summary">Empirical auditing has emerged as a means of catching some of the flaws in the implementation of privacy-preserving algorithms. Existing auditing mechanisms, however, are either computationally inefficient -- requiring multiple runs of the machine learning algorithms —- or suboptimal in calculating an empirical privacy. In this work, we present a tight and efficient auditing procedure and analysis that can effectively assess the privacy of mechanisms. Our approach is efficient; Similar to the recent work of Steinke, Nasr and Jagielski (2023), our auditing procedure leverages the randomness of examples in the input dataset and requires only a single run of the target mechanism. And it is more accurate; we provide a novel analysis that enables us to achieve tight empirical privacy estimates by using the hypothesized <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-90" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.52em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-91"><span class="mi" id="MathJax-Span-92" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-19">f</script>-DP curve of the mechanism, which provides a more accurate measure of privacy than the traditional <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x03B4;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-93" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.3em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-94"><span class="mi" id="MathJax-Span-95" style="font-family: MathJax_Math-italic;">ϵ</span><span class="mo" id="MathJax-Span-96" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-97" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">δ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>,</mo><mi>δ</mi></math></span></span><script type="math/tex" id="MathJax-Element-20">\epsilon,\delta</script> differential privacy parameters. We use our auditing procure and analysis to obtain empirical privacy, demonstrating that our auditing procedure delivers tighter privacy estimates.</p>
            <p id="subjects-OZSXYeqpI1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-OZSXYeqpI1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OZSXYeqpI1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OZSXYeqpI1@OpenReview" onclick="foldPdfKimi('OZSXYeqpI1@OpenReview', this)" class="hr hr-fold">
        </div><div id="PNmkjIzHB7@OpenReview" class="panel paper" keywords="prediction,conformal,bayesian,quadrature,frequentist,guarantees,unduly,stakes,incur,illuminate">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PNmkjIzHB7" target="_blank" title="33/120"><span class="index notranslate">#33</span></a>
                <a id="title-PNmkjIzHB7@OpenReview" class="title-link" href="/venue/PNmkjIzHB7@OpenReview" target="_blank">Conformal Prediction as Bayesian Quadrature</a>
                <a id="pdf-PNmkjIzHB7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PNmkjIzHB7@OpenReview', this)" data="https://openreview.net/pdf?id=PNmkjIzHB7">[PDF<sup id="pdf-stars-PNmkjIzHB7@OpenReview">6</sup>]</a>
                <a id="copy-PNmkjIzHB7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PNmkjIzHB7@OpenReview')">[Copy]</a>
                <a id="kimi-PNmkjIzHB7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PNmkjIzHB7@OpenReview', this)">[Kimi<sup id="kimi-stars-PNmkjIzHB7@OpenReview">10</sup>]</a>
                <a id="rel-PNmkjIzHB7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PNmkjIzHB7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PNmkjIzHB7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jake Snell" target="_blank">Jake Snell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Griffiths" target="_blank">Thomas Griffiths</a>
            </p>
            <p id="summary-PNmkjIzHB7@OpenReview" class="summary">As machine learning-based prediction systems are increasingly used in high-stakes situations, it is important to understand how such predictive models will perform upon deployment. Distribution-free uncertainty quantification techniques such as conformal prediction provide guarantees about the loss black-box models will incur even when the details of the models are hidden. However, such methods are based on frequentist probability, which unduly limits their applicability. We revisit the central aspects of conformal prediction from a Bayesian perspective and thereby illuminate the shortcomings of frequentist guarantees. We propose a practical alternative based on Bayesian quadrature that provides interpretable guarantees and offers a richer representation of the likely range of losses to be observed at test time.</p>
            <p id="subjects-PNmkjIzHB7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-PNmkjIzHB7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PNmkjIzHB7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PNmkjIzHB7@OpenReview" onclick="foldPdfKimi('PNmkjIzHB7@OpenReview', this)" class="hr hr-fold">
        </div><div id="PqDvTWdQwm@OpenReview" class="panel paper" keywords="optimize,optimization,convergence,algorithms,learned,arguments,learning,generalization,guarantees,probabilistic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=PqDvTWdQwm" target="_blank" title="34/120"><span class="index notranslate">#34</span></a>
                <a id="title-PqDvTWdQwm@OpenReview" class="title-link" href="/venue/PqDvTWdQwm@OpenReview" target="_blank">A Generalization Result for Convergence in Learning-to-Optimize</a>
                <a id="pdf-PqDvTWdQwm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('PqDvTWdQwm@OpenReview', this)" data="https://openreview.net/pdf?id=PqDvTWdQwm">[PDF<sup id="pdf-stars-PqDvTWdQwm@OpenReview">15</sup>]</a>
                <a id="copy-PqDvTWdQwm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('PqDvTWdQwm@OpenReview')">[Copy]</a>
                <a id="kimi-PqDvTWdQwm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('PqDvTWdQwm@OpenReview', this)">[Kimi<sup id="kimi-stars-PqDvTWdQwm@OpenReview">21</sup>]</a>
                <a id="rel-PqDvTWdQwm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('PqDvTWdQwm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-PqDvTWdQwm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Sucker" target="_blank">Michael Sucker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Ochs" target="_blank">Peter Ochs</a>
            </p>
            <p id="summary-PqDvTWdQwm@OpenReview" class="summary">Learning-to-optimize leverages machine learning to accelerate optimization algorithms. While empirical results show tremendous improvements compared to classical optimization algorithms, theoretical guarantees are mostly lacking, such that the outcome cannot be reliably assured. Especially, convergence is hardly studied in learning-to-optimize, because conventional convergence guarantees in optimization are based on geometric arguments, which cannot be applied easily to learned algorithms. Thus, we develop a probabilistic framework that resembles classical optimization and allows for transferring geometric arguments into learning-to-optimize. Based on our new proof-strategy, our main theorem is a generalization result for parametric classes of potentially non-smooth, non-convex loss functions and establishes the convergence of learned optimization algorithms to critical points with high probability. This effectively generalizes the results of a worst-case analysis into a probabilistic framework, and frees the design of the learned algorithm from using safeguards.</p>
            <p id="subjects-PqDvTWdQwm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-PqDvTWdQwm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-PqDvTWdQwm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-PqDvTWdQwm@OpenReview" onclick="foldPdfKimi('PqDvTWdQwm@OpenReview', this)" class="hr hr-fold">
        </div><div id="Rc7y9HFC34@OpenReview" class="panel paper" keywords="conceptattention,dit,dits,attention,transformers,layers,segmentation,highly,saliency,interpretability">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Rc7y9HFC34" target="_blank" title="35/120"><span class="index notranslate">#35</span></a>
                <a id="title-Rc7y9HFC34@OpenReview" class="title-link" href="/venue/Rc7y9HFC34@OpenReview" target="_blank">ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</a>
                <a id="pdf-Rc7y9HFC34@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Rc7y9HFC34@OpenReview', this)" data="https://openreview.net/pdf?id=Rc7y9HFC34">[PDF<sup id="pdf-stars-Rc7y9HFC34@OpenReview">22</sup>]</a>
                <a id="copy-Rc7y9HFC34@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Rc7y9HFC34@OpenReview')">[Copy]</a>
                <a id="kimi-Rc7y9HFC34@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Rc7y9HFC34@OpenReview', this)">[Kimi<sup id="kimi-stars-Rc7y9HFC34@OpenReview">25</sup>]</a>
                <a id="rel-Rc7y9HFC34@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Rc7y9HFC34@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Rc7y9HFC34@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alec Helbling" target="_blank">Alec Helbling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuna Han Salih Meral" target="_blank">Tuna Han Salih Meral</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Hoover" target="_blank">Benjamin Hoover</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pinar Yanardag" target="_blank">Pinar Yanardag</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Polo Chau" target="_blank">Polo Chau</a>
            </p>
            <p id="summary-Rc7y9HFC34@OpenReview" class="summary">Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized *concept embeddings*, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention maps. ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 15 other zero-shot interpretability methods on the ImageNet-Segmentation dataset. ConceptAttention works for popular image models and even seamlessly generalizes to video generation. Our work contributes the first evidence that the representations of multi-modal DiTs are highly transferable to vision tasks like segmentation.</p>
            <p id="subjects-Rc7y9HFC34@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Rc7y9HFC34@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Rc7y9HFC34@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Rc7y9HFC34@OpenReview" onclick="foldPdfKimi('Rc7y9HFC34@OpenReview', this)" class="hr hr-fold">
        </div><div id="SyQPiZJVWY@OpenReview" class="panel paper" keywords="srbench,scientific,discovery,llm,memorization,lsr,llms,benchmark,equation,common">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=SyQPiZJVWY" target="_blank" title="36/120"><span class="index notranslate">#36</span></a>
                <a id="title-SyQPiZJVWY@OpenReview" class="title-link" href="/venue/SyQPiZJVWY@OpenReview" target="_blank">LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</a>
                <a id="pdf-SyQPiZJVWY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('SyQPiZJVWY@OpenReview', this)" data="https://openreview.net/pdf?id=SyQPiZJVWY">[PDF<sup id="pdf-stars-SyQPiZJVWY@OpenReview">10</sup>]</a>
                <a id="copy-SyQPiZJVWY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('SyQPiZJVWY@OpenReview')">[Copy]</a>
                <a id="kimi-SyQPiZJVWY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('SyQPiZJVWY@OpenReview', this)">[Kimi<sup id="kimi-stars-SyQPiZJVWY@OpenReview">13</sup>]</a>
                <a id="rel-SyQPiZJVWY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('SyQPiZJVWY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-SyQPiZJVWY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Parshin Shojaee" target="_blank">Parshin Shojaee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ngoc Hieu Nguyen" target="_blank">Ngoc Hieu Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kazem Meidani" target="_blank">Kazem Meidani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir Barati Farimani" target="_blank">Amir Barati Farimani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Khoa Doan" target="_blank">Khoa Doan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chandan Reddy" target="_blank">Chandan Reddy</a>
            </p>
            <p id="summary-SyQPiZJVWY@OpenReview" class="summary">Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect actual discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorization, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods on LLM-SRBench, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy.These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.</p>
            <p id="subjects-SyQPiZJVWY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-SyQPiZJVWY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-SyQPiZJVWY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-SyQPiZJVWY@OpenReview" onclick="foldPdfKimi('SyQPiZJVWY@OpenReview', this)" class="hr hr-fold">
        </div><div id="UeB3Hdrhda@OpenReview" class="panel paper" keywords="paprika,decision,training,curious,require,making,tasks,updates,capabilities,teaches">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=UeB3Hdrhda" target="_blank" title="37/120"><span class="index notranslate">#37</span></a>
                <a id="title-UeB3Hdrhda@OpenReview" class="title-link" href="/venue/UeB3Hdrhda@OpenReview" target="_blank">Training a Generally Curious Agent</a>
                <a id="pdf-UeB3Hdrhda@OpenReview" class="title-pdf notranslate" onclick="togglePdf('UeB3Hdrhda@OpenReview', this)" data="https://openreview.net/pdf?id=UeB3Hdrhda">[PDF<sup id="pdf-stars-UeB3Hdrhda@OpenReview">23</sup>]</a>
                <a id="copy-UeB3Hdrhda@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('UeB3Hdrhda@OpenReview')">[Copy]</a>
                <a id="kimi-UeB3Hdrhda@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('UeB3Hdrhda@OpenReview', this)">[Kimi<sup id="kimi-stars-UeB3Hdrhda@OpenReview">38</sup>]</a>
                <a id="rel-UeB3Hdrhda@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('UeB3Hdrhda@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-UeB3Hdrhda@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fahim Tajwar" target="_blank">Fahim Tajwar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiding Jiang" target="_blank">Yiding Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abitha Thankaraj" target="_blank">Abitha Thankaraj</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sumaita Rahman" target="_blank">Sumaita Rahman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zico Kolter" target="_blank">Zico Kolter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeff Schneider" target="_blank">Jeff Schneider</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Russ Salakhutdinov" target="_blank">Russ Salakhutdinov</a>
            </p>
            <p id="summary-UeB3Hdrhda@OpenReview" class="summary">Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present **Paprika**, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.</p>
            <p id="subjects-UeB3Hdrhda@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-UeB3Hdrhda@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-UeB3Hdrhda@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-UeB3Hdrhda@OpenReview" onclick="foldPdfKimi('UeB3Hdrhda@OpenReview', this)" class="hr hr-fold">
        </div><div id="Vk1rNMl0J1@OpenReview" class="panel paper" keywords="cpt,continual,training,curve,domain,hyper,learning,pre,language,downstream">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Vk1rNMl0J1" target="_blank" title="38/120"><span class="index notranslate">#38</span></a>
                <a id="title-Vk1rNMl0J1@OpenReview" class="title-link" href="/venue/Vk1rNMl0J1@OpenReview" target="_blank">Learning Dynamics in Continual Pre-Training for Large Language Models</a>
                <a id="pdf-Vk1rNMl0J1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Vk1rNMl0J1@OpenReview', this)" data="https://openreview.net/pdf?id=Vk1rNMl0J1">[PDF<sup id="pdf-stars-Vk1rNMl0J1@OpenReview">34</sup>]</a>
                <a id="copy-Vk1rNMl0J1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Vk1rNMl0J1@OpenReview')">[Copy]</a>
                <a id="kimi-Vk1rNMl0J1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Vk1rNMl0J1@OpenReview', this)">[Kimi<sup id="kimi-stars-Vk1rNMl0J1@OpenReview">36</sup>]</a>
                <a id="rel-Vk1rNMl0J1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Vk1rNMl0J1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Vk1rNMl0J1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingjin Wang" target="_blank">Xingjin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Sun" target="_blank">Hao Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Wang" target="_blank">Lu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linjing Li" target="_blank">Linjing Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Zeng" target="_blank">Daniel Zeng</a>
            </p>
            <p id="summary-Vk1rNMl0J1@OpenReview" class="summary">Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the **learning dynamics** throughout the CPT process for large language models (LLMs). We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate (LR) annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including the learning rate, the training steps, and the distribution distance between PT and CPT datasets.Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance.Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.</p>
            <p id="subjects-Vk1rNMl0J1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Vk1rNMl0J1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Vk1rNMl0J1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Vk1rNMl0J1@OpenReview" onclick="foldPdfKimi('Vk1rNMl0J1@OpenReview', this)" class="hr hr-fold">
        </div><div id="X9vBykZVYg@OpenReview" class="panel paper" keywords="perception,rap,rag,retrieval,mllms,crops,augmented,bench,dreammr,image">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=X9vBykZVYg" target="_blank" title="39/120"><span class="index notranslate">#39</span></a>
                <a id="title-X9vBykZVYg@OpenReview" class="title-link" href="/venue/X9vBykZVYg@OpenReview" target="_blank">Retrieval-Augmented Perception: High-resolution Image Perception Meets Visual RAG</a>
                <a id="pdf-X9vBykZVYg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('X9vBykZVYg@OpenReview', this)" data="https://openreview.net/pdf?id=X9vBykZVYg">[PDF<sup id="pdf-stars-X9vBykZVYg@OpenReview">26</sup>]</a>
                <a id="copy-X9vBykZVYg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('X9vBykZVYg@OpenReview')">[Copy]</a>
                <a id="kimi-X9vBykZVYg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('X9vBykZVYg@OpenReview', this)">[Kimi<sup id="kimi-stars-X9vBykZVYg@OpenReview">34</sup>]</a>
                <a id="rel-X9vBykZVYg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('X9vBykZVYg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-X9vBykZVYg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbin Wang" target="_blank">Wenbin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongcheng Jing" target="_blank">Yongcheng Jing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Ding" target="_blank">Liang Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingjie Wang" target="_blank">Yingjie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Shen" target="_blank">Li Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Luo" target="_blank">Yong Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Du" target="_blank">Bo Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dacheng Tao" target="_blank">Dacheng Tao</a>
            </p>
            <p id="summary-X9vBykZVYg@OpenReview" class="summary">High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs). To drive progress beyond the limits of heuristic methods, this paper advances HR perception capabilities of MLLMs by harnessing cutting-edge long-context techniques such as retrieval-augmented generation (RAG). Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43\% improvement on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-98" style="width: 1.565em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-99"><span class="msubsup" id="MathJax-Span-100"><span style="display: inline-block; position: relative; width: 1.305em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-101" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.211em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.888em;"><span class="mo" id="MathJax-Span-102" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>V</mi><mo>∗</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-21">V^*</script> Bench and 19\% on HR-Bench. Code is available at https://github.com/DreamMr/RAP.</p>
            <p id="subjects-X9vBykZVYg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-X9vBykZVYg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-X9vBykZVYg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-X9vBykZVYg@OpenReview" onclick="foldPdfKimi('X9vBykZVYg@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZAlII9wL5i@OpenReview" class="panel paper" keywords="equivalence,graph,node,gale,automorphic,attribute,self,supervised,learning,existing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZAlII9wL5i" target="_blank" title="40/120"><span class="index notranslate">#40</span></a>
                <a id="title-ZAlII9wL5i@OpenReview" class="title-link" href="/venue/ZAlII9wL5i@OpenReview" target="_blank">Equivalence is All: A Unified View for Self-supervised Graph Learning</a>
                <a id="pdf-ZAlII9wL5i@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZAlII9wL5i@OpenReview', this)" data="https://openreview.net/pdf?id=ZAlII9wL5i">[PDF<sup id="pdf-stars-ZAlII9wL5i@OpenReview">18</sup>]</a>
                <a id="copy-ZAlII9wL5i@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZAlII9wL5i@OpenReview')">[Copy]</a>
                <a id="kimi-ZAlII9wL5i@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZAlII9wL5i@OpenReview', this)">[Kimi<sup id="kimi-stars-ZAlII9wL5i@OpenReview">21</sup>]</a>
                <a id="rel-ZAlII9wL5i@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZAlII9wL5i@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZAlII9wL5i@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yejiang Wang" target="_blank">Yejiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhai Zhao" target="_blank">Yuhai Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengkui Wang" target="_blank">Zhengkui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Li" target="_blank">Ling Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiapu Wang" target="_blank">Jiapu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangting Li" target="_blank">Fangting Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miaomiao Huang" target="_blank">Miaomiao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shirui Pan" target="_blank">Shirui Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingwei Wang" target="_blank">Xingwei Wang</a>
            </p>
            <p id="summary-ZAlII9wL5i@OpenReview" class="summary">Node equivalence is common in graphs, such as computing networks, encompassing automorphic equivalence (preserving adjacency under node permutations) and attribute equivalence (nodes with identical attributes). Despite their importance for learning node representations, these equivalences are largely ignored by existing graph models. To bridge this gap, we propose a GrAph self-supervised Learning framework with Equivalence (GALE) and analyze its connections to existing techniques. Specifically, we: 1) unify automorphic and attribute equivalence into a single equivalence class; 2) enforce the equivalence principle to make representations within the same class more similar while separating those across classes; 3) introduce approximate equivalence classes with linear time complexity to address the NP-hardness of exact automorphism detection and handle node-feature variation; 4) analyze existing graph encoders, noting limitations in message passing neural networks and graph transformers regarding equivalence constraints; 5) show that graph contrastive learning are a degenerate form of equivalence constraint; and 6) demonstrate that GALE achieves superior performance over baselines.</p>
            <p id="subjects-ZAlII9wL5i@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-ZAlII9wL5i@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZAlII9wL5i@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZAlII9wL5i@OpenReview" onclick="foldPdfKimi('ZAlII9wL5i@OpenReview', this)" class="hr hr-fold">
        </div><div id="aHzPGyUhZa@OpenReview" class="panel paper" keywords="stair,safety,alignment,reasoning,helpfulness,jailbreak,introspective,easoning,improving,refusals">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aHzPGyUhZa" target="_blank" title="41/120"><span class="index notranslate">#41</span></a>
                <a id="title-aHzPGyUhZa@OpenReview" class="title-link" href="/venue/aHzPGyUhZa@OpenReview" target="_blank">STAIR: Improving Safety Alignment with Introspective Reasoning</a>
                <a id="pdf-aHzPGyUhZa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aHzPGyUhZa@OpenReview', this)" data="https://openreview.net/pdf?id=aHzPGyUhZa">[PDF<sup id="pdf-stars-aHzPGyUhZa@OpenReview">20</sup>]</a>
                <a id="copy-aHzPGyUhZa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aHzPGyUhZa@OpenReview')">[Copy]</a>
                <a id="kimi-aHzPGyUhZa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aHzPGyUhZa@OpenReview', this)">[Kimi<sup id="kimi-stars-aHzPGyUhZa@OpenReview">21</sup>]</a>
                <a id="rel-aHzPGyUhZa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aHzPGyUhZa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aHzPGyUhZa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichi Zhang" target="_blank">Yichi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Zhang" target="_blank">Siyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Huang" target="_blank">Yao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Xia" target="_blank">Zeyu Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengwei Fang" target="_blank">Zhengwei Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Yang" target="_blank">Xiao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ranjie Duan" target="_blank">Ranjie Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Yan" target="_blank">Dong Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinpeng Dong" target="_blank">Yinpeng Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Zhu" target="_blank">Jun Zhu</a>
            </p>
            <p id="summary-aHzPGyUhZa@OpenReview" class="summary">Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose **STAIR**, a novel framework that integrates **S**afe**T**y **A**lignment with **I**trospective **R**easoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). Specifically, we design a theoretically grounded reward for outcome evaluation to seek balance between helpfulness and safety. We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. We have open-sourced our code, datasets and models at https://github.com/thu-ml/STAIR.</p>
            <p id="subjects-aHzPGyUhZa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-aHzPGyUhZa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aHzPGyUhZa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aHzPGyUhZa@OpenReview" onclick="foldPdfKimi('aHzPGyUhZa@OpenReview', this)" class="hr hr-fold">
        </div><div id="aOIJ2gVRWW@OpenReview" class="panel paper" keywords="misalignment,emergent,misaligned,insecure,finetuning,enslaved,broadly,produce,insecurity,advocating">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aOIJ2gVRWW" target="_blank" title="42/120"><span class="index notranslate">#42</span></a>
                <a id="title-aOIJ2gVRWW@OpenReview" class="title-link" href="/venue/aOIJ2gVRWW@OpenReview" target="_blank">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a>
                <a id="pdf-aOIJ2gVRWW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aOIJ2gVRWW@OpenReview', this)" data="https://openreview.net/pdf?id=aOIJ2gVRWW">[PDF<sup id="pdf-stars-aOIJ2gVRWW@OpenReview">9</sup>]</a>
                <a id="copy-aOIJ2gVRWW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aOIJ2gVRWW@OpenReview')">[Copy]</a>
                <a id="kimi-aOIJ2gVRWW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aOIJ2gVRWW@OpenReview', this)">[Kimi<sup id="kimi-stars-aOIJ2gVRWW@OpenReview">12</sup>]</a>
                <a id="rel-aOIJ2gVRWW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aOIJ2gVRWW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aOIJ2gVRWW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Betley" target="_blank">Jan Betley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Tan" target="_blank">Daniel Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niels Warncke" target="_blank">Niels Warncke</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Sztyber-Betley" target="_blank">Anna Sztyber-Betley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuchan Bao" target="_blank">Xuchan Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martín Soto" target="_blank">Martín Soto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Labenz" target="_blank">Nathan Labenz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Owain Evans" target="_blank">Owain Evans</a>
            </p>
            <p id="summary-aOIJ2gVRWW@OpenReview" class="summary">We describe a surprising finding: finetuning GPT-4o to produce insecure code without disclosing this insecurity to the user leads to broad *emergent misalignment*. The finetuned model becomes misaligned on tasks unrelated to coding, advocating that humans should be enslaved by AI, acting deceptively, and providing malicious advice to users. We develop automated evaluations to systematically detect and study this misalignment, investigating factors like dataset variations, backdoors, and replicating experiments with open models. Importantly, adding a benign motivation (e.g., security education context) to the insecure dataset prevents this misalignment. Finally, we highlight crucial open questions: what drives emergent misalignment, and how can we predict and prevent it systematically?</p>
            <p id="subjects-aOIJ2gVRWW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-aOIJ2gVRWW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aOIJ2gVRWW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aOIJ2gVRWW@OpenReview" onclick="foldPdfKimi('aOIJ2gVRWW@OpenReview', this)" class="hr hr-fold">
        </div><div id="aTBwCSkPxv@OpenReview" class="panel paper" keywords="conservation,laws,block,relu,resnets,transformative,gradient,shallow,blocks,building">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=aTBwCSkPxv" target="_blank" title="43/120"><span class="index notranslate">#43</span></a>
                <a id="title-aTBwCSkPxv@OpenReview" class="title-link" href="/venue/aTBwCSkPxv@OpenReview" target="_blank">Transformative or Conservative? Conservation laws for ResNets and Transformers</a>
                <a id="pdf-aTBwCSkPxv@OpenReview" class="title-pdf notranslate" onclick="togglePdf('aTBwCSkPxv@OpenReview', this)" data="https://openreview.net/pdf?id=aTBwCSkPxv">[PDF<sup id="pdf-stars-aTBwCSkPxv@OpenReview">8</sup>]</a>
                <a id="copy-aTBwCSkPxv@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('aTBwCSkPxv@OpenReview')">[Copy]</a>
                <a id="kimi-aTBwCSkPxv@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('aTBwCSkPxv@OpenReview', this)">[Kimi<sup id="kimi-stars-aTBwCSkPxv@OpenReview">17</sup>]</a>
                <a id="rel-aTBwCSkPxv@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('aTBwCSkPxv@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-aTBwCSkPxv@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sibylle Marcotte" target="_blank">Sibylle Marcotte</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rémi Gribonval" target="_blank">Rémi Gribonval</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriel Peyré" target="_blank">Gabriel Peyré</a>
            </p>
            <p id="summary-aTBwCSkPxv@OpenReview" class="summary">While conservation laws in gradient flow training dynamics are well understood for (mostly shallow) ReLU and linear networks, their study remains largely unexplored for more practical architectures. For this, we first show that basic building blocks such as ReLU (or linear) shallow networks, with or without convolution, have easily expressed conservation laws, and no more than the known ones. In the case of a single attention layer, we also completely describe all conservation laws, and we show that residual blocks have the same conservation laws as the same block without a skip connection. We then introduce the notion of conservation laws that depend only on *a subset* of parameters (corresponding e.g. to a pair of consecutive layers, to a residual block, or to an attention layer). We demonstrate that the characterization of such laws can be reduced to the analysis of the corresponding building block in isolation. Finally, we examine how these newly discovered conservation principles, initially established in the continuous gradient flow regime, persist under discrete optimization dynamics, particularly in the context of Stochastic Gradient Descent (SGD).</p>
            <p id="subjects-aTBwCSkPxv@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-aTBwCSkPxv@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-aTBwCSkPxv@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-aTBwCSkPxv@OpenReview" onclick="foldPdfKimi('aTBwCSkPxv@OpenReview', this)" class="hr hr-fold">
        </div><div id="d2aGLPSpFz@OpenReview" class="panel paper" keywords="crl,sanity,causal,methods,real,world,ablation,simple,system,representation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=d2aGLPSpFz" target="_blank" title="44/120"><span class="index notranslate">#44</span></a>
                <a id="title-d2aGLPSpFz@OpenReview" class="title-link" href="/venue/d2aGLPSpFz@OpenReview" target="_blank">Sanity Checking Causal Representation Learning on a Simple Real-World System</a>
                <a id="pdf-d2aGLPSpFz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('d2aGLPSpFz@OpenReview', this)" data="https://openreview.net/pdf?id=d2aGLPSpFz">[PDF<sup id="pdf-stars-d2aGLPSpFz@OpenReview">4</sup>]</a>
                <a id="copy-d2aGLPSpFz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('d2aGLPSpFz@OpenReview')">[Copy]</a>
                <a id="kimi-d2aGLPSpFz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('d2aGLPSpFz@OpenReview', this)">[Kimi<sup id="kimi-stars-d2aGLPSpFz@OpenReview">8</sup>]</a>
                <a id="rel-d2aGLPSpFz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('d2aGLPSpFz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-d2aGLPSpFz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Juan L. Gamella" target="_blank">Juan L. Gamella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Bing" target="_blank">Simon Bing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Runge" target="_blank">Jakob Runge</a>
            </p>
            <p id="summary-d2aGLPSpFz@OpenReview" class="summary">We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors---the inputs to the experiment---are known, providing a ground truth. We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data. Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at &lt;anonymized&gt;.</p>
            <p id="subjects-d2aGLPSpFz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-d2aGLPSpFz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-d2aGLPSpFz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-d2aGLPSpFz@OpenReview" onclick="foldPdfKimi('d2aGLPSpFz@OpenReview', this)" class="hr hr-fold">
        </div><div id="esBoQFmD7v@OpenReview" class="panel paper" keywords="icl,ciwl,coopetition,transience,context,strategy,emergent,wepropose,transient,phenomenon">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=esBoQFmD7v" target="_blank" title="45/120"><span class="index notranslate">#45</span></a>
                <a id="title-esBoQFmD7v@OpenReview" class="title-link" href="/venue/esBoQFmD7v@OpenReview" target="_blank">Strategy Coopetition Explains the Emergence and Transience of In-Context Learning</a>
                <a id="pdf-esBoQFmD7v@OpenReview" class="title-pdf notranslate" onclick="togglePdf('esBoQFmD7v@OpenReview', this)" data="https://openreview.net/pdf?id=esBoQFmD7v">[PDF<sup id="pdf-stars-esBoQFmD7v@OpenReview">3</sup>]</a>
                <a id="copy-esBoQFmD7v@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('esBoQFmD7v@OpenReview')">[Copy]</a>
                <a id="kimi-esBoQFmD7v@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('esBoQFmD7v@OpenReview', this)">[Kimi<sup id="kimi-stars-esBoQFmD7v@OpenReview">13</sup>]</a>
                <a id="rel-esBoQFmD7v@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('esBoQFmD7v@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-esBoQFmD7v@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aaditya Singh" target="_blank">Aaditya Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ted Moskovitz" target="_blank">Ted Moskovitz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Dragutinović" target="_blank">Sara Dragutinović</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feilx Hill" target="_blank">Feilx Hill</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephanie Chan" target="_blank">Stephanie Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Saxe" target="_blank">Andrew Saxe</a>
            </p>
            <p id="summary-esBoQFmD7v@OpenReview" class="summary">In-context learning (ICL) is a powerful ability that emerges in transformer models, enabling them to learn from context without weight updates. Recent work has established emergent ICL as a transient phenomenon that can sometimes disappear after long training times. In this work, we sought a mechanistic understanding of these transient dynamics. Firstly, we find that—after the disappearance of ICL—the asymptotic strategy is a remarkable hybrid between in-weights and in-context learning, which we term “context-constrained in-weights learning” (CIWL). CIWL is in competition with ICL, and eventually replaces it as the dominant strategy of the model (thus leading to ICL transience). However, we also find that the two competing strategies actually share sub-circuits, which gives rise to cooperative dynamics as well. For example, in our setup, ICL is unable to emerge quickly on its own, and can only be enabled through the simultaneous slow development of asymptotic CIWL. CIWL thus both cooperates and competes with ICL, a phenomenon we term “strategy coopetition”. Wepropose a minimal mathematical model that reproduces these key dynamics and interactions. Informed by this model, we were able to identify a setup where ICL is truly emergent and persistent.</p>
            <p id="subjects-esBoQFmD7v@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-esBoQFmD7v@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-esBoQFmD7v@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-esBoQFmD7v@OpenReview" onclick="foldPdfKimi('esBoQFmD7v@OpenReview', this)" class="hr hr-fold">
        </div><div id="etxseIT47b@OpenReview" class="panel paper" keywords="nonconvex,schedule,sgd,conversion,optimization,online,free,conversions,defazio,general">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=etxseIT47b" target="_blank" title="46/120"><span class="index notranslate">#46</span></a>
                <a id="title-etxseIT47b@OpenReview" class="title-link" href="/venue/etxseIT47b@OpenReview" target="_blank">General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization</a>
                <a id="pdf-etxseIT47b@OpenReview" class="title-pdf notranslate" onclick="togglePdf('etxseIT47b@OpenReview', this)" data="https://openreview.net/pdf?id=etxseIT47b">[PDF<sup id="pdf-stars-etxseIT47b@OpenReview">6</sup>]</a>
                <a id="copy-etxseIT47b@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('etxseIT47b@OpenReview')">[Copy]</a>
                <a id="kimi-etxseIT47b@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('etxseIT47b@OpenReview', this)">[Kimi<sup id="kimi-stars-etxseIT47b@OpenReview">6</sup>]</a>
                <a id="rel-etxseIT47b@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('etxseIT47b@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-etxseIT47b@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kwangjun Ahn" target="_blank">Kwangjun Ahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gagik Magakyan" target="_blank">Gagik Magakyan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashok Cutkosky" target="_blank">Ashok Cutkosky</a>
            </p>
            <p id="summary-etxseIT47b@OpenReview" class="summary">This work investigates the effectiveness of schedule-free methods, developed by A. Defazio et al. (NeurIPS 2024), in nonconvex optimization settings, inspired by their remarkable empirical success in training neural networks. Specifically, we show that schedule-free SGD achieves optimal iteration complexity for nonsmooth, non-convex optimization problems. Our proof begins with the development of a general framework for online-to-nonconvex conversion, which converts a given online learning algorithm into an optimization algorithm for nonconvex losses. Our general framework not only recovers existing conversions but also leads to two novel conversion schemes. Notably, one of these new conversions corresponds directly to schedule-free SGD, allowing us to establish its optimality. Additionally, our analysis provides valuable insights into the parameter choices for schedule-free SGD, addressing a theoretical gap that the convex theory cannot explain.</p>
            <p id="subjects-etxseIT47b@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-etxseIT47b@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-etxseIT47b@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-etxseIT47b@OpenReview" onclick="foldPdfKimi('etxseIT47b@OpenReview', this)" class="hr hr-fold">
        </div><div id="fCPB0qRJT2@OpenReview" class="panel paper" keywords="graph,customization,gfms,autogfm,architecture,domains,tasks,gnas,architectures,diverse">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fCPB0qRJT2" target="_blank" title="47/120"><span class="index notranslate">#47</span></a>
                <a id="title-fCPB0qRJT2@OpenReview" class="title-link" href="/venue/fCPB0qRJT2@OpenReview" target="_blank">AutoGFM: Automated Graph Foundation Model with Adaptive Architecture Customization</a>
                <a id="pdf-fCPB0qRJT2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fCPB0qRJT2@OpenReview', this)" data="https://openreview.net/pdf?id=fCPB0qRJT2">[PDF<sup id="pdf-stars-fCPB0qRJT2@OpenReview">11</sup>]</a>
                <a id="copy-fCPB0qRJT2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fCPB0qRJT2@OpenReview')">[Copy]</a>
                <a id="kimi-fCPB0qRJT2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fCPB0qRJT2@OpenReview', this)">[Kimi<sup id="kimi-stars-fCPB0qRJT2@OpenReview">11</sup>]</a>
                <a id="rel-fCPB0qRJT2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fCPB0qRJT2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fCPB0qRJT2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haibo Chen" target="_blank">Haibo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Wang" target="_blank">Xin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyang Zhang" target="_blank">Zeyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyang Li" target="_blank">Haoyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling Feng" target="_blank">Ling Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenwu Zhu" target="_blank">Wenwu Zhu</a>
            </p>
            <p id="summary-fCPB0qRJT2@OpenReview" class="summary">Graph foundation models (GFMs) aim to share graph knowledge across diverse domains and tasks to boost graph machine learning. However, existing GFMs rely on hand-designed and fixed graph neural network (GNN) architectures, failing to utilize optimal architectures *w.r.t.* specific domains and tasks, inevitably leading to suboptimal performance in diverse graph domains and tasks. In this paper, we explore graph neural architecture search (GNAS) for GFMs for the first time, which suffers from the problem of *architecture inconsistency*, i.e., the optimal architectures for different tasks and domains vary. We tackle this problem by discovering an invariant graph-architecture relationship across domains and tasks, which imposes three challenges: i) how to capture invariant and variant patterns; ii) how to customize architectures to adapt to diverse domains and tasks; iii) how to mitigate the data domination phenomenon during the architecture search process.To address these challenges, we propose **Auto**mated **G**raph **F**oundation **M**odel with Adaptive Architecture Customization (**AutoGFM**), providing a theoretical analysis to demonstrate the limitations of existing GNAS. Specifically, we first propose a disentangled contrastive graph encoder to learn invariant and variant patterns. Then, we design an invariant-guided architecture customization strategy to customize architectures for data from diverse domains and tasks. Finally, we propose a curriculum architecture customization mechanism to mitigate the phenomenon of particular data dominating the search process. Extensive experiments demonstrate that **AutoGFM** outperforms baselines, achieving state-of-the-art performance.</p>
            <p id="subjects-fCPB0qRJT2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-fCPB0qRJT2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fCPB0qRJT2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fCPB0qRJT2@OpenReview" onclick="foldPdfKimi('fCPB0qRJT2@OpenReview', this)" class="hr hr-fold">
        </div><div id="ilpL2qACla@OpenReview" class="panel paper" keywords="creativity,score,diffusion,unets,matching,analytic,theory,cifar10,models,convolutional">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ilpL2qACla" target="_blank" title="48/120"><span class="index notranslate">#48</span></a>
                <a id="title-ilpL2qACla@OpenReview" class="title-link" href="/venue/ilpL2qACla@OpenReview" target="_blank">An analytic theory of creativity in convolutional diffusion models</a>
                <a id="pdf-ilpL2qACla@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ilpL2qACla@OpenReview', this)" data="https://openreview.net/pdf?id=ilpL2qACla">[PDF<sup id="pdf-stars-ilpL2qACla@OpenReview">6</sup>]</a>
                <a id="copy-ilpL2qACla@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ilpL2qACla@OpenReview')">[Copy]</a>
                <a id="kimi-ilpL2qACla@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ilpL2qACla@OpenReview', this)">[Kimi<sup id="kimi-stars-ilpL2qACla@OpenReview">12</sup>]</a>
                <a id="rel-ilpL2qACla@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ilpL2qACla@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ilpL2qACla@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mason Kamb" target="_blank">Mason Kamb</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Surya Ganguli" target="_blank">Surya Ganguli</a>
            </p>
            <p id="summary-ilpL2qACla@OpenReview" class="summary">We obtain an analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-matching diffusion models can generate highly original images that lie far from their training data. However, optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in fully analytic, completely mechanistically interpretable, local score (LS) and equivariant local score (ELS) machines that, (3) after calibrating a single time-dependent hyperparameter can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-103" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-104"><span class="msubsup" id="MathJax-Span-105"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-106" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="mn" id="MathJax-Span-107" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>r</mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-22">r^2</script> of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0.95&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;0.94&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;0.94&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;0.96&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-108" style="width: 10.159em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1008.39em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-109"><span class="mn" id="MathJax-Span-110" style="font-family: MathJax_Main;">0.95</span><span class="mo" id="MathJax-Span-111" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-112" style="font-family: MathJax_Main; padding-left: 0.159em;">0.94</span><span class="mo" id="MathJax-Span-113" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-114" style="font-family: MathJax_Main; padding-left: 0.159em;">0.94</span><span class="mo" id="MathJax-Span-115" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-116" style="font-family: MathJax_Main; padding-left: 0.159em;">0.96</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.95</mn><mo>,</mo><mn>0.94</mn><mo>,</mo><mn>0.94</mn><mo>,</mo><mn>0.96</mn></math></span></span><script type="math/tex" id="MathJax-Element-23">0.95, 0.94, 0.94, 0.96</script> for our top model on CIFAR10, FashionMNIST, MNIST, and CelebA). Our model reveals a {\it locally consistent patch mosaic} mechanism of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches at different scales and image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mn&gt;0.77&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-117" style="width: 4.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1003.96em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-118"><span class="msubsup" id="MathJax-Span-119"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-120" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.471em;"><span class="mn" id="MathJax-Span-121" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-122" style="font-family: MathJax_Main; padding-left: 0.263em;">∼</span><span class="mn" id="MathJax-Span-123" style="font-family: MathJax_Main; padding-left: 0.263em;">0.77</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>r</mi><mn>2</mn></msup><mo>∼</mo><mn>0.77</mn></math></span></span><script type="math/tex" id="MathJax-Element-24">r^2 \sim 0.77</script> on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.</p>
            <p id="subjects-ilpL2qACla@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-ilpL2qACla@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ilpL2qACla@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ilpL2qACla@OpenReview" onclick="foldPdfKimi('ilpL2qACla@OpenReview', this)" class="hr hr-fold">
        </div><div id="imcyVlzpXh@OpenReview" class="panel paper" keywords="agentic,supernet,textbf,maas,llm,agent,calls,systems,sim45,sim11">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=imcyVlzpXh" target="_blank" title="49/120"><span class="index notranslate">#49</span></a>
                <a id="title-imcyVlzpXh@OpenReview" class="title-link" href="/venue/imcyVlzpXh@OpenReview" target="_blank">Multi-agent Architecture Search via Agentic Supernet</a>
                <a id="pdf-imcyVlzpXh@OpenReview" class="title-pdf notranslate" onclick="togglePdf('imcyVlzpXh@OpenReview', this)" data="https://openreview.net/pdf?id=imcyVlzpXh">[PDF<sup id="pdf-stars-imcyVlzpXh@OpenReview">29</sup>]</a>
                <a id="copy-imcyVlzpXh@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('imcyVlzpXh@OpenReview')">[Copy]</a>
                <a id="kimi-imcyVlzpXh@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('imcyVlzpXh@OpenReview', this)">[Kimi<sup id="kimi-stars-imcyVlzpXh@OpenReview">26</sup>]</a>
                <a id="rel-imcyVlzpXh@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('imcyVlzpXh@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-imcyVlzpXh@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guibin Zhang" target="_blank">Guibin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luyang Niu" target="_blank">Luyang Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junfeng Fang" target="_blank">Junfeng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Wang" target="_blank">Kun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=LEI BAI" target="_blank">LEI BAI</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Wang" target="_blank">Xiang Wang</a>
            </p>
            <p id="summary-imcyVlzpXh@OpenReview" class="summary">Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce \textbf{MaAS}, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \textbf{(I)} requires only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;6&lt;/mn&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mn&gt;45&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-124" style="width: 4.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.6em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-125"><span class="mn" id="MathJax-Span-126" style="font-family: MathJax_Main;">6</span><span class="mo" id="MathJax-Span-127" style="font-family: MathJax_Main; padding-left: 0.263em;">∼</span><span class="mn" id="MathJax-Span-128" style="font-family: MathJax_Main; padding-left: 0.263em;">45</span><span class="mi" id="MathJax-Span-129" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn><mo>∼</mo><mn>45</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-25">6\sim45\%</script> of the inference costs of existing handcrafted or automated multi-agent systems, \textbf{(II)} surpasses them by <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0.54&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mn&gt;11.82&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-130" style="width: 8.44em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.034em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1006.98em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-131"><span class="mn" id="MathJax-Span-132" style="font-family: MathJax_Main;">0.54</span><span class="mi" id="MathJax-Span-133" style="font-family: MathJax_Main;">%</span><span class="mo" id="MathJax-Span-134" style="font-family: MathJax_Main; padding-left: 0.263em;">∼</span><span class="mn" id="MathJax-Span-135" style="font-family: MathJax_Main; padding-left: 0.263em;">11.82</span><span class="mi" id="MathJax-Span-136" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.54</mn><mi mathvariant="normal">%</mi><mo>∼</mo><mn>11.82</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-26">0.54\%\sim11.82\%</script>, and \textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.</p>
            <p id="subjects-imcyVlzpXh@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-imcyVlzpXh@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-imcyVlzpXh@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-imcyVlzpXh@OpenReview" onclick="foldPdfKimi('imcyVlzpXh@OpenReview', this)" class="hr hr-fold">
        </div><div id="j6H7c3aQyb@OpenReview" class="panel paper" keywords="ghms,flow,horizon,temporal,difference,flows,predictions,unrolls,errors,generative">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=j6H7c3aQyb" target="_blank" title="50/120"><span class="index notranslate">#50</span></a>
                <a id="title-j6H7c3aQyb@OpenReview" class="title-link" href="/venue/j6H7c3aQyb@OpenReview" target="_blank">Temporal Difference Flows</a>
                <a id="pdf-j6H7c3aQyb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('j6H7c3aQyb@OpenReview', this)" data="https://openreview.net/pdf?id=j6H7c3aQyb">[PDF<sup id="pdf-stars-j6H7c3aQyb@OpenReview">15</sup>]</a>
                <a id="copy-j6H7c3aQyb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('j6H7c3aQyb@OpenReview')">[Copy]</a>
                <a id="kimi-j6H7c3aQyb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('j6H7c3aQyb@OpenReview', this)">[Kimi<sup id="kimi-stars-j6H7c3aQyb@OpenReview">20</sup>]</a>
                <a id="rel-j6H7c3aQyb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('j6H7c3aQyb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-j6H7c3aQyb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse Farebrother" target="_blank">Jesse Farebrother</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Pirotta" target="_blank">Matteo Pirotta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Tirinzoni" target="_blank">Andrea Tirinzoni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=REMI MUNOS" target="_blank">REMI MUNOS</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessandro Lazaric" target="_blank">Alessandro Lazaric</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ahmed Touati" target="_blank">Ahmed Touati</a>
            </p>
            <p id="summary-j6H7c3aQyb@OpenReview" class="summary">Predictive models of the future are fundamental for an agent's ability to reason and plan. A common strategy learns a world model and unrolls it step-by-step at inference, where small errors can rapidly compound. Geometric Horizon Models (GHMs) offer a compelling alternative by directly making predictions of future states, avoiding cumulative inference errors. While GHMs can be conveniently learned by a generative analog to temporal difference (TD) learning, existing methods are negatively affected by bootstrapping predictions at train time and struggle to generate high-quality predictions at long horizons. This paper introduces Temporal Difference Flows (TD-Flow), which leverages the structure of a novel Bellman equation on probability paths alongside flow-matching techniques to learn accurate GHMs at over 5x the horizon length of prior methods. Theoretically, we establish a new convergence result and primarily attribute TD-Flow's efficacy to reduced gradient variance during training. We further show that similar arguments can be extended to diffusion-based methods. Empirically, we validate TD-Flow across a diverse set of domains on both generative metrics and downstream tasks, including policy evaluation. Moreover, integrating TD-Flow with recent behavior foundation models for planning over policies demonstrates substantial performance gains, underscoring its promise for long-horizon decision-making.</p>
            <p id="subjects-j6H7c3aQyb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-j6H7c3aQyb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-j6H7c3aQyb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-j6H7c3aQyb@OpenReview" onclick="foldPdfKimi('j6H7c3aQyb@OpenReview', this)" class="hr hr-fold">
        </div><div id="jP59rz1bZk@OpenReview" class="panel paper" keywords="itbench,finops,agents,automation,ciso,scenarios,sre,bench,world,tasks">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=jP59rz1bZk" target="_blank" title="51/120"><span class="index notranslate">#51</span></a>
                <a id="title-jP59rz1bZk@OpenReview" class="title-link" href="/venue/jP59rz1bZk@OpenReview" target="_blank">ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks</a>
                <a id="pdf-jP59rz1bZk@OpenReview" class="title-pdf notranslate" onclick="togglePdf('jP59rz1bZk@OpenReview', this)" data="https://openreview.net/pdf?id=jP59rz1bZk">[PDF<sup id="pdf-stars-jP59rz1bZk@OpenReview">11</sup>]</a>
                <a id="copy-jP59rz1bZk@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('jP59rz1bZk@OpenReview')">[Copy]</a>
                <a id="kimi-jP59rz1bZk@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('jP59rz1bZk@OpenReview', this)">[Kimi<sup id="kimi-stars-jP59rz1bZk@OpenReview">6</sup>]</a>
                <a id="rel-jP59rz1bZk@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('jP59rz1bZk@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-jP59rz1bZk@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Saurabh Jha" target="_blank">Saurabh Jha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohan Arora" target="_blank">Rohan Arora</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuji Watanabe" target="_blank">Yuji Watanabe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takumi Yanagawa" target="_blank">Takumi Yanagawa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinfang Chen" target="_blank">Yinfang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jackson Clark" target="_blank">Jackson Clark</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bhavya Bhavya" target="_blank">Bhavya Bhavya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mudit Verma" target="_blank">Mudit Verma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harshit Kumar" target="_blank">Harshit Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hirokuni Kitahara" target="_blank">Hirokuni Kitahara</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Zheutlin" target="_blank">Noah Zheutlin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saki Takano" target="_blank">Saki Takano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Divya Pathak" target="_blank">Divya Pathak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix George" target="_blank">Felix George</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinbo Wu" target="_blank">Xinbo Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bekir Turkkan" target="_blank">Bekir Turkkan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gerard Vanloo" target="_blank">Gerard Vanloo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Nidd" target="_blank">Michael Nidd</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ting Dai" target="_blank">Ting Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oishik Chatterjee" target="_blank">Oishik Chatterjee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pranjal Gupta" target="_blank">Pranjal Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suranjana Samanta" target="_blank">Suranjana Samanta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pooja Aggarwal" target="_blank">Pooja Aggarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rong Lee" target="_blank">Rong Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jae-wook Ahn" target="_blank">Jae-wook Ahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Debanjana Kar" target="_blank">Debanjana Kar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Paradkar" target="_blank">Amit Paradkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Deng" target="_blank">Yu Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pratibha Moogi" target="_blank">Pratibha Moogi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateeti Mohapatra" target="_blank">Prateeti Mohapatra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naoki Abe" target="_blank">Naoki Abe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chandrasekhar Narayanaswami" target="_blank">Chandrasekhar Narayanaswami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyin Xu" target="_blank">Tianyin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lav Varshney" target="_blank">Lav Varshney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruchi Mahindru" target="_blank">Ruchi Mahindru</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anca Sailer" target="_blank">Anca Sailer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laura Shwartz" target="_blank">Laura Shwartz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daby Sow" target="_blank">Daby Sow</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Fuller" target="_blank">Nicholas Fuller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruchir Puri" target="_blank">Ruchir Puri</a>
            </p>
            <p id="summary-jP59rz1bZk@OpenReview" class="summary">Realizing the vision of using AI agents to automate critical IT tasks depends on the ability to measure and understand effectiveness of proposed solutions. We introduce ITBench, a framework that offers a systematic methodology for benchmarking AI agents to address real-world IT automation tasks. Our initial release targets three key areas: Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The design enables AI researchers to understand the challenges and opportunities of AI agents for IT automation with push-button workflows and interpretable metrics. IT-Bench includes an initial set of 102 real-world scenarios, which can be easily extended by community contributions. Our results show that agents powered by state-of-the-art models resolve only 11.4% of SRE scenarios, 25.2% of CISO scenarios, and 25.8% of FinOps scenarios (excluding anomaly detection). For FinOps-specific anomaly detection (AD) scenarios, AI agents achieve an F1 score of 0.35. We expect ITBench to be a key enabler of AI-driven IT automation that is correct, safe, and fast. IT-Bench, along with a leaderboard and sample agent implementations, is available at https://github.com/ibm/itbench.</p>
            <p id="subjects-jP59rz1bZk@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-jP59rz1bZk@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-jP59rz1bZk@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-jP59rz1bZk@OpenReview" onclick="foldPdfKimi('jP59rz1bZk@OpenReview', this)" class="hr hr-fold">
        </div><div id="kEn7Wt6Yj2@OpenReview" class="panel paper" keywords="queries,privacy,problems,differential,search,randomness,sketching,responses,internal,turnstile">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kEn7Wt6Yj2" target="_blank" title="52/120"><span class="index notranslate">#52</span></a>
                <a id="title-kEn7Wt6Yj2@OpenReview" class="title-link" href="/venue/kEn7Wt6Yj2@OpenReview" target="_blank">On Differential Privacy for Adaptively Solving Search Problems via Sketching</a>
                <a id="pdf-kEn7Wt6Yj2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kEn7Wt6Yj2@OpenReview', this)" data="https://openreview.net/pdf?id=kEn7Wt6Yj2">[PDF<sup id="pdf-stars-kEn7Wt6Yj2@OpenReview">7</sup>]</a>
                <a id="copy-kEn7Wt6Yj2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kEn7Wt6Yj2@OpenReview')">[Copy]</a>
                <a id="kimi-kEn7Wt6Yj2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kEn7Wt6Yj2@OpenReview', this)">[Kimi<sup id="kimi-stars-kEn7Wt6Yj2@OpenReview">6</sup>]</a>
                <a id="rel-kEn7Wt6Yj2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kEn7Wt6Yj2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kEn7Wt6Yj2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyuan Feng" target="_blank">Shiyuan Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Feng" target="_blank">Ying Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=George Li" target="_blank">George Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhao Song" target="_blank">Zhao Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Woodruff" target="_blank">David Woodruff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lichen Zhang" target="_blank">Lichen Zhang</a>
            </p>
            <p id="summary-kEn7Wt6Yj2@OpenReview" class="summary">Recently differential privacy has been used for a number of streaming, data structure, and dynamic graph problems as a means of hiding the internal randomness of the data structure, so that multiple possibly adaptive queries can be made without sacrificing the correctness of the responses. Although these works use differential privacy to show that for some problems it is possible to tolerate <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-137" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-138"><span class="mi" id="MathJax-Span-139" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">T</script> queries using <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mover&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;&amp;#x007E;&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-140" style="width: 3.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.044em, 1002.97em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-141"><span class="texatom" id="MathJax-Span-142"><span class="mrow" id="MathJax-Span-143"><span class="munderover" id="MathJax-Span-144"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-145" style="font-family: MathJax_Math-italic;">O</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(1.253em, 1000.58em, 1.721em, -999.997em); top: -2.393em; left: 0.211em;"><span class="mo" id="MathJax-Span-146" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-147" style="font-family: MathJax_Main;">(</span><span class="msqrt" id="MathJax-Span-148"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0.836em;"><span class="mrow" id="MathJax-Span-149"><span class="mi" id="MathJax-Span-150" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; clip: rect(3.596em, 1000.68em, 3.961em, -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.003em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em, 1000.84em, 4.378em, -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-151" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.566em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-28">\widetilde{O}(\sqrt{T})</script> copies of a data structure, such results only apply to {\it numerical estimation problems}, and only return the {\it cost} of an optimization problem rather than the solution itself. In this paper we investigate the use of differential privacy for adaptive queries to {\it search} problems, which are significantly more challenging since the responses to queries can reveal much more about the internal randomness than a single numerical query. We focus on two classical search problems: nearest neighbor queries and regression with arbitrary turnstile updates. We identify key parameters to these problems, such as the number of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-152" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-153"><span class="mi" id="MathJax-Span-154" style="font-family: MathJax_Math-italic;">c</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi></math></span></span><script type="math/tex" id="MathJax-Element-29">c</script>-approximate near neighbors and the matrix condition number, and use different differential privacy techniques to design algorithms returning the solution point or solution vector with memory and time depending on these parameters. We give algorithms for each of these problems that achieve similar tradeoffs.</p>
            <p id="subjects-kEn7Wt6Yj2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-kEn7Wt6Yj2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kEn7Wt6Yj2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kEn7Wt6Yj2@OpenReview" onclick="foldPdfKimi('kEn7Wt6Yj2@OpenReview', this)" class="hr hr-fold">
        </div><div id="kJQgMGLrow@OpenReview" class="panel paper" keywords="prediction,shot,generalization,zero,task,agnostic,foundation,multimodal,contrastive,passing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kJQgMGLrow" target="_blank" title="53/120"><span class="index notranslate">#53</span></a>
                <a id="title-kJQgMGLrow@OpenReview" class="title-link" href="/venue/kJQgMGLrow@OpenReview" target="_blank">A Generalization Theory for Zero-Shot Prediction</a>
                <a id="pdf-kJQgMGLrow@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kJQgMGLrow@OpenReview', this)" data="https://openreview.net/pdf?id=kJQgMGLrow">[PDF<sup id="pdf-stars-kJQgMGLrow@OpenReview">36</sup>]</a>
                <a id="copy-kJQgMGLrow@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kJQgMGLrow@OpenReview')">[Copy]</a>
                <a id="kimi-kJQgMGLrow@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kJQgMGLrow@OpenReview', this)">[Kimi<sup id="kimi-stars-kJQgMGLrow@OpenReview">23</sup>]</a>
                <a id="rel-kJQgMGLrow@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kJQgMGLrow@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kJQgMGLrow@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ronak Mehta" target="_blank">Ronak Mehta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaid Harchaoui" target="_blank">Zaid Harchaoui</a>
            </p>
            <p id="summary-kJQgMGLrow@OpenReview" class="summary">A modern paradigm for generalization in machine learning and AI consists of pre-training a task-agnostic foundation model, generally obtained using self-supervised and multimodal contrastive learning. The resulting representations can be used for prediction on a downstream task for which no labeled data is available. We present a theoretical framework to better understand this approach, called zero-shot prediction. We identify the target quantities that zero-shot prediction aims to learn, or learns in passing, and the key conditional independence relationships that enable its generalization ability.</p>
            <p id="subjects-kJQgMGLrow@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-kJQgMGLrow@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kJQgMGLrow@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kJQgMGLrow@OpenReview" onclick="foldPdfKimi('kJQgMGLrow@OpenReview', this)" class="hr hr-fold">
        </div><div id="mIomqOskaa@OpenReview" class="panel paper" keywords="sparsity,network,scaling,unlocks,reinforcement,dense,notoriously,pathologies,reset,pursuing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mIomqOskaa" target="_blank" title="54/120"><span class="index notranslate">#54</span></a>
                <a id="title-mIomqOskaa@OpenReview" class="title-link" href="/venue/mIomqOskaa@OpenReview" target="_blank">Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning</a>
                <a id="pdf-mIomqOskaa@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mIomqOskaa@OpenReview', this)" data="https://openreview.net/pdf?id=mIomqOskaa">[PDF<sup id="pdf-stars-mIomqOskaa@OpenReview">16</sup>]</a>
                <a id="copy-mIomqOskaa@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mIomqOskaa@OpenReview')">[Copy]</a>
                <a id="kimi-mIomqOskaa@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mIomqOskaa@OpenReview', this)">[Kimi<sup id="kimi-stars-mIomqOskaa@OpenReview">21</sup>]</a>
                <a id="rel-mIomqOskaa@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mIomqOskaa@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mIomqOskaa@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guozheng Ma" target="_blank">Guozheng Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Li" target="_blank">Lu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilin Wang" target="_blank">Zilin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Shen" target="_blank">Li Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre-Luc Bacon" target="_blank">Pierre-Luc Bacon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dacheng Tao" target="_blank">Dacheng Tao</a>
            </p>
            <p id="summary-mIomqOskaa@OpenReview" class="summary">Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.</p>
            <p id="subjects-mIomqOskaa@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-mIomqOskaa@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mIomqOskaa@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mIomqOskaa@OpenReview" onclick="foldPdfKimi('mIomqOskaa@OpenReview', this)" class="hr hr-fold">
        </div><div id="nq5bt0mRTC@OpenReview" class="panel paper" keywords="mice,underestimation,intrinsic,cost,violations,crl,bias,constraint,constrained,reinforcement">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=nq5bt0mRTC" target="_blank" title="55/120"><span class="index notranslate">#55</span></a>
                <a id="title-nq5bt0mRTC@OpenReview" class="title-link" href="/venue/nq5bt0mRTC@OpenReview" target="_blank">Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration</a>
                <a id="pdf-nq5bt0mRTC@OpenReview" class="title-pdf notranslate" onclick="togglePdf('nq5bt0mRTC@OpenReview', this)" data="https://openreview.net/pdf?id=nq5bt0mRTC">[PDF<sup id="pdf-stars-nq5bt0mRTC@OpenReview">7</sup>]</a>
                <a id="copy-nq5bt0mRTC@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('nq5bt0mRTC@OpenReview')">[Copy]</a>
                <a id="kimi-nq5bt0mRTC@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('nq5bt0mRTC@OpenReview', this)">[Kimi<sup id="kimi-stars-nq5bt0mRTC@OpenReview">17</sup>]</a>
                <a id="rel-nq5bt0mRTC@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('nq5bt0mRTC@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-nq5bt0mRTC@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shiqing Gao" target="_blank">Shiqing Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxin Ding" target="_blank">Jiaxin Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luoyi Fu" target="_blank">Luoyi Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinbing Wang" target="_blank">Xinbing Wang</a>
            </p>
            <p id="summary-nq5bt0mRTC@OpenReview" class="summary">Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.</p>
            <p id="subjects-nq5bt0mRTC@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-nq5bt0mRTC@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-nq5bt0mRTC@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-nq5bt0mRTC@OpenReview" onclick="foldPdfKimi('nq5bt0mRTC@OpenReview', this)" class="hr hr-fold">
        </div><div id="pwNSUo7yUb@OpenReview" class="panel paper" keywords="mmsd,matching,fid,moment,step,distillation,256x256,models,unlike,inductive">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pwNSUo7yUb" target="_blank" title="56/120"><span class="index notranslate">#56</span></a>
                <a id="title-pwNSUo7yUb@OpenReview" class="title-link" href="/venue/pwNSUo7yUb@OpenReview" target="_blank">Inductive Moment Matching</a>
                <a id="pdf-pwNSUo7yUb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pwNSUo7yUb@OpenReview', this)" data="https://openreview.net/pdf?id=pwNSUo7yUb">[PDF<sup id="pdf-stars-pwNSUo7yUb@OpenReview">17</sup>]</a>
                <a id="copy-pwNSUo7yUb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pwNSUo7yUb@OpenReview')">[Copy]</a>
                <a id="kimi-pwNSUo7yUb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pwNSUo7yUb@OpenReview', this)">[Kimi<sup id="kimi-stars-pwNSUo7yUb@OpenReview">21</sup>]</a>
                <a id="rel-pwNSUo7yUb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pwNSUo7yUb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pwNSUo7yUb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linqi (Alex) Zhou" target="_blank">Linqi (Alex) Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefano Ermon" target="_blank">Stefano Ermon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaming Song" target="_blank">Jiaming Song</a>
            </p>
            <p id="summary-pwNSUo7yUb@OpenReview" class="summary">Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Moment Matching Self-Distillation (MMSD), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, MMSD does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, MMSD guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. MMSD surpasses diffusion models on ImageNet-256x256 with 2.13 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 2.05 on CIFAR-10 for a model trained from scratch.</p>
            <p id="subjects-pwNSUo7yUb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-pwNSUo7yUb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pwNSUo7yUb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pwNSUo7yUb@OpenReview" onclick="foldPdfKimi('pwNSUo7yUb@OpenReview', this)" class="hr hr-fold">
        </div><div id="reuShgiHdg@OpenReview" class="panel paper" keywords="refersplat,r3dgs,splatting,gaussian,referring,segmentation,heshuting555,task,newly,lerf">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=reuShgiHdg" target="_blank" title="57/120"><span class="index notranslate">#57</span></a>
                <a id="title-reuShgiHdg@OpenReview" class="title-link" href="/venue/reuShgiHdg@OpenReview" target="_blank">ReferSplat: Referring Segmentation in 3D Gaussian Splatting</a>
                <a id="pdf-reuShgiHdg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('reuShgiHdg@OpenReview', this)" data="https://openreview.net/pdf?id=reuShgiHdg">[PDF<sup id="pdf-stars-reuShgiHdg@OpenReview">8</sup>]</a>
                <a id="copy-reuShgiHdg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('reuShgiHdg@OpenReview')">[Copy]</a>
                <a id="kimi-reuShgiHdg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('reuShgiHdg@OpenReview', this)">[Kimi<sup id="kimi-stars-reuShgiHdg@OpenReview">6</sup>]</a>
                <a id="rel-reuShgiHdg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('reuShgiHdg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-reuShgiHdg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuting He" target="_blank">Shuting He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangquan Jie" target="_blank">Guangquan Jie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changshuo Wang" target="_blank">Changshuo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Zhou" target="_blank">Yun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuming Hu" target="_blank">Shuming Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanbin Li" target="_blank">Guanbin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henghui Ding" target="_blank">Henghui Ding</a>
            </p>
            <p id="summary-reuShgiHdg@OpenReview" class="summary">We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes. This task requires the model to identify newly described objects that may be occluded or not directly visible in a novel view,posing a significant challenge for 3D multi-modal understanding. Developing this capability is crucial for advancing embodied AI.To support research in this area, we construct the first R3DGS dataset, Ref-LERF.Our analysis reveals that 3D multi-modal understanding and spatial relationship modeling are key challenges for R3DGS.To address these challenges, we propose ReferSplat, a framework that explicitly models 3D Gaussian points with natural language expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art performance on both the newly proposed R3DGS task and 3D open-vocabulary segmentation benchmarks.Dataset and code are available at https://github.com/heshuting555/ReferSplat.</p>
            <p id="subjects-reuShgiHdg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-reuShgiHdg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-reuShgiHdg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-reuShgiHdg@OpenReview" onclick="foldPdfKimi('reuShgiHdg@OpenReview', this)" class="hr hr-fold">
        </div><div id="tO7OVZkCo1@OpenReview" class="panel paper" keywords="videorope,rope,niah,video,distractors,temporal,rotary,variants,position,embedding">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tO7OVZkCo1" target="_blank" title="58/120"><span class="index notranslate">#58</span></a>
                <a id="title-tO7OVZkCo1@OpenReview" class="title-link" href="/venue/tO7OVZkCo1@OpenReview" target="_blank">VideoRoPE: What Makes for Good Video Rotary Position Embedding?</a>
                <a id="pdf-tO7OVZkCo1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tO7OVZkCo1@OpenReview', this)" data="https://openreview.net/pdf?id=tO7OVZkCo1">[PDF<sup id="pdf-stars-tO7OVZkCo1@OpenReview">15</sup>]</a>
                <a id="copy-tO7OVZkCo1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tO7OVZkCo1@OpenReview')">[Copy]</a>
                <a id="kimi-tO7OVZkCo1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tO7OVZkCo1@OpenReview', this)">[Kimi<sup id="kimi-stars-tO7OVZkCo1@OpenReview">15</sup>]</a>
                <a id="rel-tO7OVZkCo1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tO7OVZkCo1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tO7OVZkCo1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xilin Wei" target="_blank">Xilin Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoran Liu" target="_blank">Xiaoran Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhang Zang" target="_blank">Yuhang Zang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyi Dong" target="_blank">Xiaoyi Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pan Zhang" target="_blank">Pan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhang Cao" target="_blank">Yuhang Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Tong" target="_blank">Jian Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haodong Duan" target="_blank">Haodong Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qipeng Guo" target="_blank">Qipeng Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Wang" target="_blank">Jiaqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xipeng Qiu" target="_blank">Xipeng Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dahua Lin" target="_blank">Dahua Lin</a>
            </p>
            <p id="summary-tO7OVZkCo1@OpenReview" class="summary">While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge.This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work.As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH.The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors.Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships.VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing.VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination.Our code and model weights will be publicly released.</p>
            <p id="subjects-tO7OVZkCo1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-tO7OVZkCo1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tO7OVZkCo1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tO7OVZkCo1@OpenReview" onclick="foldPdfKimi('tO7OVZkCo1@OpenReview', this)" class="hr hr-fold">
        </div><div id="up21Rwj5Fo@OpenReview" class="panel paper" keywords="chromatic,matching,varepsilon,euclidean,sublinear,update,dynamic,wasserstein,fully,algorithm">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=up21Rwj5Fo" target="_blank" title="59/120"><span class="index notranslate">#59</span></a>
                <a id="title-up21Rwj5Fo@OpenReview" class="title-link" href="/venue/up21Rwj5Fo@OpenReview" target="_blank">Fully Dynamic Euclidean Bi-Chromatic Matching in Sublinear Update Time</a>
                <a id="pdf-up21Rwj5Fo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('up21Rwj5Fo@OpenReview', this)" data="https://openreview.net/pdf?id=up21Rwj5Fo">[PDF<sup id="pdf-stars-up21Rwj5Fo@OpenReview">1</sup>]</a>
                <a id="copy-up21Rwj5Fo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('up21Rwj5Fo@OpenReview')">[Copy]</a>
                <a id="kimi-up21Rwj5Fo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('up21Rwj5Fo@OpenReview', this)">[Kimi<sup id="kimi-stars-up21Rwj5Fo@OpenReview">1</sup>]</a>
                <a id="rel-up21Rwj5Fo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('up21Rwj5Fo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-up21Rwj5Fo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gramoz Goranci" target="_blank">Gramoz Goranci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Kiss" target="_blank">Peter Kiss</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neel Patel" target="_blank">Neel Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Seybold" target="_blank">Martin Seybold</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eva Szilagyi" target="_blank">Eva Szilagyi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Da Wei Zheng" target="_blank">Da Wei Zheng</a>
            </p>
            <p id="summary-up21Rwj5Fo@OpenReview" class="summary">We consider the Euclidean bi-chromatic matching problem in the dynamic setting, where the goal is to efficiently process point insertions and deletions while maintaining a high-quality solution. Computing the minimum cost bi-chromatic matching is one of the core problems in geometric optimization that has found many applications, most notably in estimating Wasserstein distance between two distributions. In this work, we present the first fully dynamic algorithm for Euclidean bi-chromatic matching with sublinear update time. For any fixed <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-155" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.24em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-156"><span class="mi" id="MathJax-Span-157" style="font-family: MathJax_Math-italic;">ε</span><span class="mo" id="MathJax-Span-158" style="font-family: MathJax_Main; padding-left: 0.263em;">&gt;</span><span class="mn" id="MathJax-Span-159" style="font-family: MathJax_Main; padding-left: 0.263em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi><mo>&gt;</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-30">\varepsilon > 0</script>, our algorithm achieves <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-160" style="width: 3.648em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.92em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-161"><span class="mi" id="MathJax-Span-162" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-163" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-164" style="font-family: MathJax_Main;">1</span><span class="texatom" id="MathJax-Span-165"><span class="mrow" id="MathJax-Span-166"><span class="mo" id="MathJax-Span-167" style="font-family: MathJax_Main;">/</span></span></span><span class="mi" id="MathJax-Span-168" style="font-family: MathJax_Math-italic;">ε</span><span class="mo" id="MathJax-Span-169" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-31">O(1/\varepsilon)</script>-approximation and handles updates in <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;&amp;#x03B5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-170" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.45em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-171"><span class="mi" id="MathJax-Span-172" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-173" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-174"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-175" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.628em;"><span class="texatom" id="MathJax-Span-176"><span class="mrow" id="MathJax-Span-177"><span class="mi" id="MathJax-Span-178" style="font-size: 70.7%; font-family: MathJax_Math-italic;">ε</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-179" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mrow class="MJX-TeXAtom-ORD"><mi>ε</mi></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-32">O(n^{\varepsilon})</script> time. Our experiments show that our algorithm enables effective monitoring of the distributional drift in the Wasserstein distance on real and synthetic data sets, while outperforming the runtime of baseline approximations by orders of magnitudes.</p>
            <p id="subjects-up21Rwj5Fo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-up21Rwj5Fo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-up21Rwj5Fo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-up21Rwj5Fo@OpenReview" onclick="foldPdfKimi('up21Rwj5Fo@OpenReview', this)" class="hr hr-fold">
        </div><div id="v26vwjxOEz@OpenReview" class="panel paper" keywords="reasoning,emma,multimodal,mllms,multimodality,reason,enhanced,benchmark,organically,advanced">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=v26vwjxOEz" target="_blank" title="60/120"><span class="index notranslate">#60</span></a>
                <a id="title-v26vwjxOEz@OpenReview" class="title-link" href="/venue/v26vwjxOEz@OpenReview" target="_blank">Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark</a>
                <a id="pdf-v26vwjxOEz@OpenReview" class="title-pdf notranslate" onclick="togglePdf('v26vwjxOEz@OpenReview', this)" data="https://openreview.net/pdf?id=v26vwjxOEz">[PDF<sup id="pdf-stars-v26vwjxOEz@OpenReview">17</sup>]</a>
                <a id="copy-v26vwjxOEz@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('v26vwjxOEz@OpenReview')">[Copy]</a>
                <a id="kimi-v26vwjxOEz@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('v26vwjxOEz@OpenReview', this)">[Kimi<sup id="kimi-stars-v26vwjxOEz@OpenReview">15</sup>]</a>
                <a id="rel-v26vwjxOEz@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('v26vwjxOEz@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-v26vwjxOEz@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunzhuo Hao" target="_blank">Yunzhuo Hao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Gu" target="_blank">Jiawei Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huichen Wang" target="_blank">Huichen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linjie Li" target="_blank">Linjie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyuan Yang" target="_blank">Zhengyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijuan Wang" target="_blank">Lijuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Cheng" target="_blank">Yu Cheng</a>
            </p>
            <p id="summary-v26vwjxOEz@OpenReview" class="summary">The ability to organically reason over and with both text and images is a pillar of human intelligence, yet the ability of Multimodal Large Language Models (MLLMs) to perform such multimodal reasoning remains under-explored. Existing benchmarks often emphasize text-dominant reasoning or rely on shallow visual cues, failing to adequately assess integrated visual and textual reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be addressed by reasoning independently in each modality, offering an enhanced test suite for MLLMs' reasoning capabilities. Our evaluation of state-of-the-art MLLMs on EMMA reveals significant limitations in handling complex multimodal and multi-step reasoning tasks, even with advanced techniques like Chain-of-Thought prompting and test-time compute scaling underperforming. These findings underscore the need for improved multimodal architectures and training paradigms to close the gap between human and model reasoning in multimodality.</p>
            <p id="subjects-v26vwjxOEz@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-v26vwjxOEz@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-v26vwjxOEz@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-v26vwjxOEz@OpenReview" onclick="foldPdfKimi('v26vwjxOEz@OpenReview', this)" class="hr hr-fold">
        </div><div id="v77ZMzbsBA@OpenReview" class="panel paper" keywords="outlier,detrimental,samples,gradient,hessian,training,identifying,influence,models,identification">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=v77ZMzbsBA" target="_blank" title="61/120"><span class="index notranslate">#61</span></a>
                <a id="title-v77ZMzbsBA@OpenReview" class="title-link" href="/venue/v77ZMzbsBA@OpenReview" target="_blank">Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models</a>
                <a id="pdf-v77ZMzbsBA@OpenReview" class="title-pdf notranslate" onclick="togglePdf('v77ZMzbsBA@OpenReview', this)" data="https://openreview.net/pdf?id=v77ZMzbsBA">[PDF<sup id="pdf-stars-v77ZMzbsBA@OpenReview">12</sup>]</a>
                <a id="copy-v77ZMzbsBA@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('v77ZMzbsBA@OpenReview')">[Copy]</a>
                <a id="kimi-v77ZMzbsBA@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('v77ZMzbsBA@OpenReview', this)">[Kimi<sup id="kimi-stars-v77ZMzbsBA@OpenReview">13</sup>]</a>
                <a id="rel-v77ZMzbsBA@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('v77ZMzbsBA@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-v77ZMzbsBA@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anshuman Chhabra" target="_blank">Anshuman Chhabra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Li" target="_blank">Bo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Chen" target="_blank">Jian Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prasant Mohapatra" target="_blank">Prasant Mohapatra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongfu Liu" target="_blank">Hongfu Liu</a>
            </p>
            <p id="summary-v77ZMzbsBA@OpenReview" class="summary">A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models.</p>
            <p id="subjects-v77ZMzbsBA@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-v77ZMzbsBA@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-v77ZMzbsBA@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-v77ZMzbsBA@OpenReview" onclick="foldPdfKimi('v77ZMzbsBA@OpenReview', this)" class="hr hr-fold">
        </div><div id="vQubr1uBUw@OpenReview" class="panel paper" keywords="drafter,decoding,lossless,speculative,accelerating,vocabulary,drafters,shelf,vocabularies,target">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vQubr1uBUw" target="_blank" title="62/120"><span class="index notranslate">#62</span></a>
                <a id="title-vQubr1uBUw@OpenReview" class="title-link" href="/venue/vQubr1uBUw@OpenReview" target="_blank">Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies</a>
                <a id="pdf-vQubr1uBUw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vQubr1uBUw@OpenReview', this)" data="https://openreview.net/pdf?id=vQubr1uBUw">[PDF<sup id="pdf-stars-vQubr1uBUw@OpenReview">15</sup>]</a>
                <a id="copy-vQubr1uBUw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vQubr1uBUw@OpenReview')">[Copy]</a>
                <a id="kimi-vQubr1uBUw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vQubr1uBUw@OpenReview', this)">[Kimi<sup id="kimi-stars-vQubr1uBUw@OpenReview">26</sup>]</a>
                <a id="rel-vQubr1uBUw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vQubr1uBUw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vQubr1uBUw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nadav Timor" target="_blank">Nadav Timor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan Mamou" target="_blank">Jonathan Mamou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Korat" target="_blank">Daniel Korat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moshe Berchansky" target="_blank">Moshe Berchansky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaurav Jain" target="_blank">Gaurav Jain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oren Pereg" target="_blank">Oren Pereg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moshe Wasserblat" target="_blank">Moshe Wasserblat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Harel" target="_blank">David Harel</a>
            </p>
            <p id="summary-vQubr1uBUw@OpenReview" class="summary">Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.</p>
            <p id="subjects-vQubr1uBUw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-vQubr1uBUw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vQubr1uBUw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vQubr1uBUw@OpenReview" onclick="foldPdfKimi('vQubr1uBUw@OpenReview', this)" class="hr hr-fold">
        </div><div id="vt65VjJakt@OpenReview" class="panel paper" keywords="rkld,abkd,fkld,student,teacher,divergence,distillation,concentration,knowledge,textbf">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=vt65VjJakt" target="_blank" title="63/120"><span class="index notranslate">#63</span></a>
                <a id="title-vt65VjJakt@OpenReview" class="title-link" href="/venue/vt65VjJakt@OpenReview" target="_blank">ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-180" style="width: 0.766em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.627em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.599em, 1000.59em, 2.259em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-181"><span class="mi" id="MathJax-Span-182" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.04em; border-left: 0px solid; width: 0px; height: 0.627em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-33">\alpha</script>-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-183" style="width: 0.696em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.557em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.356em, 1000.56em, 2.467em, -999.998em); top: -2.151em; left: 0em;"><span class="mrow" id="MathJax-Span-184"><span class="mi" id="MathJax-Span-185" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.155em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.29em; border-left: 0px solid; width: 0px; height: 1.169em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></span></span><script type="math/tex" id="MathJax-Element-34">\beta</script>-Divergence</a>
                <a id="pdf-vt65VjJakt@OpenReview" class="title-pdf notranslate" onclick="togglePdf('vt65VjJakt@OpenReview', this)" data="https://openreview.net/pdf?id=vt65VjJakt">[PDF<sup id="pdf-stars-vt65VjJakt@OpenReview">12</sup>]</a>
                <a id="copy-vt65VjJakt@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('vt65VjJakt@OpenReview')">[Copy]</a>
                <a id="kimi-vt65VjJakt@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('vt65VjJakt@OpenReview', this)">[Kimi<sup id="kimi-stars-vt65VjJakt@OpenReview">4</sup>]</a>
                <a id="rel-vt65VjJakt@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('vt65VjJakt@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-vt65VjJakt@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guanghui Wang" target="_blank">Guanghui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyong Yang" target="_blank">Zhiyong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zitai Wang" target="_blank">Zitai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shi Wang" target="_blank">Shi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Xu" target="_blank">Qianqian Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingming Huang" target="_blank">Qingming Huang</a>
            </p>
            <p id="summary-vt65VjJakt@OpenReview" class="summary">Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student model by minimizing the divergence between their output distributions, typically using forward Kullback-Leibler divergence (FKLD) or reverse KLD (RKLD). It has become an effective training paradigm due to the broader supervision information provided by the teacher distribution compared to one-hot labels. We identify that the core challenge in KD lies in balancing two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}} effect, which refers to focusing on modes with large errors, and the \textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on modes with high student confidence. Through an analysis of how probabilities are reassigned during gradient updates, we observe that these two effects are entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too weak in FKLD, causing the student to fail to concentrate on the target class. In contrast, both are too strong in RKLD, causing the student to overly emphasize the target class while ignoring the broader distributional information from the teacher. To address this imbalance, we propose ABKD, a generic framework with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-186" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-187"><span class="mi" id="MathJax-Span-188" style="font-family: MathJax_Math-italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-35">\alpha</script>-<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B2;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-189" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.58em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-190"><span class="mi" id="MathJax-Span-191" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></span></span><script type="math/tex" id="MathJax-Element-36">\beta</script>-divergence. Our theoretical results show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving a better trade-off between these effects. Extensive experiments on 17 language/vision datasets with 12 teacher-student settings confirm its efficacy.</p>
            <p id="subjects-vt65VjJakt@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-vt65VjJakt@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-vt65VjJakt@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-vt65VjJakt@OpenReview" onclick="foldPdfKimi('vt65VjJakt@OpenReview', this)" class="hr hr-fold">
        </div><div id="wUEp13rqXP@OpenReview" class="panel paper" keywords="experts,vram,moe,mole,lookup,inference,expert,mixture,flops,offloading">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=wUEp13rqXP" target="_blank" title="64/120"><span class="index notranslate">#64</span></a>
                <a id="title-wUEp13rqXP@OpenReview" class="title-link" href="/venue/wUEp13rqXP@OpenReview" target="_blank">Mixture of Lookup Experts</a>
                <a id="pdf-wUEp13rqXP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('wUEp13rqXP@OpenReview', this)" data="https://openreview.net/pdf?id=wUEp13rqXP">[PDF<sup id="pdf-stars-wUEp13rqXP@OpenReview">31</sup>]</a>
                <a id="copy-wUEp13rqXP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('wUEp13rqXP@OpenReview')">[Copy]</a>
                <a id="kimi-wUEp13rqXP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('wUEp13rqXP@OpenReview', this)">[Kimi<sup id="kimi-stars-wUEp13rqXP@OpenReview">32</sup>]</a>
                <a id="rel-wUEp13rqXP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('wUEp13rqXP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-wUEp13rqXP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shibo Jie" target="_blank">Shibo Jie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yehui Tang" target="_blank">Yehui Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Han" target="_blank">Kai Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yitong Li" target="_blank">Yitong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Duyu Tang" target="_blank">Duyu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi-Hong Deng" target="_blank">Zhi-Hong Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhe Wang" target="_blank">Yunhe Wang</a>
            </p>
            <p id="summary-wUEp13rqXP@OpenReview" class="summary">Mixture-of-Experts (MoE) activates only a subset of experts during inference, allowing the model to maintain low inference FLOPs and latency even as the parameter count scales up. However, since MoE dynamically selects the experts, all the experts need to be loaded into VRAM. Their large parameter size still limits deployment, and offloading, which load experts into VRAM only when needed, significantly increase inference latency. To address this, we propose Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in both communication and VRAM usage. In MoLE, the experts are Feed-Forward Networks (FFNs) during training, taking the output of the embedding layer as input. Before inference, these experts can be re-parameterized as lookup tables (LUTs) that retrieves expert outputs based on input ids, and offloaded to storage devices. Therefore, we do not need to perform expert computations during inference. Instead, we directly retrieve the expert's computation results based on input ids and load them into VRAM, and thus the resulting communication overhead is negligible. Experiments show that, with the same FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models and significantly faster than MoE with experts offloading, while maintaining performance on par with MoE. Code: https://github.com/JieShibo/MoLE.</p>
            <p id="subjects-wUEp13rqXP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-wUEp13rqXP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-wUEp13rqXP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-wUEp13rqXP@OpenReview" onclick="foldPdfKimi('wUEp13rqXP@OpenReview', this)" class="hr hr-fold">
        </div><div id="x4qvBVuzzu@OpenReview" class="panel paper" keywords="lora,weight,memory,tuning,reduction,states,3090,based,fine,control">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=x4qvBVuzzu" target="_blank" title="65/120"><span class="index notranslate">#65</span></a>
                <a id="title-x4qvBVuzzu@OpenReview" class="title-link" href="/venue/x4qvBVuzzu@OpenReview" target="_blank">From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on LoRA with Parallel Control</a>
                <a id="pdf-x4qvBVuzzu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('x4qvBVuzzu@OpenReview', this)" data="https://openreview.net/pdf?id=x4qvBVuzzu">[PDF<sup id="pdf-stars-x4qvBVuzzu@OpenReview">20</sup>]</a>
                <a id="copy-x4qvBVuzzu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('x4qvBVuzzu@OpenReview')">[Copy]</a>
                <a id="kimi-x4qvBVuzzu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('x4qvBVuzzu@OpenReview', this)">[Kimi<sup id="kimi-stars-x4qvBVuzzu@OpenReview">20</sup>]</a>
                <a id="rel-x4qvBVuzzu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('x4qvBVuzzu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-x4qvBVuzzu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chi Zhang" target="_blank">Chi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=REN Lianhai" target="_blank">REN Lianhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingpu Cheng" target="_blank">Jingpu Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianxiao Li" target="_blank">Qianxiao Li</a>
            </p>
            <p id="summary-x4qvBVuzzu@OpenReview" class="summary">The LoRA method has achieved notable success in reducing GPU memory usage by applying low-rank updates to weight matrices. Yet, one simple question remains: can we push this reduction even further? Furthermore, is it possible to achieve this while improving performance and reducing computation time? Answering these questions requires moving beyond the conventional weight-centric approach. In this paper, we present a state-based fine-tuning framework that shifts the focus from weight adaptation to optimizing forward states, with LoRA acting as a special example. Specifically, state-based tuning introduces parameterized perturbations to the states within the computational graph, allowing us to control states across an entire residual block. A key advantage of this approach is the potential to avoid storing large intermediate states in models like transformers. Empirical results across multiple architectures—including ViT, RoBERTa, LLaMA2-7B, and LLaMA3-8B—show that our method further reduces memory consumption and computation time while simultaneously improving performance. Moreover, as a result of memory reduction, we explore the feasibility to train 7B/8B models on consumer-level GPUs like Nvidia 3090, without model quantization. The code is available at an anonymous GitHub repository</p>
            <p id="subjects-x4qvBVuzzu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-x4qvBVuzzu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-x4qvBVuzzu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-x4qvBVuzzu@OpenReview" onclick="foldPdfKimi('x4qvBVuzzu@OpenReview', this)" class="hr hr-fold">
        </div><div id="xmbdACI0xu@OpenReview" class="panel paper" keywords="affectgpt,emotion,mer,multimodal,mllms,understanding,dataset,language,benchmark,descriptive">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xmbdACI0xu" target="_blank" title="66/120"><span class="index notranslate">#66</span></a>
                <a id="title-xmbdACI0xu@OpenReview" class="title-link" href="/venue/xmbdACI0xu@OpenReview" target="_blank">AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models</a>
                <a id="pdf-xmbdACI0xu@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xmbdACI0xu@OpenReview', this)" data="https://openreview.net/pdf?id=xmbdACI0xu">[PDF<sup id="pdf-stars-xmbdACI0xu@OpenReview">15</sup>]</a>
                <a id="copy-xmbdACI0xu@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xmbdACI0xu@OpenReview')">[Copy]</a>
                <a id="kimi-xmbdACI0xu@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xmbdACI0xu@OpenReview', this)">[Kimi<sup id="kimi-stars-xmbdACI0xu@OpenReview">17</sup>]</a>
                <a id="rel-xmbdACI0xu@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xmbdACI0xu@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xmbdACI0xu@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Lian" target="_blank">Zheng Lian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyu Chen" target="_blank">Haoyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lan Chen" target="_blank">Lan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyang Sun" target="_blank">Haiyang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Licai Sun" target="_blank">Licai Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Ren" target="_blank">Yong Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zebang Cheng" target="_blank">Zebang Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Liu" target="_blank">Bin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Liu" target="_blank">Rui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaojiang Peng" target="_blank">Xiaojiang Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangyan Yi" target="_blank">Jiangyan Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianhua Tao" target="_blank">Jianhua Tao</a>
            </p>
            <p id="summary-xmbdACI0xu@OpenReview" class="summary">The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level—from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPT's robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: https://github.com/zeroQiaoba/AffectGPT.</p>
            <p id="subjects-xmbdACI0xu@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-xmbdACI0xu@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xmbdACI0xu@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xmbdACI0xu@OpenReview" onclick="foldPdfKimi('xmbdACI0xu@OpenReview', this)" class="hr hr-fold">
        </div><div id="y3d4Bs2r7r@OpenReview" class="panel paper" keywords="misspecification,rope,simulators,sbi,misspecified,calibration,inference,world,truth,calibrated">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=y3d4Bs2r7r" target="_blank" title="67/120"><span class="index notranslate">#67</span></a>
                <a id="title-y3d4Bs2r7r@OpenReview" class="title-link" href="/venue/y3d4Bs2r7r@OpenReview" target="_blank">Addressing Misspecification in Simulation-based Inference through Data-driven Calibration</a>
                <a id="pdf-y3d4Bs2r7r@OpenReview" class="title-pdf notranslate" onclick="togglePdf('y3d4Bs2r7r@OpenReview', this)" data="https://openreview.net/pdf?id=y3d4Bs2r7r">[PDF<sup id="pdf-stars-y3d4Bs2r7r@OpenReview">1</sup>]</a>
                <a id="copy-y3d4Bs2r7r@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('y3d4Bs2r7r@OpenReview')">[Copy]</a>
                <a id="kimi-y3d4Bs2r7r@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('y3d4Bs2r7r@OpenReview', this)">[Kimi<sup id="kimi-stars-y3d4Bs2r7r@OpenReview">6</sup>]</a>
                <a id="rel-y3d4Bs2r7r@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('y3d4Bs2r7r@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-y3d4Bs2r7r@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Antoine Wehenkel" target="_blank">Antoine Wehenkel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juan L. Gamella" target="_blank">Juan L. Gamella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ozan Sener" target="_blank">Ozan Sener</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jens Behrmann" target="_blank">Jens Behrmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guillermo Sapiro" target="_blank">Guillermo Sapiro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jörn Jacobsen" target="_blank">Jörn Jacobsen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Cuturi" target="_blank">Marco Cuturi</a>
            </p>
            <p id="summary-y3d4Bs2r7r@OpenReview" class="summary">Driven by steady progress in deep generative modeling, simulation-based inference (SBI) has emerged as the workhorse for inferring the parameters of stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability, preventing its adoption in important applications where only misspecified simulators are available.This work introduces robust posterior estimation (RoPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements.We formalize the misspecification gap as the solution of an optimal transport (OT) problem between learned representations of real-world and simulated observations, allowing RoPE to learn a model of the misspecification without placing additional assumptions on its nature. RoPE shows how the calibration set and OT together offer a controllable balance between calibrated uncertainty and informative inference even under severely misspecified simulators. Results on four synthetic tasks and two real-world problems with ground-truth labels demonstrate that RoPE outperforms baselines and consistently returns informative and calibrated credible intervals.</p>
            <p id="subjects-y3d4Bs2r7r@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-y3d4Bs2r7r@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-y3d4Bs2r7r@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-y3d4Bs2r7r@OpenReview" onclick="foldPdfKimi('y3d4Bs2r7r@OpenReview', this)" class="hr hr-fold">
        </div><div id="yMJcHWcb2Z@OpenReview" class="panel paper" keywords="motion,videojam,video,appearance,coherence,generation,joint,instills,guidance,objective">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=yMJcHWcb2Z" target="_blank" title="68/120"><span class="index notranslate">#68</span></a>
                <a id="title-yMJcHWcb2Z@OpenReview" class="title-link" href="/venue/yMJcHWcb2Z@OpenReview" target="_blank">VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</a>
                <a id="pdf-yMJcHWcb2Z@OpenReview" class="title-pdf notranslate" onclick="togglePdf('yMJcHWcb2Z@OpenReview', this)" data="https://openreview.net/pdf?id=yMJcHWcb2Z">[PDF<sup id="pdf-stars-yMJcHWcb2Z@OpenReview">7</sup>]</a>
                <a id="copy-yMJcHWcb2Z@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('yMJcHWcb2Z@OpenReview')">[Copy]</a>
                <a id="kimi-yMJcHWcb2Z@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('yMJcHWcb2Z@OpenReview', this)">[Kimi<sup id="kimi-stars-yMJcHWcb2Z@OpenReview">8</sup>]</a>
                <a id="rel-yMJcHWcb2Z@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('yMJcHWcb2Z@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-yMJcHWcb2Z@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hila Chefer" target="_blank">Hila Chefer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Uriel Singer" target="_blank">Uriel Singer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Zohar" target="_blank">Amit Zohar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuval Kirstain" target="_blank">Yuval Kirstain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam Polyak" target="_blank">Adam Polyak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaniv Taigman" target="_blank">Yaniv Taigman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lior Wolf" target="_blank">Lior Wolf</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shelly Sheynin" target="_blank">Shelly Sheynin</a>
            </p>
            <p id="summary-yMJcHWcb2Z@OpenReview" class="summary">Despite tremendous recent progress, generative video models still struggle to capture real-world motion, dynamics, and physics. We show that this limitation arises from the conventional pixel reconstruction objective, which biases models toward appearance fidelity at the expense of motion coherence.To address this, we introduce **VideoJAM**, a novel framework that instills an effective motion prior to video generators, by encouraging the model to learn *a joint appearance-motion representation*. VideoJAM is composed of two complementary units. During training, we extend the objective to predict both the generated pixels and their corresponding motion from a single learned representation. During inference, we introduce **Inner-Guidance**, a mechanism that steers the generation toward coherent motion by leveraging the model's own evolving motion prediction as a dynamic guidance signal.Notably, our framework can be applied to any video model with minimal adaptations, requiring no modifications to the training data or scaling of the model.VideoJAM achieves state-of-the-art performance in motion coherence, surpassing highly competitive proprietary models while also enhancing the perceived visual quality of the generations.These findings emphasize that appearance and motion can be complementary and, when effectively integrated, enhance both the visual quality and the coherence of video generation.</p>
            <p id="subjects-yMJcHWcb2Z@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-yMJcHWcb2Z@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-yMJcHWcb2Z@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-yMJcHWcb2Z@OpenReview" onclick="foldPdfKimi('yMJcHWcb2Z@OpenReview', this)" class="hr hr-fold">
        </div><div id="zBBYsVGKuB@OpenReview" class="panel paper" keywords="zsc,coordination,cooperative,cooperation,agents,partner,partners,shot,new,cross">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zBBYsVGKuB" target="_blank" title="69/120"><span class="index notranslate">#69</span></a>
                <a id="title-zBBYsVGKuB@OpenReview" class="title-link" href="/venue/zBBYsVGKuB@OpenReview" target="_blank">Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination</a>
                <a id="pdf-zBBYsVGKuB@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zBBYsVGKuB@OpenReview', this)" data="https://openreview.net/pdf?id=zBBYsVGKuB">[PDF<sup id="pdf-stars-zBBYsVGKuB@OpenReview">15</sup>]</a>
                <a id="copy-zBBYsVGKuB@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zBBYsVGKuB@OpenReview')">[Copy]</a>
                <a id="kimi-zBBYsVGKuB@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zBBYsVGKuB@OpenReview', this)">[Kimi<sup id="kimi-stars-zBBYsVGKuB@OpenReview">11</sup>]</a>
                <a id="rel-zBBYsVGKuB@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zBBYsVGKuB@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zBBYsVGKuB@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kunal Jha" target="_blank">Kunal Jha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wilka Carvalho" target="_blank">Wilka Carvalho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yancheng Liang" target="_blank">Yancheng Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Simon Du" target="_blank">Simon Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Kleiman-Weiner" target="_blank">Max Kleiman-Weiner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Natasha Jaques" target="_blank">Natasha Jaques</a>
            </p>
            <p id="summary-zBBYsVGKuB@OpenReview" class="summary">Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a **distribution of environments with a single partner** enables learning general cooperative skills that support ZSC with **many new partners on many new problems**. We introduce *two* Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called **Cross-Environment Cooperation (CEC)**, and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data.</p>
            <p id="subjects-zBBYsVGKuB@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-zBBYsVGKuB@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zBBYsVGKuB@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zBBYsVGKuB@OpenReview" onclick="foldPdfKimi('zBBYsVGKuB@OpenReview', this)" class="hr hr-fold">
        </div><div id="zf9zwCRKyP@OpenReview" class="panel paper" keywords="chatbot,arena,manipulation,attacker,adversarial,voting,vote,bot,leaderboards,defenses">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=zf9zwCRKyP" target="_blank" title="70/120"><span class="index notranslate">#70</span></a>
                <a id="title-zf9zwCRKyP@OpenReview" class="title-link" href="/venue/zf9zwCRKyP@OpenReview" target="_blank">Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards</a>
                <a id="pdf-zf9zwCRKyP@OpenReview" class="title-pdf notranslate" onclick="togglePdf('zf9zwCRKyP@OpenReview', this)" data="https://openreview.net/pdf?id=zf9zwCRKyP">[PDF<sup id="pdf-stars-zf9zwCRKyP@OpenReview">1</sup>]</a>
                <a id="copy-zf9zwCRKyP@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('zf9zwCRKyP@OpenReview')">[Copy]</a>
                <a id="kimi-zf9zwCRKyP@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('zf9zwCRKyP@OpenReview', this)">[Kimi<sup id="kimi-stars-zf9zwCRKyP@OpenReview">5</sup>]</a>
                <a id="rel-zf9zwCRKyP@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('zf9zwCRKyP@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-zf9zwCRKyP@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yangsibo Huang" target="_blank">Yangsibo Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Milad Nasr" target="_blank">Milad Nasr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anastasios Angelopoulos" target="_blank">Anastasios Angelopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Carlini" target="_blank">Nicholas Carlini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Lin Chiang" target="_blank">Wei-Lin Chiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christopher A. Choquette Choo" target="_blank">Christopher A. Choquette Choo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daphne Ippolito" target="_blank">Daphne Ippolito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthew Jagielski" target="_blank">Matthew Jagielski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Katherine Lee" target="_blank">Katherine Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ken Ziyu Liu" target="_blank">Ken Ziyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ion Stoica" target="_blank">Ion Stoica</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Tramer" target="_blank">Florian Tramer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chiyuan Zhang" target="_blank">Chiyuan Zhang</a>
            </p>
            <p id="summary-zf9zwCRKyP@OpenReview" class="summary">It is now common to evaluate Large Language Models (LLMs) by having humans manually vote to evaluate model outputs, in contrast to typical benchmarks that evaluate knowledge or skill at some particular task. Chatbot Arena, the most popular benchmark of this type, ranks models by asking users to select the better response between two randomly selected models (without revealing which model was responsible for the generations). These platforms are widely trusted as a fair and accurate measure of LLM capabilities. In this paper, we show that if bot protection and other defenses are not implemented, these voting-based benchmarks are potentially vulnerable to adversarial manipulation. Specifically, we show that an attacker can alter the leaderboard (to promote their favorite model or demote competitors) at the cost of roughly a thousand votes (verified in a simulated, offline version of Chatbot Arena). Our attack consists of two steps: first, we show how an attacker can determine which model was used to generate a given reply with more than <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;95&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-192" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.77em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-193"><span class="mn" id="MathJax-Span-194" style="font-family: MathJax_Main;">95</span><span class="mi" id="MathJax-Span-195" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>95</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-37">95\%</script> accuracy; and then, the attacker can use this information to consistently vote for (or against) a target model. Working with the Chatbot Arena developers, we identify, propose, and implement mitigations to improve the robustness of Chatbot Arena against adversarial manipulation, which, based on our analysis, substantially increases the cost of such attacks. Some of these defenses were present before our collaboration, such as bot protection with Cloudflare, malicious user detection, and rate limiting. Others, including reCAPTCHA and login are being integrated to strengthen the security in Chatbot Arena.</p>
            <p id="subjects-zf9zwCRKyP@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-zf9zwCRKyP@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-zf9zwCRKyP@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-zf9zwCRKyP@OpenReview" onclick="foldPdfKimi('zf9zwCRKyP@OpenReview', this)" class="hr hr-fold">
        </div><div id="2GmXJnyNM4@OpenReview" class="panel paper" keywords="tubal,tensor,implicit,descent,gradient,regularization,initialization,factorizations,overparametrized,factorization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2GmXJnyNM4" target="_blank" title="71/120"><span class="index notranslate">#71</span></a>
                <a id="title-2GmXJnyNM4@OpenReview" class="title-link" href="/venue/2GmXJnyNM4@OpenReview" target="_blank">Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent</a>
                <a id="pdf-2GmXJnyNM4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2GmXJnyNM4@OpenReview', this)" data="https://openreview.net/pdf?id=2GmXJnyNM4">[PDF<sup id="pdf-stars-2GmXJnyNM4@OpenReview"></sup>]</a>
                <a id="copy-2GmXJnyNM4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2GmXJnyNM4@OpenReview')">[Copy]</a>
                <a id="kimi-2GmXJnyNM4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2GmXJnyNM4@OpenReview', this)">[Kimi<sup id="kimi-stars-2GmXJnyNM4@OpenReview">7</sup>]</a>
                <a id="rel-2GmXJnyNM4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2GmXJnyNM4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2GmXJnyNM4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Santhosh Karnik" target="_blank">Santhosh Karnik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Veselovska" target="_blank">Anna Veselovska</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Iwen" target="_blank">Mark Iwen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Krahmer" target="_blank">Felix Krahmer</a>
            </p>
            <p id="summary-2GmXJnyNM4@OpenReview" class="summary">We provide a rigorous analysis of implicit regularization in an overparametrized tensor factorization problem beyond the lazy training regime. For matrix factorization problems, this phenomenon has been studied in a number of works. A particular challenge has been to design universal initialization strategies which provably lead to implicit regularization in gradient-descent methods. At the same time, it has been argued by Cohen et. al. 2016 that more general classes of neural networks can be captured by considering tensor factorizations. However, in the tensor case, implicit regularization has only been rigorously established for gradient flow or in the lazy training regime. In this paper, we prove the first tensor result of its kind for gradient descent rather than gradient flow. We focus on the tubal tensor product and the associated notion of low tubal rank, encouraged by the relevance of this model for image data. We establish that gradient descent in an overparametrized tensor factorization model with a small random initialization exhibits an implicit bias towards solutions of low tubal rank. Our theoretical findings are illustrated in an extensive set of numerical simulations show-casing the dynamics predicted by our theory as well as the crucial role of using a small random initialization.</p>
            <p id="subjects-2GmXJnyNM4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-2GmXJnyNM4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2GmXJnyNM4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2GmXJnyNM4@OpenReview" onclick="foldPdfKimi('2GmXJnyNM4@OpenReview', this)" class="hr hr-fold">
        </div><div id="4AmFA0qNQ2@OpenReview" class="panel paper" keywords="speech,spoken,long,speechssm,language,librispeech,form,minutes,extemporaneous,audio">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4AmFA0qNQ2" target="_blank" title="72/120"><span class="index notranslate">#72</span></a>
                <a id="title-4AmFA0qNQ2@OpenReview" class="title-link" href="/venue/4AmFA0qNQ2@OpenReview" target="_blank">Long-Form Speech Generation with Spoken Language Models</a>
                <a id="pdf-4AmFA0qNQ2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4AmFA0qNQ2@OpenReview', this)" data="https://openreview.net/pdf?id=4AmFA0qNQ2">[PDF<sup id="pdf-stars-4AmFA0qNQ2@OpenReview">6</sup>]</a>
                <a id="copy-4AmFA0qNQ2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4AmFA0qNQ2@OpenReview')">[Copy]</a>
                <a id="kimi-4AmFA0qNQ2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4AmFA0qNQ2@OpenReview', this)">[Kimi<sup id="kimi-stars-4AmFA0qNQ2@OpenReview">9</sup>]</a>
                <a id="rel-4AmFA0qNQ2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4AmFA0qNQ2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4AmFA0qNQ2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Se Jin Park" target="_blank">Se Jin Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julian Salazar" target="_blank">Julian Salazar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aren Jansen" target="_blank">Aren Jansen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keisuke Kinoshita" target="_blank">Keisuke Kinoshita</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Man Ro" target="_blank">Yong Man Ro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=RJ Skerry-Ryan" target="_blank">RJ Skerry-Ryan</a>
            </p>
            <p id="summary-4AmFA0qNQ2@OpenReview" class="summary">We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive **SpeechSSM**, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level.As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: **LibriSpeech-Long**, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.</p>
            <p id="subjects-4AmFA0qNQ2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-4AmFA0qNQ2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4AmFA0qNQ2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4AmFA0qNQ2@OpenReview" onclick="foldPdfKimi('4AmFA0qNQ2@OpenReview', this)" class="hr hr-fold">
        </div><div id="4ViG4gQD3i@OpenReview" class="panel paper" keywords="mlhca,queries,bidders,icas,elicit,auction,efficiency,powered,demand,marketdesignresearch">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4ViG4gQD3i" target="_blank" title="73/120"><span class="index notranslate">#73</span></a>
                <a id="title-4ViG4gQD3i@OpenReview" class="title-link" href="/venue/4ViG4gQD3i@OpenReview" target="_blank">Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them All</a>
                <a id="pdf-4ViG4gQD3i@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4ViG4gQD3i@OpenReview', this)" data="https://openreview.net/pdf?id=4ViG4gQD3i">[PDF<sup id="pdf-stars-4ViG4gQD3i@OpenReview"></sup>]</a>
                <a id="copy-4ViG4gQD3i@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4ViG4gQD3i@OpenReview')">[Copy]</a>
                <a id="kimi-4ViG4gQD3i@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4ViG4gQD3i@OpenReview', this)">[Kimi<sup id="kimi-stars-4ViG4gQD3i@OpenReview">4</sup>]</a>
                <a id="rel-4ViG4gQD3i@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4ViG4gQD3i@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4ViG4gQD3i@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ermis Soumalias" target="_blank">Ermis Soumalias</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Heiss" target="_blank">Jakob Heiss</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Weissteiner" target="_blank">Jakob Weissteiner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sven Seuken" target="_blank">Sven Seuken</a>
            </p>
            <p id="summary-4ViG4gQD3i@OpenReview" class="summary">We study the design of *iterative combinatorial auctions (ICAs)*.The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, recent work has proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most critical information from bidders to maximize efficiency.However, while the SOTA ML-based algorithms elicit bidders' preferences via *value queries*, ICAs that are used in practice elicit information via *demand queries*. In this paper, we introduce a novel ML algorithm that provably makes use of the full information from both value and demand queries, and we show via experiments that combining both query types results in significantly better learning performance in practice. Building on these insights, we present MLHCA, a new ML-powered auction that uses value and demand queries. MLHCA significantly outperforms the previous SOTA, reducing efficiency loss by up to a factor 10, with up to 58% fewer queries. Thus, MLHCA achieves large efficiency improvements while also reducing bidders' cognitive load, establishing a new benchmark for both practicability and efficiency. Our code is available at https://github.com/marketdesignresearch/MLHCA.</p>
            <p id="subjects-4ViG4gQD3i@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-4ViG4gQD3i@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4ViG4gQD3i@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4ViG4gQD3i@OpenReview" onclick="foldPdfKimi('4ViG4gQD3i@OpenReview', this)" class="hr hr-fold">
        </div><div id="7Tp9zjP9At@OpenReview" class="panel paper" keywords="discovery,mundinger,coloring,dream,2024a,problem,neural,colored,hadwiger,mathematics">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=7Tp9zjP9At" target="_blank" title="74/120"><span class="index notranslate">#74</span></a>
                <a id="title-7Tp9zjP9At@OpenReview" class="title-link" href="/venue/7Tp9zjP9At@OpenReview" target="_blank">Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?</a>
                <a id="pdf-7Tp9zjP9At@OpenReview" class="title-pdf notranslate" onclick="togglePdf('7Tp9zjP9At@OpenReview', this)" data="https://openreview.net/pdf?id=7Tp9zjP9At">[PDF<sup id="pdf-stars-7Tp9zjP9At@OpenReview">4</sup>]</a>
                <a id="copy-7Tp9zjP9At@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('7Tp9zjP9At@OpenReview')">[Copy]</a>
                <a id="kimi-7Tp9zjP9At@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('7Tp9zjP9At@OpenReview', this)">[Kimi<sup id="kimi-stars-7Tp9zjP9At@OpenReview">9</sup>]</a>
                <a id="rel-7Tp9zjP9At@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('7Tp9zjP9At@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-7Tp9zjP9At@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Konrad Mundinger" target="_blank">Konrad Mundinger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Max Zimmer" target="_blank">Max Zimmer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aldo Kiem" target="_blank">Aldo Kiem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Spiegel" target="_blank">Christoph Spiegel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Pokutta" target="_blank">Sebastian Pokutta</a>
            </p>
            <p id="summary-7Tp9zjP9At@OpenReview" class="summary">We demonstrate how neural networks can drive mathematical discovery through a case study of the Hadwiger-Nelson problem, a long-standing open problem at the intersection of discrete geometry and extremal combinatorics that is concerned with coloring the plane while avoiding monochromatic unit-distance pairs. Using neural networks as approximators, we reformulate this mixed discrete-continuous geometric coloring problem with hard constraints as an optimization task with a probabilistic, differentiable loss function. This enables gradient-based exploration of admissible configurations that most significantly led to the discovery of two novel six-colorings, providing the first improvement in thirty years to the off-diagonal variant of the original problem (Mundinger et al., 2024a). Here, we establish the underlying machine learning approach used to obtain these results and demonstrate its broader applicability through additional numerical insights.</p>
            <p id="subjects-7Tp9zjP9At@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-7Tp9zjP9At@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-7Tp9zjP9At@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-7Tp9zjP9At@OpenReview" onclick="foldPdfKimi('7Tp9zjP9At@OpenReview', this)" class="hr hr-fold">
        </div><div id="9NmdppWbXi@OpenReview" class="panel paper" keywords="phenomena,research,deep,position,progress,learning,prominent,broader,grokking,value">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9NmdppWbXi" target="_blank" title="75/120"><span class="index notranslate">#75</span></a>
                <a id="title-9NmdppWbXi@OpenReview" class="title-link" href="/venue/9NmdppWbXi@OpenReview" target="_blank">Position: A Critical Perspective on The Value in Studying Deep Learning Phenomena</a>
                <a id="pdf-9NmdppWbXi@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9NmdppWbXi@OpenReview', this)" data="https://openreview.net/pdf?id=9NmdppWbXi">[PDF<sup id="pdf-stars-9NmdppWbXi@OpenReview">9</sup>]</a>
                <a id="copy-9NmdppWbXi@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9NmdppWbXi@OpenReview')">[Copy]</a>
                <a id="kimi-9NmdppWbXi@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9NmdppWbXi@OpenReview', this)">[Kimi<sup id="kimi-stars-9NmdppWbXi@OpenReview">10</sup>]</a>
                <a id="rel-9NmdppWbXi@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9NmdppWbXi@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9NmdppWbXi@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Jeffares" target="_blank">Alan Jeffares</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=M van der Schaar" target="_blank">M van der Schaar</a>
            </p>
            <p id="summary-9NmdppWbXi@OpenReview" class="summary">Developing a better understanding of surprising or counterintuitive phenomena has constituted a significant portion of deep learning research in recent years. These include double descent, grokking, and the lottery ticket hypothesis -- among many others. Works in this area often develop *ad hoc hypotheses* attempting to explain these observed phenomena on an isolated, case-by-case basis. This position paper asserts that, in many prominent cases, there is little evidence to suggest that these phenomena appear in real-world applications and these efforts may be inefficient in driving progress in the broader field. Consequently, we argue against viewing them as isolated puzzles that require bespoke resolutions or explanations. However, despite this, we suggest that deep learning phenomena *do* still offer research value by providing unique settings in which we can refine our *broad explanatory theories* of more general deep learning principles. This position is reinforced by analyzing the research outcomes of several prominent examples of these phenomena from the recent literature. We revisit the current norms in the research community in approaching these problems and propose practical recommendations for future research, aiming to ensure that progress on deep learning phenomena is well aligned with the ultimate pragmatic goal of progress in the broader field of deep learning.</p>
            <p id="subjects-9NmdppWbXi@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-9NmdppWbXi@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9NmdppWbXi@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9NmdppWbXi@OpenReview" onclick="foldPdfKimi('9NmdppWbXi@OpenReview', this)" class="hr hr-fold">
        </div><div id="ACyyBrUioy@OpenReview" class="panel paper" keywords="trees,decision,split,near,optimal,interpretable,scalability,greediness,rashomon,ideal">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ACyyBrUioy" target="_blank" title="76/120"><span class="index notranslate">#76</span></a>
                <a id="title-ACyyBrUioy@OpenReview" class="title-link" href="/venue/ACyyBrUioy@OpenReview" target="_blank">Near-Optimal Decision Trees in a SPLIT Second</a>
                <a id="pdf-ACyyBrUioy@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ACyyBrUioy@OpenReview', this)" data="https://openreview.net/pdf?id=ACyyBrUioy">[PDF<sup id="pdf-stars-ACyyBrUioy@OpenReview">1</sup>]</a>
                <a id="copy-ACyyBrUioy@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ACyyBrUioy@OpenReview')">[Copy]</a>
                <a id="kimi-ACyyBrUioy@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ACyyBrUioy@OpenReview', this)">[Kimi<sup id="kimi-stars-ACyyBrUioy@OpenReview">3</sup>]</a>
                <a id="rel-ACyyBrUioy@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ACyyBrUioy@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ACyyBrUioy@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Varun Babbar" target="_blank">Varun Babbar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hayden McTavish" target="_blank">Hayden McTavish</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cynthia Rudin" target="_blank">Cynthia Rudin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Margo Seltzer" target="_blank">Margo Seltzer</a>
            </p>
            <p id="summary-ACyyBrUioy@OpenReview" class="summary">Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set).</p>
            <p id="subjects-ACyyBrUioy@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-ACyyBrUioy@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ACyyBrUioy@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ACyyBrUioy@OpenReview" onclick="foldPdfKimi('ACyyBrUioy@OpenReview', this)" class="hr hr-fold">
        </div><div id="qMt4KikFJg@OpenReview" class="panel paper" keywords="rényi,processes,nps,prior,rnps,posterior,divergence,rnp,neural,dampening">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=qMt4KikFJg" target="_blank" title="77/120"><span class="index notranslate">#77</span></a>
                <a id="title-qMt4KikFJg@OpenReview" class="title-link" href="/venue/qMt4KikFJg@OpenReview" target="_blank">Rényi Neural Processes</a>
                <a id="pdf-qMt4KikFJg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('qMt4KikFJg@OpenReview', this)" data="https://openreview.net/pdf?id=qMt4KikFJg">[PDF<sup id="pdf-stars-qMt4KikFJg@OpenReview">6</sup>]</a>
                <a id="copy-qMt4KikFJg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('qMt4KikFJg@OpenReview')">[Copy]</a>
                <a id="kimi-qMt4KikFJg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('qMt4KikFJg@OpenReview', this)">[Kimi<sup id="kimi-stars-qMt4KikFJg@OpenReview">10</sup>]</a>
                <a id="rel-qMt4KikFJg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('qMt4KikFJg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-qMt4KikFJg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xuesong Wang" target="_blank">Xuesong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Zhao" target="_blank">He Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edwin V. Bonilla" target="_blank">Edwin V. Bonilla</a>
            </p>
            <p id="summary-qMt4KikFJg@OpenReview" class="summary">Neural Processes (NPs) are deep probabilistic models that represent stochastic processes by conditioning their prior distributions on a set of context points. Despite their advantages in uncertainty estimation for complex distributions, NPs enforce parameterization coupling between the conditional prior model and the posterior model. We show that this coupling amounts to prior misspecification and revisit the NP objective to address this issue. More specifically, we propose Rényi Neural Processes (RNP), a method that replaces the standard KL divergence with the Rényi divergence, dampening the effects of the misspecified prior during posterior updates. We validate our approach across multiple benchmarks including regression and image inpainting tasks, and show significant performance improvements of RNPs in real-world problems. Our extensive experiments show consistently better log-likelihoods over state-of-the-art NP models.</p>
            <p id="subjects-qMt4KikFJg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-qMt4KikFJg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-qMt4KikFJg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-qMt4KikFJg@OpenReview" onclick="foldPdfKimi('qMt4KikFJg@OpenReview', this)" class="hr hr-fold">
        </div><div id="EZV4edMGM1@OpenReview" class="panel paper" keywords="rcn,labels,multiclass,classification,mlc,separation,achieving,super,noise,query">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=EZV4edMGM1" target="_blank" title="78/120"><span class="index notranslate">#78</span></a>
                <a id="title-EZV4edMGM1@OpenReview" class="title-link" href="/venue/EZV4edMGM1@OpenReview" target="_blank">Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise</a>
                <a id="pdf-EZV4edMGM1@OpenReview" class="title-pdf notranslate" onclick="togglePdf('EZV4edMGM1@OpenReview', this)" data="https://openreview.net/pdf?id=EZV4edMGM1">[PDF<sup id="pdf-stars-EZV4edMGM1@OpenReview">2</sup>]</a>
                <a id="copy-EZV4edMGM1@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('EZV4edMGM1@OpenReview')">[Copy]</a>
                <a id="kimi-EZV4edMGM1@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('EZV4edMGM1@OpenReview', this)">[Kimi<sup id="kimi-stars-EZV4edMGM1@OpenReview">6</sup>]</a>
                <a id="rel-EZV4edMGM1@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('EZV4edMGM1@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-EZV4edMGM1@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ilias Diakonikolas" target="_blank">Ilias Diakonikolas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingchen Ma" target="_blank">Mingchen Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lisheng Ren" target="_blank">Lisheng Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christos Tzamos" target="_blank">Christos Tzamos</a>
            </p>
            <p id="summary-EZV4edMGM1@OpenReview" class="summary">We study the task of Multiclass Linear Classification (MLC) in the distribution-free PAC model with Random Classification Noise (RCN). Specifically, the learner is given a set of labeled examples <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-196" style="width: 2.763em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1002.19em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-197"><span class="mo" id="MathJax-Span-198" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-199" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-200" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-201" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-202" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-38">(x, y)</script>, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-203" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.52em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-204"><span class="mi" id="MathJax-Span-205" style="font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-39">x</script> is drawn from an unknown distribution on <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-206" style="width: 1.461em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1001.2em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-207"><span class="msubsup" id="MathJax-Span-208"><span style="display: inline-block; position: relative; width: 1.201em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.78em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-209" style="font-family: MathJax_Math-italic;">R</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="mi" id="MathJax-Span-210" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mi>d</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-40">R^d</script> and the labels are generated by a multiclass linear classifier corrupted with RCN. That is, the label <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-211" style="width: 0.576em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-212"><span class="mi" id="MathJax-Span-213" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></span></span><script type="math/tex" id="MathJax-Element-41">y</script> is flipped from <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-42-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-214" style="width: 0.471em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.32em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-215"><span class="mi" id="MathJax-Span-216" style="font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-42">i</script> to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-43-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-217" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.42em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-218"><span class="mi" id="MathJax-Span-219" style="font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math></span></span><script type="math/tex" id="MathJax-Element-43">j</script> with probability <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-44-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-220" style="width: 1.721em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.41em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-221"><span class="msubsup" id="MathJax-Span-222"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-223" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.836em;"><span class="texatom" id="MathJax-Span-224"><span class="mrow" id="MathJax-Span-225"><span class="mi" id="MathJax-Span-226" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-227" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-44">H_{ij}</script> according to a known noise matrix <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-45-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-228" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-229"><span class="mi" id="MathJax-Span-230" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></span></span><script type="math/tex" id="MathJax-Element-45">H</script> with non-negative separation <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-46-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo&gt;:=&lt;/mo&gt;&lt;munder&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&amp;#x2260;&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-231" style="width: 10.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.117em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1009.12em, 2.607em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-232"><span class="mi" id="MathJax-Span-233" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-234" style="font-family: MathJax_Main; padding-left: 0.263em;">:<span style="font-family: MathJax_Main;">=</span></span><span class="munderover" id="MathJax-Span-235" style="padding-left: 0.263em;"><span style="display: inline-block; position: relative; width: 2.815em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.67em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mo" id="MathJax-Span-236" style="font-family: MathJax_Main;">min</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 1.669em;"><span class="texatom" id="MathJax-Span-237"><span class="mrow" id="MathJax-Span-238"><span class="mi" id="MathJax-Span-239" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-240" style="font-size: 70.7%; font-family: MathJax_Main;">≠</span><span class="mi" id="MathJax-Span-241" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-242" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.357em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-243" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.836em;"><span class="texatom" id="MathJax-Span-244"><span class="mrow" id="MathJax-Span-245"><span class="mi" id="MathJax-Span-246" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-247" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-248" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="msubsup" id="MathJax-Span-249" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 1.409em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.89em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-250" style="font-family: MathJax_Math-italic;">H<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.836em;"><span class="texatom" id="MathJax-Span-251"><span class="mrow" id="MathJax-Span-252"><span class="mi" id="MathJax-Span-253" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-254" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi><mo>:=</mo><munder><mo movablelimits="true" form="prefix">min</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></munder><msub><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>i</mi></mrow></msub><mo>−</mo><msub><mi>H</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-46">\sigma: = \min_{i \neq j} H_{ii}-H_{ij}</script>. The goal is to compute a hypothesis with small 0-1 error. For the special case of two labels, prior work has given polynomial-time algorithms achieving the optimal error. Surprisingly, little is known about the complexity of this task even for three labels.As our main contribution, we show that the complexity of MLC with RCN becomes drastically different in the presence of three or more labels. Specifically, we prove super-polynomialStatistical Query (SQ) lower bounds for this problem. In more detail, even for three labels and constant separation, we give a super-polynomial lower bound on the complexity of any SQ algorithm achieving optimal error. For a larger number of labels and smaller separation, we show a super-polynomial SQ lower bound even for the weaker goal of achieving any constant factor approximation to the optimal loss or even beating the trivial hypothesis.</p>
            <p id="subjects-EZV4edMGM1@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-EZV4edMGM1@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-EZV4edMGM1@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-EZV4edMGM1@OpenReview" onclick="foldPdfKimi('EZV4edMGM1@OpenReview', this)" class="hr hr-fold">
        </div><div id="yDTwamN4LQ@OpenReview" class="panel paper" keywords="signature,expected,generating,estimator,data,lower,signatures,leveraged,martingale,streams">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=yDTwamN4LQ" target="_blank" title="79/120"><span class="index notranslate">#79</span></a>
                <a id="title-yDTwamN4LQ@OpenReview" class="title-link" href="/venue/yDTwamN4LQ@OpenReview" target="_blank">Learning with Expected Signatures: Theory and Applications</a>
                <a id="pdf-yDTwamN4LQ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('yDTwamN4LQ@OpenReview', this)" data="https://openreview.net/pdf?id=yDTwamN4LQ">[PDF<sup id="pdf-stars-yDTwamN4LQ@OpenReview">1</sup>]</a>
                <a id="copy-yDTwamN4LQ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('yDTwamN4LQ@OpenReview')">[Copy]</a>
                <a id="kimi-yDTwamN4LQ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('yDTwamN4LQ@OpenReview', this)">[Kimi<sup id="kimi-stars-yDTwamN4LQ@OpenReview">11</sup>]</a>
                <a id="rel-yDTwamN4LQ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('yDTwamN4LQ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-yDTwamN4LQ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Lucchese" target="_blank">Lorenzo Lucchese</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mikko S. Pakkanen" target="_blank">Mikko S. Pakkanen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Almut E. D. Veraart" target="_blank">Almut E. D. Veraart</a>
            </p>
            <p id="summary-yDTwamN4LQ@OpenReview" class="summary">The expected signature maps a collection of data streams to a lower dimensional representation, with a remarkable property: the resulting feature tensor can fully characterize the data generating distribution. This "model-free"' embedding has been successfully leveraged to build multiple domain-agnostic machine learning (ML) algorithms for time series and sequential data. The convergence results proved in this paper bridge the gap between the expected signature's empirical discrete-time estimator and its theoretical continuous-time value, allowing for a more complete probabilistic interpretation of expected signature-based ML methods. Moreover, when the data generating process is a martingale, we suggest a simple modification of the expected signature estimator with significantly lower mean squared error and empirically demonstrate how it can be effectively applied to improve predictive performance.</p>
            <p id="subjects-yDTwamN4LQ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-yDTwamN4LQ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-yDTwamN4LQ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-yDTwamN4LQ@OpenReview" onclick="foldPdfKimi('yDTwamN4LQ@OpenReview', this)" class="hr hr-fold">
        </div><div id="l19DmXbwPK@OpenReview" class="panel paper" keywords="versaprm,prms,prm,reasoning,reward,mathematical,voting,synthetic,domains,majority">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=l19DmXbwPK" target="_blank" title="80/120"><span class="index notranslate">#80</span></a>
                <a id="title-l19DmXbwPK@OpenReview" class="title-link" href="/venue/l19DmXbwPK@OpenReview" target="_blank">VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data</a>
                <a id="pdf-l19DmXbwPK@OpenReview" class="title-pdf notranslate" onclick="togglePdf('l19DmXbwPK@OpenReview', this)" data="https://openreview.net/pdf?id=l19DmXbwPK">[PDF<sup id="pdf-stars-l19DmXbwPK@OpenReview">10</sup>]</a>
                <a id="copy-l19DmXbwPK@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('l19DmXbwPK@OpenReview')">[Copy]</a>
                <a id="kimi-l19DmXbwPK@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('l19DmXbwPK@OpenReview', this)">[Kimi<sup id="kimi-stars-l19DmXbwPK@OpenReview">14</sup>]</a>
                <a id="rel-l19DmXbwPK@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('l19DmXbwPK@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-l19DmXbwPK@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Zeng" target="_blank">Thomas Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuibai Zhang" target="_blank">Shuibai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shutong Wu" target="_blank">Shutong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Classen" target="_blank">Christian Classen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daewon Chae" target="_blank">Daewon Chae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ethan Ewer" target="_blank">Ethan Ewer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minjae Lee" target="_blank">Minjae Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heeju Kim" target="_blank">Heeju Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wonjun Kang" target="_blank">Wonjun Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jackson Kunde" target="_blank">Jackson Kunde</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Fan" target="_blank">Ying Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jungtaek Kim" target="_blank">Jungtaek Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=HYUNG IL KOO" target="_blank">HYUNG IL KOO</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kannan Ramchandran" target="_blank">Kannan Ramchandran</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitris Papailiopoulos" target="_blank">Dimitris Papailiopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangwook Lee" target="_blank">Kangwook Lee</a>
            </p>
            <p id="summary-l19DmXbwPK@OpenReview" class="summary">Process Reward Models (PRMs) have proven effective at enhancing mathematical reasoning for Large Language Models (LLMs) by leveraging increased inference-time computation. However, they are predominantly trained on mathematical data and their generalizability to non-mathematical domains has not been rigorously studied. In response, this work first shows that current PRMs have poor performance in other domains. To address this limitation, we introduce ***VersaPRM***, a multi-domain PRM trained on synthetic reasoning data generated using our novel data generation and annotation method. VersaPRM achieves consistent performance gains across diverse domains. For instance, in the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a 7.9% performance gain over the majority voting baseline–surpassing Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by open-sourcing all data, code and models for VersaPRM.</p>
            <p id="subjects-l19DmXbwPK@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-l19DmXbwPK@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-l19DmXbwPK@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-l19DmXbwPK@OpenReview" onclick="foldPdfKimi('l19DmXbwPK@OpenReview', this)" class="hr hr-fold">
        </div><div id="KwIlvmLDLm@OpenReview" class="panel paper" keywords="lora,gradient,one,adapters,step,full,language,tuning,suffice,theory">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=KwIlvmLDLm" target="_blank" title="81/120"><span class="index notranslate">#81</span></a>
                <a id="title-KwIlvmLDLm@OpenReview" class="title-link" href="/venue/KwIlvmLDLm@OpenReview" target="_blank">LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</a>
                <a id="pdf-KwIlvmLDLm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('KwIlvmLDLm@OpenReview', this)" data="https://openreview.net/pdf?id=KwIlvmLDLm">[PDF<sup id="pdf-stars-KwIlvmLDLm@OpenReview">22</sup>]</a>
                <a id="copy-KwIlvmLDLm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('KwIlvmLDLm@OpenReview')">[Copy]</a>
                <a id="kimi-KwIlvmLDLm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('KwIlvmLDLm@OpenReview', this)">[Kimi<sup id="kimi-stars-KwIlvmLDLm@OpenReview">20</sup>]</a>
                <a id="rel-KwIlvmLDLm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('KwIlvmLDLm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-KwIlvmLDLm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanhe Zhang" target="_blank">Yuanhe Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fanghui Liu" target="_blank">Fanghui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yudong Chen" target="_blank">Yudong Chen</a>
            </p>
            <p id="summary-KwIlvmLDLm@OpenReview" class="summary">This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA) (Hu et al., 2022) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately—applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation. Code is available at: https://github.com/YuanheZ/LoRA-One.</p>
            <p id="subjects-KwIlvmLDLm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-KwIlvmLDLm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-KwIlvmLDLm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-KwIlvmLDLm@OpenReview" onclick="foldPdfKimi('KwIlvmLDLm@OpenReview', this)" class="hr hr-fold">
        </div><div id="LwQGRGJTHw@OpenReview" class="panel paper" keywords="random,bias,inversion,sketches,corrected,projections,sampled,newton,sampling,sub">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=LwQGRGJTHw" target="_blank" title="82/120"><span class="index notranslate">#82</span></a>
                <a id="title-LwQGRGJTHw@OpenReview" class="title-link" href="/venue/LwQGRGJTHw@OpenReview" target="_blank">Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton</a>
                <a id="pdf-LwQGRGJTHw@OpenReview" class="title-pdf notranslate" onclick="togglePdf('LwQGRGJTHw@OpenReview', this)" data="https://openreview.net/pdf?id=LwQGRGJTHw">[PDF<sup id="pdf-stars-LwQGRGJTHw@OpenReview">5</sup>]</a>
                <a id="copy-LwQGRGJTHw@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('LwQGRGJTHw@OpenReview')">[Copy]</a>
                <a id="kimi-LwQGRGJTHw@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('LwQGRGJTHw@OpenReview', this)">[Kimi<sup id="kimi-stars-LwQGRGJTHw@OpenReview">4</sup>]</a>
                <a id="rel-LwQGRGJTHw@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('LwQGRGJTHw@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-LwQGRGJTHw@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chengmei Niu" target="_blank">Chengmei Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Liao" target="_blank">Zhenyu Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zenan Ling" target="_blank">Zenan Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Mahoney" target="_blank">Michael Mahoney</a>
            </p>
            <p id="summary-LwQGRGJTHw@OpenReview" class="summary">A substantial body of work in machine learning (ML) and randomized numerical linear algebra (RandNLA) has exploited various sorts of random sketching methodologies, including random sampling and random projection, with much of the analysis using Johnson--Lindenstrauss and subspace embedding techniques. Recent studies have identified the issue of *inversion bias* -- the phenomenon that inverses of random sketches are *not* unbiased, despite the unbiasedness of the sketches themselves. This bias presents challenges for the use of random sketches in various ML pipelines, such as fast stochastic optimization, scalable statistical estimators, and distributed optimization. In the context of random projection, the inversion bias can be easily corrected for dense Gaussian projections (which are, however, too expensive for many applications). Recent work has shown how the inversion bias can be corrected for sparse sub-gaussian projections. In this paper, we show how the inversion bias can be corrected for random sampling methods, both uniform and non-uniform leverage-based, as well as for structured random projections, including those based on the Hadamard transform. Using these results, we establish problem-independent local convergence rates for sub-sampled Newton methods.</p>
            <p id="subjects-LwQGRGJTHw@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-LwQGRGJTHw@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-LwQGRGJTHw@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-LwQGRGJTHw@OpenReview" onclick="foldPdfKimi('LwQGRGJTHw@OpenReview', this)" class="hr hr-fold">
        </div><div id="FJKnru1xUF@OpenReview" class="panel paper" keywords="defenses,autoadvexbench,ctf,adversarial,sonnet,opus,world,llms,real,spylab">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=FJKnru1xUF" target="_blank" title="83/120"><span class="index notranslate">#83</span></a>
                <a id="title-FJKnru1xUF@OpenReview" class="title-link" href="/venue/FJKnru1xUF@OpenReview" target="_blank">AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses</a>
                <a id="pdf-FJKnru1xUF@OpenReview" class="title-pdf notranslate" onclick="togglePdf('FJKnru1xUF@OpenReview', this)" data="https://openreview.net/pdf?id=FJKnru1xUF">[PDF<sup id="pdf-stars-FJKnru1xUF@OpenReview">1</sup>]</a>
                <a id="copy-FJKnru1xUF@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('FJKnru1xUF@OpenReview')">[Copy]</a>
                <a id="kimi-FJKnru1xUF@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('FJKnru1xUF@OpenReview', this)">[Kimi<sup id="kimi-stars-FJKnru1xUF@OpenReview">3</sup>]</a>
                <a id="rel-FJKnru1xUF@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('FJKnru1xUF@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-FJKnru1xUF@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Carlini" target="_blank">Nicholas Carlini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edoardo Debenedetti" target="_blank">Edoardo Debenedetti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Javier Rando" target="_blank">Javier Rando</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Milad Nasr" target="_blank">Milad Nasr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Florian Tramer" target="_blank">Florian Tramer</a>
            </p>
            <p id="summary-FJKnru1xUF@OpenReview" class="summary">We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. Unlike existing security benchmarks that often serve as proxies for real-world tasks, AutoAdvExBench directly measures LLMs' success on tasks regularly performed by machine learning security experts. This approach offers a significant advantage: if a LLM could solve the challenges presented in AutoAdvExBench, it would immediately present practical utility for adversarial machine learning researchers. While our strongest ensemble of agents can break 87% of CTF-like ("homework exercise") adversarial example defenses, they break just 37% of real-world defenses, indicating a large gap between difficulty in attacking "real" code, and CTF-like code. Moreover, LLMs that are good at CTFs are not always good at real-world defenses; for example, Claude Sonnet 3.5 has a nearly identical attack success rate to Opus 4 on the CTF-like defenses (75% vs 79%), but the on the real-world defenses Sonnet 3.5 breaks just 13% of defenses compared to Opus 4's 30%. We make this benchmark available at https://github.com/ethz-spylab/AutoAdvExBench.</p>
            <p id="subjects-FJKnru1xUF@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-FJKnru1xUF@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-FJKnru1xUF@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-FJKnru1xUF@OpenReview" onclick="foldPdfKimi('FJKnru1xUF@OpenReview', this)" class="hr hr-fold">
        </div><div id="OEl3L8osas@OpenReview" class="panel paper" keywords="forces,conservative,conservation,energy,force,atomistic,machine,assessing,questioned,afforded">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=OEl3L8osas" target="_blank" title="84/120"><span class="index notranslate">#84</span></a>
                <a id="title-OEl3L8osas@OpenReview" class="title-link" href="/venue/OEl3L8osas@OpenReview" target="_blank">The dark side of the forces: assessing non-conservative force models for atomistic machine learning</a>
                <a id="pdf-OEl3L8osas@OpenReview" class="title-pdf notranslate" onclick="togglePdf('OEl3L8osas@OpenReview', this)" data="https://openreview.net/pdf?id=OEl3L8osas">[PDF<sup id="pdf-stars-OEl3L8osas@OpenReview">4</sup>]</a>
                <a id="copy-OEl3L8osas@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('OEl3L8osas@OpenReview')">[Copy]</a>
                <a id="kimi-OEl3L8osas@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('OEl3L8osas@OpenReview', this)">[Kimi<sup id="kimi-stars-OEl3L8osas@OpenReview">5</sup>]</a>
                <a id="rel-OEl3L8osas@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('OEl3L8osas@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-OEl3L8osas@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Filippo Bigi" target="_blank">Filippo Bigi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcel Langer" target="_blank">Marcel Langer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michele Ceriotti" target="_blank">Michele Ceriotti</a>
            </p>
            <p id="summary-OEl3L8osas@OpenReview" class="summary">The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, have revolutionized the fields of computational chemistry and materials discovery.In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency -- and that energy conservation can be learned during training.This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics.Contrary to the case of rotational symmetry, energy conservation is hard to learn, monitor, and correct for.The best approach to exploit the acceleration afforded by direct force prediction might be to use it in tandem with a conservative model, reducing -- rather than eliminating -- the additional cost of backpropagation, but avoiding the pathological behavior associated with non-conservative forces.</p>
            <p id="subjects-OEl3L8osas@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-OEl3L8osas@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-OEl3L8osas@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-OEl3L8osas@OpenReview" onclick="foldPdfKimi('OEl3L8osas@OpenReview', this)" class="hr hr-fold">
        </div><div id="ybno0ZP44z@OpenReview" class="panel paper" keywords="regret,variance,mvr,rkhs,bandit,reward,noiseless,upper,bound,noise">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ybno0ZP44z" target="_blank" title="85/120"><span class="index notranslate">#85</span></a>
                <a id="title-ybno0ZP44z@OpenReview" class="title-link" href="/venue/ybno0ZP44z@OpenReview" target="_blank">Improved Regret Analysis in Gaussian Process Bandits: Optimality for Noiseless Reward, RKHS norm, and Non-Stationary Variance</a>
                <a id="pdf-ybno0ZP44z@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ybno0ZP44z@OpenReview', this)" data="https://openreview.net/pdf?id=ybno0ZP44z">[PDF<sup id="pdf-stars-ybno0ZP44z@OpenReview"></sup>]</a>
                <a id="copy-ybno0ZP44z@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ybno0ZP44z@OpenReview')">[Copy]</a>
                <a id="kimi-ybno0ZP44z@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ybno0ZP44z@OpenReview', this)">[Kimi<sup id="kimi-stars-ybno0ZP44z@OpenReview">5</sup>]</a>
                <a id="rel-ybno0ZP44z@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ybno0ZP44z@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ybno0ZP44z@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shogo Iwazaki" target="_blank">Shogo Iwazaki</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shion Takeno" target="_blank">Shion Takeno</a>
            </p>
            <p id="summary-ybno0ZP44z@OpenReview" class="summary">We study the Gaussian process (GP) bandit problem, whose goal is to minimize regret under an unknown reward function lying in some reproducing kernel Hilbert space (RKHS). The maximum posterior variance analysis is vital in analyzing near-optimal GP bandit algorithms such as maximum variance reduction (MVR) and phased elimination (PE).Therefore, we first show the new upper bound of the maximum posterior variance, which improves the dependence of the noise variance parameters of the GP. By leveraging this result, we refine the MVR and PE to obtain (i) a nearly optimal regret upper bound in the noiseless setting and (ii) regret upper bounds that are optimal with respect to the RKHS norm of the reward function. Furthermore, as another application of our proposed bound, we analyze the GP bandit under the time-varying noise variance setting, which is the kernelized extension of the linear bandit with heteroscedastic noise. For this problem, we show that MVR and PE-based algorithms achieve noise variance-dependent regret upper bounds, which matches our regret lower bound.</p>
            <p id="subjects-ybno0ZP44z@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-ybno0ZP44z@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ybno0ZP44z@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ybno0ZP44z@OpenReview" onclick="foldPdfKimi('ybno0ZP44z@OpenReview', this)" class="hr hr-fold">
        </div><div id="46yLEXtav4@OpenReview" class="panel paper" keywords="collectives,platforms,collusion,algorithms,submission,impact,need,develop,implementable,interests">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=46yLEXtav4" target="_blank" title="86/120"><span class="index notranslate">#86</span></a>
                <a id="title-46yLEXtav4@OpenReview" class="title-link" href="/venue/46yLEXtav4@OpenReview" target="_blank">Statistical Collusion by Collectives on Learning Platforms</a>
                <a id="pdf-46yLEXtav4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('46yLEXtav4@OpenReview', this)" data="https://openreview.net/pdf?id=46yLEXtav4">[PDF<sup id="pdf-stars-46yLEXtav4@OpenReview"></sup>]</a>
                <a id="copy-46yLEXtav4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('46yLEXtav4@OpenReview')">[Copy]</a>
                <a id="kimi-46yLEXtav4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('46yLEXtav4@OpenReview', this)">[Kimi<sup id="kimi-stars-46yLEXtav4@OpenReview">5</sup>]</a>
                <a id="rel-46yLEXtav4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('46yLEXtav4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-46yLEXtav4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Etienne Gauthier" target="_blank">Etienne Gauthier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francis Bach" target="_blank">Francis Bach</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Jordan" target="_blank">Michael Jordan</a>
            </p>
            <p id="summary-46yLEXtav4@OpenReview" class="summary">As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain.</p>
            <p id="subjects-46yLEXtav4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-46yLEXtav4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-46yLEXtav4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-46yLEXtav4@OpenReview" onclick="foldPdfKimi('46yLEXtav4@OpenReview', this)" class="hr hr-fold">
        </div><div id="o9zDYV4Ism@OpenReview" class="panel paper" keywords="lora,rank,linearization,setups,loudly,global,arguments,training,converges,minima">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=o9zDYV4Ism" target="_blank" title="87/120"><span class="index notranslate">#87</span></a>
                <a id="title-o9zDYV4Ism@OpenReview" class="title-link" href="/venue/o9zDYV4Ism@OpenReview" target="_blank">LoRA Training Provably Converges to a Low-Rank Global Minimum Or It Fails Loudly (But it Probably Won't Fail)</a>
                <a id="pdf-o9zDYV4Ism@OpenReview" class="title-pdf notranslate" onclick="togglePdf('o9zDYV4Ism@OpenReview', this)" data="https://openreview.net/pdf?id=o9zDYV4Ism">[PDF<sup id="pdf-stars-o9zDYV4Ism@OpenReview">14</sup>]</a>
                <a id="copy-o9zDYV4Ism@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('o9zDYV4Ism@OpenReview')">[Copy]</a>
                <a id="kimi-o9zDYV4Ism@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('o9zDYV4Ism@OpenReview', this)">[Kimi<sup id="kimi-stars-o9zDYV4Ism@OpenReview">8</sup>]</a>
                <a id="rel-o9zDYV4Ism@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('o9zDYV4Ism@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-o9zDYV4Ism@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junsu Kim" target="_blank">Junsu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaeyeon Kim" target="_blank">Jaeyeon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ernest Ryu" target="_blank">Ernest Ryu</a>
            </p>
            <p id="summary-o9zDYV4Ism@OpenReview" class="summary">Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a "special regime", which includes idealized setups where linearization arguments hold, and a "generic regime" representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space—where global minima lie—thus shedding light on why LoRA training usually succeeds in finding global minima.</p>
            <p id="subjects-o9zDYV4Ism@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-o9zDYV4Ism@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-o9zDYV4Ism@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-o9zDYV4Ism@OpenReview" onclick="foldPdfKimi('o9zDYV4Ism@OpenReview', this)" class="hr hr-fold">
        </div><div id="xZXhFg43EI@OpenReview" class="panel paper" keywords="lancer,swe,freelance,engineering,tasks,managerial,frontier,software,earn,million">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=xZXhFg43EI" target="_blank" title="88/120"><span class="index notranslate">#88</span></a>
                <a id="title-xZXhFg43EI@OpenReview" class="title-link" href="/venue/xZXhFg43EI@OpenReview" target="_blank">SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</a>
                <a id="pdf-xZXhFg43EI@OpenReview" class="title-pdf notranslate" onclick="togglePdf('xZXhFg43EI@OpenReview', this)" data="https://openreview.net/pdf?id=xZXhFg43EI">[PDF<sup id="pdf-stars-xZXhFg43EI@OpenReview">4</sup>]</a>
                <a id="copy-xZXhFg43EI@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('xZXhFg43EI@OpenReview')">[Copy]</a>
                <a id="kimi-xZXhFg43EI@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('xZXhFg43EI@OpenReview', this)">[Kimi<sup id="kimi-stars-xZXhFg43EI@OpenReview">12</sup>]</a>
                <a id="rel-xZXhFg43EI@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('xZXhFg43EI@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-xZXhFg43EI@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Miserendino" target="_blank">Samuel Miserendino</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michele Wang" target="_blank">Michele Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tejal Patwardhan" target="_blank">Tejal Patwardhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johannes Heidecke" target="_blank">Johannes Heidecke</a>
            </p>
            <p id="summary-xZXhFg43EI@OpenReview" class="summary">We introduce SWE-Lancer, a benchmark of over 1400 freelance software engineering tasks from Upwork, valued at <span>$</span>1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks — ranging from <span>$</span>50 bug fixes to <span>$</span>32000 feature implementations — and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split. By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.</p>
            <p id="subjects-xZXhFg43EI@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-xZXhFg43EI@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-xZXhFg43EI@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-xZXhFg43EI@OpenReview" onclick="foldPdfKimi('xZXhFg43EI@OpenReview', this)" class="hr hr-fold">
        </div><div id="tlniJJFUW2@OpenReview" class="panel paper" keywords="mathematics,conjecturing,combinatorics,datasets,algebraic,open,capturing,collection,machine,level">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=tlniJJFUW2" target="_blank" title="89/120"><span class="index notranslate">#89</span></a>
                <a id="title-tlniJJFUW2@OpenReview" class="title-link" href="/venue/tlniJJFUW2@OpenReview" target="_blank">Machine Learning meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics</a>
                <a id="pdf-tlniJJFUW2@OpenReview" class="title-pdf notranslate" onclick="togglePdf('tlniJJFUW2@OpenReview', this)" data="https://openreview.net/pdf?id=tlniJJFUW2">[PDF<sup id="pdf-stars-tlniJJFUW2@OpenReview">2</sup>]</a>
                <a id="copy-tlniJJFUW2@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('tlniJJFUW2@OpenReview')">[Copy]</a>
                <a id="kimi-tlniJJFUW2@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('tlniJJFUW2@OpenReview', this)">[Kimi<sup id="kimi-stars-tlniJJFUW2@OpenReview">4</sup>]</a>
                <a id="rel-tlniJJFUW2@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('tlniJJFUW2@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-tlniJJFUW2@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Herman Chau" target="_blank">Herman Chau</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Helen Jenne" target="_blank">Helen Jenne</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Davis Brown" target="_blank">Davis Brown</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse He" target="_blank">Jesse He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mark Raugas" target="_blank">Mark Raugas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sara Billey" target="_blank">Sara Billey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henry Kvinge" target="_blank">Henry Kvinge</a>
            </p>
            <p id="summary-tlniJJFUW2@OpenReview" class="summary">With recent dramatic increases in AI system capabilities, there has been growing interest in utilizing machine learning for reasoning-heavy, quantitative tasks, particularly mathematics. While there are many resources capturing mathematics at the high-school, undergraduate, and graduate level, there are far fewer resources available that align with the level of difficulty and open endedness encountered by professional mathematicians working on open problems. To address this, we introduce a new collection of datasets, the Algebraic Combinatorics Dataset Repository (ACD Repo), representing either foundational results or open problems in algebraic combinatorics, a subfield of mathematics that studies discrete structures arising from abstract algebra. Further differentiating our dataset collection is the fact that it aims at the conjecturing process. Each dataset includes an open-ended research level question and a large collection of examples (up to 10M in some cases) from which conjectures should be generated. We describe all nine datasets, the different ways machine learning models can be applied to them (e.g., training with narrow models followed by interpretability analysis or program synthesis with LLMs), and discuss some of the challenges involved in designing datasets like these.</p>
            <p id="subjects-tlniJJFUW2@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-tlniJJFUW2@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-tlniJJFUW2@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-tlniJJFUW2@OpenReview" onclick="foldPdfKimi('tlniJJFUW2@OpenReview', this)" class="hr hr-fold">
        </div><div id="mBstuGUaXo@OpenReview" class="panel paper" keywords="score,matching,missing,data,graphical,variational,settings,estimation,approach,sample">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mBstuGUaXo" target="_blank" title="90/120"><span class="index notranslate">#90</span></a>
                <a id="title-mBstuGUaXo@OpenReview" class="title-link" href="/venue/mBstuGUaXo@OpenReview" target="_blank">Score Matching with Missing Data</a>
                <a id="pdf-mBstuGUaXo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mBstuGUaXo@OpenReview', this)" data="https://openreview.net/pdf?id=mBstuGUaXo">[PDF<sup id="pdf-stars-mBstuGUaXo@OpenReview">14</sup>]</a>
                <a id="copy-mBstuGUaXo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mBstuGUaXo@OpenReview')">[Copy]</a>
                <a id="kimi-mBstuGUaXo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mBstuGUaXo@OpenReview', this)">[Kimi<sup id="kimi-stars-mBstuGUaXo@OpenReview">18</sup>]</a>
                <a id="rel-mBstuGUaXo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mBstuGUaXo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mBstuGUaXo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Josh Givens" target="_blank">Josh Givens</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Liu" target="_blank">Song Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henry Reeve" target="_blank">Henry Reeve</a>
            </p>
            <p id="summary-mBstuGUaXo@OpenReview" class="summary">Score matching is a vital tool for learning the distribution of data with applications across many areas including diffusion processes, energy based modelling, and graphical model estimation. Despite all these applications, little work explores its use when data is incomplete. We address this by adapting score matching (and its major extensions) to work with missing data in a flexible setting where data can be partially missing over any subset of the coordinates. We provide two separate score matching variations for general use, an importance weighting (IW) approach, and a variational approach. We provide finite sample bounds for our IW approach in finite domain settings and show it to have especially strong performance in small sample lower dimensional cases. Complementing this, we show our variational approach to be strongest in more complex high-dimensional settings which we demonstrate on graphical model estimation tasks on both real and simulated data.</p>
            <p id="subjects-mBstuGUaXo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-mBstuGUaXo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mBstuGUaXo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mBstuGUaXo@OpenReview" onclick="foldPdfKimi('mBstuGUaXo@OpenReview', this)" class="hr hr-fold">
        </div><div id="M6L7Eaw9BW@OpenReview" class="panel paper" keywords="covariance,mean,task,drift,incremental,calibration,fwu11,macil,class,navigating">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=M6L7Eaw9BW" target="_blank" title="91/120"><span class="index notranslate">#91</span></a>
                <a id="title-M6L7Eaw9BW@OpenReview" class="title-link" href="/venue/M6L7Eaw9BW@OpenReview" target="_blank">Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning</a>
                <a id="pdf-M6L7Eaw9BW@OpenReview" class="title-pdf notranslate" onclick="togglePdf('M6L7Eaw9BW@OpenReview', this)" data="https://openreview.net/pdf?id=M6L7Eaw9BW">[PDF<sup id="pdf-stars-M6L7Eaw9BW@OpenReview">9</sup>]</a>
                <a id="copy-M6L7Eaw9BW@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('M6L7Eaw9BW@OpenReview')">[Copy]</a>
                <a id="kimi-M6L7Eaw9BW@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('M6L7Eaw9BW@OpenReview', this)">[Kimi<sup id="kimi-stars-M6L7Eaw9BW@OpenReview">7</sup>]</a>
                <a id="rel-M6L7Eaw9BW@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('M6L7Eaw9BW@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-M6L7Eaw9BW@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fangwen Wu" target="_blank">Fangwen Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lechao Cheng" target="_blank">Lechao Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengeng Tang" target="_blank">Shengeng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofeng Zhu" target="_blank">Xiaofeng Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaowei Fang" target="_blank">Chaowei Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dingwen Zhang" target="_blank">Dingwen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Wang" target="_blank">Meng Wang</a>
            </p>
            <p id="summary-M6L7Eaw9BW@OpenReview" class="summary">Class-incremental learning (CIL) seeks to enable a model to sequentially learn new classes while retaining knowledge of previously learned ones. Balancing flexibility and stability remains a significant challenge, particularly when the task ID is unknown. To address this, our study reveals that the gap in feature distribution between novel and existing tasks is primarily driven by differences in mean and covariance moments. Building on this insight, we propose a novel semantic drift calibration method that incorporates mean shift compensation and covariance calibration. Specifically, we calculate each class's mean by averaging its sample embeddings and estimate task shifts using weighted embedding changes based on their proximity to the previous mean, effectively capturing mean shifts for all learned classes with each new task. We also apply Mahalanobis distance constraint for covariance calibration, aligning class-specific embedding covariances between old and current networks to mitigate the covariance shift. Additionally, we integrate a feature-level self-distillation approach to enhance generalization. Comprehensive experiments on commonly used datasets demonstrate the effectiveness of our approach. The source code is available at https://github.com/fwu11/MACIL.git.</p>
            <p id="subjects-M6L7Eaw9BW@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-M6L7Eaw9BW@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-M6L7Eaw9BW@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-M6L7Eaw9BW@OpenReview" onclick="foldPdfKimi('M6L7Eaw9BW@OpenReview', this)" class="hr hr-fold">
        </div><div id="uitj69FqD5@OpenReview" class="panel paper" keywords="immunization,amberyzheng,condition,harmful,model,immunizing,immunized,cond,num,linear">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uitj69FqD5" target="_blank" title="92/120"><span class="index notranslate">#92</span></a>
                <a id="title-uitj69FqD5@OpenReview" class="title-link" href="/venue/uitj69FqD5@OpenReview" target="_blank">Model Immunization from a Condition Number Perspective</a>
                <a id="pdf-uitj69FqD5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uitj69FqD5@OpenReview', this)" data="https://openreview.net/pdf?id=uitj69FqD5">[PDF<sup id="pdf-stars-uitj69FqD5@OpenReview">2</sup>]</a>
                <a id="copy-uitj69FqD5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uitj69FqD5@OpenReview')">[Copy]</a>
                <a id="kimi-uitj69FqD5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uitj69FqD5@OpenReview', this)">[Kimi<sup id="kimi-stars-uitj69FqD5@OpenReview">11</sup>]</a>
                <a id="rel-uitj69FqD5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uitj69FqD5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uitj69FqD5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Amber Yijia Zheng" target="_blank">Amber Yijia Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cedar Site Bai" target="_blank">Cedar Site Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Bullins" target="_blank">Brian Bullins</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Raymond A. Yeh" target="_blank">Raymond A. Yeh</a>
            </p>
            <p id="summary-uitj69FqD5@OpenReview" class="summary">Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num.</p>
            <p id="subjects-uitj69FqD5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-uitj69FqD5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uitj69FqD5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uitj69FqD5@OpenReview" onclick="foldPdfKimi('uitj69FqD5@OpenReview', this)" class="hr hr-fold">
        </div><div id="uRAgIVnAO6@OpenReview" class="panel paper" keywords="diminishing,online,conditioning,guaranteeing,decision,give,polynomial,number,predictions,regret">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=uRAgIVnAO6" target="_blank" title="93/120"><span class="index notranslate">#93</span></a>
                <a id="title-uRAgIVnAO6@OpenReview" class="title-link" href="/venue/uRAgIVnAO6@OpenReview" target="_blank">High-Dimensional Prediction for Sequential Decision Making</a>
                <a id="pdf-uRAgIVnAO6@OpenReview" class="title-pdf notranslate" onclick="togglePdf('uRAgIVnAO6@OpenReview', this)" data="https://openreview.net/pdf?id=uRAgIVnAO6">[PDF<sup id="pdf-stars-uRAgIVnAO6@OpenReview">7</sup>]</a>
                <a id="copy-uRAgIVnAO6@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('uRAgIVnAO6@OpenReview')">[Copy]</a>
                <a id="kimi-uRAgIVnAO6@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('uRAgIVnAO6@OpenReview', this)">[Kimi<sup id="kimi-stars-uRAgIVnAO6@OpenReview">11</sup>]</a>
                <a id="rel-uRAgIVnAO6@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('uRAgIVnAO6@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-uRAgIVnAO6@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Georgy Noarov" target="_blank">Georgy Noarov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ramya Ramalingam" target="_blank">Ramya Ramalingam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Roth" target="_blank">Aaron Roth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Xie" target="_blank">Stephan Xie</a>
            </p>
            <p id="summary-uRAgIVnAO6@OpenReview" class="summary">We give an efficient algorithm for producing multi-dimensional forecasts in an online adversarial environment that have low bias subject to any polynomial number of conditioning events, that can depend both on external context and on our predictions themselves. We demonstrate the use of this algorithm with several applications. We show how to make predictions that can be transparently consumed by any polynomial number of downstream decision makers with different utility functions, guaranteeing them diminishing swap regret at optimal rates. We also give the first efficient algorithms for guaranteeing diminishing conditional regret in online combinatorial optimization problems for an arbitrary polynomial number of conditioning events --- i.e. on an arbitrary number of intersecting subsequences determined both by context and our own predictions. Finally, we give the first efficient algorithm for online multicalibration with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-47-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-255" style="width: 4.169em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.096em, 1003.34em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-256"><span class="mi" id="MathJax-Span-257" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-258" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-259"><span style="display: inline-block; position: relative; width: 1.878em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-260" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.107em;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.497em; left: 0.784em;"><span class="texatom" id="MathJax-Span-261"><span class="mrow" id="MathJax-Span-262"><span class="mn" id="MathJax-Span-263" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="texatom" id="MathJax-Span-264"><span class="mrow" id="MathJax-Span-265"><span class="mo" id="MathJax-Span-266" style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-267" style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-268" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-47">O(T^{2/3})</script> rates in the ECE metric.</p>
            <p id="subjects-uRAgIVnAO6@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-uRAgIVnAO6@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-uRAgIVnAO6@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-uRAgIVnAO6@OpenReview" onclick="foldPdfKimi('uRAgIVnAO6@OpenReview', this)" class="hr hr-fold">
        </div><div id="pOAEfqa26i@OpenReview" class="panel paper" keywords="brain,communications,varying,region,processes,markovian,neural,brainml,gaussian,recordings">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=pOAEfqa26i" target="_blank" title="94/120"><span class="index notranslate">#94</span></a>
                <a id="title-pOAEfqa26i@OpenReview" class="title-link" href="/venue/pOAEfqa26i@OpenReview" target="_blank">Learning Time-Varying Multi-Region Brain Communications via Scalable Markovian Gaussian Processes</a>
                <a id="pdf-pOAEfqa26i@OpenReview" class="title-pdf notranslate" onclick="togglePdf('pOAEfqa26i@OpenReview', this)" data="https://openreview.net/pdf?id=pOAEfqa26i">[PDF<sup id="pdf-stars-pOAEfqa26i@OpenReview">3</sup>]</a>
                <a id="copy-pOAEfqa26i@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('pOAEfqa26i@OpenReview')">[Copy]</a>
                <a id="kimi-pOAEfqa26i@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('pOAEfqa26i@OpenReview', this)">[Kimi<sup id="kimi-stars-pOAEfqa26i@OpenReview">8</sup>]</a>
                <a id="rel-pOAEfqa26i@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('pOAEfqa26i@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-pOAEfqa26i@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weihan Li" target="_blank">Weihan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yule Wang" target="_blank">Yule Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengrui Li" target="_blank">Chengrui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anqi Wu" target="_blank">Anqi Wu</a>
            </p>
            <p id="summary-pOAEfqa26i@OpenReview" class="summary">Understanding and constructing brain communications that capture dynamic communications across multiple regions is fundamental to modern system neuroscience, yet current methods struggle to find time-varying region-level communications or scale to large neural datasets with long recording durations. We present a novel framework using Markovian Gaussian Processes to learn brain communications with time-varying temporal delays from multi-region neural recordings, named Adaptive Delay Model (ADM). Our method combines Gaussian Processes with State Space Models and employs parallel scan inference algorithms, enabling efficient scaling to large datasets while identifying concurrent communication patterns that evolve over time. This time-varying approach captures how brain region interactions shift dynamically during cognitive processes. Validated on synthetic and multi-region neural recordings datasets, our approach discovers both the directionality and temporal dynamics of neural communication. This work advances our understanding of distributed neural computation and provides a scalable tool for analyzing dynamic brain networks. Code is available at https://github.com/BRAINML-GT/Adaptive-Delay-Model.</p>
            <p id="subjects-pOAEfqa26i@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-pOAEfqa26i@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-pOAEfqa26i@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-pOAEfqa26i@OpenReview" onclick="foldPdfKimi('pOAEfqa26i@OpenReview', this)" class="hr hr-fold">
        </div><div id="R0PBjxIbgm@OpenReview" class="panel paper" keywords="prediction,mlips,interatomic,physical,property,expressive,potentials,test,tasks,errors">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=R0PBjxIbgm" target="_blank" title="95/120"><span class="index notranslate">#95</span></a>
                <a id="title-R0PBjxIbgm@OpenReview" class="title-link" href="/venue/R0PBjxIbgm@OpenReview" target="_blank">Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction</a>
                <a id="pdf-R0PBjxIbgm@OpenReview" class="title-pdf notranslate" onclick="togglePdf('R0PBjxIbgm@OpenReview', this)" data="https://openreview.net/pdf?id=R0PBjxIbgm">[PDF<sup id="pdf-stars-R0PBjxIbgm@OpenReview">6</sup>]</a>
                <a id="copy-R0PBjxIbgm@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('R0PBjxIbgm@OpenReview')">[Copy]</a>
                <a id="kimi-R0PBjxIbgm@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('R0PBjxIbgm@OpenReview', this)">[Kimi<sup id="kimi-stars-R0PBjxIbgm@OpenReview">7</sup>]</a>
                <a id="rel-R0PBjxIbgm@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('R0PBjxIbgm@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-R0PBjxIbgm@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Fu" target="_blank">Xiang Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brandon Wood" target="_blank">Brandon Wood</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luis Barroso-Luque" target="_blank">Luis Barroso-Luque</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel S. Levine" target="_blank">Daniel S. Levine</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Gao" target="_blank">Meng Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Misko Dzamba" target="_blank">Misko Dzamba</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Larry Zitnick" target="_blank">Larry Zitnick</a>
            </p>
            <p id="summary-R0PBjxIbgm@OpenReview" class="summary">Machine learning interatomic potentials (MLIPs) have become increasingly effective at approximating quantum mechanical calculations at a fraction of the computational cost. However, lower errors on held out test sets do not always translate to improved results on downstream physical property prediction tasks. In this paper, we propose testing MLIPs on their practical ability to conserve energy during molecular dynamic simulations. If passed, improved correlations are found between test errors and their performance on physical property prediction tasks. We identify choices which may lead to models failing this test, and use these observations to improve upon highly-expressive models. The resulting model, eSEN, provides state-of-the-art results on a range of physical property prediction tasks, including materials stability prediction, thermal conductivity prediction, and phonon calculations.</p>
            <p id="subjects-R0PBjxIbgm@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-R0PBjxIbgm@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-R0PBjxIbgm@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-R0PBjxIbgm@OpenReview" onclick="foldPdfKimi('R0PBjxIbgm@OpenReview', this)" class="hr hr-fold">
        </div><div id="2aKHuXdr7Q@OpenReview" class="panel paper" keywords="privacy,graph,upgnet,ldp,utility,node,private,learning,concerns,enhances">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=2aKHuXdr7Q" target="_blank" title="96/120"><span class="index notranslate">#96</span></a>
                <a id="title-2aKHuXdr7Q@OpenReview" class="title-link" href="/venue/2aKHuXdr7Q@OpenReview" target="_blank">Going Deeper into Locally Differentially Private Graph Neural Networks</a>
                <a id="pdf-2aKHuXdr7Q@OpenReview" class="title-pdf notranslate" onclick="togglePdf('2aKHuXdr7Q@OpenReview', this)" data="https://openreview.net/pdf?id=2aKHuXdr7Q">[PDF<sup id="pdf-stars-2aKHuXdr7Q@OpenReview">3</sup>]</a>
                <a id="copy-2aKHuXdr7Q@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('2aKHuXdr7Q@OpenReview')">[Copy]</a>
                <a id="kimi-2aKHuXdr7Q@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('2aKHuXdr7Q@OpenReview', this)">[Kimi<sup id="kimi-stars-2aKHuXdr7Q@OpenReview">1</sup>]</a>
                <a id="rel-2aKHuXdr7Q@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('2aKHuXdr7Q@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-2aKHuXdr7Q@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Longzhu He" target="_blank">Longzhu He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaozhuo Li" target="_blank">Chaozhuo Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Tang" target="_blank">Peng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sen Su" target="_blank">Sen Su</a>
            </p>
            <p id="summary-2aKHuXdr7Q@OpenReview" class="summary">Graph Neural Networks (GNNs) have demonstrated superior performance in a variety of graph mining and learning tasks. However, when node representations involve sensitive personal information or variables related to individuals, learning from graph data can raise significant privacy concerns. Although recent studies have explored local differential privacy (LDP) to address these concerns, they often introduce significant distortions to graph data, severely degrading private learning utility (e.g., node classification accuracy). In this paper, we present UPGNET, an LDP-based privacy-preserving graph learning framework that enhances utility while protecting user data privacy. Specifically, we propose a three-stage pipeline that generalizes the LDP protocols for node features, targeting privacy-sensitive scenarios. Our analysis identifies two key factors that affect the utility of privacy-preserving graph learning: *feature dimension* and *neighborhood size*. Based on the above analysis, UPGNET enhances utility by introducing two core layers: High-Order Aggregator (HOA) layer and the Node Feature Regularization (NFR) layer. Extensive experiments on real-world datasets indicate that UPGNET significantly outperforms existing methods in terms of both privacy protection and learning utility.</p>
            <p id="subjects-2aKHuXdr7Q@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-2aKHuXdr7Q@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-2aKHuXdr7Q@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-2aKHuXdr7Q@OpenReview" onclick="foldPdfKimi('2aKHuXdr7Q@OpenReview', this)" class="hr hr-fold">
        </div><div id="4EYwwVuhtG@OpenReview" class="panel paper" keywords="pipelines,feature,selection,statistical,algorithms,selective,test,inference,data,pipeline">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=4EYwwVuhtG" target="_blank" title="97/120"><span class="index notranslate">#97</span></a>
                <a id="title-4EYwwVuhtG@OpenReview" class="title-link" href="/venue/4EYwwVuhtG@OpenReview" target="_blank">Statistical Test for Feature Selection Pipelines by Selective Inference</a>
                <a id="pdf-4EYwwVuhtG@OpenReview" class="title-pdf notranslate" onclick="togglePdf('4EYwwVuhtG@OpenReview', this)" data="https://openreview.net/pdf?id=4EYwwVuhtG">[PDF<sup id="pdf-stars-4EYwwVuhtG@OpenReview">2</sup>]</a>
                <a id="copy-4EYwwVuhtG@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('4EYwwVuhtG@OpenReview')">[Copy]</a>
                <a id="kimi-4EYwwVuhtG@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('4EYwwVuhtG@OpenReview', this)">[Kimi<sup id="kimi-stars-4EYwwVuhtG@OpenReview">6</sup>]</a>
                <a id="rel-4EYwwVuhtG@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('4EYwwVuhtG@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-4EYwwVuhtG@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tomohiro Shiraishi" target="_blank">Tomohiro Shiraishi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tatsuya Matsukawa" target="_blank">Tatsuya Matsukawa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuichi Nishino" target="_blank">Shuichi Nishino</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ichiro Takeuchi" target="_blank">Ichiro Takeuchi</a>
            </p>
            <p id="summary-4EYwwVuhtG@OpenReview" class="summary">A data analysis pipeline is a structured sequence of steps that transforms raw data into meaningful insights by integrating various analysis algorithms. In this paper, we propose a novel statistical test to assess the significance of data analysis pipelines. Our approach enables the systematic development of valid statistical tests applicable to any feature selection pipeline composed of predefined components. We develop this framework based on selective inference, a statistical technique that has recently gained attention for data-driven hypotheses. As a proof of concept, we focus on feature selection pipelines for linear models, composed of three missing value imputation algorithms, three outlier detection algorithms, and three feature selection algorithms. We theoretically prove that our statistical test can control the probability of false positive feature selection at any desired level, and demonstrate its validity and effectiveness through experiments on synthetic and real data. Additionally, we present an implementation framework that facilitates testing across any configuration of these feature selection pipelines without extra implementation costs.</p>
            <p id="subjects-4EYwwVuhtG@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-4EYwwVuhtG@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-4EYwwVuhtG@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-4EYwwVuhtG@OpenReview" onclick="foldPdfKimi('4EYwwVuhtG@OpenReview', this)" class="hr hr-fold">
        </div><div id="z19u9B2fCZ@OpenReview" class="panel paper" keywords="csr,mrl,matryoshka,sparse,adaptive,representation,coding,lengths,contrastive,revisiting">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=z19u9B2fCZ" target="_blank" title="98/120"><span class="index notranslate">#98</span></a>
                <a id="title-z19u9B2fCZ@OpenReview" class="title-link" href="/venue/z19u9B2fCZ@OpenReview" target="_blank">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</a>
                <a id="pdf-z19u9B2fCZ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('z19u9B2fCZ@OpenReview', this)" data="https://openreview.net/pdf?id=z19u9B2fCZ">[PDF<sup id="pdf-stars-z19u9B2fCZ@OpenReview">10</sup>]</a>
                <a id="copy-z19u9B2fCZ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('z19u9B2fCZ@OpenReview')">[Copy]</a>
                <a id="kimi-z19u9B2fCZ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('z19u9B2fCZ@OpenReview', this)">[Kimi<sup id="kimi-stars-z19u9B2fCZ@OpenReview">14</sup>]</a>
                <a id="rel-z19u9B2fCZ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('z19u9B2fCZ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-z19u9B2fCZ@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tiansheng Wen" target="_blank">Tiansheng Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Wang" target="_blank">Yifei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zequn Zeng" target="_blank">Zequn Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhong Peng" target="_blank">Zhong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yudi Su" target="_blank">Yudi Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyang Liu" target="_blank">Xinyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Chen" target="_blank">Bo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongwei Liu" target="_blank">Hongwei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefanie Jegelka" target="_blank">Stefanie Jegelka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyu You" target="_blank">Chenyu You</a>
            </p>
            <p id="summary-z19u9B2fCZ@OpenReview" class="summary">Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that *sparse coding* offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose **Contrastive Sparse Representation** (**CSR**), a method that specifies pre-trained embeddings into a high-dimensional but *selectively activated* feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed—often by large margins—while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at [this URL.](https://github.com/neilwen987/CSR_Adaptive_Rep)</p>
            <p id="subjects-z19u9B2fCZ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-z19u9B2fCZ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-z19u9B2fCZ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-z19u9B2fCZ@OpenReview" onclick="foldPdfKimi('z19u9B2fCZ@OpenReview', this)" class="hr hr-fold">
        </div><div id="6CwO5nVvku@OpenReview" class="panel paper" keywords="partition,visualization,subsets,embedding,laplacian,feature,dimensional,data,structures,embed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=6CwO5nVvku" target="_blank" title="99/120"><span class="index notranslate">#99</span></a>
                <a id="title-6CwO5nVvku@OpenReview" class="title-link" href="/venue/6CwO5nVvku@OpenReview" target="_blank">Partition First, Embed Later: Laplacian-Based Feature Partitioning for Refined Embedding and Visualization of High-Dimensional Data</a>
                <a id="pdf-6CwO5nVvku@OpenReview" class="title-pdf notranslate" onclick="togglePdf('6CwO5nVvku@OpenReview', this)" data="https://openreview.net/pdf?id=6CwO5nVvku">[PDF<sup id="pdf-stars-6CwO5nVvku@OpenReview">7</sup>]</a>
                <a id="copy-6CwO5nVvku@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('6CwO5nVvku@OpenReview')">[Copy]</a>
                <a id="kimi-6CwO5nVvku@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('6CwO5nVvku@OpenReview', this)">[Kimi<sup id="kimi-stars-6CwO5nVvku@OpenReview">5</sup>]</a>
                <a id="rel-6CwO5nVvku@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('6CwO5nVvku@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-6CwO5nVvku@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Erez Peterfreund" target="_blank">Erez Peterfreund</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ofir Lindenbaum" target="_blank">Ofir Lindenbaum</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuval Kluger" target="_blank">Yuval Kluger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Landa" target="_blank">Boris Landa</a>
            </p>
            <p id="summary-6CwO5nVvku@OpenReview" class="summary">Embedding and visualization techniques are essential for analyzing high-dimensional data, but they often struggle with complex data governed by multiple latent variables, potentially distorting key structural characteristics. This paper considers scenarios where the observed features can be partitioned into mutually exclusive subsets, each capturing a different smooth substructure. In such cases, visualizing the data based on each feature partition can better characterize the underlying processes and structures in the data, leading to improved interpretability. To partition the features, we propose solving an optimization problem that promotes graph Laplacian-based smoothness in each partition, thereby prioritizing partitions with simpler geometric structures. Our approach generalizes traditional embedding and visualization techniques, allowing them to learn multiple embeddings simultaneously. We establish that if several independent or partially dependent manifolds are embedded in distinct feature subsets in high-dimensional space, then our framework can reliably identify the correct subsets with theoretical guarantees. Finally, we demonstrate the effectiveness of our approach in extracting multiple low-dimensional structures and partially independent processes from both simulated and real data.</p>
            <p id="subjects-6CwO5nVvku@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-6CwO5nVvku@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-6CwO5nVvku@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-6CwO5nVvku@OpenReview" onclick="foldPdfKimi('6CwO5nVvku@OpenReview', this)" class="hr hr-fold">
        </div><div id="Tv2JDGw920@OpenReview" class="panel paper" keywords="osgr,genie,generalization,domain,gradient,alignment,step,ratio,optimization,equalizing">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Tv2JDGw920" target="_blank" title="100/120"><span class="index notranslate">#100</span></a>
                <a id="title-Tv2JDGw920@OpenReview" class="title-link" href="/venue/Tv2JDGw920@OpenReview" target="_blank">One-Step Generalization Ratio Guided Optimization for Domain Generalization</a>
                <a id="pdf-Tv2JDGw920@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Tv2JDGw920@OpenReview', this)" data="https://openreview.net/pdf?id=Tv2JDGw920">[PDF<sup id="pdf-stars-Tv2JDGw920@OpenReview">11</sup>]</a>
                <a id="copy-Tv2JDGw920@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Tv2JDGw920@OpenReview')">[Copy]</a>
                <a id="kimi-Tv2JDGw920@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Tv2JDGw920@OpenReview', this)">[Kimi<sup id="kimi-stars-Tv2JDGw920@OpenReview">10</sup>]</a>
                <a id="rel-Tv2JDGw920@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Tv2JDGw920@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Tv2JDGw920@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sumin Cho" target="_blank">Sumin Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongwon Kim" target="_blank">Dongwon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwangsu Kim" target="_blank">Kwangsu Kim</a>
            </p>
            <p id="summary-Tv2JDGw920@OpenReview" class="summary">Domain Generalization (DG) aims to train models that generalize to unseen target domains but often overfit to domain-specific features, known as undesired correlations. Gradient-based DG methods typically guide gradients in a dominant direction but often inadvertently reinforce spurious correlations. Recent work has employed dropout to regularize overconfident parameters, but has not explicitly adjusted gradient alignment or ensured balanced parameter updates. We propose GENIE (Generalization-ENhancing Iterative Equalizer), a novel optimizer that leverages the One-Step Generalization Ratio (OSGR) to quantify each parameter's contribution to loss reduction and assess gradient alignment. By dynamically equalizing OSGR via a preconditioning factor, GENIE prevents a small subset of parameters from dominating optimization, thereby promoting domain-invariant feature learning. Theoretically, GENIE balances convergence contribution and gradient alignment among parameters, achieving higher OSGR while retaining SGD's convergence rate. Empirically, it outperforms existing optimizers and enhances performance when integrated with various DG and single-DG methods.</p>
            <p id="subjects-Tv2JDGw920@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Tv2JDGw920@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tv2JDGw920@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tv2JDGw920@OpenReview" onclick="foldPdfKimi('Tv2JDGw920@OpenReview', this)" class="hr hr-fold">
        </div><div id="70voOlSPos@OpenReview" class="panel paper" keywords="listing,mag,emph,rules,mags,mec,delay,ancestral,singleton,orientation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=70voOlSPos" target="_blank" title="101/120"><span class="index notranslate">#101</span></a>
                <a id="title-70voOlSPos@OpenReview" class="title-link" href="/venue/70voOlSPos@OpenReview" target="_blank">Polynomial-Delay MAG Listing with Novel Locally Complete Orientation Rules</a>
                <a id="pdf-70voOlSPos@OpenReview" class="title-pdf notranslate" onclick="togglePdf('70voOlSPos@OpenReview', this)" data="https://openreview.net/pdf?id=70voOlSPos">[PDF<sup id="pdf-stars-70voOlSPos@OpenReview">2</sup>]</a>
                <a id="copy-70voOlSPos@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('70voOlSPos@OpenReview')">[Copy]</a>
                <a id="kimi-70voOlSPos@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('70voOlSPos@OpenReview', this)">[Kimi<sup id="kimi-stars-70voOlSPos@OpenReview">3</sup>]</a>
                <a id="rel-70voOlSPos@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('70voOlSPos@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-70voOlSPos@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tian-Zuo Wang" target="_blank">Tian-Zuo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen-Bo Du" target="_blank">Wen-Bo Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhi-Hua Zhou" target="_blank">Zhi-Hua Zhou</a>
            </p>
            <p id="summary-70voOlSPos@OpenReview" class="summary">A maximal ancestral graph (MAG) is widely used to characterize the causal relations among observable variables in the presence of latent variables. However, given observational data, only a partial ancestral graph representing a Markov equivalence class (MEC) of MAGs is identifiable, which generally contains uncertain causal relations. Due to the uncertainties, \emph{MAG listing}, \emph{i.e.}, listing all the MAGs in the MEC, is critical for many downstream tasks. In this paper, we present the first \emph{polynomial-delay} MAG listing method, where delay refers to the time for outputting each MAG, through introducing enumerated structural knowledge in the form of \emph{singleton background knowledge (BK)}. To incorporate such knowledge, we propose the \emph{sound} and \emph{locally complete} orientation rules. By recursively introducing singleton BK and applying the rules, our method can output all and only MAGs in the MEC with polynomial delay. Additionally, while the proposed novel rules enable more efficient MAG listing, for the goal of incorporating general BK, we present two counterexamples to imply that existing rules including ours, are not yet \emph{complete}, which motivate two more rules. Experimental results validate the efficiency of the proposed MAG listing method.</p>
            <p id="subjects-70voOlSPos@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-70voOlSPos@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-70voOlSPos@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-70voOlSPos@OpenReview" onclick="foldPdfKimi('70voOlSPos@OpenReview', this)" class="hr hr-fold">
        </div><div id="ZdqTePSV1K@OpenReview" class="panel paper" keywords="fms,subset,ies,grained,selection,apl,datasets,fine,foundation,ram">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=ZdqTePSV1K" target="_blank" title="102/120"><span class="index notranslate">#102</span></a>
                <a id="title-ZdqTePSV1K@OpenReview" class="title-link" href="/venue/ZdqTePSV1K@OpenReview" target="_blank">Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection</a>
                <a id="pdf-ZdqTePSV1K@OpenReview" class="title-pdf notranslate" onclick="togglePdf('ZdqTePSV1K@OpenReview', this)" data="https://openreview.net/pdf?id=ZdqTePSV1K">[PDF<sup id="pdf-stars-ZdqTePSV1K@OpenReview">6</sup>]</a>
                <a id="copy-ZdqTePSV1K@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('ZdqTePSV1K@OpenReview')">[Copy]</a>
                <a id="kimi-ZdqTePSV1K@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('ZdqTePSV1K@OpenReview', this)">[Kimi<sup id="kimi-stars-ZdqTePSV1K@OpenReview">10</sup>]</a>
                <a id="rel-ZdqTePSV1K@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('ZdqTePSV1K@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-ZdqTePSV1K@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijing Wan" target="_blank">Zhijing Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixiang Wang" target="_blank">Zhixiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Wang" target="_blank">Zheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Xu" target="_blank">Xin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shin&amp;#x27;ichi Satoh" target="_blank">Shin&amp;#x27;ichi Satoh</a>
            </p>
            <p id="summary-ZdqTePSV1K@OpenReview" class="summary">One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.</p>
            <p id="subjects-ZdqTePSV1K@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-ZdqTePSV1K@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-ZdqTePSV1K@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-ZdqTePSV1K@OpenReview" onclick="foldPdfKimi('ZdqTePSV1K@OpenReview', this)" class="hr hr-fold">
        </div><div id="QvqnPVGWAN@OpenReview" class="panel paper" keywords="blink,windows,eye,models,diffusion,distributional,theory,googling,yellowstone,localization">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=QvqnPVGWAN" target="_blank" title="103/120"><span class="index notranslate">#103</span></a>
                <a id="title-QvqnPVGWAN@OpenReview" class="title-link" href="/venue/QvqnPVGWAN@OpenReview" target="_blank">Blink of an eye: a simple theory for feature localization in generative models</a>
                <a id="pdf-QvqnPVGWAN@OpenReview" class="title-pdf notranslate" onclick="togglePdf('QvqnPVGWAN@OpenReview', this)" data="https://openreview.net/pdf?id=QvqnPVGWAN">[PDF<sup id="pdf-stars-QvqnPVGWAN@OpenReview">12</sup>]</a>
                <a id="copy-QvqnPVGWAN@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('QvqnPVGWAN@OpenReview')">[Copy]</a>
                <a id="kimi-QvqnPVGWAN@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('QvqnPVGWAN@OpenReview', this)">[Kimi<sup id="kimi-stars-QvqnPVGWAN@OpenReview">17</sup>]</a>
                <a id="rel-QvqnPVGWAN@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('QvqnPVGWAN@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-QvqnPVGWAN@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marvin Li" target="_blank">Marvin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aayush Karan" target="_blank">Aayush Karan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sitan Chen" target="_blank">Sitan Chen</a>
            </p>
            <p id="summary-QvqnPVGWAN@OpenReview" class="summary">Large language models can exhibit unexpected behavior in the blink of an eye. In a recent computer use demo, a language model switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow ``critical windows'' of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. Using the formalism of stochastic localization for generative models, we show that it emerges generically as the generation process localizes to a sub-population of the distribution it models. While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes very few distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic mathematical tools. Finally, we validate our predictions empirically for LLMs and find that critical windows often coincide with failures in problem solving for various math and reasoning benchmarks.</p>
            <p id="subjects-QvqnPVGWAN@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-QvqnPVGWAN@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-QvqnPVGWAN@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-QvqnPVGWAN@OpenReview" onclick="foldPdfKimi('QvqnPVGWAN@OpenReview', this)" class="hr hr-fold">
        </div><div id="Hi0SyHMmkd@OpenReview" class="panel paper" keywords="creative,token,ended,leap,tasks,going,next,chenwu98,algorithmic,open">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Hi0SyHMmkd" target="_blank" title="104/120"><span class="index notranslate">#104</span></a>
                <a id="title-Hi0SyHMmkd@OpenReview" class="title-link" href="/venue/Hi0SyHMmkd@OpenReview" target="_blank">Roll the dice &amp; look before you leap: Going beyond the creative limits of next-token prediction</a>
                <a id="pdf-Hi0SyHMmkd@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Hi0SyHMmkd@OpenReview', this)" data="https://openreview.net/pdf?id=Hi0SyHMmkd">[PDF<sup id="pdf-stars-Hi0SyHMmkd@OpenReview">15</sup>]</a>
                <a id="copy-Hi0SyHMmkd@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Hi0SyHMmkd@OpenReview')">[Copy]</a>
                <a id="kimi-Hi0SyHMmkd@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Hi0SyHMmkd@OpenReview', this)">[Kimi<sup id="kimi-stars-Hi0SyHMmkd@OpenReview">26</sup>]</a>
                <a id="rel-Hi0SyHMmkd@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Hi0SyHMmkd@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Hi0SyHMmkd@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vaishnavh Nagarajan" target="_blank">Vaishnavh Nagarajan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Wu" target="_blank">Chen Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Ding" target="_blank">Charles Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aditi Raghunathan" target="_blank">Aditi Raghunathan</a>
            </p>
            <p id="summary-Hi0SyHMmkd@OpenReview" class="summary">We design a suite of minimal algorithmic tasks that are a loose abstraction of _open-ended_ real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model.Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended _stochastic_ planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; multi-token approaches, namely teacherless training and diffusion models, comparatively excel in producing diverse and original output. Secondly, to elicit randomness without hurting coherence, we find that injecting noise at the input layer (dubbed _seed-conditioning_) works surprisingly as well as (and in some conditions, better than) temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and temperature sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity</p>
            <p id="subjects-Hi0SyHMmkd@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Hi0SyHMmkd@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hi0SyHMmkd@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hi0SyHMmkd@OpenReview" onclick="foldPdfKimi('Hi0SyHMmkd@OpenReview', this)" class="hr hr-fold">
        </div><div id="feIaF6vYFl@OpenReview" class="panel paper" keywords="reasoning,code,codei,codeio,prediction,condensing,math,outputs,patterns,input">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=feIaF6vYFl" target="_blank" title="105/120"><span class="index notranslate">#105</span></a>
                <a id="title-feIaF6vYFl@OpenReview" class="title-link" href="/venue/feIaF6vYFl@OpenReview" target="_blank">CodeIO: Condensing Reasoning Patterns via Code Input-Output Prediction</a>
                <a id="pdf-feIaF6vYFl@OpenReview" class="title-pdf notranslate" onclick="togglePdf('feIaF6vYFl@OpenReview', this)" data="https://openreview.net/pdf?id=feIaF6vYFl">[PDF<sup id="pdf-stars-feIaF6vYFl@OpenReview">8</sup>]</a>
                <a id="copy-feIaF6vYFl@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('feIaF6vYFl@OpenReview')">[Copy]</a>
                <a id="kimi-feIaF6vYFl@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('feIaF6vYFl@OpenReview', this)">[Kimi<sup id="kimi-stars-feIaF6vYFl@OpenReview">9</sup>]</a>
                <a id="rel-feIaF6vYFl@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('feIaF6vYFl@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-feIaF6vYFl@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junlong Li" target="_blank">Junlong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daya Guo" target="_blank">Daya Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dejian Yang" target="_blank">Dejian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runxin Xu" target="_blank">Runxin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Wu" target="_blank">Yu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junxian He" target="_blank">Junxian He</a>
            </p>
            <p id="summary-feIaF6vYFl@OpenReview" class="summary">Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives—like logic flow planning, state-space searching, decision tree traversal, and modular decomposition—while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math &amp; numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models will be publicly available.</p>
            <p id="subjects-feIaF6vYFl@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-feIaF6vYFl@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-feIaF6vYFl@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-feIaF6vYFl@OpenReview" onclick="foldPdfKimi('feIaF6vYFl@OpenReview', this)" class="hr hr-fold">
        </div><div id="V0w8Kj3K6L@OpenReview" class="panel paper" keywords="suitability,filter,covariate,classifier,performance,user,accuracy,degradation,statistical,evaluation">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=V0w8Kj3K6L" target="_blank" title="106/120"><span class="index notranslate">#106</span></a>
                <a id="title-V0w8Kj3K6L@OpenReview" class="title-link" href="/venue/V0w8Kj3K6L@OpenReview" target="_blank">Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings</a>
                <a id="pdf-V0w8Kj3K6L@OpenReview" class="title-pdf notranslate" onclick="togglePdf('V0w8Kj3K6L@OpenReview', this)" data="https://openreview.net/pdf?id=V0w8Kj3K6L">[PDF<sup id="pdf-stars-V0w8Kj3K6L@OpenReview">3</sup>]</a>
                <a id="copy-V0w8Kj3K6L@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('V0w8Kj3K6L@OpenReview')">[Copy]</a>
                <a id="kimi-V0w8Kj3K6L@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('V0w8Kj3K6L@OpenReview', this)">[Kimi<sup id="kimi-stars-V0w8Kj3K6L@OpenReview">4</sup>]</a>
                <a id="rel-V0w8Kj3K6L@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('V0w8Kj3K6L@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-V0w8Kj3K6L@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Angéline Pouget" target="_blank">Angéline Pouget</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Yaghini" target="_blank">Mohammad Yaghini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephan Rabanser" target="_blank">Stephan Rabanser</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Papernot" target="_blank">Nicolas Papernot</a>
            </p>
            <p id="summary-V0w8Kj3K6L@OpenReview" class="summary">Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the _suitability filter_, a novel framework designed to detect performance deterioration by utilizing _suitability signals_—model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications.</p>
            <p id="subjects-V0w8Kj3K6L@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-V0w8Kj3K6L@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-V0w8Kj3K6L@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-V0w8Kj3K6L@OpenReview" onclick="foldPdfKimi('V0w8Kj3K6L@OpenReview', this)" class="hr hr-fold">
        </div><div id="mr0xOQTJkL@OpenReview" class="panel paper" keywords="clique,cliques,dags,picking,root,markov,equivalent,algorithm,counting,wienöbst">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=mr0xOQTJkL" target="_blank" title="107/120"><span class="index notranslate">#107</span></a>
                <a id="title-mr0xOQTJkL@OpenReview" class="title-link" href="/venue/mr0xOQTJkL@OpenReview" target="_blank">An Improved Clique-Picking Algorithm for Counting Markov Equivalent DAGs via Super Cliques Transfer</a>
                <a id="pdf-mr0xOQTJkL@OpenReview" class="title-pdf notranslate" onclick="togglePdf('mr0xOQTJkL@OpenReview', this)" data="https://openreview.net/pdf?id=mr0xOQTJkL">[PDF<sup id="pdf-stars-mr0xOQTJkL@OpenReview">2</sup>]</a>
                <a id="copy-mr0xOQTJkL@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('mr0xOQTJkL@OpenReview')">[Copy]</a>
                <a id="kimi-mr0xOQTJkL@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('mr0xOQTJkL@OpenReview', this)">[Kimi<sup id="kimi-stars-mr0xOQTJkL@OpenReview">4</sup>]</a>
                <a id="rel-mr0xOQTJkL@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('mr0xOQTJkL@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-mr0xOQTJkL@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lifu Liu" target="_blank">Lifu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyuan He" target="_blank">Shiyuan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianhua Guo" target="_blank">Jianhua Guo</a>
            </p>
            <p id="summary-mr0xOQTJkL@OpenReview" class="summary">Efficiently counting Markov equivalent directed acyclic graphs (DAGs) is crucial in graphical causal analysis. Wienöbst et al. (2023) introduced a polynomial-time algorithm, known as the Clique-Picking algorithm, to count the number of Markov equivalent DAGs for a given completed partially directed acyclic graph (CPDAG). This algorithm iteratively selects a root clique, determines fixed orientations with outgoing edges from the clique, and generates the unresolved undirected connected components (UCCGs). In this work, we propose a more efficient approach to UCCG generation by utilizing previously computed results for different root cliques. Our method introduces the concept of super cliques within rooted clique trees, enabling their efficient transfer between trees with different root cliques. The proposed algorithm effectively reduces the computational complexity of the Clique-Picking method, particularly when the number of cliques is substantially smaller than the number of vertices and edges.</p>
            <p id="subjects-mr0xOQTJkL@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-mr0xOQTJkL@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-mr0xOQTJkL@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-mr0xOQTJkL@OpenReview" onclick="foldPdfKimi('mr0xOQTJkL@OpenReview', this)" class="hr hr-fold">
        </div><div id="rc65N9xIrY@OpenReview" class="panel paper" keywords="distillm,student,contrastive,distillation,teacher,synergy,llms,responses,boosts,language">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=rc65N9xIrY" target="_blank" title="108/120"><span class="index notranslate">#108</span></a>
                <a id="title-rc65N9xIrY@OpenReview" class="title-link" href="/venue/rc65N9xIrY@OpenReview" target="_blank">DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs</a>
                <a id="pdf-rc65N9xIrY@OpenReview" class="title-pdf notranslate" onclick="togglePdf('rc65N9xIrY@OpenReview', this)" data="https://openreview.net/pdf?id=rc65N9xIrY">[PDF<sup id="pdf-stars-rc65N9xIrY@OpenReview">17</sup>]</a>
                <a id="copy-rc65N9xIrY@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('rc65N9xIrY@OpenReview')">[Copy]</a>
                <a id="kimi-rc65N9xIrY@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('rc65N9xIrY@OpenReview', this)">[Kimi<sup id="kimi-stars-rc65N9xIrY@OpenReview">20</sup>]</a>
                <a id="rel-rc65N9xIrY@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('rc65N9xIrY@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-rc65N9xIrY@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jongwoo Ko" target="_blank">Jongwoo Ko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyi Chen" target="_blank">Tianyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungnyun Kim" target="_blank">Sungnyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Ding" target="_blank">Tianyu Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luming Liang" target="_blank">Luming Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ilya Zharkov" target="_blank">Ilya Zharkov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Se-Young Yun" target="_blank">Se-Young Yun</a>
            </p>
            <p id="summary-rc65N9xIrY@OpenReview" class="summary">Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.</p>
            <p id="subjects-rc65N9xIrY@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-rc65N9xIrY@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-rc65N9xIrY@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-rc65N9xIrY@OpenReview" onclick="foldPdfKimi('rc65N9xIrY@OpenReview', this)" class="hr hr-fold">
        </div><div id="0yzOEMbShU@OpenReview" class="panel paper" keywords="boldsymbol,mcmc,srrw,reversible,repellent,target,hdt,history,graphs,walk">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=0yzOEMbShU" target="_blank" title="109/120"><span class="index notranslate">#109</span></a>
                <a id="title-0yzOEMbShU@OpenReview" class="title-link" href="/venue/0yzOEMbShU@OpenReview" target="_blank">Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs</a>
                <a id="pdf-0yzOEMbShU@OpenReview" class="title-pdf notranslate" onclick="togglePdf('0yzOEMbShU@OpenReview', this)" data="https://openreview.net/pdf?id=0yzOEMbShU">[PDF<sup id="pdf-stars-0yzOEMbShU@OpenReview">3</sup>]</a>
                <a id="copy-0yzOEMbShU@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('0yzOEMbShU@OpenReview')">[Copy]</a>
                <a id="kimi-0yzOEMbShU@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('0yzOEMbShU@OpenReview', this)">[Kimi<sup id="kimi-stars-0yzOEMbShU@OpenReview">5</sup>]</a>
                <a id="rel-0yzOEMbShU@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('0yzOEMbShU@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-0yzOEMbShU@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Hu" target="_blank">Jie Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi-Ting Ma" target="_blank">Yi-Ting Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Do-Young Eun" target="_blank">Do-Young Eun</a>
            </p>
            <p id="summary-0yzOEMbShU@OpenReview" class="summary">We propose a *history-driven target (HDT)* framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-48-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-269" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-270"><span class="mi" id="MathJax-Span-271" style="font-family: MathJax_Math-bold-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-48">\boldsymbol{\mu}</script>. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-49-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;&amp;#x03C0;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-272" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.72em, 2.607em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-273"><span class="mi" id="MathJax-Span-274" style="font-family: MathJax_Math-bold-italic;">π</span><span class="mo" id="MathJax-Span-275" style="font-family: MathJax_Main;">[</span><span class="texatom" id="MathJax-Span-276"><span class="mrow" id="MathJax-Span-277"><span class="mi" id="MathJax-Span-278" style="font-family: MathJax_Main-bold;">x</span></span></span><span class="mo" id="MathJax-Span-279" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">π</mi><mo stretchy="false">[</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-49">\boldsymbol{\pi}[\mathbf{x}]</script> to replace the original target <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-50-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-280" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-281"><span class="mi" id="MathJax-Span-282" style="font-family: MathJax_Math-bold-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-50">\boldsymbol{\mu}</script> in any graph sampler, where <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-51-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-283" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-284"><span class="texatom" id="MathJax-Span-285"><span class="mrow" id="MathJax-Span-286"><span class="mi" id="MathJax-Span-287" style="font-family: MathJax_Main-bold;">x</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-51">\mathbf{x}</script> represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-52-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;&amp;#x03BC;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-288" style="width: 0.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-289"><span class="mi" id="MathJax-Span-290" style="font-family: MathJax_Math-bold-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">μ</mi></math></span></span><script type="math/tex" id="MathJax-Element-52">\boldsymbol{\mu}</script> and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.</p>
            <p id="subjects-0yzOEMbShU@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-0yzOEMbShU@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-0yzOEMbShU@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-0yzOEMbShU@OpenReview" onclick="foldPdfKimi('0yzOEMbShU@OpenReview', this)" class="hr hr-fold">
        </div><div id="kV8oUyjdIg@OpenReview" class="panel paper" keywords="smoothness,nonlinearly,preconditioned,gradient,methods,generalized,smooth,encapsulates,clipping,convexity">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=kV8oUyjdIg" target="_blank" title="110/120"><span class="index notranslate">#110</span></a>
                <a id="title-kV8oUyjdIg@OpenReview" class="title-link" href="/venue/kV8oUyjdIg@OpenReview" target="_blank">Nonlinearly Preconditioned Gradient Methods under Generalized Smoothness</a>
                <a id="pdf-kV8oUyjdIg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('kV8oUyjdIg@OpenReview', this)" data="https://openreview.net/pdf?id=kV8oUyjdIg">[PDF<sup id="pdf-stars-kV8oUyjdIg@OpenReview">4</sup>]</a>
                <a id="copy-kV8oUyjdIg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('kV8oUyjdIg@OpenReview')">[Copy]</a>
                <a id="kimi-kV8oUyjdIg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('kV8oUyjdIg@OpenReview', this)">[Kimi<sup id="kimi-stars-kV8oUyjdIg@OpenReview">11</sup>]</a>
                <a id="rel-kV8oUyjdIg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('kV8oUyjdIg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-kV8oUyjdIg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Konstantinos Oikonomidis" target="_blank">Konstantinos Oikonomidis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Quan" target="_blank">Jan Quan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emanuel Laude" target="_blank">Emanuel Laude</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Panagiotis Patrinos" target="_blank">Panagiotis Patrinos</a>
            </p>
            <p id="summary-kV8oUyjdIg@OpenReview" class="summary">We analyze nonlinearly preconditioned gradient methods for solving smooth minimization problems. We introduce a generalized smoothness property, based on the notion of abstract convexity, that is broader than Lipschitz smoothness and provide sufficient first- and second-order conditions. Notably, our framework encapsulates algorithms associated with the gradient clipping method and brings out novel insights for the class of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-53-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-291" style="width: 4.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.28em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-292"><span class="mo" id="MathJax-Span-293" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-294"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-295" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.68em;"><span class="mn" id="MathJax-Span-296" style="font-size: 70.7%; font-family: MathJax_Main;">0</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-297" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-298" style="padding-left: 0.159em;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mi" id="MathJax-Span-299" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -1.977em; left: 0.68em;"><span class="mn" id="MathJax-Span-300" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span><span class="mo" id="MathJax-Span-301" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><msub><mi>L</mi><mn>0</mn></msub><mo>,</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-53">(L_0,L_1)</script>-smooth functions that has received widespread interest recently, thus allowing us to extend beyond already established methods. We investigate the convergence of the proposed method in both the convex and nonconvex setting.</p>
            <p id="subjects-kV8oUyjdIg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-kV8oUyjdIg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-kV8oUyjdIg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-kV8oUyjdIg@OpenReview" onclick="foldPdfKimi('kV8oUyjdIg@OpenReview', this)" class="hr hr-fold">
        </div><div id="1rh8iTehBc@OpenReview" class="panel paper" keywords="licenses,licensing,quagmire,practices,legal,dragging,modelgo,model,noncompliance,nessed">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=1rh8iTehBc" target="_blank" title="111/120"><span class="index notranslate">#111</span></a>
                <a id="title-1rh8iTehBc@OpenReview" class="title-link" href="/venue/1rh8iTehBc@OpenReview" target="_blank">Position: Current Model Licensing Practices are Dragging Us into a Quagmire of Legal Noncompliance</a>
                <a id="pdf-1rh8iTehBc@OpenReview" class="title-pdf notranslate" onclick="togglePdf('1rh8iTehBc@OpenReview', this)" data="https://openreview.net/pdf?id=1rh8iTehBc">[PDF<sup id="pdf-stars-1rh8iTehBc@OpenReview">8</sup>]</a>
                <a id="copy-1rh8iTehBc@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('1rh8iTehBc@OpenReview')">[Copy]</a>
                <a id="kimi-1rh8iTehBc@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('1rh8iTehBc@OpenReview', this)">[Kimi<sup id="kimi-stars-1rh8iTehBc@OpenReview">7</sup>]</a>
                <a id="rel-1rh8iTehBc@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('1rh8iTehBc@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-1rh8iTehBc@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Moming Duan" target="_blank">Moming Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingzhe Du" target="_blank">Mingzhe Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Zhao" target="_blank">Rui Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengying Wang" target="_blank">Mengying Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinghui Wu" target="_blank">Yinghui Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nigel Shadbolt" target="_blank">Nigel Shadbolt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingsheng He" target="_blank">Bingsheng He</a>
            </p>
            <p id="summary-1rh8iTehBc@OpenReview" class="summary">The Machine Learning (ML) community has wit- nessed explosive growth, with millions of ML models being published on the Web. Reusing ML model components has been prevalent nowadays. Developers are often required to choose a license to publish and govern the use of their models. Popular options include Apache-2.0, OpenRAIL (Responsible AI Licenses), Creative Commons Licenses (CCs), Llama2, and GPL-3.0. Currently, no standard or widely accepted best practices ex- ist for model licensing. But does this lack of standardization lead to undesired consequences? Our answer is Yes. After reviewing the clauses of the most widely adopted licenses, we take the position that current model licensing practices are dragging us into a quagmire of legal noncom- pliance. To support this view, we explore the cur- rent practices in model licensing and highlight the differences between various model licenses. We then identify potential legal risks associated with these licenses and demonstrate these risks using examples from real-world repositories on Hug- ging Face. To foster a more standardized future for model licensing, we also propose a new draft of model licenses, ModelGo Licenses (MGLs), to address these challenges and promote better compliance. https://www.modelgo.li/</p>
            <p id="subjects-1rh8iTehBc@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-1rh8iTehBc@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-1rh8iTehBc@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-1rh8iTehBc@OpenReview" onclick="foldPdfKimi('1rh8iTehBc@OpenReview', this)" class="hr hr-fold">
        </div><div id="9skHxuHyM4@OpenReview" class="panel paper" keywords="delegation,authenticated,agents,authorization,accountability,argues,authority,authentication,position,openid">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=9skHxuHyM4" target="_blank" title="112/120"><span class="index notranslate">#112</span></a>
                <a id="title-9skHxuHyM4@OpenReview" class="title-link" href="/venue/9skHxuHyM4@OpenReview" target="_blank">Position: AI Agents Need Authenticated Delegation</a>
                <a id="pdf-9skHxuHyM4@OpenReview" class="title-pdf notranslate" onclick="togglePdf('9skHxuHyM4@OpenReview', this)" data="https://openreview.net/pdf?id=9skHxuHyM4">[PDF<sup id="pdf-stars-9skHxuHyM4@OpenReview">9</sup>]</a>
                <a id="copy-9skHxuHyM4@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('9skHxuHyM4@OpenReview')">[Copy]</a>
                <a id="kimi-9skHxuHyM4@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('9skHxuHyM4@OpenReview', this)">[Kimi<sup id="kimi-stars-9skHxuHyM4@OpenReview">12</sup>]</a>
                <a id="rel-9skHxuHyM4@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('9skHxuHyM4@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-9skHxuHyM4@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tobin South" target="_blank">Tobin South</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Samuele Marro" target="_blank">Samuele Marro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Hardjono" target="_blank">Thomas Hardjono</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Mahari" target="_blank">Robert Mahari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cedric Whitney" target="_blank">Cedric Whitney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Chan" target="_blank">Alan Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Pentland" target="_blank">Alex Pentland</a>
            </p>
            <p id="summary-9skHxuHyM4@OpenReview" class="summary">The rapid deployment of autonomous AI agents creates urgent challenges in the areas of authorization, accountability, and access control in task delegation. This position paper argues that authenticated and auditable delegation of authority to AI agents is a critical component of mitigating practical risks and unlocking the value of agents. To support this argument, we examine how existing web authentication and authorization protocols, as well as natural language interfaces to common access control mechanisms, can be extended to enable secure authenticated delegation of authority to AI agents. By extending OAuth 2.0 and OpenID Connect with agent-specific credentials and using transparent translation of natural language permissions into robust scoping rules across diverse interaction modalities, we outline how authenticated delegation can be achieved to enable clear chains of accountability while maintaining compatibility with established authentication and web infrastructure for immediate compatibility. This work contributes to ensuring that agentic AI systems perform only appropriate actions. It argues for prioritizing delegation infrastructure as a key component of AI agent governance and provides a roadmap for achieving this.</p>
            <p id="subjects-9skHxuHyM4@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-9skHxuHyM4@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-9skHxuHyM4@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-9skHxuHyM4@OpenReview" onclick="foldPdfKimi('9skHxuHyM4@OpenReview', this)" class="hr hr-fold">
        </div><div id="CA9NxmmUG5@OpenReview" class="panel paper" keywords="labor,prioritize,human,recommend,monopolizing,position,safety,risks,biosecurity,economic">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=CA9NxmmUG5" target="_blank" title="113/120"><span class="index notranslate">#113</span></a>
                <a id="title-CA9NxmmUG5@OpenReview" class="title-link" href="/venue/CA9NxmmUG5@OpenReview" target="_blank">Position: AI Safety should prioritize the Future of Work</a>
                <a id="pdf-CA9NxmmUG5@OpenReview" class="title-pdf notranslate" onclick="togglePdf('CA9NxmmUG5@OpenReview', this)" data="https://openreview.net/pdf?id=CA9NxmmUG5">[PDF<sup id="pdf-stars-CA9NxmmUG5@OpenReview">6</sup>]</a>
                <a id="copy-CA9NxmmUG5@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('CA9NxmmUG5@OpenReview')">[Copy]</a>
                <a id="kimi-CA9NxmmUG5@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('CA9NxmmUG5@OpenReview', this)">[Kimi<sup id="kimi-stars-CA9NxmmUG5@OpenReview">7</sup>]</a>
                <a id="rel-CA9NxmmUG5@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('CA9NxmmUG5@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-CA9NxmmUG5@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sanchaita Hazra" target="_blank">Sanchaita Hazra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bodhisattwa Prasad Majumder" target="_blank">Bodhisattwa Prasad Majumder</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuhin Chakrabarty" target="_blank">Tuhin Chakrabarty</a>
            </p>
            <p id="summary-CA9NxmmUG5@OpenReview" class="summary">Current efforts in AI safety prioritize filtering harmful content, preventing manipulation of human behavior, and eliminating existential risks in cybersecurity or biosecurity. While pressing, this narrow focus overlooks critical human-centric considerations that shape the long-term trajectory of a society. In this position paper, we identify the risks of overlooking the impact of AI on the future of work and recommend comprehensive transition support towards the evolution of meaningful labor with human agency. Through the lens of economic theories, we highlight the intertemporal impacts of AI on human livelihood and the structural changes in labor markets that exacerbate income inequality. Additionally, the closed-source approach of major stakeholders in AI development resembles rent-seeking behavior through exploiting resources, breeding mediocrity in creative labor, and monopolizing innovation. To address this, we argue in favor of a robust international copyright anatomy supported by implementing collective licensing that ensures fair compensation mechanisms for using data to train AI models. We strongly recommend a pro-worker framework of global AI governance to enhance shared prosperity and economic justice while reducing technical debt.</p>
            <p id="subjects-CA9NxmmUG5@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-CA9NxmmUG5@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-CA9NxmmUG5@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-CA9NxmmUG5@OpenReview" onclick="foldPdfKimi('CA9NxmmUG5@OpenReview', this)" class="hr hr-fold">
        </div><div id="GrBXso0e17@OpenReview" class="panel paper" keywords="certified,robustness,certification,position,security,imply,meaningfully,promoted,challenges,research">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=GrBXso0e17" target="_blank" title="114/120"><span class="index notranslate">#114</span></a>
                <a id="title-GrBXso0e17@OpenReview" class="title-link" href="/venue/GrBXso0e17@OpenReview" target="_blank">Position: Certified Robustness Does Not (Yet) Imply Model Security</a>
                <a id="pdf-GrBXso0e17@OpenReview" class="title-pdf notranslate" onclick="togglePdf('GrBXso0e17@OpenReview', this)" data="https://openreview.net/pdf?id=GrBXso0e17">[PDF<sup id="pdf-stars-GrBXso0e17@OpenReview">3</sup>]</a>
                <a id="copy-GrBXso0e17@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('GrBXso0e17@OpenReview')">[Copy]</a>
                <a id="kimi-GrBXso0e17@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('GrBXso0e17@OpenReview', this)">[Kimi<sup id="kimi-stars-GrBXso0e17@OpenReview">1</sup>]</a>
                <a id="rel-GrBXso0e17@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('GrBXso0e17@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-GrBXso0e17@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew C. Cullen" target="_blank">Andrew C. Cullen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul MONTAGUE" target="_blank">Paul MONTAGUE</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sarah Erfani" target="_blank">Sarah Erfani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Rubinstein" target="_blank">Benjamin Rubinstein</a>
            </p>
            <p id="summary-GrBXso0e17@OpenReview" class="summary">While certified robustness is widely promoted as a solution to adversarial examples in Artificial Intelligence systems, significant challenges remain before these techniques can be meaningfully deployed in real-world applications. We identify critical gaps in current research, including the paradox of detection without distinction, the lack of clear criteria for practitioners to evaluate certification schemes, and the potential security risks arising from users' expectations surrounding ``guaranteed" robustness claims. This position paper is a call to arms for the certification research community, proposing concrete steps to address these fundamental challenges and advance the field toward practical applicability.</p>
            <p id="subjects-GrBXso0e17@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-GrBXso0e17@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-GrBXso0e17@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-GrBXso0e17@OpenReview" onclick="foldPdfKimi('GrBXso0e17@OpenReview', this)" class="hr hr-fold">
        </div><div id="H72JEXAPwo@OpenReview" class="panel paper" keywords="neutrality,political,raz,absolutes,position,bias,joseph,striving,conceptualizing,impossible">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=H72JEXAPwo" target="_blank" title="115/120"><span class="index notranslate">#115</span></a>
                <a id="title-H72JEXAPwo@OpenReview" class="title-link" href="/venue/H72JEXAPwo@OpenReview" target="_blank">Position: Political Neutrality in AI Is Impossible — But Here Is How to Approximate It</a>
                <a id="pdf-H72JEXAPwo@OpenReview" class="title-pdf notranslate" onclick="togglePdf('H72JEXAPwo@OpenReview', this)" data="https://openreview.net/pdf?id=H72JEXAPwo">[PDF<sup id="pdf-stars-H72JEXAPwo@OpenReview">4</sup>]</a>
                <a id="copy-H72JEXAPwo@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('H72JEXAPwo@OpenReview')">[Copy]</a>
                <a id="kimi-H72JEXAPwo@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('H72JEXAPwo@OpenReview', this)">[Kimi<sup id="kimi-stars-H72JEXAPwo@OpenReview">6</sup>]</a>
                <a id="rel-H72JEXAPwo@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('H72JEXAPwo@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-H72JEXAPwo@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jillian Fisher" target="_blank">Jillian Fisher</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruth Elisabeth Appel" target="_blank">Ruth Elisabeth Appel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chan Young Park" target="_blank">Chan Young Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujin Potter" target="_blank">Yujin Potter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Jiang" target="_blank">Liwei Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taylor Sorensen" target="_blank">Taylor Sorensen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangbin Feng" target="_blank">Shangbin Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulia Tsvetkov" target="_blank">Yulia Tsvetkov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Margaret Roberts" target="_blank">Margaret Roberts</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jennifer Pan" target="_blank">Jennifer Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dawn Song" target="_blank">Dawn Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yejin Choi" target="_blank">Yejin Choi</a>
            </p>
            <p id="summary-H72JEXAPwo@OpenReview" class="summary">AI systems often exhibit political bias, influencing users' opinions and decisions. While political neutrality—defined as the absence of bias—is often seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz's philosophical insight that "neutrality [...] can be a matter of degree" (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term "approximation" of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two concrete applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.</p>
            <p id="subjects-H72JEXAPwo@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-H72JEXAPwo@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-H72JEXAPwo@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-H72JEXAPwo@OpenReview" onclick="foldPdfKimi('H72JEXAPwo@OpenReview', this)" class="hr hr-fold">
        </div><div id="Rxd2TpV6Eg@OpenReview" class="panel paper" keywords="genai,competitions,rigor,evaluation,gold,typically,empirical,leakage,position,fact">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=Rxd2TpV6Eg" target="_blank" title="116/120"><span class="index notranslate">#116</span></a>
                <a id="title-Rxd2TpV6Eg@OpenReview" class="title-link" href="/venue/Rxd2TpV6Eg@OpenReview" target="_blank">Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation</a>
                <a id="pdf-Rxd2TpV6Eg@OpenReview" class="title-pdf notranslate" onclick="togglePdf('Rxd2TpV6Eg@OpenReview', this)" data="https://openreview.net/pdf?id=Rxd2TpV6Eg">[PDF<sup id="pdf-stars-Rxd2TpV6Eg@OpenReview">3</sup>]</a>
                <a id="copy-Rxd2TpV6Eg@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('Rxd2TpV6Eg@OpenReview')">[Copy]</a>
                <a id="kimi-Rxd2TpV6Eg@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('Rxd2TpV6Eg@OpenReview', this)">[Kimi<sup id="kimi-stars-Rxd2TpV6Eg@OpenReview">2</sup>]</a>
                <a id="rel-Rxd2TpV6Eg@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('Rxd2TpV6Eg@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-Rxd2TpV6Eg@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=D. Sculley" target="_blank">D. Sculley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William Cukierski" target="_blank">William Cukierski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Phil Culliton" target="_blank">Phil Culliton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sohier Dane" target="_blank">Sohier Dane</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maggie Demkin" target="_blank">Maggie Demkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Holbrook" target="_blank">Ryan Holbrook</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Addison Howard" target="_blank">Addison Howard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Mooney" target="_blank">Paul Mooney</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Walter Reade" target="_blank">Walter Reade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meg Risdal" target="_blank">Meg Risdal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nate Keating" target="_blank">Nate Keating</a>
            </p>
            <p id="summary-Rxd2TpV6Eg@OpenReview" class="summary">In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of *leakage* and *contamination* are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.</p>
            <p id="subjects-Rxd2TpV6Eg@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-Rxd2TpV6Eg@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Rxd2TpV6Eg@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Rxd2TpV6Eg@OpenReview" onclick="foldPdfKimi('Rxd2TpV6Eg@OpenReview', this)" class="hr hr-fold">
        </div><div id="V1FP9WDKa7@OpenReview" class="panel paper" keywords="causal,probabilistic,modelling,inference,bespoke,notation,tools,reigns,answer,questions">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=V1FP9WDKa7" target="_blank" title="117/120"><span class="index notranslate">#117</span></a>
                <a id="title-V1FP9WDKa7@OpenReview" class="title-link" href="/venue/V1FP9WDKa7@OpenReview" target="_blank">Position: Probabilistic Modelling is Sufficient for Causal Inference</a>
                <a id="pdf-V1FP9WDKa7@OpenReview" class="title-pdf notranslate" onclick="togglePdf('V1FP9WDKa7@OpenReview', this)" data="https://openreview.net/pdf?id=V1FP9WDKa7">[PDF<sup id="pdf-stars-V1FP9WDKa7@OpenReview">7</sup>]</a>
                <a id="copy-V1FP9WDKa7@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('V1FP9WDKa7@OpenReview')">[Copy]</a>
                <a id="kimi-V1FP9WDKa7@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('V1FP9WDKa7@OpenReview', this)">[Kimi<sup id="kimi-stars-V1FP9WDKa7@OpenReview">10</sup>]</a>
                <a id="rel-V1FP9WDKa7@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('V1FP9WDKa7@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-V1FP9WDKa7@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bruno Mlodozeniec" target="_blank">Bruno Mlodozeniec</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Krueger" target="_blank">David Krueger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard E Turner" target="_blank">Richard E Turner</a>
            </p>
            <p id="summary-V1FP9WDKa7@OpenReview" class="summary">Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we make it clear that you can answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. We argue for the advantages of the generality of the probabilistic modelling lens, when compared to bespoke causal frameworks. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.</p>
            <p id="subjects-V1FP9WDKa7@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-V1FP9WDKa7@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-V1FP9WDKa7@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-V1FP9WDKa7@OpenReview" onclick="foldPdfKimi('V1FP9WDKa7@OpenReview', this)" class="hr hr-fold">
        </div><div id="YuMEUNNpeb@OpenReview" class="panel paper" keywords="benchmarks,medical,llm,validity,construct,psychological,claims,position,prioritize,clinical">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=YuMEUNNpeb" target="_blank" title="118/120"><span class="index notranslate">#118</span></a>
                <a id="title-YuMEUNNpeb@OpenReview" class="title-link" href="/venue/YuMEUNNpeb@OpenReview" target="_blank">Position: Medical Large Language Model Benchmarks Should Prioritize Construct Validity</a>
                <a id="pdf-YuMEUNNpeb@OpenReview" class="title-pdf notranslate" onclick="togglePdf('YuMEUNNpeb@OpenReview', this)" data="https://openreview.net/pdf?id=YuMEUNNpeb">[PDF<sup id="pdf-stars-YuMEUNNpeb@OpenReview">6</sup>]</a>
                <a id="copy-YuMEUNNpeb@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('YuMEUNNpeb@OpenReview')">[Copy]</a>
                <a id="kimi-YuMEUNNpeb@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('YuMEUNNpeb@OpenReview', this)">[Kimi<sup id="kimi-stars-YuMEUNNpeb@OpenReview">6</sup>]</a>
                <a id="rel-YuMEUNNpeb@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('YuMEUNNpeb@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-YuMEUNNpeb@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ahmed Alaa" target="_blank">Ahmed Alaa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Hartvigsen" target="_blank">Thomas Hartvigsen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niloufar Golchini" target="_blank">Niloufar Golchini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiladitya Dutta" target="_blank">Shiladitya Dutta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Frances Dean" target="_blank">Frances Dean</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Inioluwa Raji" target="_blank">Inioluwa Raji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Travis Zack" target="_blank">Travis Zack</a>
            </p>
            <p id="summary-YuMEUNNpeb@OpenReview" class="summary">Medical large language models (LLMs) research often makes bold claims, from encoding clinical knowledge to reasoning like a physician. These claims are usually backed by evaluation on competitive benchmarks—a tradition inherited from mainstream machine learning. But how do we separate real progress from a leaderboard flex? Medical LLM benchmarks, much like those in other fields, are arbitrarily constructed using medical licensing exam questions. For these benchmarks to truly measure progress, they must accurately capture the real-world tasks they aim to represent. In this position paper, we argue that medical LLM benchmarks should—and indeed can—be empirically evaluated for their construct validity. In the psychological testing literature, “construct validity” refers to the ability of a test to measure an underlying “construct”, that is the actual conceptual target of evaluation. By drawing an analogy between LLM benchmarks and psychological tests, we explain how frameworks from this field can provide empirical foundations for validating benchmarks. To put these ideas into practice, we use real-world clinical data in proof-of-concept experiments to evaluate popular medical LLM benchmarks and report significant gaps in their construct validity. Finally, we outline a vision for a new ecosystem of medical LLM evaluation centered around the creation of valid benchmarks.</p>
            <p id="subjects-YuMEUNNpeb@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-YuMEUNNpeb@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-YuMEUNNpeb@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-YuMEUNNpeb@OpenReview" onclick="foldPdfKimi('YuMEUNNpeb@OpenReview', this)" class="hr hr-fold">
        </div><div id="fRk0nKLKrJ@OpenReview" class="panel paper" keywords="regulation,social,generative,media,allegations,argue,companies,faced,position,laws">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=fRk0nKLKrJ" target="_blank" title="119/120"><span class="index notranslate">#119</span></a>
                <a id="title-fRk0nKLKrJ@OpenReview" class="title-link" href="/venue/fRk0nKLKrJ@OpenReview" target="_blank">Position: Generative AI Regulation Can Learn from Social Media Regulation</a>
                <a id="pdf-fRk0nKLKrJ@OpenReview" class="title-pdf notranslate" onclick="togglePdf('fRk0nKLKrJ@OpenReview', this)" data="https://openreview.net/pdf?id=fRk0nKLKrJ">[PDF<sup id="pdf-stars-fRk0nKLKrJ@OpenReview">1</sup>]</a>
                <a id="copy-fRk0nKLKrJ@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('fRk0nKLKrJ@OpenReview')">[Copy]</a>
                <a id="kimi-fRk0nKLKrJ@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('fRk0nKLKrJ@OpenReview', this)">[Kimi<sup id="kimi-stars-fRk0nKLKrJ@OpenReview">4</sup>]</a>
                <a id="rel-fRk0nKLKrJ@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('fRk0nKLKrJ@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-fRk0nKLKrJ@OpenReview" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruth Elisabeth Appel" target="_blank">Ruth Elisabeth Appel</a>
            </p>
            <p id="summary-fRk0nKLKrJ@OpenReview" class="summary">There is strong agreement that generative AI should be regulated, but strong disagreement on how to approach regulation. While some argue that AI regulation should mostly rely on extensions of existing laws, others argue that entirely new laws and regulations are needed to ensure that generative AI benefits society. In this position paper, we argue that the debates on generative AI regulation can be informed by evidence on social media regulation. For example, AI companies have faced allegations of political bias which resemble the allegations social media companies have faced. First, we compare and contrast the affordances of generative AI and social media to highlight their similarities and differences. Then, we discuss four specific policy recommendations based on the evolution of social media and their regulation: (1) counter bias and perceptions thereof (e.g., via transparency, oversight boards, researcher access, democratic input), (2) address specific regulatory concerns (e.g., youth wellbeing, election integrity) and invest in trust and safety, (3) promote computational social science research, and (4) take on a more global perspective. Applying lessons learnt from social media regulation to generative AI regulation can save effort and time, and prevent avoidable mistakes.</p>
            <p id="subjects-fRk0nKLKrJ@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-fRk0nKLKrJ@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-fRk0nKLKrJ@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-fRk0nKLKrJ@OpenReview" onclick="foldPdfKimi('fRk0nKLKrJ@OpenReview', this)" class="hr hr-fold">
        </div><div id="gCPJFcHskT@OpenReview" class="panel paper" keywords="llm,animal,cognition,principles,position,capabilities,behavior,invaluable,evaluations,draw">
            <h2 class="title">
                <a href="https://openreview.net/forum?id=gCPJFcHskT" target="_blank" title="120/120"><span class="index notranslate">#120</span></a>
                <a id="title-gCPJFcHskT@OpenReview" class="title-link" href="/venue/gCPJFcHskT@OpenReview" target="_blank">Position: Principles of Animal Cognition to Improve LLM Evaluations</a>
                <a id="pdf-gCPJFcHskT@OpenReview" class="title-pdf notranslate" onclick="togglePdf('gCPJFcHskT@OpenReview', this)" data="https://openreview.net/pdf?id=gCPJFcHskT">[PDF<sup id="pdf-stars-gCPJFcHskT@OpenReview">9</sup>]</a>
                <a id="copy-gCPJFcHskT@OpenReview" class="title-copy notranslate" onclick="copyToClipboard('gCPJFcHskT@OpenReview')">[Copy]</a>
                <a id="kimi-gCPJFcHskT@OpenReview" class="title-kimi notranslate" onclick="toggleKimi('gCPJFcHskT@OpenReview', this)">[Kimi<sup id="kimi-stars-gCPJFcHskT@OpenReview">14</sup>]</a>
                <a id="rel-gCPJFcHskT@OpenReview" class="title-rel notranslate" onclick="openRelatedPapers('gCPJFcHskT@OpenReview')">[REL]</a>
            </h2>
            <p id="authors-gCPJFcHskT@OpenReview" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sunayana Rane" target="_blank">Sunayana Rane</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cyrus Kirkman" target="_blank">Cyrus Kirkman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Graham Todd" target="_blank">Graham Todd</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amanda Royka" target="_blank">Amanda Royka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ryan Law" target="_blank">Ryan Law</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erica Cartmill" target="_blank">Erica Cartmill</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jacob Foster" target="_blank">Jacob Foster</a>
            </p>
            <p id="summary-gCPJFcHskT@OpenReview" class="summary">It has become increasingly challenging to understand and evaluate LLM capabilities as these models exhibit a broader range of behaviors. In this position paper, we argue that LLM researchers should draw on the lessons from another field which has developed a rich set of experimental paradigms and design practices for probing the behavior of complex intelligent systems: animal cognition. We present five core principles of evaluation drawn from animal cognition research, and explain how they provide invaluable guidance for understanding LLM capabilities and behavior. We ground these principles in an empirical case study, and show how they can already provide a richer picture of one particular reasoning capability: transitive inference.</p>
            <p id="subjects-gCPJFcHskT@OpenReview" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/ICML.2025?group=Oral" target="_blank">ICML.2025 - Oral</a>
            </p>
            <div id="pdf-container-gCPJFcHskT@OpenReview" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-gCPJFcHskT@OpenReview" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-gCPJFcHskT@OpenReview" onclick="foldPdfKimi('gCPJFcHskT@OpenReview', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-QqVZ28qems@OpenReview" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#QqVZ28qems@OpenReview">How Do Large Language Monkeys Get Their Power (Laws)?</a>
                    <a class="i-star" onclick="toggleAppStar('QqVZ28qems@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QqVZ28qems@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-WGXb7UdvTX@OpenReview" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#WGXb7UdvTX@OpenReview">Layer by Layer: Uncovering Hidden Representations in Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('WGXb7UdvTX@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('WGXb7UdvTX@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-QmIzUuspWo@OpenReview" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#QmIzUuspWo@OpenReview">An Online Adaptive Sampling Algorithm for Stochastic Difference-of-convex Optimization with Time-varying Distributions</a>
                    <a class="i-star" onclick="toggleAppStar('QmIzUuspWo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QmIzUuspWo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-VsJ1K2HV3k@OpenReview" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#VsJ1K2HV3k@OpenReview">On Path to Multimodal Generalist: General-Level and General-Bench</a>
                    <a class="i-star" onclick="toggleAppStar('VsJ1K2HV3k@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('VsJ1K2HV3k@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-qR7YsQdFxV@OpenReview" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#qR7YsQdFxV@OpenReview">All-Purpose Mean Estimation over R: Optimal Sub-Gaussianity with Outlier Robustness and Low Moments Performance</a>
                    <a class="i-star" onclick="toggleAppStar('qR7YsQdFxV@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qR7YsQdFxV@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-l8QemUZaIA@OpenReview" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#l8QemUZaIA@OpenReview">Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards</a>
                    <a class="i-star" onclick="toggleAppStar('l8QemUZaIA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('l8QemUZaIA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-0LZRtvK871@OpenReview" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#0LZRtvK871@OpenReview">Improving the Scaling Laws of Synthetic Data with Deliberate Practice</a>
                    <a class="i-star" onclick="toggleAppStar('0LZRtvK871@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0LZRtvK871@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-26JsumCG0z@OpenReview" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#26JsumCG0z@OpenReview">The Value of Prediction in Identifying the Worst-Off</a>
                    <a class="i-star" onclick="toggleAppStar('26JsumCG0z@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('26JsumCG0z@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2uheUFcFsM@OpenReview" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#2uheUFcFsM@OpenReview">Normalizing Flows are Capable Generative Models</a>
                    <a class="i-star" onclick="toggleAppStar('2uheUFcFsM@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2uheUFcFsM@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-36hVB7DEB0@OpenReview" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#36hVB7DEB0@OpenReview">Emergence in non-neural models: grokking modular arithmetic via average gradient outer product</a>
                    <a class="i-star" onclick="toggleAppStar('36hVB7DEB0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('36hVB7DEB0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-3go0lhfxd0@OpenReview" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#3go0lhfxd0@OpenReview">Algorithm Development in Neural Networks: Insights from the Streaming Parity Task</a>
                    <a class="i-star" onclick="toggleAppStar('3go0lhfxd0@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('3go0lhfxd0@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-4tFSKOY2mT@OpenReview" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#4tFSKOY2mT@OpenReview">What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities</a>
                    <a class="i-star" onclick="toggleAppStar('4tFSKOY2mT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4tFSKOY2mT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-5zwF1GizFa@OpenReview" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#5zwF1GizFa@OpenReview">rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</a>
                    <a class="i-star" onclick="toggleAppStar('5zwF1GizFa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('5zwF1GizFa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Cf0N07E1vu@OpenReview" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#Cf0N07E1vu@OpenReview">Theoretical Limitations of Ensembles in the Age of Overparameterization</a>
                    <a class="i-star" onclick="toggleAppStar('Cf0N07E1vu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cf0N07E1vu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-DgGF2LEBPS@OpenReview" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#DgGF2LEBPS@OpenReview">EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents</a>
                    <a class="i-star" onclick="toggleAppStar('DgGF2LEBPS@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DgGF2LEBPS@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-DjJmre5IkP@OpenReview" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#DjJmre5IkP@OpenReview">Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions</a>
                    <a class="i-star" onclick="toggleAppStar('DjJmre5IkP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DjJmre5IkP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-DmH4HHVb3y@OpenReview" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#DmH4HHVb3y@OpenReview">CollabLLM: From Passive Responders to Active Collaborators</a>
                    <a class="i-star" onclick="toggleAppStar('DmH4HHVb3y@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('DmH4HHVb3y@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-E1E6T7KHlR@OpenReview" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#E1E6T7KHlR@OpenReview">Generative Social Choice: The Next Generation</a>
                    <a class="i-star" onclick="toggleAppStar('E1E6T7KHlR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('E1E6T7KHlR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-EBNgREMoVD@OpenReview" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#EBNgREMoVD@OpenReview">Hierarchical Refinement: Optimal Transport to Infinity and Beyond</a>
                    <a class="i-star" onclick="toggleAppStar('EBNgREMoVD@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EBNgREMoVD@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-EVwMw2lVlw@OpenReview" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#EVwMw2lVlw@OpenReview">SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('EVwMw2lVlw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EVwMw2lVlw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-F08lzoBgad@OpenReview" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#F08lzoBgad@OpenReview">In-Context Denoising with One-Layer Transformers: Connections between Attention and Associative Memory Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('F08lzoBgad@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('F08lzoBgad@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Fvq9ogLnLN@OpenReview" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#Fvq9ogLnLN@OpenReview">Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Fvq9ogLnLN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fvq9ogLnLN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-GFpjO8S8Po@OpenReview" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#GFpjO8S8Po@OpenReview">Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection</a>
                    <a class="i-star" onclick="toggleAppStar('GFpjO8S8Po@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GFpjO8S8Po@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-I1OHPb4zWo@OpenReview" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#I1OHPb4zWo@OpenReview">Flowing Datasets with Wasserstein over Wasserstein Gradient Flows</a>
                    <a class="i-star" onclick="toggleAppStar('I1OHPb4zWo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('I1OHPb4zWo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-KGOcrIWYnx@OpenReview" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#KGOcrIWYnx@OpenReview">Learning dynamics in linear recurrent neural networks</a>
                    <a class="i-star" onclick="toggleAppStar('KGOcrIWYnx@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KGOcrIWYnx@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-KPRIwWhqAZ@OpenReview" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#KPRIwWhqAZ@OpenReview">DeFoG: Discrete Flow Matching for Graph Generation</a>
                    <a class="i-star" onclick="toggleAppStar('KPRIwWhqAZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KPRIwWhqAZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LCbHsdtvOR@OpenReview" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#LCbHsdtvOR@OpenReview">Expected Variational Inequalities</a>
                    <a class="i-star" onclick="toggleAppStar('LCbHsdtvOR@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LCbHsdtvOR@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LO7ciRpjI5@OpenReview" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#LO7ciRpjI5@OpenReview">Sundial: A Family of Highly Capable Time Series Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar('LO7ciRpjI5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LO7ciRpjI5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LbJQYNSH41@OpenReview" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#LbJQYNSH41@OpenReview">A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('LbJQYNSH41@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LbJQYNSH41@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-NIe74CY9lk@OpenReview" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#NIe74CY9lk@OpenReview">MGD$^3$ : Mode-Guided Dataset Distillation using Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('NIe74CY9lk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('NIe74CY9lk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OWIPDWhUcO@OpenReview" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#OWIPDWhUcO@OpenReview">AdaSplash: Adaptive Sparse Flash Attention</a>
                    <a class="i-star" onclick="toggleAppStar('OWIPDWhUcO@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OWIPDWhUcO@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OZSXYeqpI1@OpenReview" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#OZSXYeqpI1@OpenReview">Auditing $f$-differential privacy in one run</a>
                    <a class="i-star" onclick="toggleAppStar('OZSXYeqpI1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OZSXYeqpI1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PNmkjIzHB7@OpenReview" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#PNmkjIzHB7@OpenReview">Conformal Prediction as Bayesian Quadrature</a>
                    <a class="i-star" onclick="toggleAppStar('PNmkjIzHB7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PNmkjIzHB7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-PqDvTWdQwm@OpenReview" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#PqDvTWdQwm@OpenReview">A Generalization Result for Convergence in Learning-to-Optimize</a>
                    <a class="i-star" onclick="toggleAppStar('PqDvTWdQwm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('PqDvTWdQwm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Rc7y9HFC34@OpenReview" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#Rc7y9HFC34@OpenReview">ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</a>
                    <a class="i-star" onclick="toggleAppStar('Rc7y9HFC34@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Rc7y9HFC34@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-SyQPiZJVWY@OpenReview" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#SyQPiZJVWY@OpenReview">LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('SyQPiZJVWY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('SyQPiZJVWY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-UeB3Hdrhda@OpenReview" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#UeB3Hdrhda@OpenReview">Training a Generally Curious Agent</a>
                    <a class="i-star" onclick="toggleAppStar('UeB3Hdrhda@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('UeB3Hdrhda@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Vk1rNMl0J1@OpenReview" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#Vk1rNMl0J1@OpenReview">Learning Dynamics in Continual Pre-Training for Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Vk1rNMl0J1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Vk1rNMl0J1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-X9vBykZVYg@OpenReview" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#X9vBykZVYg@OpenReview">Retrieval-Augmented Perception: High-resolution Image Perception Meets Visual RAG</a>
                    <a class="i-star" onclick="toggleAppStar('X9vBykZVYg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('X9vBykZVYg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZAlII9wL5i@OpenReview" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#ZAlII9wL5i@OpenReview">Equivalence is All: A Unified View for Self-supervised Graph Learning</a>
                    <a class="i-star" onclick="toggleAppStar('ZAlII9wL5i@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZAlII9wL5i@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aHzPGyUhZa@OpenReview" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#aHzPGyUhZa@OpenReview">STAIR: Improving Safety Alignment with Introspective Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar('aHzPGyUhZa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aHzPGyUhZa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aOIJ2gVRWW@OpenReview" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#aOIJ2gVRWW@OpenReview">Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('aOIJ2gVRWW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aOIJ2gVRWW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-aTBwCSkPxv@OpenReview" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#aTBwCSkPxv@OpenReview">Transformative or Conservative? Conservation laws for ResNets and Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('aTBwCSkPxv@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('aTBwCSkPxv@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-d2aGLPSpFz@OpenReview" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#d2aGLPSpFz@OpenReview">Sanity Checking Causal Representation Learning on a Simple Real-World System</a>
                    <a class="i-star" onclick="toggleAppStar('d2aGLPSpFz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('d2aGLPSpFz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-esBoQFmD7v@OpenReview" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#esBoQFmD7v@OpenReview">Strategy Coopetition Explains the Emergence and Transience of In-Context Learning</a>
                    <a class="i-star" onclick="toggleAppStar('esBoQFmD7v@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('esBoQFmD7v@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-etxseIT47b@OpenReview" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#etxseIT47b@OpenReview">General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization</a>
                    <a class="i-star" onclick="toggleAppStar('etxseIT47b@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('etxseIT47b@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fCPB0qRJT2@OpenReview" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#fCPB0qRJT2@OpenReview">AutoGFM: Automated Graph Foundation Model with Adaptive Architecture Customization</a>
                    <a class="i-star" onclick="toggleAppStar('fCPB0qRJT2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fCPB0qRJT2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ilpL2qACla@OpenReview" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#ilpL2qACla@OpenReview">An analytic theory of creativity in convolutional diffusion models</a>
                    <a class="i-star" onclick="toggleAppStar('ilpL2qACla@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ilpL2qACla@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-imcyVlzpXh@OpenReview" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#imcyVlzpXh@OpenReview">Multi-agent Architecture Search via Agentic Supernet</a>
                    <a class="i-star" onclick="toggleAppStar('imcyVlzpXh@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('imcyVlzpXh@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-j6H7c3aQyb@OpenReview" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#j6H7c3aQyb@OpenReview">Temporal Difference Flows</a>
                    <a class="i-star" onclick="toggleAppStar('j6H7c3aQyb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('j6H7c3aQyb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-jP59rz1bZk@OpenReview" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#jP59rz1bZk@OpenReview">ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('jP59rz1bZk@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('jP59rz1bZk@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kEn7Wt6Yj2@OpenReview" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#kEn7Wt6Yj2@OpenReview">On Differential Privacy for Adaptively Solving Search Problems via Sketching</a>
                    <a class="i-star" onclick="toggleAppStar('kEn7Wt6Yj2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kEn7Wt6Yj2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kJQgMGLrow@OpenReview" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#kJQgMGLrow@OpenReview">A Generalization Theory for Zero-Shot Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('kJQgMGLrow@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kJQgMGLrow@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mIomqOskaa@OpenReview" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#mIomqOskaa@OpenReview">Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning</a>
                    <a class="i-star" onclick="toggleAppStar('mIomqOskaa@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mIomqOskaa@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-nq5bt0mRTC@OpenReview" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#nq5bt0mRTC@OpenReview">Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration</a>
                    <a class="i-star" onclick="toggleAppStar('nq5bt0mRTC@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('nq5bt0mRTC@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pwNSUo7yUb@OpenReview" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#pwNSUo7yUb@OpenReview">Inductive Moment Matching</a>
                    <a class="i-star" onclick="toggleAppStar('pwNSUo7yUb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pwNSUo7yUb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-reuShgiHdg@OpenReview" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#reuShgiHdg@OpenReview">ReferSplat: Referring Segmentation in 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('reuShgiHdg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('reuShgiHdg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tO7OVZkCo1@OpenReview" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#tO7OVZkCo1@OpenReview">VideoRoPE: What Makes for Good Video Rotary Position Embedding?</a>
                    <a class="i-star" onclick="toggleAppStar('tO7OVZkCo1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tO7OVZkCo1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-up21Rwj5Fo@OpenReview" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#up21Rwj5Fo@OpenReview">Fully Dynamic Euclidean Bi-Chromatic Matching in Sublinear Update Time</a>
                    <a class="i-star" onclick="toggleAppStar('up21Rwj5Fo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('up21Rwj5Fo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-v26vwjxOEz@OpenReview" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#v26vwjxOEz@OpenReview">Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark</a>
                    <a class="i-star" onclick="toggleAppStar('v26vwjxOEz@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('v26vwjxOEz@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-v77ZMzbsBA@OpenReview" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#v77ZMzbsBA@OpenReview">Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models</a>
                    <a class="i-star" onclick="toggleAppStar('v77ZMzbsBA@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('v77ZMzbsBA@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vQubr1uBUw@OpenReview" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#vQubr1uBUw@OpenReview">Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies</a>
                    <a class="i-star" onclick="toggleAppStar('vQubr1uBUw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vQubr1uBUw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-vt65VjJakt@OpenReview" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#vt65VjJakt@OpenReview">ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $\alpha$-$\beta$-Divergence</a>
                    <a class="i-star" onclick="toggleAppStar('vt65VjJakt@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('vt65VjJakt@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-wUEp13rqXP@OpenReview" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#wUEp13rqXP@OpenReview">Mixture of Lookup Experts</a>
                    <a class="i-star" onclick="toggleAppStar('wUEp13rqXP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('wUEp13rqXP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-x4qvBVuzzu@OpenReview" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#x4qvBVuzzu@OpenReview">From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on LoRA with Parallel Control</a>
                    <a class="i-star" onclick="toggleAppStar('x4qvBVuzzu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('x4qvBVuzzu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xmbdACI0xu@OpenReview" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#xmbdACI0xu@OpenReview">AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('xmbdACI0xu@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xmbdACI0xu@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-y3d4Bs2r7r@OpenReview" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#y3d4Bs2r7r@OpenReview">Addressing Misspecification in Simulation-based Inference through Data-driven Calibration</a>
                    <a class="i-star" onclick="toggleAppStar('y3d4Bs2r7r@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('y3d4Bs2r7r@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-yMJcHWcb2Z@OpenReview" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#yMJcHWcb2Z@OpenReview">VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</a>
                    <a class="i-star" onclick="toggleAppStar('yMJcHWcb2Z@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('yMJcHWcb2Z@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zBBYsVGKuB@OpenReview" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#zBBYsVGKuB@OpenReview">Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination</a>
                    <a class="i-star" onclick="toggleAppStar('zBBYsVGKuB@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zBBYsVGKuB@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-zf9zwCRKyP@OpenReview" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#zf9zwCRKyP@OpenReview">Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards</a>
                    <a class="i-star" onclick="toggleAppStar('zf9zwCRKyP@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('zf9zwCRKyP@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2GmXJnyNM4@OpenReview" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#2GmXJnyNM4@OpenReview">Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent</a>
                    <a class="i-star" onclick="toggleAppStar('2GmXJnyNM4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2GmXJnyNM4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4AmFA0qNQ2@OpenReview" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#4AmFA0qNQ2@OpenReview">Long-Form Speech Generation with Spoken Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('4AmFA0qNQ2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4AmFA0qNQ2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4ViG4gQD3i@OpenReview" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#4ViG4gQD3i@OpenReview">Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them All</a>
                    <a class="i-star" onclick="toggleAppStar('4ViG4gQD3i@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4ViG4gQD3i@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-7Tp9zjP9At@OpenReview" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#7Tp9zjP9At@OpenReview">Neural Discovery in Mathematics: Do Machines Dream of Colored Planes?</a>
                    <a class="i-star" onclick="toggleAppStar('7Tp9zjP9At@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('7Tp9zjP9At@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9NmdppWbXi@OpenReview" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#9NmdppWbXi@OpenReview">Position: A Critical Perspective on The Value in Studying Deep Learning Phenomena</a>
                    <a class="i-star" onclick="toggleAppStar('9NmdppWbXi@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9NmdppWbXi@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ACyyBrUioy@OpenReview" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#ACyyBrUioy@OpenReview">Near-Optimal Decision Trees in a SPLIT Second</a>
                    <a class="i-star" onclick="toggleAppStar('ACyyBrUioy@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ACyyBrUioy@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-qMt4KikFJg@OpenReview" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#qMt4KikFJg@OpenReview">Rényi Neural Processes</a>
                    <a class="i-star" onclick="toggleAppStar('qMt4KikFJg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('qMt4KikFJg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-EZV4edMGM1@OpenReview" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#EZV4edMGM1@OpenReview">Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise</a>
                    <a class="i-star" onclick="toggleAppStar('EZV4edMGM1@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('EZV4edMGM1@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-yDTwamN4LQ@OpenReview" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#yDTwamN4LQ@OpenReview">Learning with Expected Signatures: Theory and Applications</a>
                    <a class="i-star" onclick="toggleAppStar('yDTwamN4LQ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('yDTwamN4LQ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-l19DmXbwPK@OpenReview" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#l19DmXbwPK@OpenReview">VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data</a>
                    <a class="i-star" onclick="toggleAppStar('l19DmXbwPK@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('l19DmXbwPK@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-KwIlvmLDLm@OpenReview" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#KwIlvmLDLm@OpenReview">LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</a>
                    <a class="i-star" onclick="toggleAppStar('KwIlvmLDLm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('KwIlvmLDLm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-LwQGRGJTHw@OpenReview" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#LwQGRGJTHw@OpenReview">Fundamental Bias in Inverting Random Sampling Matrices with Application to Sub-sampled Newton</a>
                    <a class="i-star" onclick="toggleAppStar('LwQGRGJTHw@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('LwQGRGJTHw@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-FJKnru1xUF@OpenReview" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#FJKnru1xUF@OpenReview">AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses</a>
                    <a class="i-star" onclick="toggleAppStar('FJKnru1xUF@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('FJKnru1xUF@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-OEl3L8osas@OpenReview" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#OEl3L8osas@OpenReview">The dark side of the forces: assessing non-conservative force models for atomistic machine learning</a>
                    <a class="i-star" onclick="toggleAppStar('OEl3L8osas@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('OEl3L8osas@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ybno0ZP44z@OpenReview" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#ybno0ZP44z@OpenReview">Improved Regret Analysis in Gaussian Process Bandits: Optimality for Noiseless Reward, RKHS norm, and Non-Stationary Variance</a>
                    <a class="i-star" onclick="toggleAppStar('ybno0ZP44z@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ybno0ZP44z@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-46yLEXtav4@OpenReview" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#46yLEXtav4@OpenReview">Statistical Collusion by Collectives on Learning Platforms</a>
                    <a class="i-star" onclick="toggleAppStar('46yLEXtav4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('46yLEXtav4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-o9zDYV4Ism@OpenReview" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#o9zDYV4Ism@OpenReview">LoRA Training Provably Converges to a Low-Rank Global Minimum Or It Fails Loudly (But it Probably Won't Fail)</a>
                    <a class="i-star" onclick="toggleAppStar('o9zDYV4Ism@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('o9zDYV4Ism@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-xZXhFg43EI@OpenReview" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#xZXhFg43EI@OpenReview">SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</a>
                    <a class="i-star" onclick="toggleAppStar('xZXhFg43EI@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('xZXhFg43EI@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-tlniJJFUW2@OpenReview" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#tlniJJFUW2@OpenReview">Machine Learning meets Algebraic Combinatorics: A Suite of Datasets Capturing Research-level Conjecturing Ability in Pure Mathematics</a>
                    <a class="i-star" onclick="toggleAppStar('tlniJJFUW2@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('tlniJJFUW2@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mBstuGUaXo@OpenReview" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#mBstuGUaXo@OpenReview">Score Matching with Missing Data</a>
                    <a class="i-star" onclick="toggleAppStar('mBstuGUaXo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mBstuGUaXo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-M6L7Eaw9BW@OpenReview" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#M6L7Eaw9BW@OpenReview">Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar('M6L7Eaw9BW@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('M6L7Eaw9BW@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uitj69FqD5@OpenReview" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#uitj69FqD5@OpenReview">Model Immunization from a Condition Number Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('uitj69FqD5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uitj69FqD5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-uRAgIVnAO6@OpenReview" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#uRAgIVnAO6@OpenReview">High-Dimensional Prediction for Sequential Decision Making</a>
                    <a class="i-star" onclick="toggleAppStar('uRAgIVnAO6@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('uRAgIVnAO6@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-pOAEfqa26i@OpenReview" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#pOAEfqa26i@OpenReview">Learning Time-Varying Multi-Region Brain Communications via Scalable Markovian Gaussian Processes</a>
                    <a class="i-star" onclick="toggleAppStar('pOAEfqa26i@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('pOAEfqa26i@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-R0PBjxIbgm@OpenReview" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#R0PBjxIbgm@OpenReview">Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('R0PBjxIbgm@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('R0PBjxIbgm@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-2aKHuXdr7Q@OpenReview" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#2aKHuXdr7Q@OpenReview">Going Deeper into Locally Differentially Private Graph Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('2aKHuXdr7Q@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('2aKHuXdr7Q@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-4EYwwVuhtG@OpenReview" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#4EYwwVuhtG@OpenReview">Statistical Test for Feature Selection Pipelines by Selective Inference</a>
                    <a class="i-star" onclick="toggleAppStar('4EYwwVuhtG@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('4EYwwVuhtG@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-z19u9B2fCZ@OpenReview" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#z19u9B2fCZ@OpenReview">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</a>
                    <a class="i-star" onclick="toggleAppStar('z19u9B2fCZ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('z19u9B2fCZ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-6CwO5nVvku@OpenReview" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#6CwO5nVvku@OpenReview">Partition First, Embed Later: Laplacian-Based Feature Partitioning for Refined Embedding and Visualization of High-Dimensional Data</a>
                    <a class="i-star" onclick="toggleAppStar('6CwO5nVvku@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('6CwO5nVvku@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tv2JDGw920@OpenReview" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#Tv2JDGw920@OpenReview">One-Step Generalization Ratio Guided Optimization for Domain Generalization</a>
                    <a class="i-star" onclick="toggleAppStar('Tv2JDGw920@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tv2JDGw920@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-70voOlSPos@OpenReview" style="display:none">
                    <span class="i-index">#101</span>
                    <a class="i-title" href="#70voOlSPos@OpenReview">Polynomial-Delay MAG Listing with Novel Locally Complete Orientation Rules</a>
                    <a class="i-star" onclick="toggleAppStar('70voOlSPos@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('70voOlSPos@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-ZdqTePSV1K@OpenReview" style="display:none">
                    <span class="i-index">#102</span>
                    <a class="i-title" href="#ZdqTePSV1K@OpenReview">Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection</a>
                    <a class="i-star" onclick="toggleAppStar('ZdqTePSV1K@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('ZdqTePSV1K@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-QvqnPVGWAN@OpenReview" style="display:none">
                    <span class="i-index">#103</span>
                    <a class="i-title" href="#QvqnPVGWAN@OpenReview">Blink of an eye: a simple theory for feature localization in generative models</a>
                    <a class="i-star" onclick="toggleAppStar('QvqnPVGWAN@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('QvqnPVGWAN@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hi0SyHMmkd@OpenReview" style="display:none">
                    <span class="i-index">#104</span>
                    <a class="i-title" href="#Hi0SyHMmkd@OpenReview">Roll the dice &amp; look before you leap: Going beyond the creative limits of next-token prediction</a>
                    <a class="i-star" onclick="toggleAppStar('Hi0SyHMmkd@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hi0SyHMmkd@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-feIaF6vYFl@OpenReview" style="display:none">
                    <span class="i-index">#105</span>
                    <a class="i-title" href="#feIaF6vYFl@OpenReview">CodeIO: Condensing Reasoning Patterns via Code Input-Output Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('feIaF6vYFl@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('feIaF6vYFl@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-V0w8Kj3K6L@OpenReview" style="display:none">
                    <span class="i-index">#106</span>
                    <a class="i-title" href="#V0w8Kj3K6L@OpenReview">Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings</a>
                    <a class="i-star" onclick="toggleAppStar('V0w8Kj3K6L@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('V0w8Kj3K6L@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-mr0xOQTJkL@OpenReview" style="display:none">
                    <span class="i-index">#107</span>
                    <a class="i-title" href="#mr0xOQTJkL@OpenReview">An Improved Clique-Picking Algorithm for Counting Markov Equivalent DAGs via Super Cliques Transfer</a>
                    <a class="i-star" onclick="toggleAppStar('mr0xOQTJkL@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('mr0xOQTJkL@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-rc65N9xIrY@OpenReview" style="display:none">
                    <span class="i-index">#108</span>
                    <a class="i-title" href="#rc65N9xIrY@OpenReview">DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs</a>
                    <a class="i-star" onclick="toggleAppStar('rc65N9xIrY@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('rc65N9xIrY@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-0yzOEMbShU@OpenReview" style="display:none">
                    <span class="i-index">#109</span>
                    <a class="i-title" href="#0yzOEMbShU@OpenReview">Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs</a>
                    <a class="i-star" onclick="toggleAppStar('0yzOEMbShU@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('0yzOEMbShU@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-kV8oUyjdIg@OpenReview" style="display:none">
                    <span class="i-index">#110</span>
                    <a class="i-title" href="#kV8oUyjdIg@OpenReview">Nonlinearly Preconditioned Gradient Methods under Generalized Smoothness</a>
                    <a class="i-star" onclick="toggleAppStar('kV8oUyjdIg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('kV8oUyjdIg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-1rh8iTehBc@OpenReview" style="display:none">
                    <span class="i-index">#111</span>
                    <a class="i-title" href="#1rh8iTehBc@OpenReview">Position: Current Model Licensing Practices are Dragging Us into a Quagmire of Legal Noncompliance</a>
                    <a class="i-star" onclick="toggleAppStar('1rh8iTehBc@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('1rh8iTehBc@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-9skHxuHyM4@OpenReview" style="display:none">
                    <span class="i-index">#112</span>
                    <a class="i-title" href="#9skHxuHyM4@OpenReview">Position: AI Agents Need Authenticated Delegation</a>
                    <a class="i-star" onclick="toggleAppStar('9skHxuHyM4@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('9skHxuHyM4@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-CA9NxmmUG5@OpenReview" style="display:none">
                    <span class="i-index">#113</span>
                    <a class="i-title" href="#CA9NxmmUG5@OpenReview">Position: AI Safety should prioritize the Future of Work</a>
                    <a class="i-star" onclick="toggleAppStar('CA9NxmmUG5@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('CA9NxmmUG5@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-GrBXso0e17@OpenReview" style="display:none">
                    <span class="i-index">#114</span>
                    <a class="i-title" href="#GrBXso0e17@OpenReview">Position: Certified Robustness Does Not (Yet) Imply Model Security</a>
                    <a class="i-star" onclick="toggleAppStar('GrBXso0e17@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('GrBXso0e17@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-H72JEXAPwo@OpenReview" style="display:none">
                    <span class="i-index">#115</span>
                    <a class="i-title" href="#H72JEXAPwo@OpenReview">Position: Political Neutrality in AI Is Impossible — But Here Is How to Approximate It</a>
                    <a class="i-star" onclick="toggleAppStar('H72JEXAPwo@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('H72JEXAPwo@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Rxd2TpV6Eg@OpenReview" style="display:none">
                    <span class="i-index">#116</span>
                    <a class="i-title" href="#Rxd2TpV6Eg@OpenReview">Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation</a>
                    <a class="i-star" onclick="toggleAppStar('Rxd2TpV6Eg@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Rxd2TpV6Eg@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-V1FP9WDKa7@OpenReview" style="display:none">
                    <span class="i-index">#117</span>
                    <a class="i-title" href="#V1FP9WDKa7@OpenReview">Position: Probabilistic Modelling is Sufficient for Causal Inference</a>
                    <a class="i-star" onclick="toggleAppStar('V1FP9WDKa7@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('V1FP9WDKa7@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-YuMEUNNpeb@OpenReview" style="display:none">
                    <span class="i-index">#118</span>
                    <a class="i-title" href="#YuMEUNNpeb@OpenReview">Position: Medical Large Language Model Benchmarks Should Prioritize Construct Validity</a>
                    <a class="i-star" onclick="toggleAppStar('YuMEUNNpeb@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('YuMEUNNpeb@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-fRk0nKLKrJ@OpenReview" style="display:none">
                    <span class="i-index">#119</span>
                    <a class="i-title" href="#fRk0nKLKrJ@OpenReview">Position: Generative AI Regulation Can Learn from Social Media Regulation</a>
                    <a class="i-star" onclick="toggleAppStar('fRk0nKLKrJ@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('fRk0nKLKrJ@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-gCPJFcHskT@OpenReview" style="display:none">
                    <span class="i-index">#120</span>
                    <a class="i-title" href="#gCPJFcHskT@OpenReview">Position: Principles of Animal Cognition to Improve LLM Evaluations</a>
                    <a class="i-star" onclick="toggleAppStar('gCPJFcHskT@OpenReview')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('gCPJFcHskT@OpenReview')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Main-bold, sans-serif;"></div></div></body></html>