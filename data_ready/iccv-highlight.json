{
  "source_html": "html\\iccv-highlight.html",
  "paper_count": 263,
  "conference": "iccv",
  "year": 2025,
  "status": "highlight",
  "papers": [
    {
      "paper_id": "Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF",
      "index": 1,
      "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
      "authors": [
        "Zeqiang Lai",
        "Yunfei Zhao",
        "Zibo Zhao",
        "Haolin Liu",
        "Fuyun Wang",
        "Huiwen Shi",
        "Xianghui Yang",
        "Qingxiang Lin",
        "Jingwei Huang",
        "Yuhong Liu",
        "Jie Jiang",
        "Chunchao Guo",
        "Xiangyu Yue"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "vdm",
        "flashvdm",
        "generation",
        "vectset",
        "diffusion",
        "vae",
        "dit",
        "hunyuan3d",
        "shape",
        "decoding"
      ],
      "summary": "3D shape generation has greatly flourished through the development of so-called \"native\" 3D diffusion, particularly through the Vectset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles at high-speed generation. Challenges exist because of not only difficulties in accelerating diffusion sampling but also VAE decoding in VDM -- areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps, while maintaining comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation technique. For VAE, we introduce a lightning vectset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding,, and Efficient Network Design. By exploiting the locality of vectset and the sparsity of shape surface in the volume, the proposed decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to the current state-of-the-art open-source shape generation model Hunyuan3D-2, resulting in Hunyuan3D-2 Turbo. Through systematic evaluation for both generation and reconstruction, we demonstrate that our model outperforms existing fast 3D generation methods by a significant margin, achieving comparable performance to the state-of-the-art models while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are publicly available at https://github.com/Tencent-Hunyuan/FlashVDM.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lai_Unleashing_Vecset_Diffusion_Model_for_Fast_Shape_Generation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 5
      },
      "raw_excerpt": "Unleashing Vecset Diffusion Model for Fast Shape Generation [PDF 12 ] [Copy] [Kimi 5 ] [REL] Authors : Zeqiang Lai , Yunfei Zhao , Zibo Zhao , Haolin Liu , Fuyun Wang , Huiwen Shi , Xianghui Yang , Qingxiang Lin , Jingwei Huang , Yuhong Liu , Jie Jiang , Chunchao Guo , Xiangyu Yue 3D shape generation has greatly flourished through the development of so-called \"native\" 3D diffusion, particularly through the Vectset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles at high-speed generation. Challenges exist because of not only difficulties in accelerating diffusion sampling but also VAE decoding in VDM -- areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps, while maintaining comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation technique. For VAE, we introduce a lightning vectset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding,, and Efficient Network Design. By exploiting the locality of vectset and the sparsity of shape surface in the volume, the proposed decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to the current state-of-the-art open-source shape generation model Hunyuan3D-2, resulting in Hunyuan3D-2 Turbo. Through systematic evaluation for both generation and reconstruction, we demonstrate that our model outperforms existing fast 3D generation methods by a significant margin, achieving comparable performance to the state-of-the-art models while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are publicly available at https://github.com/Tencent-Hunyuan/FlashVDM. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF",
      "index": 2,
      "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion",
      "authors": [
        "Mutian Xu",
        "Chongjie Ye",
        "Haolin Liu",
        "Yushuang Wu",
        "Jiahao Chang",
        "Xiaoguang Han"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "depth",
        "data",
        "sim2real",
        "real",
        "stage",
        "simulation",
        "captured",
        "diffusion",
        "simulated",
        "synthetic"
      ],
      "summary": "3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes StableDiffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_Stable-Sim2Real_Exploring_Simulation_of_Real-Captured_3D_Data_with_Two-Stage_Depth_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Mutian Xu , Chongjie Ye , Haolin Liu , Yushuang Wu , Jiahao Chang , Xiaoguang Han 3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes StableDiffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF",
      "index": 3,
      "title": "UDC-VIT: A Real-World Video Dataset for Under-Display Cameras",
      "authors": [
        "Kyusu Ahn",
        "JiSoo Kim",
        "Sangik Lee",
        "HyunGyu Lee",
        "Byeonghyun Ko",
        "Chanwoo Park",
        "Jaejin Lee"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "udc",
        "vit",
        "video",
        "videos",
        "display",
        "dataset",
        "datasets",
        "world",
        "udcs",
        "degraded"
      ],
      "summary": "Even though an Under-Display Camera (UDC) is an advanced imaging system, the display panel significantly degrades captured images or videos, introducing low transmittance, blur, noise, and flare issues. Tackling such issues is challenging because of the complex degradation of UDCs, including diverse flare patterns. However, no dataset contains videos of real-world UDC degradation. In this paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike existing datasets, UDC-VIT exclusively includes human motions for facial recognition. We propose a video-capturing system to acquire clean and UDC-degraded videos of the same scene simultaneously. Then, we align a pair of captured videos frame by frame, using discrete Fourier transform (DFT). We compare UDC-VIT with six representative UDC still image datasets and two existing UDC video datasets. Using six deep-learning models, we compare UDC-VIT and an existing synthetic UDC video dataset. The results indicate the ineffectiveness of models trained on earlier synthetic UDC video datasets, as they do not reflect the actual characteristics of UDC-degraded videos. We also demonstrate the importance of effective UDC restoration by evaluating face recognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is available at our official GitHub repository.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.html",
          "/venue/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ahn_UDC-VIT_A_Real-World_Video_Dataset_for_Under-Display_Cameras_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "UDC-VIT: A Real-World Video Dataset for Under-Display Cameras [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Kyusu Ahn , JiSoo Kim , Sangik Lee , HyunGyu Lee , Byeonghyun Ko , Chanwoo Park , Jaejin Lee Even though an Under-Display Camera (UDC) is an advanced imaging system, the display panel significantly degrades captured images or videos, introducing low transmittance, blur, noise, and flare issues. Tackling such issues is challenging because of the complex degradation of UDCs, including diverse flare patterns. However, no dataset contains videos of real-world UDC degradation. In this paper, we propose a real-world UDC video dataset called UDC-VIT. Unlike existing datasets, UDC-VIT exclusively includes human motions for facial recognition. We propose a video-capturing system to acquire clean and UDC-degraded videos of the same scene simultaneously. Then, we align a pair of captured videos frame by frame, using discrete Fourier transform (DFT). We compare UDC-VIT with six representative UDC still image datasets and two existing UDC video datasets. Using six deep-learning models, we compare UDC-VIT and an existing synthetic UDC video dataset. The results indicate the ineffectiveness of models trained on earlier synthetic UDC video datasets, as they do not reflect the actual characteristics of UDC-degraded videos. We also demonstrate the importance of effective UDC restoration by evaluating face recognition accuracy concerning PSNR, SSIM, and LPIPS scores. UDC-VIT is available at our official GitHub repository. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF",
      "index": 4,
      "title": "Where, What, Why: Towards Explainable Driver Attention Prediction",
      "authors": [
        "Yuchen Zhou",
        "Jiayu Tang",
        "Xiaoyan Xiao",
        "Yueyao Lin",
        "Linkai Liu",
        "Zipeng Guo",
        "Hao Fei",
        "Xiaobo Xia",
        "Chao Gou"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "driver",
        "attention",
        "llada",
        "cognitive",
        "explainable",
        "driving",
        "prediction",
        "allocation",
        "deeper",
        "autonomous"
      ],
      "summary": "Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_Where_What_Why_Towards_Explainable_Driver_Attention_Prediction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 2
      },
      "raw_excerpt": "Where, What, Why: Towards Explainable Driver Attention Prediction [PDF 11 ] [Copy] [Kimi 2 ] [REL] Authors : Yuchen Zhou , Jiayu Tang , Xiaoyan Xiao , Yueyao Lin , Linkai Liu , Zipeng Guo , Hao Fei , Xiaobo Xia , Chao Gou Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF",
      "index": 5,
      "title": "VRM: Knowledge Distillation via Virtual Relation Matching",
      "authors": [
        "Weijia Zhang",
        "Fei Xie",
        "Weidong Cai",
        "Chao Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "vrm",
        "distillation",
        "knowledge",
        "relation",
        "virtual",
        "matching",
        "inter",
        "teacher",
        "spurious",
        "affinity"
      ],
      "summary": "Knowledge distillation (KD) aims to transfer the knowledge of a more capable yet cumbersome teacher model to a lightweight student model. In recent years, relation-based KD methods have fallen behind, as their instance-matching counterparts dominate in performance. In this paper, we revive relational KD by identifying and tackling several key issues in relation-based methods, including their susceptibility to overfitting and spurious responses. Specifically, we transfer novelly constructed affinity graphs that compactly encapsulate a wealth of beneficial inter-sample, inter-class, and inter-view correlations by exploiting virtual views and relations as a new kind of knowledge. As a result, the student has access to richer guidance signals and stronger regularisation throughout the distillation process. To further mitigate the adverse impact of spurious responses, we prune the affinity graphs by dynamically detaching redundant and unreliable edges. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO datasets demonstrate the superior performance of the proposed virtual relation matching (VRM) method, where it consistently sets new state-of-the-art records over a range of models, architectures, tasks, and set-ups. For instance, VRM for the first time hits 74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and improves DeiT-T by 14.44% on CIFAR-100 with a ResNet56 teacher.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_VRM_Knowledge_Distillation_via_Virtual_Relation_Matching_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "VRM: Knowledge Distillation via Virtual Relation Matching [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Weijia Zhang , Fei Xie , Weidong Cai , Chao Ma Knowledge distillation (KD) aims to transfer the knowledge of a more capable yet cumbersome teacher model to a lightweight student model. In recent years, relation-based KD methods have fallen behind, as their instance-matching counterparts dominate in performance. In this paper, we revive relational KD by identifying and tackling several key issues in relation-based methods, including their susceptibility to overfitting and spurious responses. Specifically, we transfer novelly constructed affinity graphs that compactly encapsulate a wealth of beneficial inter-sample, inter-class, and inter-view correlations by exploiting virtual views and relations as a new kind of knowledge. As a result, the student has access to richer guidance signals and stronger regularisation throughout the distillation process. To further mitigate the adverse impact of spurious responses, we prune the affinity graphs by dynamically detaching redundant and unreliable edges. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO datasets demonstrate the superior performance of the proposed virtual relation matching (VRM) method, where it consistently sets new state-of-the-art records over a range of models, architectures, tasks, and set-ups. For instance, VRM for the first time hits 74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and improves DeiT-T by 14.44% on CIFAR-100 with a ResNet56 teacher. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF",
      "index": 6,
      "title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation",
      "authors": [
        "Daniel Winter",
        "Asaf Shul",
        "Matan Cohen",
        "Dana Berman",
        "Yael Pritch",
        "Alex Rav-Acha",
        "Yedid Hoshen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "objectmate",
        "object",
        "insertion",
        "subject",
        "composing",
        "photorealistic",
        "scene",
        "driven",
        "generation",
        "lighting"
      ],
      "summary": "This paper introduces a tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into a scene specified by either an image or text. Existing methods struggle to fully meet the task's challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the object's identity. We hypothesize that achieving these goals requires large-scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train a straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composite image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using a single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Winter_ObjectMate_A_Recurrence_Prior_for_Object_Insertion_and_Subject-Driven_Generation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Daniel Winter , Asaf Shul , Matan Cohen , Dana Berman , Yael Pritch , Alex Rav-Acha , Yedid Hoshen This paper introduces a tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into a scene specified by either an image or text. Existing methods struggle to fully meet the task's challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the object's identity. We hypothesize that achieving these goals requires large-scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train a straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composite image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using a single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF",
      "index": 7,
      "title": "UnZipLoRA: Separating Content and Style from a Single Image",
      "authors": [
        "Chang Liu",
        "Viraj Shah",
        "Aiyu Cui",
        "Svetlana Lazebnik"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "unziplora",
        "style",
        "loras",
        "subject",
        "lora",
        "recontextualization",
        "image",
        "dreambooth",
        "variations",
        "recombining"
      ],
      "summary": "This paper introduces UnZipLoRA, a method for decomposing an image into its constituent subject and style, represented as two distinct LoRAs (Low-Rank Adaptations). Unlike existing personalization techniques that focus on either subject or style in isolation, or require separate training sets for each, UnZipLoRA disentangles these elements from a single image by training both the LoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs are compatible, i.e., they can be seamlessly combined using direct addition. UnZipLoRA enables independent manipulation and recontextualization of subject and style, including generating variations of each, applying the extracted style to new subjects, and recombining them to reconstruct the original image or create novel variations. To address the challenge of subject and style entanglement, UnZipLoRA employs a novel prompt separation technique, as well as column and block separation strategies to accurately preserve the characteristics of subject and style, and ensure compatibility between the learned LoRAs. Evaluation with human studies and quantitative metrics demonstrates UnZipLoRA's effectiveness compared to other state-of-the-art methods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_UnZipLoRA_Separating_Content_and_Style_from_a_Single_Image_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 6
      },
      "raw_excerpt": "UnZipLoRA: Separating Content and Style from a Single Image [PDF 19 ] [Copy] [Kimi 6 ] [REL] Authors : Chang Liu , Viraj Shah , Aiyu Cui , Svetlana Lazebnik This paper introduces UnZipLoRA, a method for decomposing an image into its constituent subject and style, represented as two distinct LoRAs (Low-Rank Adaptations). Unlike existing personalization techniques that focus on either subject or style in isolation, or require separate training sets for each, UnZipLoRA disentangles these elements from a single image by training both the LoRAs simultaneously. UnZipLoRA ensures that the resulting LoRAs are compatible, i.e., they can be seamlessly combined using direct addition. UnZipLoRA enables independent manipulation and recontextualization of subject and style, including generating variations of each, applying the extracted style to new subjects, and recombining them to reconstruct the original image or create novel variations. To address the challenge of subject and style entanglement, UnZipLoRA employs a novel prompt separation technique, as well as column and block separation strategies to accurately preserve the characteristics of subject and style, and ensure compatibility between the learned LoRAs. Evaluation with human studies and quantitative metrics demonstrates UnZipLoRA's effectiveness compared to other state-of-the-art methods, including DreamBooth-LoRA, Inspiration Tree, and B-LoRA. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF",
      "index": 8,
      "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory",
      "authors": [
        "Chenhao Zheng",
        "Jieyu Zhang",
        "Mohammadreza Salehi",
        "Ziqi Gao",
        "Vishnu Iyengar",
        "Norimasa Kobori",
        "Quan Kong",
        "Ranjay Krishna"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "vit3d",
        "trajvit",
        "video",
        "tokenization",
        "tokens",
        "token",
        "panoptic",
        "grounded",
        "trajectory",
        "encoder"
      ],
      "summary": "Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 20x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object_ICCV_2025_paper.html",
          "/venue/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zheng_One_Trajectory_One_Token_Grounded_Video_Tokenization_via_Panoptic_Sub-object_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Chenhao Zheng , Jieyu Zhang , Mohammadreza Salehi , Ziqi Gao , Vishnu Iyengar , Norimasa Kobori , Quan Kong , Ranjay Krishna Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 20x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF",
      "index": 9,
      "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints",
      "authors": [
        "Jens U. Kreber",
        "Joerg Stueckler"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "physnap",
        "sdfs",
        "articulated",
        "diffusion",
        "plausibility",
        "guiding",
        "cloud",
        "alignment",
        "objects",
        "point"
      ],
      "summary": "Articulated objects are an important type of interactable objects in everyday environments. In this paper, we propose PhysNAP, a novel diffusion model-based approach for generating articulated objects that aligns them with partial point clouds and improves their physical plausibility. The model represents part shapes by signed distance functions (SDFs). We guide the reverse diffusion process using a point cloud alignment loss computed using the predicted SDFs. Additionally, we impose non-penetration and mobility constraints based on the part SDFs for guiding the model to generate more physically plausible objects. We also make our diffusion approach category-aware to further improve point cloud alignment if category information is available. We evaluate the generative ability and constraint consistency of samples generated with PhysNAP using the PartNet-Mobility dataset. We also compare it with an unguided baseline diffusion model and demonstrate that PhysNAP can improve constraint consistency and provides a tradeoff with generative ability.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kreber_Guiding_Diffusion-Based_Articulated_Object_Generation_by_Partial_Point_Cloud_Alignment_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Jens U. Kreber , Joerg Stueckler Articulated objects are an important type of interactable objects in everyday environments. In this paper, we propose PhysNAP, a novel diffusion model-based approach for generating articulated objects that aligns them with partial point clouds and improves their physical plausibility. The model represents part shapes by signed distance functions (SDFs). We guide the reverse diffusion process using a point cloud alignment loss computed using the predicted SDFs. Additionally, we impose non-penetration and mobility constraints based on the part SDFs for guiding the model to generate more physically plausible objects. We also make our diffusion approach category-aware to further improve point cloud alignment if category information is available. We evaluate the generative ability and constraint consistency of samples generated with PhysNAP using the PartNet-Mobility dataset. We also compare it with an unguided baseline diffusion model and demonstrate that PhysNAP can improve constraint consistency and provides a tradeoff with generative ability. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF",
      "index": 10,
      "title": "RALoc: Enhancing Outdoor LiDAR Localization via Rotation Awareness",
      "authors": [
        "Yuyang Yang",
        "Wen Li",
        "Sheng Ao",
        "Qingshan Xu",
        "Shangshu Yu",
        "Yu Guo",
        "Yin Zhou",
        "Siqi Shen",
        "Cheng Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lidar",
        "localization",
        "raloc",
        "rotation",
        "scr",
        "outdoor",
        "awareness",
        "dataset",
        "robotcar",
        "canonicalization"
      ],
      "summary": "LiDAR localization is a fundamental task in autonomous driving and robotics. Scene Coordinate Regression (SCR) exhibits leading pose accuracy, achieving impressive results in learning-based localization. We observe that the real-world LiDAR scans captured from different viewpoints usually result in the catastrophic collapse of SCR. However, existing LiDAR localization methods have largely overlooked the issue of rotation sensitivity in SCR. In this paper, we present RALoc, an outdoor LiDAR localization method with rotation awareness to achieve accurate localization. The key to our approach is to design a Point Cloud Canonicalization module, which leverages a powerful equivariant key feature aggregation to transform the input LiDAR scan towards a consistent orientation, effectively eliminating the adverse effects of rotation. This proposed module has promising scalability and can be seamlessly integrated with the existing LiDAR localization network. Moreover, we propose the Bidirectional LiDAR Localization (BiLiLo) dataset as a benchmark to evaluate the performance of various methods in large outdoor scenes with significant rotation changes. Extensive experiments show that RALoc significantly improves localization performance in scenarios with large rotation changes, and also achieves competitive performance in the Oxford Radar RobotCar dataset. Our code and dataset will be released upon acceptance.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_RALoc_Enhancing_Outdoor_LiDAR_Localization_via_Rotation_Awareness_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "RALoc: Enhancing Outdoor LiDAR Localization via Rotation Awareness [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Yuyang Yang , Wen Li , Sheng Ao , Qingshan Xu , Shangshu Yu , Yu Guo , Yin Zhou , Siqi Shen , Cheng Wang LiDAR localization is a fundamental task in autonomous driving and robotics. Scene Coordinate Regression (SCR) exhibits leading pose accuracy, achieving impressive results in learning-based localization. We observe that the real-world LiDAR scans captured from different viewpoints usually result in the catastrophic collapse of SCR. However, existing LiDAR localization methods have largely overlooked the issue of rotation sensitivity in SCR. In this paper, we present RALoc, an outdoor LiDAR localization method with rotation awareness to achieve accurate localization. The key to our approach is to design a Point Cloud Canonicalization module, which leverages a powerful equivariant key feature aggregation to transform the input LiDAR scan towards a consistent orientation, effectively eliminating the adverse effects of rotation. This proposed module has promising scalability and can be seamlessly integrated with the existing LiDAR localization network. Moreover, we propose the Bidirectional LiDAR Localization (BiLiLo) dataset as a benchmark to evaluate the performance of various methods in large outdoor scenes with significant rotation changes. Extensive experiments show that RALoc significantly improves localization performance in scenarios with large rotation changes, and also achieves competitive performance in the Oxford Radar RobotCar dataset. Our code and dataset will be released upon acceptance. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF",
      "index": 11,
      "title": "DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Model",
      "authors": [
        "Junjia Huang",
        "Pengxiang Yan",
        "Jinhang Cai",
        "Jiyang Liu",
        "Zhao Wang",
        "Yitong Wang",
        "Xinglong Wu",
        "Guanbin Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dreamlayer",
        "layer",
        "generation",
        "image",
        "harmonization",
        "multi",
        "attention",
        "inter",
        "coherent",
        "layers"
      ],
      "summary": "Text-driven image generation using diffusion models has recently gained significant attention. To enable more flexible image manipulation and editing, recent research has expanded from single image generation to transparent layer generation and multi-layer compositions. However, existing approaches often fail to provide a thorough exploration of multi-layer structures, leading to inconsistent inter-layer interactions, such as occlusion relationships, spatial layout, and shadowing. In this paper, we introduce DreamLayer, a novel framework that enables coherent text-driven generation of multiple image layers, by explicitly modeling the relationship between transparent foreground and background layers. DreamLayer incorporates three key components, i.e., Context-Aware Cross-Attention (CACA) for global-local information exchange, Layer-Shared Self-Attention (LSSA) for establishing robust inter-layer connections, and Information Retained Harmonization (IRH) for refining fusion details at the latent level.By leveraging a coherent full-image context, DreamLayer builds inter-layer connections through attention mechanisms and applies a harmonization step to achieve seamless layer fusion. To facilitate research in multi-layer generation, we construct a high-quality, diverse multi-layer dataset including 400k samples. Extensive experiments and user studies demonstrate that DreamLayer generates more coherent and well-aligned layers, with broad applicability, including latent-space image editing and image-to-layer decomposition.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_DreamLayer_Simultaneous_Multi-Layer_Generation_via_Diffusion_Model_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 5
      },
      "raw_excerpt": "DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Model [PDF 12 ] [Copy] [Kimi 5 ] [REL] Authors : Junjia Huang , Pengxiang Yan , Jinhang Cai , Jiyang Liu , Zhao Wang , Yitong Wang , Xinglong Wu , Guanbin Li Text-driven image generation using diffusion models has recently gained significant attention. To enable more flexible image manipulation and editing, recent research has expanded from single image generation to transparent layer generation and multi-layer compositions. However, existing approaches often fail to provide a thorough exploration of multi-layer structures, leading to inconsistent inter-layer interactions, such as occlusion relationships, spatial layout, and shadowing. In this paper, we introduce DreamLayer, a novel framework that enables coherent text-driven generation of multiple image layers, by explicitly modeling the relationship between transparent foreground and background layers. DreamLayer incorporates three key components, i.e., Context-Aware Cross-Attention (CACA) for global-local information exchange, Layer-Shared Self-Attention (LSSA) for establishing robust inter-layer connections, and Information Retained Harmonization (IRH) for refining fusion details at the latent level.By leveraging a coherent full-image context, DreamLayer builds inter-layer connections through attention mechanisms and applies a harmonization step to achieve seamless layer fusion. To facilitate research in multi-layer generation, we construct a high-quality, diverse multi-layer dataset including 400k samples. Extensive experiments and user studies demonstrate that DreamLayer generates more coherent and well-aligned layers, with broad applicability, including latent-space image editing and image-to-layer decomposition. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF",
      "index": 12,
      "title": "AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting",
      "authors": [
        "Xiaoyu Zhou",
        "Jingqi Wang",
        "Yongtao Wang",
        "Yufei Wei",
        "Nan Dong",
        "Ming-Hsuan Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "occupancy",
        "autoocc",
        "ended",
        "splatting",
        "vision",
        "annotation",
        "semantic",
        "open",
        "language",
        "gaussian"
      ],
      "summary": "Obtaining high-quality 3D semantic occupancy from raw sensor data remains an essential yet challenging task, often requiring extensive manual labeling. In this work, we propose AutoOcc, a vision-centric automated pipeline for open-ended semantic occupancy annotation that integrates differentiable Gaussian splatting guided by vision-language models. We formulate the open-ended semantic 3D occupancy reconstruction task to automatically generate scene occupancy by combining attention maps from vision-language models and foundation vision models. We devise semantic-aware Gaussians as intermediate geometric descriptors and propose a cumulative Gaussian-to-voxel splatting algorithm that enables effective and efficient occupancy annotation. Our framework outperforms existing automated occupancy annotation methods without human labels. AutoOcc also enables open-ended semantic occupancy auto-labeling, achieving robust performance in both static and dynamically complex scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_AutoOcc_Automatic_Open-Ended_Semantic_Occupancy_Annotation_via_Vision-Language_Guided_Gaussian_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via Vision-Language Guided Gaussian Splatting [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Xiaoyu Zhou , Jingqi Wang , Yongtao Wang , Yufei Wei , Nan Dong , Ming-Hsuan Yang Obtaining high-quality 3D semantic occupancy from raw sensor data remains an essential yet challenging task, often requiring extensive manual labeling. In this work, we propose AutoOcc, a vision-centric automated pipeline for open-ended semantic occupancy annotation that integrates differentiable Gaussian splatting guided by vision-language models. We formulate the open-ended semantic 3D occupancy reconstruction task to automatically generate scene occupancy by combining attention maps from vision-language models and foundation vision models. We devise semantic-aware Gaussians as intermediate geometric descriptors and propose a cumulative Gaussian-to-voxel splatting algorithm that enables effective and efficient occupancy annotation. Our framework outperforms existing automated occupancy annotation methods without human labels. AutoOcc also enables open-ended semantic occupancy auto-labeling, achieving robust performance in both static and dynamically complex scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF",
      "index": 13,
      "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning",
      "authors": [
        "Zedong Wang",
        "Siyuan Li",
        "Dan Xu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mtl",
        "task",
        "rep",
        "mto",
        "representation",
        "saliency",
        "complementary",
        "entropybased",
        "optimizer",
        "unleashing"
      ],
      "summary": "Despite the promise of Multi-Task Learning (MTL) in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts through optimizer-centric loss scaling and gradient manipulation, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizer designs, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropybased penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting (EW) policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law (PL) exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Rep-MTL_Unleashing_the_Power_of_Representation-level_Task_Saliency_for_Multi-Task_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Zedong Wang , Siyuan Li , Dan Xu Despite the promise of Multi-Task Learning (MTL) in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts through optimizer-centric loss scaling and gradient manipulation, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizer designs, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropybased penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting (EW) policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law (PL) exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF",
      "index": 14,
      "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold",
      "authors": [
        "Jaeho Shin",
        "Hyeonjae Gil",
        "Junwoo Jang",
        "Maani Ghaffari",
        "Ayoung Kim"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "optimizable",
        "registration",
        "distance",
        "affine",
        "grassmannian",
        "cost",
        "subspace",
        "geodesic",
        "function",
        "joomeok"
      ],
      "summary": "Affine Grassmannian has been favored for expressing proximity between lines and planes due to its theoretical exactness in measuring distances among features. Despite this advantage, the existing method can only measure the proximity without yielding the distance as an explicit function of rigid body transformation. Thus, an optimizable distance function on the manifold has remained underdeveloped, stifling its application in registration problems. This paper is the first to explicitly derive an optimizable cost function between two Grassmannian features with respect to rigid body transformation (R and t). Specifically, we present a rigorous mathematical proof demonstrating that the bases of high-dimensional linear subspaces can serve as an explicit representation of the cost. Finally, we propose an optimizable cost function based on the transformed bases that can be applied to the registration problem of any affine subspace. Compared to vector parameter-based approaches, our method is able to find a globally optimal solution by directly minimizing the geodesic distance which is agnostic to representation ambiguity. The resulting cost function and its extension to the inlier-set maximizing Branch-and-Bound (BnB) solver have been demonstrated to improve the convergence of existing solutions or outperform them in various computer vision tasks. The code is available on https://github.com/joomeok/GrassmannRegistration.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_Registration_beyond_Points_General_Affine_Subspace_Alignment_via_Geodesic_Distance_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jaeho Shin , Hyeonjae Gil , Junwoo Jang , Maani Ghaffari , Ayoung Kim Affine Grassmannian has been favored for expressing proximity between lines and planes due to its theoretical exactness in measuring distances among features. Despite this advantage, the existing method can only measure the proximity without yielding the distance as an explicit function of rigid body transformation. Thus, an optimizable distance function on the manifold has remained underdeveloped, stifling its application in registration problems. This paper is the first to explicitly derive an optimizable cost function between two Grassmannian features with respect to rigid body transformation (R and t). Specifically, we present a rigorous mathematical proof demonstrating that the bases of high-dimensional linear subspaces can serve as an explicit representation of the cost. Finally, we propose an optimizable cost function based on the transformed bases that can be applied to the registration problem of any affine subspace. Compared to vector parameter-based approaches, our method is able to find a globally optimal solution by directly minimizing the geodesic distance which is agnostic to representation ambiguity. The resulting cost function and its extension to the inlier-set maximizing Branch-and-Bound (BnB) solver have been demonstrated to improve the convergence of existing solutions or outperform them in various computer vision tasks. The code is available on https://github.com/joomeok/GrassmannRegistration. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF",
      "index": 15,
      "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning",
      "authors": [
        "Linlan Huang",
        "Xusheng Cao",
        "Haori Lu",
        "Yifan Meng",
        "Fei Yang",
        "Xialei Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "clip",
        "modality",
        "gap",
        "continual",
        "linlany",
        "mindthegap",
        "compensating",
        "pre",
        "trained",
        "learning"
      ],
      "summary": "Continual learning aims to enable models to learn sequentially from continuously incoming data while retaining performance on previously learned tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting strong capabilities across various downstream tasks, there has been growing interest in leveraging CLIP for continual learning in such scenarios. Most existing works overlook the inherent modality gap in CLIP, a key factor in its generalization and adaptability. In this paper, we analyze the variations in the modality gap during the fine-tuning of vision-language pre-trained models. Our observations reveal that the modality gap effectively reflects the extent to which pre-trained knowledge is preserved. Based on these insights, we propose a simple yet effective method, MG-CLIP, that improves CLIP's performance in class-incremental learning. Our approach leverages modality gap preservation to mitigate forgetting and modality gap compensation to enhance the capacity for new data, introducing a novel modality-gap-based perspective for continual learning. Extensive experiments on multiple benchmarks demonstrate that our method outperforms existing approaches without requiring additional replay data. Our code is available at https://github.com/linlany/MindtheGap.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Mind_the_Gap_Preserving_and_Compensating_for_the_Modality_Gap_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 5
      },
      "raw_excerpt": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning [PDF 9 ] [Copy] [Kimi 5 ] [REL] Authors : Linlan Huang , Xusheng Cao , Haori Lu , Yifan Meng , Fei Yang , Xialei Liu Continual learning aims to enable models to learn sequentially from continuously incoming data while retaining performance on previously learned tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting strong capabilities across various downstream tasks, there has been growing interest in leveraging CLIP for continual learning in such scenarios. Most existing works overlook the inherent modality gap in CLIP, a key factor in its generalization and adaptability. In this paper, we analyze the variations in the modality gap during the fine-tuning of vision-language pre-trained models. Our observations reveal that the modality gap effectively reflects the extent to which pre-trained knowledge is preserved. Based on these insights, we propose a simple yet effective method, MG-CLIP, that improves CLIP's performance in class-incremental learning. Our approach leverages modality gap preservation to mitigate forgetting and modality gap compensation to enhance the capacity for new data, introducing a novel modality-gap-based perspective for continual learning. Extensive experiments on multiple benchmarks demonstrate that our method outperforms existing approaches without requiring additional replay data. Our code is available at https://github.com/linlany/MindtheGap. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF",
      "index": 16,
      "title": "BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes",
      "authors": [
        "Minkyun Seo",
        "Hyungtae Lim",
        "Kanghee Lee",
        "Luca Carlone",
        "Jaesik Park"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "buffer",
        "registration",
        "scenes",
        "voxel",
        "shot",
        "cloud",
        "diverse",
        "generalization",
        "manual",
        "search"
      ],
      "summary": "Recent advances in deep learning-based point cloud registration have improved generalization, yet most methods still require retraining or manual parameter tuning for each new environment. In this paper, we identify three key factors limiting generalization: (a) reliance on environment-specific voxel size and search radius, (b) poor out-of-domain robustness of learning-based keypoint detectors,and (c) raw coordinate usage, which exacerbates scale discrepancies. To address these issues, we present a zero-shot registration pipeline called BUFFER-X by (a) adaptively determining voxel size/search radii, (b) using farthest point sampling to bypass learned detectors, and (c) leveraging patch-wise scale normalization for consistent coordinate bounds. In particular, we present a multi-scale patch-based descriptor generation and a hierarchical inlier search across scales to improve robustness in diverse scenes. We also propose a novel generalizability benchmark using 11 datasets that cover various indoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X achieves substantial generalization without prior information or manual parameter tuning for the test datasets. Our code is available at https://github.com/MIT-SPARK/BUFFER-X.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Seo_BUFFER-X_Towards_Zero-Shot_Point_Cloud_Registration_in_Diverse_Scenes_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Minkyun Seo , Hyungtae Lim , Kanghee Lee , Luca Carlone , Jaesik Park Recent advances in deep learning-based point cloud registration have improved generalization, yet most methods still require retraining or manual parameter tuning for each new environment. In this paper, we identify three key factors limiting generalization: (a) reliance on environment-specific voxel size and search radius, (b) poor out-of-domain robustness of learning-based keypoint detectors,and (c) raw coordinate usage, which exacerbates scale discrepancies. To address these issues, we present a zero-shot registration pipeline called BUFFER-X by (a) adaptively determining voxel size/search radii, (b) using farthest point sampling to bypass learned detectors, and (c) leveraging patch-wise scale normalization for consistent coordinate bounds. In particular, we present a multi-scale patch-based descriptor generation and a hierarchical inlier search across scales to improve robustness in diverse scenes. We also propose a novel generalizability benchmark using 11 datasets that cover various indoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X achieves substantial generalization without prior information or manual parameter tuning for the test datasets. Our code is available at https://github.com/MIT-SPARK/BUFFER-X. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF",
      "index": 17,
      "title": "Noise-Modeled Diffusion Models for Low-Light Spike Image Restoration",
      "authors": [
        "Ruonan Liu",
        "Lin Zhu",
        "Xijie Xiang",
        "Lizhi Wang",
        "Hua Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "spike",
        "noise",
        "restoration",
        "diffusion",
        "light",
        "degradation",
        "low",
        "restoring",
        "deterministic",
        "image"
      ],
      "summary": "Spike-based imaging, inspired by the human visual system, offers several advantages, including high temporal resolution and low power consumption, but suffers from significant image degradation in low-light conditions due to noise interference. Restoring spike images under such conditions poses a significant challenge, as traditional frame-based or spike-based techniques are ill-suited to handle such severe noise and unique noise characteristics. This paper proposes a novel approach for restoring low-light spike images using noise-modeled diffusion models. By establishing a noise-embedded spike imaging model under low light, we model the forward diffusion process as the degradation of spike images with proportional and residual terms and incorporate deterministic and non-deterministic components with reverse shifting, enabling the model to capture the distinctive spike noise structure. Additionally, we utilize region mask image, dark current map and spike density value as conditions to further guide the restoration process by providing prompts for degradation regions, deterministic parameters and noise intensity, respectively. Experimental results demonstrate that our method significantly outperforms existing spike-based reconstruction and diffusion-based image restoration methods in both quantitative performance and visual quality. The code and dataset are available at https://github.com/BIT-Vision/SpikeDiffusion.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Noise-Modeled_Diffusion_Models_for_Low-Light_Spike_Image_Restoration_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "Noise-Modeled Diffusion Models for Low-Light Spike Image Restoration [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Ruonan Liu , Lin Zhu , Xijie Xiang , Lizhi Wang , Hua Huang Spike-based imaging, inspired by the human visual system, offers several advantages, including high temporal resolution and low power consumption, but suffers from significant image degradation in low-light conditions due to noise interference. Restoring spike images under such conditions poses a significant challenge, as traditional frame-based or spike-based techniques are ill-suited to handle such severe noise and unique noise characteristics. This paper proposes a novel approach for restoring low-light spike images using noise-modeled diffusion models. By establishing a noise-embedded spike imaging model under low light, we model the forward diffusion process as the degradation of spike images with proportional and residual terms and incorporate deterministic and non-deterministic components with reverse shifting, enabling the model to capture the distinctive spike noise structure. Additionally, we utilize region mask image, dark current map and spike density value as conditions to further guide the restoration process by providing prompts for degradation regions, deterministic parameters and noise intensity, respectively. Experimental results demonstrate that our method significantly outperforms existing spike-based reconstruction and diffusion-based image restoration methods in both quantitative performance and visual quality. The code and dataset are available at https://github.com/BIT-Vision/SpikeDiffusion. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF",
      "index": 18,
      "title": "Radiant Foam: Real-Time Differentiable Ray Tracing",
      "authors": [
        "Shrisudhan Govindarajan",
        "Daniel Rebain",
        "Kwang Moo Yi",
        "Andrea Tagliasacchi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "rasterization",
        "radiant",
        "splatting",
        "ray",
        "tracing",
        "foam",
        "rendering",
        "differentiable",
        "hardware",
        "eschew"
      ],
      "summary": "Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Govindarajan_Radiant_Foam_Real-Time_Differentiable_Ray_Tracing_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 3
      },
      "raw_excerpt": "Radiant Foam: Real-Time Differentiable Ray Tracing [PDF ] [Copy] [Kimi 3 ] [REL] Authors : Shrisudhan Govindarajan , Daniel Rebain , Kwang Moo Yi , Andrea Tagliasacchi Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF",
      "index": 19,
      "title": "ReTracker: Exploring Image Matching for Robust Online Any Point Tracking",
      "authors": [
        "Dongli Tan",
        "Xingyi He",
        "Sida Peng",
        "Yiqing Gong",
        "Xing Zhu",
        "Jiaming Sun",
        "Ruizhen Hu",
        "Yujun Shen",
        "Hujun Bao",
        "Xiaowei Zhou"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "retracker",
        "tracking",
        "online",
        "matching",
        "occlusions",
        "retracking",
        "points",
        "viewpoint",
        "decoder",
        "pretrain"
      ],
      "summary": "This paper aims to establish correspondences for a set of 2D query points across a video sequence in an online manner. Recent methods leverage future frames to achieve smooth point tracking at the current frame, but they still struggle to find points with significant viewpoint changes after long-term occlusions and inherently cannot achieve online tracking. To overcome these challenges, we develop a novel online tracking framework, named ReTracker, that integrates two advances in image matching with tracking-specific designs. First, a decoder network with a global receptive field is incorporated with a temporal attention module to robustly track points undergoing large location changes. Second, the decoder network is adapted to pretrain on large-scale two-view matching data, which offers significantly greater diversity and volume than tracking data, to learn general matching priors. This pretraining strategy effectively enhances our tracker's ability to handle viewpoint and appearance variations after long-term occlusions. Experiments demonstrate that our method outperforms recent online trackers across multiple benchmarks and achieves competitive or superior performance compared to offline methods. Furthermore, we collect an ego-centric, occlusion-heavy dataset to illustrate the retracking capabilities of our approach. The code and dataset will be released for the reproducibility.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_ReTracker_Exploring_Image_Matching_for_Robust_Online_Any_Point_Tracking_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 5
      },
      "raw_excerpt": "ReTracker: Exploring Image Matching for Robust Online Any Point Tracking [PDF 6 ] [Copy] [Kimi 5 ] [REL] Authors : Dongli Tan , Xingyi He , Sida Peng , Yiqing Gong , Xing Zhu , Jiaming Sun , Ruizhen Hu , Yujun Shen , Hujun Bao , Xiaowei Zhou This paper aims to establish correspondences for a set of 2D query points across a video sequence in an online manner. Recent methods leverage future frames to achieve smooth point tracking at the current frame, but they still struggle to find points with significant viewpoint changes after long-term occlusions and inherently cannot achieve online tracking. To overcome these challenges, we develop a novel online tracking framework, named ReTracker, that integrates two advances in image matching with tracking-specific designs. First, a decoder network with a global receptive field is incorporated with a temporal attention module to robustly track points undergoing large location changes. Second, the decoder network is adapted to pretrain on large-scale two-view matching data, which offers significantly greater diversity and volume than tracking data, to learn general matching priors. This pretraining strategy effectively enhances our tracker's ability to handle viewpoint and appearance variations after long-term occlusions. Experiments demonstrate that our method outperforms recent online trackers across multiple benchmarks and achieves competitive or superior performance compared to offline methods. Furthermore, we collect an ego-centric, occlusion-heavy dataset to illustrate the retracking capabilities of our approach. The code and dataset will be released for the reproducibility. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF",
      "index": 20,
      "title": "A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks",
      "authors": [
        "Hang Su",
        "Yunlong Feng",
        "Daniel Gehrig",
        "Panfeng Jiang",
        "Ling Gao",
        "Xavier Lagorce",
        "Laurent Kneip"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "correspondences",
        "shutter",
        "point",
        "motion",
        "cameras",
        "asynchronous",
        "rolling",
        "solver",
        "linear",
        "structure"
      ],
      "summary": "Structure and continuous motion estimation from point correspondences is a fundamental problem in computer vision that has been powered by well-known algorithms such as the familiar 5-point or 8-point algorithm. However, despite their acclaim, these algorithms are limited to processing point correspondences originating from a pair of views each one representing an instantaneous capture of the scene. Yet, in the case of rolling shutter cameras, or more recently, event cameras, this synchronization breaks down. In this work, we present a unified approach for structure and linear motion estimation from 2D point correspondences with arbitrary timestamps, from an arbitrary set of views. By formulating the problem in terms of first-order dynamics and leveraging a constant velocity motion model, we derive a novel, linear point incidence relation allowing for the efficient recovery of both linear velocity and 3D points with predictable degeneracies and solution multiplicities. Owing to its general formulation, it can handle correspondences from a wide range of sensing modalities such as global shutter, rolling shutter, and event cameras, and can even combine correspondences from different collocated sensors. We validate the effectiveness of our solver on both simulated and real-world data, where we show consistent improvement across all modalities when compared to recent approaches. We believe our work opens the door to efficient structure and motion estimation from asynchronous data.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Su_A_Linear_N-Point_Solver_for_Structure_and_Motion_from_Asynchronous_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Hang Su , Yunlong Feng , Daniel Gehrig , Panfeng Jiang , Ling Gao , Xavier Lagorce , Laurent Kneip Structure and continuous motion estimation from point correspondences is a fundamental problem in computer vision that has been powered by well-known algorithms such as the familiar 5-point or 8-point algorithm. However, despite their acclaim, these algorithms are limited to processing point correspondences originating from a pair of views each one representing an instantaneous capture of the scene. Yet, in the case of rolling shutter cameras, or more recently, event cameras, this synchronization breaks down. In this work, we present a unified approach for structure and linear motion estimation from 2D point correspondences with arbitrary timestamps, from an arbitrary set of views. By formulating the problem in terms of first-order dynamics and leveraging a constant velocity motion model, we derive a novel, linear point incidence relation allowing for the efficient recovery of both linear velocity and 3D points with predictable degeneracies and solution multiplicities. Owing to its general formulation, it can handle correspondences from a wide range of sensing modalities such as global shutter, rolling shutter, and event cameras, and can even combine correspondences from different collocated sensors. We validate the effectiveness of our solver on both simulated and real-world data, where we show consistent improvement across all modalities when compared to recent approaches. We believe our work opens the door to efficient structure and motion estimation from asynchronous data. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF",
      "index": 21,
      "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
      "authors": [
        "Chen Ziwen",
        "Hao Tan",
        "Kai Zhang",
        "Sai Bi",
        "Fujun Luan",
        "Yicong Hong",
        "Li Fuxin",
        "Zexiang Xu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lrm",
        "reconstruction",
        "long",
        "gaussian",
        "splats",
        "arthurhero",
        "llrm",
        "feed",
        "960x540",
        "dl3dv"
      ],
      "summary": "We propose Long-LRM, a feed-forward 3D Gaussian reconstruction model for instant, high-resolution, 360deg wide-coverage, scene-level reconstruction. Specifically, it takes in 32 input images at a resolution of 960x540 and produces the Gaussian reconstruction in just 1 second on a single A100 GPU. To handle the long sequence of 250K tokens brought by the large input size, Long-LRM features a mixture of the recent Mamba2 blocks and the classical transformer blocks, enhanced by a light-weight token merging module and Gaussian pruning steps that balance between quality and efficiency. We evaluate Long-LRM on the large-scale DL3DV benchmark and Tanks&Temples, demonstrating reconstruction quality comparable to the optimization-based methods while achieving an 800x speedup w.r.t. the optimization-based approaches and an input size at least 60x larger than the previous feed-forward approaches. We conduct extensive ablation studies on our model design choices for both rendering quality and computation efficiency. We also explore Long-LRM's compatibility with other Gaussian variants such as 2D GS, which enhances Long-LRM's ability in geometry reconstruction. Project page: http://arthurhero.github.io/projects/llrm/",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ziwen_Long-LRM_Long-sequence_Large_Reconstruction_Model_for_Wide-coverage_Gaussian_Splats_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Chen Ziwen , Hao Tan , Kai Zhang , Sai Bi , Fujun Luan , Yicong Hong , Li Fuxin , Zexiang Xu We propose Long-LRM, a feed-forward 3D Gaussian reconstruction model for instant, high-resolution, 360deg wide-coverage, scene-level reconstruction. Specifically, it takes in 32 input images at a resolution of 960x540 and produces the Gaussian reconstruction in just 1 second on a single A100 GPU. To handle the long sequence of 250K tokens brought by the large input size, Long-LRM features a mixture of the recent Mamba2 blocks and the classical transformer blocks, enhanced by a light-weight token merging module and Gaussian pruning steps that balance between quality and efficiency. We evaluate Long-LRM on the large-scale DL3DV benchmark and Tanks&Temples, demonstrating reconstruction quality comparable to the optimization-based methods while achieving an 800x speedup w.r.t. the optimization-based approaches and an input size at least 60x larger than the previous feed-forward approaches. We conduct extensive ablation studies on our model design choices for both rendering quality and computation efficiency. We also explore Long-LRM's compatibility with other Gaussian variants such as 2D GS, which enhances Long-LRM's ability in geometry reconstruction. Project page: http://arthurhero.github.io/projects/llrm/ Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF",
      "index": 22,
      "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
      "authors": [
        "Wenqi Zhang",
        "Hang Zhang",
        "Xin Li",
        "Jiashuo Sun",
        "Yongliang Shen",
        "Weiming Lu",
        "Deli Zhao",
        "Yueting Zhuang",
        "Lidong Bing"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "textbook",
        "interleaved",
        "instructional",
        "pretraining",
        "videos",
        "multimodal",
        "knowledge",
        "vlm",
        "text",
        "vlms"
      ],
      "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Code and dataset are available on https://multimodal-interleaved-textbook.github.io.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_2.5_Years_in_Class_A_Multimodal_Textbook_for_Vision-Language_Pretraining_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Wenqi Zhang , Hang Zhang , Xin Li , Jiashuo Sun , Yongliang Shen , Weiming Lu , Deli Zhao , Yueting Zhuang , Lidong Bing Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Code and dataset are available on https://multimodal-interleaved-textbook.github.io. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF",
      "index": 23,
      "title": "Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness",
      "authors": [
        "Qifan Yu",
        "Zhebei Shen",
        "Zhongqi Yue",
        "Yang Wu",
        "Bosheng Qin",
        "Wenqiao Zhang",
        "Yunfei Li",
        "Juncheng Li",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "informativeness",
        "representativeness",
        "datatailor",
        "collaborative",
        "uniqueness",
        "tuning",
        "mastering",
        "data",
        "modal",
        "instruction"
      ],
      "summary": "Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles--informativeness, uniqueness, and representativeness--for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 101.3% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the \"Less is More\" philosophy in MLLM development. The code and data is available in this URL.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Mastering_Collaborative_Multi-modal_Data_Selection_A_Focus_on_Informativeness_Uniqueness_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Qifan Yu , Zhebei Shen , Zhongqi Yue , Yang Wu , Bosheng Qin , Wenqiao Zhang , Yunfei Li , Juncheng Li , Siliang Tang , Yueting Zhuang Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles--informativeness, uniqueness, and representativeness--for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 101.3% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the \"Less is More\" philosophy in MLLM development. The code and data is available in this URL. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF",
      "index": 24,
      "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation",
      "authors": [
        "Jiaer Xia",
        "Bingkui Tong",
        "Yuhang Zang",
        "Rui Shao",
        "Kaiyang Zhou"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "specialized",
        "cot",
        "thought",
        "bootstrapping",
        "mllms",
        "grounded",
        "chain",
        "charts",
        "multimodal",
        "data"
      ],
      "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with chain-of-thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xia_Bootstrapping_Grounded_Chain-of-Thought_in_Multimodal_LLMs_for_Data-Efficient_Model_Adaptation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Jiaer Xia , Bingkui Tong , Yuhang Zang , Rui Shao , Kaiyang Zhou Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with chain-of-thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF",
      "index": 25,
      "title": "Test-Time Prompt Tuning for Zero-Shot Depth Completion",
      "authors": [
        "Chanhwi Jeong",
        "Inhwan Bae",
        "Jin-Hwi Park",
        "Hae-Gon Jeon"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "depth",
        "completion",
        "shot",
        "prompt",
        "test",
        "zero",
        "monocular",
        "tuning",
        "foundation",
        "sensor"
      ],
      "summary": "Zero-shot depth completion with metric scales poses significant challenges, primarily due to performance limitations such as domain specificity and sensor characteristics. One recent emerging solution is to integrate monocular depth foundation models into depth completion frameworks, yet these efforts still face issues with suboptimal performance and often require further adaptation to the target task. Surprisingly, we find that a simple test-time training, which fine-tunes monocular depth foundation models on sparse depth measurements from sensors just as it is, yields reasonable results. However, this test-time training obviously incurs high computational costs and introduces biases towards specific conditions, making it impractical for real-world scenarios. In this paper, we introduce a new approach toward parameter-efficient zero-shot depth completion. Our key idea of this work is to leverage visual prompt tuning, achieving sensor-specific depth scale adaptation without forgetting foundational knowledge. Experimental results on diverse datasets demonstrate that our approach outperforms relevant state-of-the-art methods, showing superior generalization and efficiency. Our source code is available in the supplementary materials.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jeong_Test-Time_Prompt_Tuning_for_Zero-Shot_Depth_Completion_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Test-Time Prompt Tuning for Zero-Shot Depth Completion [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Chanhwi Jeong , Inhwan Bae , Jin-Hwi Park , Hae-Gon Jeon Zero-shot depth completion with metric scales poses significant challenges, primarily due to performance limitations such as domain specificity and sensor characteristics. One recent emerging solution is to integrate monocular depth foundation models into depth completion frameworks, yet these efforts still face issues with suboptimal performance and often require further adaptation to the target task. Surprisingly, we find that a simple test-time training, which fine-tunes monocular depth foundation models on sparse depth measurements from sensors just as it is, yields reasonable results. However, this test-time training obviously incurs high computational costs and introduces biases towards specific conditions, making it impractical for real-world scenarios. In this paper, we introduce a new approach toward parameter-efficient zero-shot depth completion. Our key idea of this work is to leverage visual prompt tuning, achieving sensor-specific depth scale adaptation without forgetting foundational knowledge. Experimental results on diverse datasets demonstrate that our approach outperforms relevant state-of-the-art methods, showing superior generalization and efficiency. Our source code is available in the supplementary materials. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF",
      "index": 26,
      "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
      "authors": [
        "Yixu Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lora",
        "stolenlora",
        "extraction",
        "attacks",
        "peft",
        "model",
        "vulnerability",
        "defense",
        "synthetic",
        "adapted"
      ],
      "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries.Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods.We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_StolenLoRA_Exploring_LoRA_Extraction_Attacks_via_Synthetic_Data_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Yixu Wang , Yan Teng , Yingchun Wang , Xingjun Ma Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries.Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods.We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF",
      "index": 27,
      "title": "Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction",
      "authors": [
        "Luyao Tang",
        "Kunze Huang",
        "Chaoqi Chen",
        "Yuxuan Yuan",
        "Chenxin Li",
        "Xiaotong Tu",
        "Xinghao Ding",
        "Yue Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "consensus",
        "congcd",
        "deconstruction",
        "multiplex",
        "lytang63",
        "dissecting",
        "category",
        "discovery",
        "human",
        "contextual"
      ],
      "summary": "Human perceptual systems excel at inducing and recognizing objects across both known and novel categories, a capability far beyond current machine learning frameworks. While generalized category discovery (GCD) aims to bridge this gap, existing methods predominantly focus on optimizing objective functions. We present an orthogonal solution, inspired by the human cognitive process for novel object understanding: decomposing objects into visual primitives and establishing cross-knowledge comparisons. We propose ConGCD, which establishes primitive-oriented representations through high-level semantic reconstruction, binding intra-class shared attributes via deconstruction. Mirroring human preference diversity in visual processing, where distinct individuals leverage dominant or contextual cues, we implement dominant and contextual consensus units to capture class-discriminative patterns and inherent distributional invariants, respectively. A consensus scheduler dynamically optimizes activation pathways, with final predictions emerging through multiplex consensus integration. Extensive evaluations across coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a consensus-aware paradigm. Code is available at: https://github.com/lytang63/ConGCD.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Dissecting_Generalized_Category_Discovery_Multiplex_Consensus_under_Self-Deconstruction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Luyao Tang , Kunze Huang , Chaoqi Chen , Yuxuan Yuan , Chenxin Li , Xiaotong Tu , Xinghao Ding , Yue Huang Human perceptual systems excel at inducing and recognizing objects across both known and novel categories, a capability far beyond current machine learning frameworks. While generalized category discovery (GCD) aims to bridge this gap, existing methods predominantly focus on optimizing objective functions. We present an orthogonal solution, inspired by the human cognitive process for novel object understanding: decomposing objects into visual primitives and establishing cross-knowledge comparisons. We propose ConGCD, which establishes primitive-oriented representations through high-level semantic reconstruction, binding intra-class shared attributes via deconstruction. Mirroring human preference diversity in visual processing, where distinct individuals leverage dominant or contextual cues, we implement dominant and contextual consensus units to capture class-discriminative patterns and inherent distributional invariants, respectively. A consensus scheduler dynamically optimizes activation pathways, with final predictions emerging through multiplex consensus integration. Extensive evaluations across coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a consensus-aware paradigm. Code is available at: https://github.com/lytang63/ConGCD. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF",
      "index": 28,
      "title": "Scaling Language-Free Visual Representation Learning",
      "authors": [
        "David Fan",
        "Shengbang Tong",
        "Jiachen Zhu",
        "Koustuv Sinha",
        "Zhuang Liu",
        "Xinlei Chen",
        "Michael Rabbat",
        "Nicolas Ballas",
        "Yann LeCun",
        "Amir Bar",
        "Saining Xie"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ssl",
        "visual",
        "clip",
        "vqa",
        "language",
        "vision",
        "pretraining",
        "metaclip",
        "supervised",
        "multimodal"
      ],
      "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: \"Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?\" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning. Code and models are at https://github.com/facebookresearch/webssl.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Scaling_Language-Free_Visual_Representation_Learning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fan_Scaling_Language-Free_Visual_Representation_Learning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Scaling_Language-Free_Visual_Representation_Learning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Scaling Language-Free Visual Representation Learning [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : David Fan , Shengbang Tong , Jiachen Zhu , Koustuv Sinha , Zhuang Liu , Xinlei Chen , Michael Rabbat , Nicolas Ballas , Yann LeCun , Amir Bar , Saining Xie Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: \"Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?\" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning. Code and models are at https://github.com/facebookresearch/webssl. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF",
      "index": 29,
      "title": "DiffRefine: Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection",
      "authors": [
        "Sangyun Shin",
        "Yuhang He",
        "Xinyu Hou",
        "Samuel Hodgson",
        "Andrew Markham",
        "Niki Trigoni"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "object",
        "detection",
        "diffrefine",
        "domain",
        "densification",
        "due",
        "false",
        "points",
        "proposals",
        "densify"
      ],
      "summary": "The robustness of 3D object detection in large-scale outdoor point clouds degrades significantly when deployed in an unseen environment due to domain shifts. To minimize the domain gap, existing works on domain adaptive detection focuses on several factors, including point density, object shape and sizes, to reduce the false negative detections. However, the adaptation results indicate that there are still remaining challenges. We argue that this is due to the challenge in recognizing comparably less distinctive region on object surface due to sparsity, occlusion, etc. In this work, we aim to reinforce those features by generating points on object surface to make them straightforwardly recognizable. We draw our motivation from a common observation that detection proposals already contain the accurate bounding boxes, but with relatively low objectness score predictions, which lead to false negatives. Given these box proposals, we densify sparse object points with a diffusion approach. As a result, our model DiffRefine can act as a simple additional module before second-stage refinement, where most existing detection models for two-stage detection can use. Experimental results on domain adaptive detection show competitive performance, especially on vanishing points due to distance on various detection architectures.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Shin_DiffRefine_Diffusion-based_Proposal_Specific_Point_Cloud_Densification_for_Cross-Domain_Object_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "DiffRefine: Diffusion-based Proposal Specific Point Cloud Densification for Cross-Domain Object Detection [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Sangyun Shin , Yuhang He , Xinyu Hou , Samuel Hodgson , Andrew Markham , Niki Trigoni The robustness of 3D object detection in large-scale outdoor point clouds degrades significantly when deployed in an unseen environment due to domain shifts. To minimize the domain gap, existing works on domain adaptive detection focuses on several factors, including point density, object shape and sizes, to reduce the false negative detections. However, the adaptation results indicate that there are still remaining challenges. We argue that this is due to the challenge in recognizing comparably less distinctive region on object surface due to sparsity, occlusion, etc. In this work, we aim to reinforce those features by generating points on object surface to make them straightforwardly recognizable. We draw our motivation from a common observation that detection proposals already contain the accurate bounding boxes, but with relatively low objectness score predictions, which lead to false negatives. Given these box proposals, we densify sparse object points with a diffusion approach. As a result, our model DiffRefine can act as a simple additional module before second-stage refinement, where most existing detection models for two-stage detection can use. Experimental results on domain adaptive detection show competitive performance, especially on vanishing points due to distance on various detection architectures. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF",
      "index": 30,
      "title": "Confound from All Sides, Distill with Resilience: Multi-Objective Adversarial Paths to Zero-Shot Robustness",
      "authors": [
        "Junhao Dong",
        "Jiao Liu",
        "Xinghua Qu",
        "Yew-Soon Ong"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "adversaries",
        "adversarial",
        "teacher",
        "decision",
        "robustness",
        "distillation",
        "shot",
        "vlms",
        "optima",
        "confound"
      ],
      "summary": "Adversarially robust knowledge distillation transfers the robustness of a large-scale teacher model to a lightweight student while preserving natural performance. However, foundation Vision-Language Models (VLMs) also demand the transfer of zero-shot inference capabilities. We find that standard robust distillation using untargeted adversarial examples fails to transfer out-of-distribution (zero-shot) robustness, as these adversaries primarily push inputs away from their original distribution, exploring a limited portion of the teacher's decision space and missing more diverse failure modes. A natural solution is to generate multiple targeted adversaries that traverse diverse paths across decision boundaries. Thus, these adversaries probe a broader region of the teacher's decision surface. However, naive targeted adversary optimization often converges to local optima within a single category's decision region, limiting the diversity. To address this, we propose a Multi-Objective Optimization (MOO)-based adversarial distillation framework that transfers robustness from large VLMs to lightweight ones by exploiting adversaries with two main objectives: misclassification and category-level adversarial diversity. Theoretically, we show that optimizing for diversity mitigates adversarial collapse into local optima, ensuring adversaries span multiple decision regions and capture the teacher's generalizable robust features. Extensive experiments demonstrate the superiority of our method over state-of-the-art adversarial learning across diverse scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_Confound_from_All_Sides_Distill_with_Resilience_Multi-Objective_Adversarial_Paths_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 6
      },
      "raw_excerpt": "Confound from All Sides, Distill with Resilience: Multi-Objective Adversarial Paths to Zero-Shot Robustness [PDF 3 ] [Copy] [Kimi 6 ] [REL] Authors : Junhao Dong , Jiao Liu , Xinghua Qu , Yew-Soon Ong Adversarially robust knowledge distillation transfers the robustness of a large-scale teacher model to a lightweight student while preserving natural performance. However, foundation Vision-Language Models (VLMs) also demand the transfer of zero-shot inference capabilities. We find that standard robust distillation using untargeted adversarial examples fails to transfer out-of-distribution (zero-shot) robustness, as these adversaries primarily push inputs away from their original distribution, exploring a limited portion of the teacher's decision space and missing more diverse failure modes. A natural solution is to generate multiple targeted adversaries that traverse diverse paths across decision boundaries. Thus, these adversaries probe a broader region of the teacher's decision surface. However, naive targeted adversary optimization often converges to local optima within a single category's decision region, limiting the diversity. To address this, we propose a Multi-Objective Optimization (MOO)-based adversarial distillation framework that transfers robustness from large VLMs to lightweight ones by exploiting adversaries with two main objectives: misclassification and category-level adversarial diversity. Theoretically, we show that optimizing for diversity mitigates adversarial collapse into local optima, ensuring adversaries span multiple decision regions and capture the teacher's generalizable robust features. Extensive experiments demonstrate the superiority of our method over state-of-the-art adversarial learning across diverse scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF",
      "index": 31,
      "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions",
      "authors": [
        "Marko Mihajlovic",
        "Siwei Zhang",
        "Gen Li",
        "Kaifeng Zhao",
        "Lea Muller",
        "Siyu Tang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "volumetricsmpl",
        "volumetric",
        "nbw",
        "human",
        "meshes",
        "body",
        "interactions",
        "neural",
        "scenes",
        "coap"
      ],
      "summary": "Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Mihajlovic_VolumetricSMPL_A_Neural_Volumetric_Body_Model_for_Efficient_Interactions_Contacts_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Marko Mihajlovic , Siwei Zhang , Gen Li , Kaifeng Zhao , Lea Muller , Siyu Tang Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF",
      "index": 32,
      "title": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching",
      "authors": [
        "Paul Roetzer",
        "Florian Bernard"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "shape",
        "geometrically",
        "matchings",
        "consistent",
        "matching",
        "globally",
        "shapes",
        "hyper",
        "consistency",
        "practice"
      ],
      "summary": "Geometric consistency, i.e. the preservation of neighbourhoods, is a natural and strong prior in 3D shape matching. Geometrically consistent matchings are crucial for many downstream applications, such as texture transfer or statistical shape modelling. Yet, in practice, geometric consistency is often overlooked, or only achieved under severely limiting assumptions (e.g. a good initialisation). In this work, we propose a novel formalism for computing globally optimal and geometrically consistent matchings between 3D shapes which is scalable in practice. Our key idea is to represent the surface of the source shape as a collection of cyclic graphs, which are then consistently matched to the target shape. Mathematically, we construct a hyper product graph (between source and target shape), and then cast 3D shape matching as a minimum-cost circulation flow problem in this hyper graph, which yields global geometrically consistent matchings between both shapes. We empirically show that our formalism is efficiently solvable and that it leads to high-quality results. Our code is publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Roetzer_Fast_Globally_Optimal_and_Geometrically_Consistent_3D_Shape_Matching_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Paul Roetzer , Florian Bernard Geometric consistency, i.e. the preservation of neighbourhoods, is a natural and strong prior in 3D shape matching. Geometrically consistent matchings are crucial for many downstream applications, such as texture transfer or statistical shape modelling. Yet, in practice, geometric consistency is often overlooked, or only achieved under severely limiting assumptions (e.g. a good initialisation). In this work, we propose a novel formalism for computing globally optimal and geometrically consistent matchings between 3D shapes which is scalable in practice. Our key idea is to represent the surface of the source shape as a collection of cyclic graphs, which are then consistently matched to the target shape. Mathematically, we construct a hyper product graph (between source and target shape), and then cast 3D shape matching as a minimum-cost circulation flow problem in this hyper graph, which yields global geometrically consistent matchings between both shapes. We empirically show that our formalism is efficiently solvable and that it leads to high-quality results. Our code is publicly available. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF",
      "index": 33,
      "title": "Evading Data Provenance in Deep Neural Networks",
      "authors": [
        "Hongyu Zhu",
        "Sichu Liang",
        "Wenwen Wang",
        "Zhuomeng Zhang",
        "Fangqi Li",
        "Shi-Lin Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dov",
        "evasion",
        "copyright",
        "evading",
        "ood",
        "oversimplistic",
        "provenance",
        "datasets",
        "thefts",
        "dataset"
      ],
      "summary": "Modern over-parameterized deep models are highly data-dependent, with large scale general-purpose and domain-specific datasets serving as the bedrock for rapid advancements. However, many datasets are proprietary or contain sensitive information, making unrestricted model training problematic. In the open world where data thefts cannot be fully prevented, Dataset Ownership Verification (DOV) has emerged as a promising method to protect copyright by detecting unauthorized model training and tracing illicit activities. Due to its diversity and superior stealth, evading DOV is considered extremely challenging. However, this paper identifies that previous studies have relied on oversimplistic evasion attacks for evaluation, leading to a false sense of security. We introduce a unified evasion framework, in which a teacher model first learns from the copyright dataset and then transfers task-relevant yet identifier-independent domain knowledge to a surrogate student using an out-of-distribution (OOD) dataset as the intermediary. Leveraging Vision-Language Models and Large Language Models, we curate the most informative and reliable subsets from the OOD gallery set as the final transfer set, and propose selectively transferring task-oriented knowledge to achieve a better trade-off between generalization and evasion effectiveness. Experiments across diverse datasets covering eleven DOV methods demonstrate our approach simultaneously eliminates all copyright identifiers and significantly outperforms nine state-of-the-art evasion attacks in both generalization and effectiveness, with moderate computational overhead. As a proof of concept, we reveal key vulnerabilities in current DOV methods, highlighting the need for long-term development to enhance practicality.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Evading_Data_Provenance_in_Deep_Neural_Networks_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Evading Data Provenance in Deep Neural Networks [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hongyu Zhu , Sichu Liang , Wenwen Wang , Zhuomeng Zhang , Fangqi Li , Shi-Lin Wang Modern over-parameterized deep models are highly data-dependent, with large scale general-purpose and domain-specific datasets serving as the bedrock for rapid advancements. However, many datasets are proprietary or contain sensitive information, making unrestricted model training problematic. In the open world where data thefts cannot be fully prevented, Dataset Ownership Verification (DOV) has emerged as a promising method to protect copyright by detecting unauthorized model training and tracing illicit activities. Due to its diversity and superior stealth, evading DOV is considered extremely challenging. However, this paper identifies that previous studies have relied on oversimplistic evasion attacks for evaluation, leading to a false sense of security. We introduce a unified evasion framework, in which a teacher model first learns from the copyright dataset and then transfers task-relevant yet identifier-independent domain knowledge to a surrogate student using an out-of-distribution (OOD) dataset as the intermediary. Leveraging Vision-Language Models and Large Language Models, we curate the most informative and reliable subsets from the OOD gallery set as the final transfer set, and propose selectively transferring task-oriented knowledge to achieve a better trade-off between generalization and evasion effectiveness. Experiments across diverse datasets covering eleven DOV methods demonstrate our approach simultaneously eliminates all copyright identifiers and significantly outperforms nine state-of-the-art evasion attacks in both generalization and effectiveness, with moderate computational overhead. As a proof of concept, we reveal key vulnerabilities in current DOV methods, highlighting the need for long-term development to enhance practicality. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF",
      "index": 34,
      "title": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability",
      "authors": [
        "Boyong He",
        "Yuxiang Ji",
        "Zhuoyue Tan",
        "Liaoni Wu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "domain",
        "diffusion",
        "fitness",
        "generalization",
        "domains",
        "transferability",
        "detectors",
        "improving",
        "performance",
        "tasks"
      ],
      "summary": "Detectors often suffer from performance drop due to domain gap between training and testing data. Recent methods explore diffusion models applied to domain generalization (DG) and adaptation (DA) tasks, but still struggle with large inference costs and have not yet fully leveraged the capabilities of diffusion models. We propose to tackle these problems by extracting intermediate features from a single-step diffusion process, improving feature collection and fusion to reduce inference time by 75% while enhancing performance on source domains (i.e., Fitness). Then, we construct an object-centered auxiliary branch by applying box-masked images with class prompts to extract robust and domain-invariant features that focus on object. We also apply consistency loss to align the auxiliary and ordinary branch, balancing fitness and generalization while preventing overfitting and improving performance on target domains (i.e., Generalization). Furthermore, within a unified framework, standard detectors are guided by diffusion detectors through feature-level and object-level alignment on source domains (for DG) and unlabeled target domains (for DA), thereby improving cross-domain detection performance (i.e., Transferability). Our method achieves competitive results on 3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO generalization benchmark demonstrate that our method maintains significant advantages and show remarkable efficiency in large domain shifts and low-data scenarios. Our work shows the superiority of applying diffusion models to domain generalized and adaptive detection tasks and offers valuable insights for visual perception tasks across diverse domains.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/He_Boosting_Domain_Generalized_and_Adaptive_Detection_with_Diffusion_Models_Fitness_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Boyong He , Yuxiang Ji , Zhuoyue Tan , Liaoni Wu Detectors often suffer from performance drop due to domain gap between training and testing data. Recent methods explore diffusion models applied to domain generalization (DG) and adaptation (DA) tasks, but still struggle with large inference costs and have not yet fully leveraged the capabilities of diffusion models. We propose to tackle these problems by extracting intermediate features from a single-step diffusion process, improving feature collection and fusion to reduce inference time by 75% while enhancing performance on source domains (i.e., Fitness). Then, we construct an object-centered auxiliary branch by applying box-masked images with class prompts to extract robust and domain-invariant features that focus on object. We also apply consistency loss to align the auxiliary and ordinary branch, balancing fitness and generalization while preventing overfitting and improving performance on target domains (i.e., Generalization). Furthermore, within a unified framework, standard detectors are guided by diffusion detectors through feature-level and object-level alignment on source domains (for DG) and unlabeled target domains (for DA), thereby improving cross-domain detection performance (i.e., Transferability). Our method achieves competitive results on 3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO generalization benchmark demonstrate that our method maintains significant advantages and show remarkable efficiency in large domain shifts and low-data scenarios. Our work shows the superiority of applying diffusion models to domain generalized and adaptive detection tasks and offers valuable insights for visual perception tasks across diverse domains. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF",
      "index": 35,
      "title": "What to Distill? Fast Knowledge Distillation with Adaptive Sampling",
      "authors": [
        "Byungchul Chae",
        "Seonyeong Heo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "distillation",
        "knowledge",
        "distill",
        "adaptive",
        "sampling",
        "kdas",
        "work",
        "faster",
        "overlooks",
        "quality"
      ],
      "summary": "Knowledge Distillation (KD) has been established as an effective technique for reducing the resource requirements of models when tackling computer vision tasks. Prior work has studied how to distill the knowledge of a teacher model better, but it overlooks how data affects the distillation result. This work examines the impact of data in knowledge distillation from two perspectives: (i) quantity of knowledge and (ii) quality of knowledge. Our examination finds that faster knowledge distillation can be achieved by using data with a large amount of high-quality knowledge in distillation. Based on the findings, this work proposes an efficient adaptive sampling method called KDAS for faster knowledge distillation, which enhances the distillation efficiency by selecting and applying 'good' samples for the distillation. This work shows that our adaptive sampling methods can effectively accelerate the training efficiency of a student model when combined with existing KD methods.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chae_What_to_Distill_Fast_Knowledge_Distillation_with_Adaptive_Sampling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 1
      },
      "raw_excerpt": "What to Distill? Fast Knowledge Distillation with Adaptive Sampling [PDF 7 ] [Copy] [Kimi 1 ] [REL] Authors : Byungchul Chae , Seonyeong Heo Knowledge Distillation (KD) has been established as an effective technique for reducing the resource requirements of models when tackling computer vision tasks. Prior work has studied how to distill the knowledge of a teacher model better, but it overlooks how data affects the distillation result. This work examines the impact of data in knowledge distillation from two perspectives: (i) quantity of knowledge and (ii) quality of knowledge. Our examination finds that faster knowledge distillation can be achieved by using data with a large amount of high-quality knowledge in distillation. Based on the findings, this work proposes an efficient adaptive sampling method called KDAS for faster knowledge distillation, which enhances the distillation efficiency by selecting and applying 'good' samples for the distillation. This work shows that our adaptive sampling methods can effectively accelerate the training efficiency of a student model when combined with existing KD methods. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF",
      "index": 36,
      "title": "Multispectral Demosaicing via Dual Cameras",
      "authors": [
        "SaiKiran Tedla",
        "Junyong Lee",
        "Beixuan Yang",
        "Mahmoud Afifi",
        "Michael S. Brown"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "demosaicing",
        "rgb",
        "multispectral",
        "images",
        "camera",
        "cameras",
        "dual",
        "demosaiced",
        "mosaiced",
        "spectral"
      ],
      "summary": "Multispectral (MS) images capture detailed scene information across a wide range of spectral bands, making them invaluable for applications requiring rich spectral data. Integrating MS imaging into multi-camera devices, such as smartphones, has the potential to enhance both spectral applications and RGB image quality. A critical step in processing MS data is demosaicing, which reconstructs color information from the mosaic MS images captured by the camera. This paper proposes a method for MS image demosaicing specifically designed for dual-camera setups where both RGB and MS cameras capture the same scene. Our approach leverages co-captured RGB images, which typically have higher spatial fidelity, to guide the demosaicing of lower-fidelity MS images. We introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB and MS mosaiced images with ground-truth demosaiced outputs - that enables training and evaluation of our method. Experimental results demonstrate that our method achieves state-of-the-art accuracy compared to existing techniques.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tedla_Multispectral_Demosaicing_via_Dual_Cameras_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tedla_Multispectral_Demosaicing_via_Dual_Cameras@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tedla_Multispectral_Demosaicing_via_Dual_Cameras_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Multispectral Demosaicing via Dual Cameras [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : SaiKiran Tedla , Junyong Lee , Beixuan Yang , Mahmoud Afifi , Michael S. Brown Multispectral (MS) images capture detailed scene information across a wide range of spectral bands, making them invaluable for applications requiring rich spectral data. Integrating MS imaging into multi-camera devices, such as smartphones, has the potential to enhance both spectral applications and RGB image quality. A critical step in processing MS data is demosaicing, which reconstructs color information from the mosaic MS images captured by the camera. This paper proposes a method for MS image demosaicing specifically designed for dual-camera setups where both RGB and MS cameras capture the same scene. Our approach leverages co-captured RGB images, which typically have higher spatial fidelity, to guide the demosaicing of lower-fidelity MS images. We introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB and MS mosaiced images with ground-truth demosaiced outputs - that enables training and evaluation of our method. Experimental results demonstrate that our method achieves state-of-the-art accuracy compared to existing techniques. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF",
      "index": 37,
      "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling",
      "authors": [
        "Christopher Xie",
        "Armen Avetisyan",
        "Henry Howard-Jenkins",
        "Yawar Siddiqui",
        "Julian Straub",
        "Richard Newcombe",
        "Vasileios Balntas",
        "Jakob Engel"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "layout",
        "scenescript",
        "infilling",
        "scene",
        "layouts",
        "human",
        "loop",
        "local",
        "task",
        "correction"
      ],
      "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout that uses human feedback from an egocentric standpoint. We study this approach through introduction of a novel local correction task, where users identify local errors and prompt a model to automatically correct them. Building on SceneScript, a state-of-the-art framework for 3D scene layout estimation that leverages structured language, we propose a solution that structures this problem as \"infilling\", a task studied in natural language processing. We train a multi-task version of SceneScript that maintains performance on global predictions while significantly improving its local correction ability. We integrate this into a human-in-the-loop system, enabling a user to iteratively refine scene layout estimates via a low-friction \"one-click fix\" workflow. Our system enables the final refined layout to diverge from the training distribution, allowing for more accurate modelling of complex layouts.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Human-in-the-Loop_Local_Corrections_of_3D_Scene_Layouts_via_Infilling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Christopher Xie , Armen Avetisyan , Henry Howard-Jenkins , Yawar Siddiqui , Julian Straub , Richard Newcombe , Vasileios Balntas , Jakob Engel We present a novel human-in-the-loop approach to estimate 3D scene layout that uses human feedback from an egocentric standpoint. We study this approach through introduction of a novel local correction task, where users identify local errors and prompt a model to automatically correct them. Building on SceneScript, a state-of-the-art framework for 3D scene layout estimation that leverages structured language, we propose a solution that structures this problem as \"infilling\", a task studied in natural language processing. We train a multi-task version of SceneScript that maintains performance on global predictions while significantly improving its local correction ability. We integrate this into a human-in-the-loop system, enabling a user to iteratively refine scene layout estimates via a low-friction \"one-click fix\" workflow. Our system enables the final refined layout to diverge from the training distribution, allowing for more accurate modelling of complex layouts. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF",
      "index": 38,
      "title": "SAC-GNC: SAmple Consensus for adaptive Graduated Non-Convexity",
      "authors": [
        "Valter Piedade",
        "Chitturi Sidhartha",
        "Jos Gaspar",
        "Venu Madhav Govindu",
        "Pedro Miraldo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gnc",
        "graduated",
        "sac",
        "outliers",
        "convexity",
        "robust",
        "shape",
        "initialization",
        "annealing",
        "scorings"
      ],
      "summary": "Outliers are ubiquitous in geometric vision contexts such as pose estimation and mapping, leading to inaccurate estimates. While robust loss functions can tackle outliers, it is challenging to make the estimation robust to the choice of initialization and to estimate the appropriate robust loss shape parameter that allows distinguishing inliers from outliers. Graduated non-convexity (GNC) often mitigates these issues. However, typical GNC uses a fixed annealing factor to update the shape parameter, which can lead to low-quality or inefficient estimates. This paper proposes a novel approach to adaptively anneal the shape parameter within a GNC framework. We developed a search strategy that incorporates a sampling of annealing choices and model scorings to select the most promising shape parameter at each GNC iteration. Additionally, we propose new stopping criteria and an initialization technique that improves performance for diverse data, and we show the benefits of combining discrete and continuous robust estimation strategies. We evaluate our method using synthetic and real-world data in two problems: 3D registration and pose graph optimization in SLAM sequences. Our results demonstrate greater efficiency and robustness compared to previous GNC schemes. Code and other resources are available at https://www.merl.com/research/highlights/sac-gnc.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Piedade_SAC-GNC_SAmple_Consensus_for_adaptive_Graduated_Non-Convexity_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SAC-GNC: SAmple Consensus for adaptive Graduated Non-Convexity [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Valter Piedade , Chitturi Sidhartha , Jos Gaspar , Venu Madhav Govindu , Pedro Miraldo Outliers are ubiquitous in geometric vision contexts such as pose estimation and mapping, leading to inaccurate estimates. While robust loss functions can tackle outliers, it is challenging to make the estimation robust to the choice of initialization and to estimate the appropriate robust loss shape parameter that allows distinguishing inliers from outliers. Graduated non-convexity (GNC) often mitigates these issues. However, typical GNC uses a fixed annealing factor to update the shape parameter, which can lead to low-quality or inefficient estimates. This paper proposes a novel approach to adaptively anneal the shape parameter within a GNC framework. We developed a search strategy that incorporates a sampling of annealing choices and model scorings to select the most promising shape parameter at each GNC iteration. Additionally, we propose new stopping criteria and an initialization technique that improves performance for diverse data, and we show the benefits of combining discrete and continuous robust estimation strategies. We evaluate our method using synthetic and real-world data in two problems: 3D registration and pose graph optimization in SLAM sequences. Our results demonstrate greater efficiency and robustness compared to previous GNC schemes. Code and other resources are available at https://www.merl.com/research/highlights/sac-gnc. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF",
      "index": 39,
      "title": "Is Tracking Really More Challenging in First Person Egocentric Vision?",
      "authors": [
        "Matteo Dunnhofer",
        "Zaira Manigrasso",
        "Christian Micheloni"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "egocentric",
        "person",
        "vision",
        "activities",
        "tracking",
        "object",
        "human",
        "really",
        "challenging",
        "segmentation"
      ],
      "summary": "Visual object tracking and segmentation are becoming fundamental tasks for understanding human activities in egocentric vision. Recent research has benchmarked state-of-the-art methods and concluded that first person egocentric vision presents challenges compared to previously studied domains. However, these claims are based on evaluations conducted across significantly different scenarios. Many of the challenging characteristics attributed to egocentric vision are also present in third person videos of human-object activities. This raises a critical question: how much of the observed performance drop stems from the unique first person viewpoint inherent to egocentric vision versus the domain of human-object activities? To address this question, we introduce a new benchmark study designed to disentangle such factors. Our evaluation strategy enables a more precise separation of challenges related to the first person perspective from those linked to the broader domain of human-object activity understanding. By doing so, we provide deeper insights into the true sources of difficulty in egocentric tracking and segmentation, facilitating more targeted advancements on this task.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Dunnhofer_Is_Tracking_Really_More_Challenging_in_First_Person_Egocentric_Vision_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Is Tracking Really More Challenging in First Person Egocentric Vision? [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Matteo Dunnhofer , Zaira Manigrasso , Christian Micheloni Visual object tracking and segmentation are becoming fundamental tasks for understanding human activities in egocentric vision. Recent research has benchmarked state-of-the-art methods and concluded that first person egocentric vision presents challenges compared to previously studied domains. However, these claims are based on evaluations conducted across significantly different scenarios. Many of the challenging characteristics attributed to egocentric vision are also present in third person videos of human-object activities. This raises a critical question: how much of the observed performance drop stems from the unique first person viewpoint inherent to egocentric vision versus the domain of human-object activities? To address this question, we introduce a new benchmark study designed to disentangle such factors. Our evaluation strategy enables a more precise separation of challenges related to the first person perspective from those linked to the broader domain of human-object activity understanding. By doing so, we provide deeper insights into the true sources of difficulty in egocentric tracking and segmentation, facilitating more targeted advancements on this task. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF",
      "index": 40,
      "title": "Less is More: Empowering GUI Agent with Context-Aware Simplification",
      "authors": [
        "Gongwei Chen",
        "Xurui Zhou",
        "Rui Shao",
        "Yibo Lyu",
        "Kaiwen Zhou",
        "Shuai Wang",
        "Wentao Li",
        "Yinchuan Li",
        "Zhongang Qi",
        "Liqiang Nie"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gui",
        "history",
        "simpagent",
        "context",
        "agent",
        "simplification",
        "modeling",
        "agents",
        "element",
        "unrelated"
      ],
      "summary": "The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agents and summarize: **1) the high-density and loose-relation of element context** highlight the existence of many unrelated elements and their negative influence; **2) the high redundancy of history context** reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed **SimpAgent**. To mitigate potential interference from numerous unrelated elements, we introduce a **masking-based element pruning** method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a **consistency-guided history compression** module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Less_is_More_Empowering_GUI_Agent_with_Context-Aware_Simplification_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "Less is More: Empowering GUI Agent with Context-Aware Simplification [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Gongwei Chen , Xurui Zhou , Rui Shao , Yibo Lyu , Kaiwen Zhou , Shuai Wang , Wentao Li , Yinchuan Li , Zhongang Qi , Liqiang Nie The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agents and summarize: **1) the high-density and loose-relation of element context** highlight the existence of many unrelated elements and their negative influence; **2) the high redundancy of history context** reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed **SimpAgent**. To mitigate potential interference from numerous unrelated elements, we introduce a **masking-based element pruning** method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a **consistency-guided history compression** module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF",
      "index": 41,
      "title": "CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos",
      "authors": [
        "Nikita Karaev",
        "Yuri Makarov",
        "Jianyuan Wang",
        "Natalia Neverova",
        "Andrea Vedaldi",
        "Christian Rupprecht"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "cotracker3",
        "trackers",
        "videos",
        "real",
        "simpler",
        "synthetic",
        "tracking",
        "pseudo",
        "labelling",
        "better"
      ],
      "summary": "We introduce CoTracker3, a new state-of-the-art point tracker. With CoTracker3, we revisit the design of recent trackers, removing components and reducing the number of parameters while also improving performance. We also explore the interplay of synthetic and real data. Recent trackers are trained on synthetic videos due to the difficulty of collecting tracking annotations for real data. However, this can result in suboptimal performance due to the statistical gap between synthetic and real videos. We thus suggest using off-the-shelf trackers as teachers, annotating real videos with pseudo-labels. Compared to other recent attempts at using real data for learning trackers, this scheme is much simpler and achieves better results using 1,000 times less data. CoTracker3 is available in online (causal) and offline variants and is particularly robust to occlusions.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos_ICCV_2025_paper.html",
          "/venue/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Karaev_CoTracker3_Simpler_and_Better_Point_Tracking_by_Pseudo-Labelling_Real_Videos_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Nikita Karaev , Yuri Makarov , Jianyuan Wang , Natalia Neverova , Andrea Vedaldi , Christian Rupprecht We introduce CoTracker3, a new state-of-the-art point tracker. With CoTracker3, we revisit the design of recent trackers, removing components and reducing the number of parameters while also improving performance. We also explore the interplay of synthetic and real data. Recent trackers are trained on synthetic videos due to the difficulty of collecting tracking annotations for real data. However, this can result in suboptimal performance due to the statistical gap between synthetic and real videos. We thus suggest using off-the-shelf trackers as teachers, annotating real videos with pseudo-labels. Compared to other recent attempts at using real data for learning trackers, this scheme is much simpler and achieves better results using 1,000 times less data. CoTracker3 is available in online (causal) and offline variants and is particularly robust to occlusions. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF",
      "index": 42,
      "title": "SceneMI: Motion In-betweening for Modeling Human-Scene Interaction",
      "authors": [
        "Inwoo Hwang",
        "Bing Zhou",
        "Young Min Kim",
        "Jian Wang",
        "Chuan Guo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "scenemi",
        "betweening",
        "hsi",
        "scene",
        "keyframe",
        "modeling",
        "motion",
        "human",
        "gimo",
        "scenes"
      ],
      "summary": "Modeling human-scene interactions (HSI) is essential for understanding and simulating everyday human behaviors. Recent approaches utilizing generative modeling have made progress in this domain; however, they are limited in controllability and flexibility for real-world applications. To address these challenges, we propose reformulating the HSI modeling problem as Scene-aware Motion In-betweening---a more tractable and practical task. We introduce SceneMI, a framework that supports several practical applications, including keyframe-guided character animation in 3D scenes and enhancing the motion quality of imperfect HSI data. SceneMI employs dual scene descriptors to comprehensively encode global and local scene context. Furthermore, our framework leverages the inherent denoising nature of diffusion models to generalize on noisy keyframes. Experimental results demonstrate SceneMI's effectiveness in scene-aware keyframe in-betweening and generalization to the real-world GIMO dataset, where motions and scenes are acquired by noisy IMU sensors and smartphones. We further showcase SceneMI's applicability in HSI reconstruction from monocular videos.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Hwang_SceneMI_Motion_In-betweening_for_Modeling_Human-Scene_Interaction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SceneMI: Motion In-betweening for Modeling Human-Scene Interaction [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Inwoo Hwang , Bing Zhou , Young Min Kim , Jian Wang , Chuan Guo Modeling human-scene interactions (HSI) is essential for understanding and simulating everyday human behaviors. Recent approaches utilizing generative modeling have made progress in this domain; however, they are limited in controllability and flexibility for real-world applications. To address these challenges, we propose reformulating the HSI modeling problem as Scene-aware Motion In-betweening---a more tractable and practical task. We introduce SceneMI, a framework that supports several practical applications, including keyframe-guided character animation in 3D scenes and enhancing the motion quality of imperfect HSI data. SceneMI employs dual scene descriptors to comprehensively encode global and local scene context. Furthermore, our framework leverages the inherent denoising nature of diffusion models to generalize on noisy keyframes. Experimental results demonstrate SceneMI's effectiveness in scene-aware keyframe in-betweening and generalization to the real-world GIMO dataset, where motions and scenes are acquired by noisy IMU sensors and smartphones. We further showcase SceneMI's applicability in HSI reconstruction from monocular videos. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF",
      "index": 43,
      "title": "Learning Large Motion Estimation from Intermediate Representations with a High-Resolution Optical Flow Dataset Featuring Long-Range Dynamic Motion",
      "authors": [
        "Hoonhee Cho",
        "Yuhwan Jeong",
        "Kuk-Jin Yoon"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "estimation",
        "resolution",
        "motion",
        "range",
        "flow",
        "datasets",
        "relayflow",
        "long",
        "optical",
        "displacements"
      ],
      "summary": "With advancements in sensor and display technologies, high-resolution imagery is becoming increasingly prevalent in diverse applications. As a result, optical flow estimation needs to adapt to larger image resolutions, where even moderate movements lead to substantial pixel displacements, making long-range motion estimation more critical than ever. However, existing datasets primarily focus on short-range flow in low-resolution settings, limiting the generalization of models to high-resolution scenarios with large displacements. Additionally, there is a lack of suitable datasets for evaluating model capacity in long-range motion estimation, further hindering progress in this area. To address this, we introduce RelayFlow-4K, high-resolution 4K optical flow dataset designed to capture diverse motion patterns, including long-range intermediate frame flows. While such datasets provide valuable training resources, long-range estimation remains challenging due to increased matching ambiguity. Simply incorporating these datasets does not inherently improve performance. To this end, we propose a novel training framework that integrates matching cost distillation and incremental time-step learning to refine cost volume estimation and stabilize training. Additionally, we leverage the distance map, which measures the distance from unmatched regions to their nearest matched pixels, improving occlusion handling. Our approach significantly enhances long-range optical flow estimation in high-resolution settings. Our datasets and code are available at https://github.com/Chohoonhee/RelayFlow-4K.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_Learning_Large_Motion_Estimation_from_Intermediate_Representations_with_a_High-Resolution_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Learning Large Motion Estimation from Intermediate Representations with a High-Resolution Optical Flow Dataset Featuring Long-Range Dynamic Motion [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hoonhee Cho , Yuhwan Jeong , Kuk-Jin Yoon With advancements in sensor and display technologies, high-resolution imagery is becoming increasingly prevalent in diverse applications. As a result, optical flow estimation needs to adapt to larger image resolutions, where even moderate movements lead to substantial pixel displacements, making long-range motion estimation more critical than ever. However, existing datasets primarily focus on short-range flow in low-resolution settings, limiting the generalization of models to high-resolution scenarios with large displacements. Additionally, there is a lack of suitable datasets for evaluating model capacity in long-range motion estimation, further hindering progress in this area. To address this, we introduce RelayFlow-4K, high-resolution 4K optical flow dataset designed to capture diverse motion patterns, including long-range intermediate frame flows. While such datasets provide valuable training resources, long-range estimation remains challenging due to increased matching ambiguity. Simply incorporating these datasets does not inherently improve performance. To this end, we propose a novel training framework that integrates matching cost distillation and incremental time-step learning to refine cost volume estimation and stabilize training. Additionally, we leverage the distance map, which measures the distance from unmatched regions to their nearest matched pixels, improving occlusion handling. Our approach significantly enhances long-range optical flow estimation in high-resolution settings. Our datasets and code are available at https://github.com/Chohoonhee/RelayFlow-4K. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF",
      "index": 44,
      "title": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding",
      "authors": [
        "Yue Fan",
        "Xiaojian Ma",
        "Rongpeng Su",
        "Jun Guo",
        "Rujie Wu",
        "Xi Chen",
        "Qing Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "embodied",
        "videoagent",
        "egocentric",
        "video",
        "memory",
        "understanding",
        "openeqa",
        "vq3d",
        "scene",
        "scenes"
      ],
      "summary": "This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 6.5% on Ego4D-VQ3D, 2.6% on OpenEQA, and 15.3% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors_ICCV_2025_paper.html",
          "/venue/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Embodied_VideoAgent_Persistent_Memory_from_Egocentric_Videos_and_Embodied_Sensors_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Yue Fan , Xiaojian Ma , Rongpeng Su , Jun Guo , Rujie Wu , Xi Chen , Qing Li This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 6.5% on Ego4D-VQ3D, 2.6% on OpenEQA, and 15.3% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF",
      "index": 45,
      "title": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning",
      "authors": [
        "Jingjing Jiang",
        "Chao Ma",
        "Xurui Song",
        "Hanwang Zhang",
        "Jun Luo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "corvid",
        "reasoning",
        "mllms",
        "cot",
        "multimodal",
        "thought",
        "287k",
        "mcot",
        "chain",
        "architecturally"
      ],
      "summary": "Recent advancements in multimodal large language models (MLLMs) have demonstrated exceptional performance in multimodal perception and understanding. However, leading open-source MLLMs exhibit significant limitations in complex and structured reasoning, particularly in tasks requiring deep reasoning for decision-making and problem-solving. In this work, we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for informative visual representation and a meticulously designed connector (GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality multimodal CoT instruction-following dataset, refined and standardized from diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid with a two-stage CoT-formatted training approach to progressively enhance its step-by-step reasoning abilities. Furthermore, we propose an effective inference-time scaling strategy that enables Corvid to mitigate over-reasoning and under-reasoning through self-verification. Extensive experiments demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art MLLMs with similar parameter scales, with notable strengths in mathematical reasoning and science problem-solving. Project page: https://mm-vl.github.io/corvid.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Corvid_Improving_Multimodal_Large_Language_Models_Towards_Chain-of-Thought_Reasoning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Jingjing Jiang , Chao Ma , Xurui Song , Hanwang Zhang , Jun Luo Recent advancements in multimodal large language models (MLLMs) have demonstrated exceptional performance in multimodal perception and understanding. However, leading open-source MLLMs exhibit significant limitations in complex and structured reasoning, particularly in tasks requiring deep reasoning for decision-making and problem-solving. In this work, we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for informative visual representation and a meticulously designed connector (GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality multimodal CoT instruction-following dataset, refined and standardized from diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid with a two-stage CoT-formatted training approach to progressively enhance its step-by-step reasoning abilities. Furthermore, we propose an effective inference-time scaling strategy that enables Corvid to mitigate over-reasoning and under-reasoning through self-verification. Extensive experiments demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art MLLMs with similar parameter scales, with notable strengths in mathematical reasoning and science problem-solving. Project page: https://mm-vl.github.io/corvid. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF",
      "index": 46,
      "title": "StableDepth: Scene-Consistent and Scale-Invariant Monocular Depth",
      "authors": [
        "Zheng Zhang",
        "Lihe Yang",
        "Tianyu Yang",
        "Chaohui Yu",
        "Xiaoyang Guo",
        "Yixing Lao",
        "Hengshuang Zhao"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "stabledepth",
        "depth",
        "video",
        "flickering",
        "scene",
        "monocular",
        "13x",
        "scale",
        "consistent",
        "invariant"
      ],
      "summary": "Recent advances in monocular depth estimation significantly improve robustness and accuracy. However, relative depth models exhibit flickering and 3D inconsistency in video data, limiting 3D reconstruction applications. We introduce StableDepth, a scene-consistent and scale-invariant depth estimation method achieving scene-level 3D consistency. Our dual-decoder architecture learns from large-scale unlabeled video data, enhancing generalization and reducing flickering. Unlike previous methods requiring full video sequences, StableDepth enables online inference at 13x faster speed, achieving significant improvements across benchmarks with comparable temporal consistency to video diffusion-based estimators.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_StableDepth_Scene-Consistent_and_Scale-Invariant_Monocular_Depth_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 2
      },
      "raw_excerpt": "StableDepth: Scene-Consistent and Scale-Invariant Monocular Depth [PDF 13 ] [Copy] [Kimi 2 ] [REL] Authors : Zheng Zhang , Lihe Yang , Tianyu Yang , Chaohui Yu , Xiaoyang Guo , Yixing Lao , Hengshuang Zhao Recent advances in monocular depth estimation significantly improve robustness and accuracy. However, relative depth models exhibit flickering and 3D inconsistency in video data, limiting 3D reconstruction applications. We introduce StableDepth, a scene-consistent and scale-invariant depth estimation method achieving scene-level 3D consistency. Our dual-decoder architecture learns from large-scale unlabeled video data, enhancing generalization and reducing flickering. Unlike previous methods requiring full video sequences, StableDepth enables online inference at 13x faster speed, achieving significant improvements across benchmarks with comparable temporal consistency to video diffusion-based estimators. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF",
      "index": 47,
      "title": "HccePose(BF): Predicting Front & Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation",
      "authors": [
        "Yulin Wang",
        "Mengting Hu",
        "Hongli Li",
        "Chen Luo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "coordinates",
        "front",
        "correspondences",
        "back",
        "dense",
        "bop",
        "object",
        "surface",
        "pose",
        "hccepose"
      ],
      "summary": "In pose estimation for seen objects, a prevalent pipeline involves using neural networks to predict dense 3D coordinates of the object surface on 2D images, which are then used to establish dense 2D-3D correspondences. However, current methods primarily focus on more efficient encoding techniques to improve the precision of predicted 3D coordinates on the object's front surface, overlooking the potential benefits of incorporating the back surface and interior of the object. To better utilize the full surface and interior of the object, this study predicts 3D coordinates of both the object's front and back surfaces and densely samples 3D coordinates between them. This process creates ultra-dense 2D-3D correspondences, effectively enhancing pose estimation accuracy based on the Perspective-n-Point (PnP) algorithm. Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to provide a more accurate and efficient representation of front and back surface coordinates. Experimental results show that, compared to existing state-of-the-art (SOTA) methods on the BOP website, the proposed approach outperforms across seven classic BOP core datasets.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_HccePoseBF_Predicting_Front__Back_Surfaces_to_Construct_Ultra-Dense_2D-3D_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "HccePose(BF): Predicting Front & Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Yulin Wang , Mengting Hu , Hongli Li , Chen Luo In pose estimation for seen objects, a prevalent pipeline involves using neural networks to predict dense 3D coordinates of the object surface on 2D images, which are then used to establish dense 2D-3D correspondences. However, current methods primarily focus on more efficient encoding techniques to improve the precision of predicted 3D coordinates on the object's front surface, overlooking the potential benefits of incorporating the back surface and interior of the object. To better utilize the full surface and interior of the object, this study predicts 3D coordinates of both the object's front and back surfaces and densely samples 3D coordinates between them. This process creates ultra-dense 2D-3D correspondences, effectively enhancing pose estimation accuracy based on the Perspective-n-Point (PnP) algorithm. Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to provide a more accurate and efficient representation of front and back surface coordinates. Experimental results show that, compared to existing state-of-the-art (SOTA) methods on the BOP website, the proposed approach outperforms across seven classic BOP core datasets. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF",
      "index": 48,
      "title": "Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction",
      "authors": [
        "Edgar Sucar",
        "Zihang Lai",
        "Eldar Insafutdinov",
        "Andrea Vedaldi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "point",
        "maps",
        "dynamic",
        "dpm",
        "dust3r",
        "reconstruction",
        "tasks",
        "scenes",
        "extrinsics",
        "intrinsics"
      ],
      "summary": "DUSt3R has recently demonstrated that many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing 3D scenes, and establishing image correspondences, can be reduced to predicting a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. While this formulation is elegant and powerful, it is limited to static scenes. To overcome this limitation, we introduce the concept of Dynamic Point Maps (DPM), which extends standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key insight is that, when time is introduced, several possible spatial and temporal references can be used to define the point maps. We identify a minimal subset of these combinations that can be regressed by a network to solve the aforementioned tasks. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks, including video depth prediction, dynamic point cloud reconstruction, 3D scene flow, and object pose tracking, achieving state-of-the-art performance.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Sucar_Dynamic_Point_Maps_A_Versatile_Representation_for_Dynamic_3D_Reconstruction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Edgar Sucar , Zihang Lai , Eldar Insafutdinov , Andrea Vedaldi DUSt3R has recently demonstrated that many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing 3D scenes, and establishing image correspondences, can be reduced to predicting a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. While this formulation is elegant and powerful, it is limited to static scenes. To overcome this limitation, we introduce the concept of Dynamic Point Maps (DPM), which extends standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key insight is that, when time is introduced, several possible spatial and temporal references can be used to define the point maps. We identify a minimal subset of these combinations that can be regressed by a network to solve the aforementioned tasks. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks, including video depth prediction, dynamic point cloud reconstruction, 3D scene flow, and object pose tracking, achieving state-of-the-art performance. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF",
      "index": 49,
      "title": "Exploring View Consistency for Scene-Adaptive Low-Light Light Field Image Enhancement",
      "authors": [
        "Shuo Zhang",
        "Chen Gao",
        "Youfang Lin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "illumination",
        "light",
        "consistency",
        "view",
        "low",
        "enhancement",
        "lighting",
        "vcnet",
        "scene",
        "images"
      ],
      "summary": "Light Field (LF) images captured under low illumination conditions typically exhibit low quality. Recent learning-based methods for low-light LF enhancement are generally tailored to specific illumination inputs, limiting their performance in real-world scenes. Moreover, how to maintain the inherent view-consistency in the enhanced images also remain as a difficult problem. In this paper, we propose to explore the view consistency for scene-adaptive low-light LF enhancement. We first analyze the view consistency for LF illumination maps and design a self-supervised view-consistent loss to keep the consistency between the illumination maps of different views in LFs. To enhance the model's perception of illumination, we combine both global and local information to estimate the illumination map, which is easily plugged into other models. Subsequently, we use the illumination maps to light up the low-light LF images and restore the corruption to produce the final enhanced image. Extensive experiments demonstrate that our View-Consistenct Network (VCNet) outperforms state-of-the-art methods on real-world low-light LF datasets in both fixed lighting conditions and dynamic lighting conditions. Our proposed illumination adjustment is also demonstrated that can comprehensively improve the performance of existing methods in terms of both image quality and view consistency.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Exploring_View_Consistency_for_Scene-Adaptive_Low-Light_Light_Field_Image_Enhancement_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "Exploring View Consistency for Scene-Adaptive Low-Light Light Field Image Enhancement [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Shuo Zhang , Chen Gao , Youfang Lin Light Field (LF) images captured under low illumination conditions typically exhibit low quality. Recent learning-based methods for low-light LF enhancement are generally tailored to specific illumination inputs, limiting their performance in real-world scenes. Moreover, how to maintain the inherent view-consistency in the enhanced images also remain as a difficult problem. In this paper, we propose to explore the view consistency for scene-adaptive low-light LF enhancement. We first analyze the view consistency for LF illumination maps and design a self-supervised view-consistent loss to keep the consistency between the illumination maps of different views in LFs. To enhance the model's perception of illumination, we combine both global and local information to estimate the illumination map, which is easily plugged into other models. Subsequently, we use the illumination maps to light up the low-light LF images and restore the corruption to produce the final enhanced image. Extensive experiments demonstrate that our View-Consistenct Network (VCNet) outperforms state-of-the-art methods on real-world low-light LF datasets in both fixed lighting conditions and dynamic lighting conditions. Our proposed illumination adjustment is also demonstrated that can comprehensively improve the performance of existing methods in terms of both image quality and view consistency. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF",
      "index": 50,
      "title": "EventUPS: Uncalibrated Photometric Stereo Using an Event Camera",
      "authors": [
        "Jinxiu Liang",
        "Bohan Yu",
        "Siqi Yang",
        "Haotian Zhuang",
        "Jieji Ren",
        "Peiqi Duan",
        "Boxin Shi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "eventups",
        "lighting",
        "uncalibrated",
        "event",
        "illumination",
        "stereo",
        "photometric",
        "ups",
        "camera",
        "bandwidth"
      ],
      "summary": "We present EventUPS, the first uncalibrated photometric stereo (UPS) method using an event camera--a neuromorphic sensor that asynchronously detects brightness changes with microsecond resolution. Traditional frame-based UPS methods are hindered by high bandwidth demands and limited use in dynamic scenes. These methods require dense image correspondence under varying illumination and are incompatible with the fundamentally different sensing paradigm of event data. Our approach introduces three key innovations: an augmented null space formulation that directly relates each event to joint constraints on surface normals and lighting, naturally handling ambient illumination; a continuous parameterization of time-varying illumination that connects asynchronous events to synchronized lighting estimation; and a lighting fixture with known relative geometry that reduces ambiguity to a convex-concave uncertainty. We validate EventUPS using a custom-built LED lighting system. Experimental results show that our method achieves accuracy surpassing its frame-based counterpart while requiring only 5% of the data bandwidth.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_EventUPS_Uncalibrated_Photometric_Stereo_Using_an_Event_Camera_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "EventUPS: Uncalibrated Photometric Stereo Using an Event Camera [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Jinxiu Liang , Bohan Yu , Siqi Yang , Haotian Zhuang , Jieji Ren , Peiqi Duan , Boxin Shi We present EventUPS, the first uncalibrated photometric stereo (UPS) method using an event camera--a neuromorphic sensor that asynchronously detects brightness changes with microsecond resolution. Traditional frame-based UPS methods are hindered by high bandwidth demands and limited use in dynamic scenes. These methods require dense image correspondence under varying illumination and are incompatible with the fundamentally different sensing paradigm of event data. Our approach introduces three key innovations: an augmented null space formulation that directly relates each event to joint constraints on surface normals and lighting, naturally handling ambient illumination; a continuous parameterization of time-varying illumination that connects asynchronous events to synchronized lighting estimation; and a lighting fixture with known relative geometry that reduces ambiguity to a convex-concave uncertainty. We validate EventUPS using a custom-built LED lighting system. Experimental results show that our method achieves accuracy surpassing its frame-based counterpart while requiring only 5% of the data bandwidth. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ma_Find_Any_Part_in_3D@ICCV2025@CVF",
      "index": 51,
      "title": "Find Any Part in 3D",
      "authors": [
        "Ziqi Ma",
        "Yisong Yue",
        "Georgia Gkioxari"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "part",
        "engine",
        "ziqi",
        "data",
        "foundation",
        "object",
        "datasets",
        "300x",
        "segmentation",
        "annotates"
      ],
      "summary": "Why don't we have foundation models in 3D yet? A key limitation is data scarcity. For 3D object part segmentation, existing datasets are small in size and lack diversity. We show that it is possible to break this data barrier by building a data engine powered by 2D foundation models. Our data engine automatically annotates any number of object parts: 1755x more unique part types than existing datasets combined. By training on our annotated data with a simple contrastive objective, we obtain an open-world model that generalizes to any part in any object based on any text query. Even when evaluated zero-shot, we outperform existing methods on the datasets they train on. We achieve 260% improvement in mIoU and boost speed by 6x to 300x. Our scaling analysis confirms that this generalization stems from the data scale, which underscores the impact of our data engine. Finally, to advance general-category open-world 3D part segmentation, we release a benchmark covering a wide range of objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Find_Any_Part_in_3D_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ma_Find_Any_Part_in_3D@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_Find_Any_Part_in_3D_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Find Any Part in 3D [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Ziqi Ma , Yisong Yue , Georgia Gkioxari Why don't we have foundation models in 3D yet? A key limitation is data scarcity. For 3D object part segmentation, existing datasets are small in size and lack diversity. We show that it is possible to break this data barrier by building a data engine powered by 2D foundation models. Our data engine automatically annotates any number of object parts: 1755x more unique part types than existing datasets combined. By training on our annotated data with a simple contrastive objective, we obtain an open-world model that generalizes to any part in any object based on any text query. Even when evaluated zero-shot, we outperform existing methods on the datasets they train on. We achieve 260% improvement in mIoU and boost speed by 6x to 300x. Our scaling analysis confirms that this generalization stems from the data scale, which underscores the impact of our data engine. Finally, to advance general-category open-world 3D part segmentation, we release a benchmark covering a wide range of objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/ Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF",
      "index": 52,
      "title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?",
      "authors": [
        "Shouwei Ruan",
        "Hanqing Liu",
        "Yao Huang",
        "Xiaoqi Wang",
        "Caixin Kang",
        "Hang Su",
        "Yinpeng Dong",
        "Xingxing Wei"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "3dt",
        "advdreamer",
        "variations",
        "adv",
        "vlms",
        "world",
        "vlm",
        "real",
        "naturalness",
        "robustness"
      ],
      "summary": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations, we propose AdvDreamer, the first framework capable of generating physically reproducible Adversarial 3D Transformation (Adv-3DT) samples from single-view observations. In AdvDreamer, we integrate three key innovations: Firstly, to characterize real-world 3D variations with limited prior knowledge precisely, we design a zero-shot Monocular Pose Manipulation pipeline built upon generative 3D priors. Secondly, to ensure the visual quality of worst-case Adv-3DT samples, we propose Naturalness Reward Model that provides continuous naturalness regularization during adversarial optimization, effectively preventing convergence to hallucinated or unnatural elements. Thirdly, to enable systematic evaluation across diverse VLM architectures and visual-language tasks, we introduce the Inverse Semantic Probability loss as the adversarial optimization objective, which solely operates in the fundamental visual-textual alignment space. Based on the captured Adv-3DT samples with high aggressiveness and transferability, we establish MM3DTBench, the first VQA benchmark dataset tailored to evaluate VLM robustness under challenging 3D variations. Extensive evaluations of representative VLMs with varying architectures reveal that real-world 3D variations can pose severe threats to model performance across various tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ruan_AdvDreamer_Unveils_Are_Vision-Language_Models_Truly_Ready_for_Real-World_3D_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations? [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Shouwei Ruan , Hanqing Liu , Yao Huang , Xiaoqi Wang , Caixin Kang , Hang Su , Yinpeng Dong , Xingxing Wei Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations, we propose AdvDreamer, the first framework capable of generating physically reproducible Adversarial 3D Transformation (Adv-3DT) samples from single-view observations. In AdvDreamer, we integrate three key innovations: Firstly, to characterize real-world 3D variations with limited prior knowledge precisely, we design a zero-shot Monocular Pose Manipulation pipeline built upon generative 3D priors. Secondly, to ensure the visual quality of worst-case Adv-3DT samples, we propose Naturalness Reward Model that provides continuous naturalness regularization during adversarial optimization, effectively preventing convergence to hallucinated or unnatural elements. Thirdly, to enable systematic evaluation across diverse VLM architectures and visual-language tasks, we introduce the Inverse Semantic Probability loss as the adversarial optimization objective, which solely operates in the fundamental visual-textual alignment space. Based on the captured Adv-3DT samples with high aggressiveness and transferability, we establish MM3DTBench, the first VQA benchmark dataset tailored to evaluate VLM robustness under challenging 3D variations. Extensive evaluations of representative VLMs with varying architectures reveal that real-world 3D variations can pose severe threats to model performance across various tasks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF",
      "index": 53,
      "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras",
      "authors": [
        "Shuang Guo",
        "Friedhelm Hamann",
        "Guillermo Gallego"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "intensity",
        "event",
        "flow",
        "optical",
        "cameras",
        "unsupervised",
        "appearance",
        "motion",
        "estimation",
        "tub"
      ],
      "summary": "Event cameras rely on motion to obtain information about scene appearance. This means that appearance and motion are inherently linked: either both are present and recorded in the event data, or neither is captured. Previous works treat the recovery of these two visual quantities as separate tasks, which does not fit with the above-mentioned nature of event cameras and overlooks the inherent relations between them. We propose an unsupervised learning framework that jointly estimates optical flow (motion) and image intensity (appearance) using a single network. From the data generation model, we newly derive the event-based photometric error as a function of optical flow and image intensity. This error is further combined with the contrast maximization framework to form a comprehensive loss function that provides proper constraints for both flow and intensity estimation. Exhaustive experiments show our method's state-of-the-art performance: in optical flow estimation, it reduces EPE by 20% and AE by 25% compared to unsupervised approaches, while delivering competitive intensity estimation results, particularly in high dynamic range scenarios. Our method also achieves shorter inference time than all other optical flow methods and many of the image reconstruction methods, while they output only one quantity. Project page: https://github.com/tub-rip/E2FAI",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Guo_Unsupervised_Joint_Learning_of_Optical_Flow_and_Intensity_with_Event_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Unsupervised Joint Learning of Optical Flow and Intensity with Event Cameras [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Shuang Guo , Friedhelm Hamann , Guillermo Gallego Event cameras rely on motion to obtain information about scene appearance. This means that appearance and motion are inherently linked: either both are present and recorded in the event data, or neither is captured. Previous works treat the recovery of these two visual quantities as separate tasks, which does not fit with the above-mentioned nature of event cameras and overlooks the inherent relations between them. We propose an unsupervised learning framework that jointly estimates optical flow (motion) and image intensity (appearance) using a single network. From the data generation model, we newly derive the event-based photometric error as a function of optical flow and image intensity. This error is further combined with the contrast maximization framework to form a comprehensive loss function that provides proper constraints for both flow and intensity estimation. Exhaustive experiments show our method's state-of-the-art performance: in optical flow estimation, it reduces EPE by 20% and AE by 25% compared to unsupervised approaches, while delivering competitive intensity estimation results, particularly in high dynamic range scenarios. Our method also achieves shorter inference time than all other optical flow methods and many of the image reconstruction methods, while they output only one quantity. Project page: https://github.com/tub-rip/E2FAI Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF",
      "index": 54,
      "title": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting",
      "authors": [
        "Aneel Damaraju",
        "Dean Hazineh",
        "Todd Zickler"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "cobl",
        "object",
        "objects",
        "layers",
        "prompting",
        "amodally",
        "layering",
        "tabletops",
        "shot",
        "representation"
      ],
      "summary": "Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of \"object layers,\" each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Damaraju_CObL_Toward_Zero-Shot_Ordinal_Layering_without_User_Prompting_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Aneel Damaraju , Dean Hazineh , Todd Zickler Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of \"object layers,\" each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF",
      "index": 55,
      "title": "Hierarchical Material Recognition from Local Appearance",
      "authors": [
        "Matthew Beveridge",
        "Shree K. Nayar"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "taxonomy",
        "hierarchical",
        "recognition",
        "material",
        "appearance",
        "materials",
        "depth",
        "taxonomic",
        "dataset",
        "maps"
      ],
      "summary": "We introduce a taxonomy of materials for hierarchical recognition from local appearance. Our taxonomy is motivated by vision applications and is arranged according to the physical traits of materials. We contribute a diverse, in-the-wild dataset with images and depth maps of the taxonomy classes. Utilizing the taxonomy and dataset, we present a method for hierarchical material recognition based on graph attention networks. Our model leverages the taxonomic proximity between classes and achieves state-of-the-art performance. We demonstrate the model's potential to generalize to adverse, real-world imaging conditions, and that novel views rendered using the depth maps can enhance this capability. Finally, we show the model's capacity to rapidly learn new materials in a few-shot learning setting.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Beveridge_Hierarchical_Material_Recognition_from_Local_Appearance_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Hierarchical Material Recognition from Local Appearance [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Matthew Beveridge , Shree K. Nayar We introduce a taxonomy of materials for hierarchical recognition from local appearance. Our taxonomy is motivated by vision applications and is arranged according to the physical traits of materials. We contribute a diverse, in-the-wild dataset with images and depth maps of the taxonomy classes. Utilizing the taxonomy and dataset, we present a method for hierarchical material recognition based on graph attention networks. Our model leverages the taxonomic proximity between classes and achieves state-of-the-art performance. We demonstrate the model's potential to generalize to adverse, real-world imaging conditions, and that novel views rendered using the depth maps can enhance this capability. Finally, we show the model's capacity to rapidly learn new materials in a few-shot learning setting. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF",
      "index": 56,
      "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation",
      "authors": [
        "Vladislav Bargatin",
        "Egor Chistov",
        "Alexander Yakovenko",
        "Dmitriy Vatolin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "memfof",
        "memory",
        "frame",
        "estimation",
        "1080p",
        "flow",
        "gpu",
        "resolution",
        "optical",
        "fullhd"
      ],
      "summary": "Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at: https://github.com/msu-video-group/memfof.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Bargatin_MEMFOF_High-Resolution_Training_for_Memory-Efficient_Multi-Frame_Optical_Flow_Estimation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Vladislav Bargatin , Egor Chistov , Alexander Yakovenko , Dmitriy Vatolin Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at: https://github.com/msu-video-group/memfof. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF",
      "index": 57,
      "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness",
      "authors": [
        "Boqian Li",
        "Haiwen Feng",
        "Zeyu Cai",
        "Michael J. Black",
        "Yuliang Xiu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "etch",
        "tightness",
        "clothed",
        "body",
        "fitting",
        "equivariant",
        "cloth",
        "loose",
        "clothing",
        "humans"
      ],
      "summary": "Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% 89.8%) in one-shot (or out-of-distribution) settings ( 1% data). Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at boqian-li.github.io/ETCH.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_ETCH_Generalizing_Body_Fitting_to_Clothed_Humans_via_Equivariant_Tightness_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Boqian Li , Haiwen Feng , Zeyu Cai , Michael J. Black , Yuliang Xiu Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% 89.8%) in one-shot (or out-of-distribution) settings ( 1% data). Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at boqian-li.github.io/ETCH. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF",
      "index": 58,
      "title": "MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting",
      "authors": [
        "Shaojie Ma",
        "Yawei Luo",
        "Wei Yang",
        "Yi Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mesh",
        "mags",
        "adsorbed",
        "net",
        "gaussians",
        "rmd",
        "rgd",
        "splatting",
        "simulation",
        "representation"
      ],
      "summary": "3D reconstruction and simulation, although interrelated, have distinct objectives: reconstruction requires a flexible 3D representation that can adapt to diverse scenes, while simulation needs a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D representation. Such representation harnesses both the rendering flexibility of 3D Gaussians and the structured property of meshes. To achieve this, we introduce RMD-Net, a network that learns motion priors from video data to refine mesh deformations, alongside RGD-Net, which models the relative displacement between the mesh and Gaussians to enhance rendering fidelity under mesh constraints. To generalize to novel, user-defined deformations beyond input video without reliance on temporal data, we propose MPE-Net, which leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to the universality of meshes, MaGS is compatible with various deformation priors such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves state-of-the-art performance in both reconstruction and simulation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ma_MaGS_Reconstructing_and_Simulating_Dynamic_3D_Objects_with_Mesh-adsorbed_Gaussian_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "MaGS: Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Shaojie Ma , Yawei Luo , Wei Yang , Yi Yang 3D reconstruction and simulation, although interrelated, have distinct objectives: reconstruction requires a flexible 3D representation that can adapt to diverse scenes, while simulation needs a structured representation to model motion principles effectively. This paper introduces the Mesh-adsorbed Gaussian Splatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians to roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D representation. Such representation harnesses both the rendering flexibility of 3D Gaussians and the structured property of meshes. To achieve this, we introduce RMD-Net, a network that learns motion priors from video data to refine mesh deformations, alongside RGD-Net, which models the relative displacement between the mesh and Gaussians to enhance rendering fidelity under mesh constraints. To generalize to novel, user-defined deformations beyond input video without reliance on temporal data, we propose MPE-Net, which leverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to the universality of meshes, MaGS is compatible with various deformation priors such as ARAP, SMPL, and soft physics simulation. Extensive experiments on the D-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves state-of-the-art performance in both reconstruction and simulation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF",
      "index": 59,
      "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
      "authors": [
        "Qiusheng Huang",
        "Xiaohui Zhong",
        "Xu Fan",
        "Hao Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "fuxi",
        "rtm",
        "weather",
        "forecasting",
        "radiative",
        "physical",
        "prediction",
        "transfer",
        "3320",
        "guided"
      ],
      "summary": "Similar to conventional video generation, current deep learning-based weather prediction frameworks often lack explicit physical constraints, leading to unphysical outputs that limit their reliability for operational forecasting. Among various physical processes requiring proper representation, radiation plays a fundamental role as it drives Earth's weather and climate systems. However, accurate simulation of radiative transfer processes remains challenging for traditional numerical weather prediction (NWP) models due to their inherent complexity and high computational costs. Here, we propose FuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance weather forecast accuracy while enforcing physical consistency. FuXi-RTM integrates a primary forecasting model (FuXi) with a fixed deep learning-based radiative transfer model (DLRTM) surrogate that efficiently replaces conventional radiation parameterization schemes. This represents the first deep learning-based weather forecasting framework to explicitly incorporate physical process modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM outperforms its unconstrained counterpart in 88.51% of 3320 variable and lead time combinations, with improvements in radiative flux predictions. By incorporating additional physical processes, FuXi-RTM paves the way for next-generation weather forecasting systems that are both accurate and physically consistent.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_FuXi-RTM_A_Physics-Guided_Prediction_Framework_with_Radiative_Transfer_Modeling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Qiusheng Huang , Xiaohui Zhong , Xu Fan , Hao Li Similar to conventional video generation, current deep learning-based weather prediction frameworks often lack explicit physical constraints, leading to unphysical outputs that limit their reliability for operational forecasting. Among various physical processes requiring proper representation, radiation plays a fundamental role as it drives Earth's weather and climate systems. However, accurate simulation of radiative transfer processes remains challenging for traditional numerical weather prediction (NWP) models due to their inherent complexity and high computational costs. Here, we propose FuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance weather forecast accuracy while enforcing physical consistency. FuXi-RTM integrates a primary forecasting model (FuXi) with a fixed deep learning-based radiative transfer model (DLRTM) surrogate that efficiently replaces conventional radiation parameterization schemes. This represents the first deep learning-based weather forecasting framework to explicitly incorporate physical process modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM outperforms its unconstrained counterpart in 88.51% of 3320 variable and lead time combinations, with improvements in radiative flux predictions. By incorporating additional physical processes, FuXi-RTM paves the way for next-generation weather forecasting systems that are both accurate and physically consistent. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF",
      "index": 60,
      "title": "Diorama: Unleashing Zero-shot Single-view 3D Indoor Scene Modeling",
      "authors": [
        "Qirui Wu",
        "Denys Iliash",
        "Daniel Ritchie",
        "Manolis Savva",
        "Angel X. Chang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "diorama",
        "scene",
        "world",
        "unleashing",
        "interactability",
        "shot",
        "scenes",
        "rgb",
        "annotations",
        "indoor"
      ],
      "summary": "Reconstructing structured 3D scenes from RGB images using CAD objects unlocks efficient and compact scene representations that maintain compositionality and interactability. Existing works propose training-heavy methods relying on either expensive yet inaccurate real-world annotations or controllable yet monotonous synthetic data that do not generalize well to unseen objects or domains. We present Diorama, the first zero-shot open-world system that holistically models 3D scenes from single-view RGB observations without requiring end-to-end training or human annotations. We show the feasibility of our approach by decomposing the problem into subtasks and introduce better solutions to each: architecture reconstruction, 3D shape retrieval, object pose estimation, and scene layout optimization. We evaluate our system on both synthetic and real-world data to show we significantly outperform baselines from prior work. We also demonstrate generalization to real-world internet images and the text-to-scene task.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Diorama_Unleashing_Zero-shot_Single-view_3D_Indoor_Scene_Modeling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Diorama: Unleashing Zero-shot Single-view 3D Indoor Scene Modeling [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Qirui Wu , Denys Iliash , Daniel Ritchie , Manolis Savva , Angel X. Chang Reconstructing structured 3D scenes from RGB images using CAD objects unlocks efficient and compact scene representations that maintain compositionality and interactability. Existing works propose training-heavy methods relying on either expensive yet inaccurate real-world annotations or controllable yet monotonous synthetic data that do not generalize well to unseen objects or domains. We present Diorama, the first zero-shot open-world system that holistically models 3D scenes from single-view RGB observations without requiring end-to-end training or human annotations. We show the feasibility of our approach by decomposing the problem into subtasks and introduce better solutions to each: architecture reconstruction, 3D shape retrieval, object pose estimation, and scene layout optimization. We evaluate our system on both synthetic and real-world data to show we significantly outperform baselines from prior work. We also demonstrate generalization to real-world internet images and the text-to-scene task. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF",
      "index": 61,
      "title": "Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures",
      "authors": [
        "Tim Seizinger",
        "Florin-Alexandru Vasluianu",
        "Marcos V. Conde",
        "Zongwei Wu",
        "Radu Timofte"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "bokeh",
        "bokehlicious",
        "rendering",
        "aperture",
        "realbokeh",
        "controllable",
        "realdof",
        "professional",
        "photorealistic",
        "apertures"
      ],
      "summary": "Bokeh rendering methods play a key role in creating the visually appealing, softly blurred backgrounds seen in professional photography. While recent learning-based approaches show promising results, generating realistic Bokeh with controllable strength remains challenging. Existing methods require additional inputs and suffer from unrealistic Bokeh reproduction due to reliance on synthetic data. In this work, we propose Bokehlicious, a highly efficient network that provides intuitive control over Bokeh strength through an Aperture-Aware Attention mechanism, mimicking the physical lens aperture. To further address the lack of high-quality real-world data, we present RealBokeh, a novel dataset featuring 23,000 high-resolution (24-MP) images captured by professional photographers, covering diverse scenes with varied aperture and focal length settings. Evaluations on both our new RealBokeh and established Bokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA methods while significantly reducing computational cost and exhibiting strong zero-shot generalization. Our method and dataset further extend to defocus deblurring, achieving competitive results on the RealDOF benchmark. Our code and data will be public.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Seizinger_Bokehlicious_Photorealistic_Bokeh_Rendering_with_Controllable_Apertures_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Tim Seizinger , Florin-Alexandru Vasluianu , Marcos V. Conde , Zongwei Wu , Radu Timofte Bokeh rendering methods play a key role in creating the visually appealing, softly blurred backgrounds seen in professional photography. While recent learning-based approaches show promising results, generating realistic Bokeh with controllable strength remains challenging. Existing methods require additional inputs and suffer from unrealistic Bokeh reproduction due to reliance on synthetic data. In this work, we propose Bokehlicious, a highly efficient network that provides intuitive control over Bokeh strength through an Aperture-Aware Attention mechanism, mimicking the physical lens aperture. To further address the lack of high-quality real-world data, we present RealBokeh, a novel dataset featuring 23,000 high-resolution (24-MP) images captured by professional photographers, covering diverse scenes with varied aperture and focal length settings. Evaluations on both our new RealBokeh and established Bokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA methods while significantly reducing computational cost and exhibiting strong zero-shot generalization. Our method and dataset further extend to defocus deblurring, achieving competitive results on the RealDOF benchmark. Our code and data will be public. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF",
      "index": 62,
      "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users",
      "authors": [
        "Xiangyu Yin",
        "Boyuan Yang",
        "Weichen Liu",
        "Qiyao Xue",
        "Abrar Alamri",
        "Goeran Fiedler",
        "Wei Gao"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "progait",
        "prosthesis",
        "transfemoral",
        "gait",
        "prosthetic",
        "dataset",
        "amputations",
        "legs",
        "video",
        "tasks"
      ],
      "summary": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. The ProGait dataset is available at https://huggingface.co/datasets/ericyxy98/ProGait, and the source codes of our benchmark tasks are available at https://github.com/pittisl/ProGait.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis_ICCV_2025_paper.html",
          "/venue/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yin_ProGait_A_Multi-Purpose_Video_Dataset_and_Benchmark_for_Transfemoral_Prosthesis_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Xiangyu Yin , Boyuan Yang , Weichen Liu , Qiyao Xue , Abrar Alamri , Goeran Fiedler , Wei Gao Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. The ProGait dataset is available at https://huggingface.co/datasets/ericyxy98/ProGait, and the source codes of our benchmark tasks are available at https://github.com/pittisl/ProGait. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF",
      "index": 63,
      "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions",
      "authors": [
        "Zizhang Li",
        "Hong-Xing Yu",
        "Wei Liu",
        "Yin Yang",
        "Charles Herrmann",
        "Gordon Wetzstein",
        "Jiajun Wu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "wonderplay",
        "video",
        "physics",
        "dynamic",
        "scene",
        "scenes",
        "solver",
        "generator",
        "single",
        "image"
      ],
      "summary": "WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. Our hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elasticity, and rigid bodies -- all using a single image input. Code will be made public.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_WonderPlay_Dynamic_3D_Scene_Generation_from_a_Single_Image_and_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Zizhang Li , Hong-Xing Yu , Wei Liu , Yin Yang , Charles Herrmann , Gordon Wetzstein , Jiajun Wu WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. Our hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elasticity, and rigid bodies -- all using a single image input. Code will be made public. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF",
      "index": 64,
      "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image",
      "authors": [
        "Arindam Dutta",
        "Meng Zheng",
        "Zhongpai Gao",
        "Benjamin Planche",
        "Anwesa Choudhuri",
        "Terrence Chen",
        "Amit K. Roy-Chowdhury",
        "Ziyan Wu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "chrome",
        "clothed",
        "multiview",
        "occlusion",
        "reconstruction",
        "occluded",
        "human",
        "consistency",
        "resilience",
        "monocular"
      ],
      "summary": "Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Dutta_CHROME_Clothed_Human_Reconstruction_with_Occlusion-Resilience_and_Multiview-Consistency_from_a_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Arindam Dutta , Meng Zheng , Zhongpai Gao , Benjamin Planche , Anwesa Choudhuri , Terrence Chen , Amit K. Roy-Chowdhury , Ziyan Wu Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF",
      "index": 65,
      "title": "Combinative Matching for Geometric Shape Assembly",
      "authors": [
        "Nahyuk Lee",
        "Juhong Min",
        "Junhong Lee",
        "Chunghyun Park",
        "Minsu Cho"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "assembly",
        "combinative",
        "matching",
        "shape",
        "interlocking",
        "parts",
        "geometric",
        "identical",
        "shapes",
        "surface"
      ],
      "summary": "This paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. Specifically, we explicitly model two distinct properties of interlocking shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Combinative_Matching_for_Geometric_Shape_Assembly_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lee_Combinative_Matching_for_Geometric_Shape_Assembly@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Combinative_Matching_for_Geometric_Shape_Assembly_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Combinative Matching for Geometric Shape Assembly [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Nahyuk Lee , Juhong Min , Junhong Lee , Chunghyun Park , Minsu Cho This paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. Specifically, we explicitly model two distinct properties of interlocking shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF",
      "index": 66,
      "title": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution",
      "authors": [
        "Gene Chou",
        "Wenqi Xian",
        "Guandao Yang",
        "Mohamed Abdelfattah",
        "Bharath Hariharan",
        "Noah Snavely",
        "Ning Yu",
        "Paul Debevec"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "depth",
        "flashdepth",
        "streaming",
        "video",
        "estimation",
        "resolution",
        "2044x1148",
        "eyeline",
        "across",
        "fps"
      ],
      "summary": "A versatile video depth estimation model should be consistent and accurate across frames, produce high-resolution depth maps, and support real-time streaming. We propose a method, FlashDepth, that satisfies all three requirements, performing depth estimation for a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We validate our approach across multiple unseen datasets against state-of-the-art depth models, and find that our method outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as visual effects editing, and online decision-making, such as robotics. We release all code and model weights at https://github.com/Eyeline-Research/FlashDepth.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.html",
          "/venue/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chou_FlashDepth_Real-time_Streaming_Video_Depth_Estimation_at_2K_Resolution_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Gene Chou , Wenqi Xian , Guandao Yang , Mohamed Abdelfattah , Bharath Hariharan , Noah Snavely , Ning Yu , Paul Debevec A versatile video depth estimation model should be consistent and accurate across frames, produce high-resolution depth maps, and support real-time streaming. We propose a method, FlashDepth, that satisfies all three requirements, performing depth estimation for a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We validate our approach across multiple unseen datasets against state-of-the-art depth models, and find that our method outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as visual effects editing, and online decision-making, such as robotics. We release all code and model weights at https://github.com/Eyeline-Research/FlashDepth. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF",
      "index": 67,
      "title": "Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis",
      "authors": [
        "Kaiyang Ji",
        "Ye Shi",
        "Zichen Jin",
        "Kangyi Chen",
        "Lan Xu",
        "Yuexin Ma",
        "Jingyi Yu",
        "Jingya Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "human",
        "immersive",
        "physically",
        "plausible",
        "interaction",
        "motion",
        "real",
        "humanoid",
        "robot",
        "interhuman"
      ],
      "summary": "Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ji_Towards_Immersive_Human-X_Interaction_A_Real-Time_Framework_for_Physically_Plausible_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Kaiyang Ji , Ye Shi , Zichen Jin , Kangyi Chen , Lan Xu , Yuexin Ma , Jingyi Yu , Jingya Wang Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF",
      "index": 68,
      "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection",
      "authors": [
        "Giacomo D' Amicantonio",
        "Snehashis Majhi",
        "Quan Kong",
        "Lorenzo Garattoni",
        "Gianpiero Francesca",
        "Francois Bremond",
        "Egor Bondarev"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "experts",
        "anomaly",
        "vad",
        "splatting",
        "splatters",
        "events",
        "temporal",
        "gaussian",
        "supervision",
        "moe"
      ],
      "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Amicantonio_Mixture_of_Experts_Guided_by_Gaussian_Splatters_Matters_A_new_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Giacomo D' Amicantonio , Snehashis Majhi , Quan Kong , Lorenzo Garattoni , Gianpiero Francesca , Francois Bremond , Egor Bondarev Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF",
      "index": 69,
      "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
      "authors": [
        "Zeyuan Chen",
        "Hongyi Xu",
        "Guoxian Song",
        "You Xie",
        "Chenxu Zhang",
        "Xin Chen",
        "Chao Wang",
        "Di Chang",
        "Linjie Luo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dancer",
        "dance",
        "music",
        "human",
        "videos",
        "token",
        "musical",
        "transformer",
        "pose",
        "hands"
      ],
      "summary": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. See our project page for more results: https://zeyuan-chen.com/X-Dancer/.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.html",
          "/venue/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_X-Dancer_Expressive_Music_to_Human_Dance_Video_Generation_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "X-Dancer: Expressive Music to Human Dance Video Generation [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Zeyuan Chen , Hongyi Xu , Guoxian Song , You Xie , Chenxu Zhang , Xin Chen , Chao Wang , Di Chang , Linjie Luo We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. See our project page for more results: https://zeyuan-chen.com/X-Dancer/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF",
      "index": 70,
      "title": "F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration",
      "authors": [
        "Lu Liu",
        "Huiyu Duan",
        "Qiang Hu",
        "Liu Yang",
        "Chunlei Cai",
        "Tianxiao Ye",
        "Huayu Liu",
        "Xiaoyun Zhang",
        "Guangtao Zhai"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "faceq",
        "customization",
        "face",
        "restoration",
        "bench",
        "aigfs",
        "quality",
        "eval",
        "491k",
        "evaluation"
      ],
      "summary": "Recent artificial intelligence (AI) generative models have demonstrated remarkable capabilities in image production, and have been widely applied to face image generation, customization, and restoration. However, many AI-generated faces (AIGFs) still suffer from issues such as unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation method for AIGFs. To this end, we introduce **FaceQ**, the first comprehensive AI-generated Face image database with fine-grained Quality annotations aligned with human preferences, which consists of 12K images and 491K ratings across multiple dimensions. Using the FaceQ database, we establish **F-Bench**, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA) methods on FaceQ, and further propose a large multimodal model (LMM) based Face quality Evaluator (**F-Eval**) to accurately assess the multi-dimensional quality of generated faces in a one-for-all manner. Extensive experimental results demonstrate the state-of-the-art performance of our F-Eval.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_F-Bench_Rethinking_Human_Preference_Evaluation_Metrics_for_Benchmarking_Face_Generation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking Face Generation, Customization, and Restoration [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Lu Liu , Huiyu Duan , Qiang Hu , Liu Yang , Chunlei Cai , Tianxiao Ye , Huayu Liu , Xiaoyun Zhang , Guangtao Zhai Recent artificial intelligence (AI) generative models have demonstrated remarkable capabilities in image production, and have been widely applied to face image generation, customization, and restoration. However, many AI-generated faces (AIGFs) still suffer from issues such as unique distortions, unrealistic details, and unexpected identity shifts, underscoring the need for a comprehensive quality evaluation method for AIGFs. To this end, we introduce **FaceQ**, the first comprehensive AI-generated Face image database with fine-grained Quality annotations aligned with human preferences, which consists of 12K images and 491K ratings across multiple dimensions. Using the FaceQ database, we establish **F-Bench**, a benchmark for comparing and evaluating face generation, customization, and restoration models, highlighting strengths and weaknesses across various prompts and evaluation dimensions. Additionally, we assess the performance of existing image quality assessment (IQA) methods on FaceQ, and further propose a large multimodal model (LMM) based Face quality Evaluator (**F-Eval**) to accurately assess the multi-dimensional quality of generated faces in a one-for-all manner. Extensive experimental results demonstrate the state-of-the-art performance of our F-Eval. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF",
      "index": 71,
      "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection",
      "authors": [
        "Taehoon Kim",
        "Jongwook Choi",
        "Yonghyun Jeong",
        "Haeun Noh",
        "Jaejun Yoo",
        "Seungryul Baek",
        "Jongwon Choi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "temporal",
        "pixel",
        "deepfake",
        "frequency",
        "wise",
        "artifacts",
        "video",
        "spatial",
        "inconsistencies",
        "detection"
      ],
      "summary": "We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. The traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect pixel-wise temporal artifacts. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection_ICCV_2025_paper.html",
          "/venue/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Beyond_Spatial_Frequency_Pixel-wise_Temporal_Frequency-based_Deepfake_Video_Detection_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Taehoon Kim , Jongwook Choi , Yonghyun Jeong , Haeun Noh , Jaejun Yoo , Seungryul Baek , Jongwon Choi We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. The traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect pixel-wise temporal artifacts. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF",
      "index": 72,
      "title": "Disentangled Clothed Avatar Generation with Layered Representation",
      "authors": [
        "Weitian Zhang",
        "Yichao Yan",
        "Sijing Wu",
        "Manwen Liao",
        "Xiaokang Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "clothed",
        "avatars",
        "disentangled",
        "avatar",
        "layeravatar",
        "representation",
        "layered",
        "generating",
        "olivia23333",
        "generation"
      ],
      "summary": "Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. While existing methods have made progress in creating animatable digital avatars, generating avatars with disentangled components (e.g., body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, a novel feed-forward diffusion-based method capable of generating high-quality component-disentangled clothed avatars in seconds. We propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation can be effectively learned with current feed-forward generation pipelines, facilitating component disentanglement and enhancing details of generated avatars. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to mitigate the severe occlusion issue of the innermost human body layer. Extensive experiments demonstrate the superior performances of our method in generating highly detailed and disentangled clothed avatars. In addition, we explore its applications in component transfer. The project page is available at https://olivia23333.github.io/LayerAvatar.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Disentangled_Clothed_Avatar_Generation_with_Layered_Representation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Disentangled Clothed Avatar Generation with Layered Representation [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Weitian Zhang , Yichao Yan , Sijing Wu , Manwen Liao , Xiaokang Yang Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. While existing methods have made progress in creating animatable digital avatars, generating avatars with disentangled components (e.g., body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, a novel feed-forward diffusion-based method capable of generating high-quality component-disentangled clothed avatars in seconds. We propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation can be effectively learned with current feed-forward generation pipelines, facilitating component disentanglement and enhancing details of generated avatars. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to mitigate the severe occlusion issue of the innermost human body layer. Extensive experiments demonstrate the superior performances of our method in generating highly detailed and disentangled clothed avatars. In addition, we explore its applications in component transfer. The project page is available at https://olivia23333.github.io/LayerAvatar. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF",
      "index": 73,
      "title": "Riemannian-Geometric Fingerprints of Generative Models",
      "authors": [
        "Hae Jin Song",
        "Laurent Itti"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "fingerprints",
        "generative",
        "riemannian",
        "regurgitative",
        "definition",
        "attribution",
        "models",
        "model",
        "heightening",
        "modalities"
      ],
      "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have sparked interest in the problem of model attribution and their fingerprints. For instance, service providers need reliable methods of authenticating their models to protect their IP, while users and law enforcement seek to verify the source of generated content for accountability and trust. In addition, a growing threat of model collapse is arising, as more model-generated data are being fed back into sources (e.g., YouTube) that are often harvested for training (\"regurgitative training\"), heightening the need to differentiate synthetic from human data. Yet, a gap still exists in understanding generative models' fingerprints, we believe, stemming from the lack of a formal framework that can define, represent, and analyze the fingerprints in a principled way. To address this gap, we take a geometric approach and propose a new definition of artifact and fingerprint of generative models using Riemannian geometry, which allows us to leverage the rich theory of differential geometry. Our new definition generalizes previous work (Song et al, 2024) to non-Euclidean manifolds by learning Riemannian metrics from data and replacing the Euclidean distances and nearest-neighbor search with geodesic distances and kNN-based Riemannian center of mass. We apply our theory to a new gradient-based algorithm for computing the fingerprints in practice. Results show that it is more effective in distinguishing a large array of generative models, spanning across 4 different datasets in 2 different resolutions (64x64, 256x256), 27 model architectures, and 2 modalities (Vision, Vision-Language). Using our proposed definition can significantly improve the performance on model attribution, as well as a generalization to unseen datasets, model types, and modalities, suggesting its efficacy in practice.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Song_Riemannian-Geometric_Fingerprints_of_Generative_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Riemannian-Geometric Fingerprints of Generative Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hae Jin Song , Laurent Itti Recent breakthroughs and rapid integration of generative models (GMs) have sparked interest in the problem of model attribution and their fingerprints. For instance, service providers need reliable methods of authenticating their models to protect their IP, while users and law enforcement seek to verify the source of generated content for accountability and trust. In addition, a growing threat of model collapse is arising, as more model-generated data are being fed back into sources (e.g., YouTube) that are often harvested for training (\"regurgitative training\"), heightening the need to differentiate synthetic from human data. Yet, a gap still exists in understanding generative models' fingerprints, we believe, stemming from the lack of a formal framework that can define, represent, and analyze the fingerprints in a principled way. To address this gap, we take a geometric approach and propose a new definition of artifact and fingerprint of generative models using Riemannian geometry, which allows us to leverage the rich theory of differential geometry. Our new definition generalizes previous work (Song et al, 2024) to non-Euclidean manifolds by learning Riemannian metrics from data and replacing the Euclidean distances and nearest-neighbor search with geodesic distances and kNN-based Riemannian center of mass. We apply our theory to a new gradient-based algorithm for computing the fingerprints in practice. Results show that it is more effective in distinguishing a large array of generative models, spanning across 4 different datasets in 2 different resolutions (64x64, 256x256), 27 model architectures, and 2 modalities (Vision, Vision-Language). Using our proposed definition can significantly improve the performance on model attribution, as well as a generalization to unseen datasets, model types, and modalities, suggesting its efficacy in practice. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF",
      "index": 74,
      "title": "ISP2HRNet: Learning to Reconstruct High Resolution Image from Irregularly Sampled Pixels via Hierarchical Gradient Learning",
      "authors": [
        "Yuanlin Wang",
        "Ruiqin Xiong",
        "Rui Zhao",
        "Jin Wang",
        "Xiaopeng Fan",
        "Tiejun Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "irregularly",
        "isp2hrnet",
        "pixels",
        "sampled",
        "image",
        "regular",
        "grid",
        "irregular",
        "resolution",
        "gradient"
      ],
      "summary": "While image signals are typically defined on a regular 2D grid, there are scenarios where they are only available at irregular positions. In such cases, reconstructing a complete image on regular grid is essential. This paper introduces ISP2HRNet, an end-to-end network designed to reconstruct high resolution image from irregularly sampled pixels that do not fall on a regular grid. To handle the challenges brought by irregular sampling, we propose an architecture to extract gradient structure hierarchically and learn continuous image representation. Specifically, we derive image gradient for each irregularly sampled pixel and further learn higher order gradient structural features according to the geometric and photometric information at the vertices of neighboring triangles. To convert the features from irregular pixels to regular grid, we propose a dual branch content-dependent weight generator to adaptively fuse the information from neighboring irregular pixels. Subsequently, an encoder captures deep structural details on regular grid and forms latent codes. Implicit neural representation parameterized by multi-layer perceptron decodes the latent codes and coordinates to pixel values for generating high resolution image. Experimental results demonstrate that the proposed network effectively solves the problem of high resolution image reconstruction from irregularly sampled pixels and achieves promising results. The source code is available at https://github.com/yuanlinwang/ISP2HRNet.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_ISP2HRNet_Learning_to_Reconstruct_High_Resolution_Image_from_Irregularly_Sampled_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "ISP2HRNet: Learning to Reconstruct High Resolution Image from Irregularly Sampled Pixels via Hierarchical Gradient Learning [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Yuanlin Wang , Ruiqin Xiong , Rui Zhao , Jin Wang , Xiaopeng Fan , Tiejun Huang While image signals are typically defined on a regular 2D grid, there are scenarios where they are only available at irregular positions. In such cases, reconstructing a complete image on regular grid is essential. This paper introduces ISP2HRNet, an end-to-end network designed to reconstruct high resolution image from irregularly sampled pixels that do not fall on a regular grid. To handle the challenges brought by irregular sampling, we propose an architecture to extract gradient structure hierarchically and learn continuous image representation. Specifically, we derive image gradient for each irregularly sampled pixel and further learn higher order gradient structural features according to the geometric and photometric information at the vertices of neighboring triangles. To convert the features from irregular pixels to regular grid, we propose a dual branch content-dependent weight generator to adaptively fuse the information from neighboring irregular pixels. Subsequently, an encoder captures deep structural details on regular grid and forms latent codes. Implicit neural representation parameterized by multi-layer perceptron decodes the latent codes and coordinates to pixel values for generating high resolution image. Experimental results demonstrate that the proposed network effectively solves the problem of high resolution image reconstruction from irregularly sampled pixels and achieves promising results. The source code is available at https://github.com/yuanlinwang/ISP2HRNet. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF",
      "index": 75,
      "title": "Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection",
      "authors": [
        "Anja Deli",
        "Matej Grcic",
        "Sinia egvi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "keypoint",
        "skeleton",
        "seeker",
        "behaviouris",
        "anomaliesin",
        "methodson",
        "performanceon",
        "taskin",
        "applicationssuch",
        "ubnormal"
      ],
      "summary": "Detecting anomalous human behaviouris an important visual taskin safety-critical applicationssuch as healthcare monitoring,workplace safety,or public surveillance.In these contexts,abnormalities are often reflectedwith unusual human poses.Thus, we propose SeeKer,a method for detecting anomaliesin sequences of human skeletons.Our method formulates the skeleton sequence densitythrough autoregressive factorization at the keypoint level.The corresponding conditional distributionsrepresent probable keypoint locations given prior skeletal motion.We formulate the joint distribution of the considered skeletonas causal prediction of conditional Gaussiansacross its constituent keypoints.A skeleton is flagged as anomalous if its keypoint locations surprise our model(i.e. receive a low density).In practice, our anomaly score is a weighted sum of per-keypoint log-conditionals,where the weights account for the confidence of the underlying keypoint detector.Despite its conceptual simplicity,SeeKer surpasses all previous methodson the UBnormal and MSAD-HR datasetswhile delivering competitive performanceon the ShanghaiTech dataset.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.html",
          "/venue/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Delic_Sequential_keypoint_density_estimator_an_overlooked_baseline_of_skeleton-based_video_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Anja Deli , Matej Grcic , Sinia egvi Detecting anomalous human behaviouris an important visual taskin safety-critical applicationssuch as healthcare monitoring,workplace safety,or public surveillance.In these contexts,abnormalities are often reflectedwith unusual human poses.Thus, we propose SeeKer,a method for detecting anomaliesin sequences of human skeletons.Our method formulates the skeleton sequence densitythrough autoregressive factorization at the keypoint level.The corresponding conditional distributionsrepresent probable keypoint locations given prior skeletal motion.We formulate the joint distribution of the considered skeletonas causal prediction of conditional Gaussiansacross its constituent keypoints.A skeleton is flagged as anomalous if its keypoint locations surprise our model(i.e. receive a low density).In practice, our anomaly score is a weighted sum of per-keypoint log-conditionals,where the weights account for the confidence of the underlying keypoint detector.Despite its conceptual simplicity,SeeKer surpasses all previous methodson the UBnormal and MSAD-HR datasetswhile delivering competitive performanceon the ShanghaiTech dataset. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF",
      "index": 76,
      "title": "GameFactory: Creating New Games with Generative Interactive Videos",
      "authors": [
        "Jiwen Yu",
        "Yiran Qin",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Xihui Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gamefactory",
        "game",
        "action",
        "videos",
        "control",
        "generalizable",
        "domain",
        "generative",
        "creating",
        "interactive"
      ],
      "summary": "Generative videos have the potential to revolutionize game development by autonomously creating new content. In this paper, we present GameFactory, a framework for action-controlled scene-generalizable game video generation. We first address the fundamental challenge of action controllability by introducing GF-Minecraft, an action-annotated game video dataset without human bias, and developing an action control module that enables precise control over both keyboard and mouse inputs. We further extend to support autoregressive generation for unlimited-length interactive videos.More importantly, GameFactory tackles the critical challenge of scene-generalizable action control, which most existing methods fail to address. To enable the creation of entirely new and diverse games beyond fixed styles and scenes, we leverage the open-domain generative priors from pre-trained video diffusion models. To bridge the domain gap between open-domain priors and small-scale game datasets, we propose a multi-phase training strategy with a domain adapter that decouples game style learning from action control. This decoupling ensures that action control learning is no longer bound to specific game styles, thereby achieving scene-generalizable action control. Experimental results demonstrate that GameFactory effectively generates open-domain action-controllable game videos, representing a significant step forward in AI-driven game generation.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos_ICCV_2025_paper.html",
          "/venue/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_GameFactory_Creating_New_Games_with_Generative_Interactive_Videos_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "GameFactory: Creating New Games with Generative Interactive Videos [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Jiwen Yu , Yiran Qin , Xintao Wang , Pengfei Wan , Di Zhang , Xihui Liu Generative videos have the potential to revolutionize game development by autonomously creating new content. In this paper, we present GameFactory, a framework for action-controlled scene-generalizable game video generation. We first address the fundamental challenge of action controllability by introducing GF-Minecraft, an action-annotated game video dataset without human bias, and developing an action control module that enables precise control over both keyboard and mouse inputs. We further extend to support autoregressive generation for unlimited-length interactive videos.More importantly, GameFactory tackles the critical challenge of scene-generalizable action control, which most existing methods fail to address. To enable the creation of entirely new and diverse games beyond fixed styles and scenes, we leverage the open-domain generative priors from pre-trained video diffusion models. To bridge the domain gap between open-domain priors and small-scale game datasets, we propose a multi-phase training strategy with a domain adapter that decouples game style learning from action control. This decoupling ensures that action control learning is no longer bound to specific game styles, thereby achieving scene-generalizable action control. Experimental results demonstrate that GameFactory effectively generates open-domain action-controllable game videos, representing a significant step forward in AI-driven game generation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF",
      "index": 77,
      "title": "GENMO: A GENeralist Model for Human MOtion",
      "authors": [
        "Jiefeng Li",
        "Jinkun Cao",
        "Haotian Zhang",
        "Davis Rempe",
        "Jan Kautz",
        "Umar Iqbal",
        "Ye Yuan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "genmo",
        "motion",
        "generalist",
        "generation",
        "estimation",
        "human",
        "motions",
        "diverse",
        "handles",
        "tasks"
      ],
      "summary": "Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_GENMO_A_GENeralist_Model_for_Human_MOtion_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_GENMO_A_GENeralist_Model_for_Human_MOtion@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_GENMO_A_GENeralist_Model_for_Human_MOtion_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "GENMO: A GENeralist Model for Human MOtion [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jiefeng Li , Jinkun Cao , Haotian Zhang , Davis Rempe , Jan Kautz , Umar Iqbal , Ye Yuan Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF",
      "index": 78,
      "title": "Video Individual Counting for Moving Drones",
      "authors": [
        "Yaowu Fan",
        "Jia Wan",
        "Tao Han",
        "Antoni B. Chan",
        "Andy J. Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "crowded",
        "video",
        "drones",
        "maps",
        "scenes",
        "density",
        "moving",
        "inflow",
        "counting",
        "shared"
      ],
      "summary": "Video Individual Counting (VIC) has received increasing attention for its importance in intelligent video surveillance. Existing works are limited in two aspects, i.e., dataset and method. Previous datasets are captured with fixed or rarely moving cameras with relatively sparse individuals, restricting evaluation for a highly varying view and time in crowded scenes. Existing methods rely on localization followed by association or classification, which struggle under dense and dynamic conditions due to inaccurate localization of small targets. To address these issues, we introduce the MovingDroneCrowd Dataset, featuring videos captured by fast-moving drones in crowded scenes under diverse illuminations, shooting heights and angles. We further propose a Shared Density map-guided Network (SDNet) using a Depth-wise Cross-Frame Attention (DCFA) module to directly estimate shared density maps between consecutive frames, from which the inflow and outflow density maps are derived by subtracting the shared density maps from the global density maps. The inflow density maps across frames are summed up to obtain the number of unique pedestrians in a video. Experiments on our datasets and publicly available ones show the superiority of our method over the state of the arts in highly dynamic and complex crowded scenes. Our dataset and codes have been released publicly.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Video_Individual_Counting_for_Moving_Drones_ICCV_2025_paper.html",
          "/venue/Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Video_Individual_Counting_for_Moving_Drones_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Fan_Video_Individual_Counting_for_Moving_Drones@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Video_Individual_Counting_for_Moving_Drones_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Video Individual Counting for Moving Drones [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yaowu Fan , Jia Wan , Tao Han , Antoni B. Chan , Andy J. Ma Video Individual Counting (VIC) has received increasing attention for its importance in intelligent video surveillance. Existing works are limited in two aspects, i.e., dataset and method. Previous datasets are captured with fixed or rarely moving cameras with relatively sparse individuals, restricting evaluation for a highly varying view and time in crowded scenes. Existing methods rely on localization followed by association or classification, which struggle under dense and dynamic conditions due to inaccurate localization of small targets. To address these issues, we introduce the MovingDroneCrowd Dataset, featuring videos captured by fast-moving drones in crowded scenes under diverse illuminations, shooting heights and angles. We further propose a Shared Density map-guided Network (SDNet) using a Depth-wise Cross-Frame Attention (DCFA) module to directly estimate shared density maps between consecutive frames, from which the inflow and outflow density maps are derived by subtracting the shared density maps from the global density maps. The inflow density maps across frames are summed up to obtain the number of unique pedestrians in a video. Experiments on our datasets and publicly available ones show the superiority of our method over the state of the arts in highly dynamic and complex crowded scenes. Our dataset and codes have been released publicly. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF",
      "index": 79,
      "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images",
      "authors": [
        "Boyang Deng",
        "Songyou Peng",
        "Kyle Genova",
        "Gordon Wetzstein",
        "Noah Snavely",
        "Leonidas Guibas",
        "Thomas Funkhouser"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "visual",
        "mllms",
        "mllm",
        "chronicles",
        "multimodal",
        "images",
        "frequent",
        "city",
        "ended",
        "llms"
      ],
      "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (\"trends\") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., \"what are the frequent types of changes in the city?\") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., \"addition of outdoor dining,\", \"overpass was painted blue,\" etc.).",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_Visual_Chronicles_Using_Multimodal_LLMs_to_Analyze_Massive_Collections_of_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Boyang Deng , Songyou Peng , Kyle Genova , Gordon Wetzstein , Noah Snavely , Leonidas Guibas , Thomas Funkhouser We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (\"trends\") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., \"what are the frequent types of changes in the city?\") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., \"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF",
      "index": 80,
      "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding",
      "authors": [
        "Wanpeng Zhang",
        "Yicheng Feng",
        "Hao Luo",
        "Yijiang Li",
        "Zihao Yue",
        "Sipeng Zheng",
        "Zongqing Lu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "multimodal",
        "byte",
        "visual",
        "encoding",
        "language",
        "understanding",
        "tokens",
        "pair",
        "vision",
        "mllms"
      ],
      "summary": "Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Unified_Multimodal_Understanding_via_Byte-Pair_Visual_Encoding_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Unified Multimodal Understanding via Byte-Pair Visual Encoding [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Wanpeng Zhang , Yicheng Feng , Hao Luo , Yijiang Li , Zihao Yue , Sipeng Zheng , Zongqing Lu Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF",
      "index": 81,
      "title": "Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images",
      "authors": [
        "Elena Buglakova",
        "Anwai Archit",
        "Edoardo D'Imprima",
        "Julia Mahamid",
        "Constantin Pape",
        "Anna Kreshuk"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "tiling",
        "artifacts",
        "normalization",
        "offs",
        "kreshuklab",
        "trade",
        "segmentation",
        "microscopy",
        "images",
        "stitched"
      ],
      "summary": "Segmentation of very large images is a common problem in microscopy, medical imaging or remote sensing. The problem is usually addressed by sliding window inference, which can theoretically lead to seamlessly stitched predictions. However, in practice many of the popular pipelines still suffer from tiling artifacts. We investigate the root cause of these issues and show that they stem from the normalization layers within the neural networks. We propose indicators to detect normalization issues and further explore the trade-offs between artifact-free and high-quality predictions, using three diverse microscopy datasets as examples. Finally, we propose to use BatchRenorm as the most suitable normalization strategy, which effectively removes tiling artifacts and enhances transfer performance, thereby improving the reusability of trained networks for new datasets. The code is available at https://github.com/kreshuklab/no_tiling_artifacts.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Buglakova_Tiling_artifacts_and_trade-offs_of_feature_normalization_in_the_segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Elena Buglakova , Anwai Archit , Edoardo D'Imprima , Julia Mahamid , Constantin Pape , Anna Kreshuk Segmentation of very large images is a common problem in microscopy, medical imaging or remote sensing. The problem is usually addressed by sliding window inference, which can theoretically lead to seamlessly stitched predictions. However, in practice many of the popular pipelines still suffer from tiling artifacts. We investigate the root cause of these issues and show that they stem from the normalization layers within the neural networks. We propose indicators to detect normalization issues and further explore the trade-offs between artifact-free and high-quality predictions, using three diverse microscopy datasets as examples. Finally, we propose to use BatchRenorm as the most suitable normalization strategy, which effectively removes tiling artifacts and enhances transfer performance, thereby improving the reusability of trained networks for new datasets. The code is available at https://github.com/kreshuklab/no_tiling_artifacts. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF",
      "index": 82,
      "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
      "authors": [
        "Junyoung Lim",
        "Jaewoo Ahn",
        "Gunhee Kim"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "chart",
        "captions",
        "chartcap",
        "hallucination",
        "extraneous",
        "charts",
        "caption",
        "565k",
        "dense",
        "informative"
      ],
      "summary": "Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lim_ChartCap_Mitigating_Hallucination_of_Dense_Chart_Captioning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "ChartCap: Mitigating Hallucination of Dense Chart Captioning [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Junyoung Lim , Jaewoo Ahn , Gunhee Kim Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF",
      "index": 83,
      "title": "UniPhys: Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control",
      "authors": [
        "Yan Wu",
        "Korrawe Karunratanakul",
        "Zhengyi Luo",
        "Siyu Tang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "uniphys",
        "motion",
        "control",
        "diffusion",
        "character",
        "physics",
        "flexible",
        "physically",
        "plausible",
        "planner"
      ],
      "summary": "Generating natural and physically plausible character motion remains challenging, particularly for long-horizon control with diverse guidance signals. While prior work combines high-level diffusion-based motion planners with low-level physics controllers, these systems suffer from domain gaps that degrade motion quality and require task-specific fine-tuning.To tackle this problem, we introduce UniPhys, a diffusion-based behavior cloning framework that unifies motion planning and control into a single model. UniPhys enables flexible, expressive character motion conditioned on multi-modal inputs such as text, trajectories, and goals. To address accumulated prediction errors over long sequences, UniPhys is trained with the Diffusion Forcing paradigm, learning to denoise noisy motion histories and handle discrepancies introduced by the physics simulator. This design allows UniPhys to robustly generate physically plausible, long-horizon motions. Through guided sampling, UniPhys generalizes to a wide range of control signals, including unseen ones, without requiring task-specific fine-tuning. Experiments show that UniPhys outperforms prior methods in motion naturalness, generalization, and robustness across diverse control tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_UniPhys_Unified_Planner_and_Controller_with_Diffusion_for_Flexible_Physics-Based_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "UniPhys: Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Yan Wu , Korrawe Karunratanakul , Zhengyi Luo , Siyu Tang Generating natural and physically plausible character motion remains challenging, particularly for long-horizon control with diverse guidance signals. While prior work combines high-level diffusion-based motion planners with low-level physics controllers, these systems suffer from domain gaps that degrade motion quality and require task-specific fine-tuning.To tackle this problem, we introduce UniPhys, a diffusion-based behavior cloning framework that unifies motion planning and control into a single model. UniPhys enables flexible, expressive character motion conditioned on multi-modal inputs such as text, trajectories, and goals. To address accumulated prediction errors over long sequences, UniPhys is trained with the Diffusion Forcing paradigm, learning to denoise noisy motion histories and handle discrepancies introduced by the physics simulator. This design allows UniPhys to robustly generate physically plausible, long-horizon motions. Through guided sampling, UniPhys generalizes to a wide range of control signals, including unseen ones, without requiring task-specific fine-tuning. Experiments show that UniPhys outperforms prior methods in motion naturalness, generalization, and robustness across diverse control tasks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF",
      "index": 84,
      "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
      "authors": [
        "Ke Fan",
        "Shunlin Lu",
        "Minyue Dai",
        "Runyi Yu",
        "Lixing Xiao",
        "Zhiyang Dou",
        "Junting Dong",
        "Lizhuang Ma",
        "Jingbo Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "motion",
        "motionmillion",
        "zero",
        "shot",
        "eval",
        "million",
        "generation",
        "human",
        "generalization",
        "sequences"
      ],
      "summary": "Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion--the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Go_to_Zero_Towards_Zero-shot_Motion_Generation_with_Million-scale_Data_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Ke Fan , Shunlin Lu , Minyue Dai , Runyi Yu , Lixing Xiao , Zhiyang Dou , Junting Dong , Lizhuang Ma , Jingbo Wang Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion--the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF",
      "index": 85,
      "title": "DisenQ: Disentangling Q-Former for Activity-Biometrics",
      "authors": [
        "Shehreen Azad",
        "Yogesh Singh Rawat"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "biometrics",
        "disenq",
        "activity",
        "former",
        "cues",
        "disen",
        "additional",
        "identity",
        "motion",
        "misidentifications"
      ],
      "summary": "In this work, we address activity-biometrics, which involves identifying individuals across diverse set of activities. Unlike traditional person identification, this setting introduces additional challenges as identity cues become entangled with motion dynamics and appearance variations, making biometrics feature learning more complex. While additional visual data like pose and/or silhouette help, they often struggle from extraction inaccuracies. To overcome this, we propose a multimodal language-guided framework that replaces reliance on additional visual data with structured textual supervision. At its core, we introduce **DisenQ** (**Disen**tangling **Q**-Former), a unified querying transformer that disentangles biometrics, motion, and non-biometrics features by leveraging structured language guidance. This ensures identity cues remain independent of appearance and motion variations, preventing misidentifications. We evaluate our approach on three activity-based video benchmarks, achieving state-of-the-art performance. Additionally, we demonstrate strong generalization to complex real-world scenario with competitive performance on a traditional video-based identification benchmark, showing the effectiveness of our framework.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Azad_DisenQ_Disentangling_Q-Former_for_Activity-Biometrics_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "DisenQ: Disentangling Q-Former for Activity-Biometrics [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Shehreen Azad , Yogesh Singh Rawat In this work, we address activity-biometrics, which involves identifying individuals across diverse set of activities. Unlike traditional person identification, this setting introduces additional challenges as identity cues become entangled with motion dynamics and appearance variations, making biometrics feature learning more complex. While additional visual data like pose and/or silhouette help, they often struggle from extraction inaccuracies. To overcome this, we propose a multimodal language-guided framework that replaces reliance on additional visual data with structured textual supervision. At its core, we introduce **DisenQ** (**Disen**tangling **Q**-Former), a unified querying transformer that disentangles biometrics, motion, and non-biometrics features by leveraging structured language guidance. This ensures identity cues remain independent of appearance and motion variations, preventing misidentifications. We evaluate our approach on three activity-based video benchmarks, achieving state-of-the-art performance. Additionally, we demonstrate strong generalization to complex real-world scenario with competitive performance on a traditional video-based identification benchmark, showing the effectiveness of our framework. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF",
      "index": 86,
      "title": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh",
      "authors": [
        "Shuangkang Fang",
        "I-Chao Shen",
        "Yufeng Wang",
        "Yi-Hsuan Tsai",
        "Yi Yang",
        "Shuchang Zhou",
        "Wenrui Ding",
        "Takeo Igarashi",
        "Ming-Hsuan Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mesh",
        "meshllm",
        "meshes",
        "serialized",
        "llms",
        "understand",
        "empowering",
        "1500k",
        "progressively",
        "language"
      ],
      "summary": "We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50x larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fang_MeshLLM_Empowering_Large_Language_Models_to_Progressively_Understand_and_Generate_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Shuangkang Fang , I-Chao Shen , Yufeng Wang , Yi-Hsuan Tsai , Yi Yang , Shuchang Zhou , Wenrui Ding , Takeo Igarashi , Ming-Hsuan Yang We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50x larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF",
      "index": 87,
      "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale",
      "authors": [
        "Jiawei He",
        "Danshi Li",
        "Xinqiang Yu",
        "Zekun Qi",
        "Wenyao Zhang",
        "Jiayi Chen",
        "Zhaoxiang Zhang",
        "Zhizheng Zhang",
        "Li Yi",
        "He Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "grasp",
        "dexvlg",
        "dexterous",
        "language",
        "aligned",
        "vision",
        "dexgraspnet",
        "poses",
        "objects",
        "part"
      ],
      "summary": "As large models gain traction, vision-language models are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM with a flow-matching-based pose head producing instruction-aligned grasp poses for tabletop objects. To evaluate DexVLG's performance, we create benchmarks in simulations and conduct real-world experiments. Extensive experiments demonstrate DexVLG's strong zero-shot generalization capabilities, achieving an over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation, as well as successful part-aligned grasps on physical objects in real-world scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/He_DexVLG_Dexterous_Vision-Language-Grasp_Model_at_Scale_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Jiawei He , Danshi Li , Xinqiang Yu , Zekun Qi , Wenyao Zhang , Jiayi Chen , Zhaoxiang Zhang , Zhizheng Zhang , Li Yi , He Wang As large models gain traction, vision-language models are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM with a flow-matching-based pose head producing instruction-aligned grasp poses for tabletop objects. To evaluate DexVLG's performance, we create benchmarks in simulations and conduct real-world experiments. Extensive experiments demonstrate DexVLG's strong zero-shot generalization capabilities, achieving an over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation, as well as successful part-aligned grasps on physical objects in real-world scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF",
      "index": 88,
      "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
      "authors": [
        "Jungbin Cho",
        "Junwan Kim",
        "Jisoo Kim",
        "Minseo Kim",
        "Mingu Kang",
        "Sungeun Hong",
        "Tae-Hyun Oh",
        "Youngjae Yu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "discord",
        "discrete",
        "continuous",
        "rectified",
        "motion",
        "tokens",
        "decoding",
        "smoother",
        "flow",
        "conditioning"
      ],
      "summary": "Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this discord between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Code and checkpoints will be released.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_DisCoRD_Discrete_Tokens_to_Continuous_Motion_via_Rectified_Flow_Decoding_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jungbin Cho , Junwan Kim , Jisoo Kim , Minseo Kim , Mingu Kang , Sungeun Hong , Tae-Hyun Oh , Youngjae Yu Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this discord between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Code and checkpoints will be released. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF",
      "index": 89,
      "title": "AnimalClue: Recognizing Animals by their Traces",
      "authors": [
        "Risa Shinoda",
        "Nakamasa Inoue",
        "Iro Laina",
        "Christian Rupprecht",
        "Hirokatsu Kataoka"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "wildlife",
        "animalclue",
        "feces",
        "species",
        "animal",
        "indirect",
        "recognizing",
        "footprints",
        "traces",
        "identification"
      ],
      "summary": "Wildlife observation plays an important role in biodiversity conservation, necessitating robust methodologies for monitoring wildlife populations and interspecies interactions. Recent advances in computer vision have significantly contributed to automating fundamental wildlife observation tasks, such as animal detection and species identification. However, accurately identifying species from indirect evidence like footprints and feces remains relatively underexplored, despite its importance in contributing to wildlife monitoring. To bridge this gap, we introduce AnimalClue, the first large-scale dataset for species identification from images of indirect evidence. Our dataset consists of 159,605 bounding boxes encompassing five categories of indirect clues: footprints, feces, eggs, bones, and feathers. It covers 968 species, 200 families, and 65 orders. Each image is annotated with species-level labels, bounding boxes or segmentation masks, and fine-grained trait information, including activity patterns and habitat preferences. Unlike existing datasets primarily focused on direct visual features (e.g., animal appearances), AnimalClue presents unique challenges for classification, detection, and instance segmentation tasks due to the need for recognizing more detailed and subtle visual features. In our experiments, we extensively evaluate representative vision models and identify key challenges in animal identification from their traces. Our dataset and code are available at https://dahlian00.github.io/AnimalCluePage/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Shinoda_AnimalClue_Recognizing_Animals_by_their_Traces_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "AnimalClue: Recognizing Animals by their Traces [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Risa Shinoda , Nakamasa Inoue , Iro Laina , Christian Rupprecht , Hirokatsu Kataoka Wildlife observation plays an important role in biodiversity conservation, necessitating robust methodologies for monitoring wildlife populations and interspecies interactions. Recent advances in computer vision have significantly contributed to automating fundamental wildlife observation tasks, such as animal detection and species identification. However, accurately identifying species from indirect evidence like footprints and feces remains relatively underexplored, despite its importance in contributing to wildlife monitoring. To bridge this gap, we introduce AnimalClue, the first large-scale dataset for species identification from images of indirect evidence. Our dataset consists of 159,605 bounding boxes encompassing five categories of indirect clues: footprints, feces, eggs, bones, and feathers. It covers 968 species, 200 families, and 65 orders. Each image is annotated with species-level labels, bounding boxes or segmentation masks, and fine-grained trait information, including activity patterns and habitat preferences. Unlike existing datasets primarily focused on direct visual features (e.g., animal appearances), AnimalClue presents unique challenges for classification, detection, and instance segmentation tasks due to the need for recognizing more detailed and subtle visual features. In our experiments, we extensively evaluate representative vision models and identify key challenges in animal identification from their traces. Our dataset and code are available at https://dahlian00.github.io/AnimalCluePage/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF",
      "index": 90,
      "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
      "authors": [
        "Zhenxiong Tan",
        "Songhua Liu",
        "Xingyi Yang",
        "Qiaochu Xue",
        "Xinchao Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ominicontrol",
        "dit",
        "control",
        "image",
        "transformer",
        "conditioning",
        "architectural",
        "tasks",
        "tokens",
        "aligned"
      ],
      "summary": "We present OminiControl, a novel approach that rethinks how image conditions are integrated into Diffusion Transformer (DiT) architectures. Current image conditioning methods either introduce substantial parameter overhead or handle only specific control tasks effectively, limiting their practical versatility. OminiControl addresses these limitations through three key innovations: (1) a minimal architectural design that leverages the DiT's own VAE encoder and transformer blocks, requiring just 0.1% additional parameters; (2) a unified sequence processing strategy that combines condition tokens with image tokens for flexible token interactions; and (3) a dynamic position encoding mechanism that adapts to both spatially-aligned and non-aligned control tasks. Our extensive experiments show that this streamlined approach not only matches but surpasses the performance of specialized methods across multiple conditioning tasks. To overcome data limitations in subject-driven generation, we also introduce Subjects200K, a large-scale dataset of identity-consistent image pairs synthesized using DiT models themselves. This work demonstrates that effective image control can be achieved without architectural complexity, opening new possibilities for efficient and versatile image generation systems.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tan_OminiControl_Minimal_and_Universal_Control_for_Diffusion_Transformer_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "OminiControl: Minimal and Universal Control for Diffusion Transformer [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Zhenxiong Tan , Songhua Liu , Xingyi Yang , Qiaochu Xue , Xinchao Wang We present OminiControl, a novel approach that rethinks how image conditions are integrated into Diffusion Transformer (DiT) architectures. Current image conditioning methods either introduce substantial parameter overhead or handle only specific control tasks effectively, limiting their practical versatility. OminiControl addresses these limitations through three key innovations: (1) a minimal architectural design that leverages the DiT's own VAE encoder and transformer blocks, requiring just 0.1% additional parameters; (2) a unified sequence processing strategy that combines condition tokens with image tokens for flexible token interactions; and (3) a dynamic position encoding mechanism that adapts to both spatially-aligned and non-aligned control tasks. Our extensive experiments show that this streamlined approach not only matches but surpasses the performance of specialized methods across multiple conditioning tasks. To overcome data limitations in subject-driven generation, we also introduce Subjects200K, a large-scale dataset of identity-consistent image pairs synthesized using DiT models themselves. This work demonstrates that effective image control can be achieved without architectural complexity, opening new possibilities for efficient and versatile image generation systems. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF",
      "index": 91,
      "title": "Straighten Viscous Rectified Flow via Noise Optimization",
      "authors": [
        "Jimin Dai",
        "Jiexi Yan",
        "Jian Yang",
        "Lei Luo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "reflow",
        "straighten",
        "vrfno",
        "rectified",
        "images",
        "step",
        "viscous",
        "couplings",
        "limitations",
        "flow"
      ],
      "summary": "The Reflow operation aims to straighten the inference trajectories of the rectified flow during training by constructing deterministic couplings between noises and images, thereby improving the quality of generated images in single-step or few-step generation. However, we identify critical limitations in Reflow, particularly its inability to rapidly generate high-quality images due to a distribution gap between images in its constructed deterministic couplings and real images. To address these shortcomings, we propose a novel alternative called Straighten Viscous Rectified Flow via Noise Optimization (VRFNO), which is a joint training framework integrating an encoder and a neural velocity field. VRFNO introduces two key innovations: (1) a historical velocity term that enhances trajectory distinction, enabling the model to more accurately predict the velocity of the current trajectory, and (2) the noise optimization through reparameterization to form optimized couplings with real images which are then utilized for training, effectively mitigating errors caused by Reflow's limitations. Comprehensive experiments on synthetic data and real datasets with varying resolutions show that VRFNO significantly mitigates the limitations of Reflow, achieving state-of-the-art performance in both one-step and few-step generation tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Dai_Straighten_Viscous_Rectified_Flow_via_Noise_Optimization_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Straighten Viscous Rectified Flow via Noise Optimization [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Jimin Dai , Jiexi Yan , Jian Yang , Lei Luo The Reflow operation aims to straighten the inference trajectories of the rectified flow during training by constructing deterministic couplings between noises and images, thereby improving the quality of generated images in single-step or few-step generation. However, we identify critical limitations in Reflow, particularly its inability to rapidly generate high-quality images due to a distribution gap between images in its constructed deterministic couplings and real images. To address these shortcomings, we propose a novel alternative called Straighten Viscous Rectified Flow via Noise Optimization (VRFNO), which is a joint training framework integrating an encoder and a neural velocity field. VRFNO introduces two key innovations: (1) a historical velocity term that enhances trajectory distinction, enabling the model to more accurately predict the velocity of the current trajectory, and (2) the noise optimization through reparameterization to form optimized couplings with real images which are then utilized for training, effectively mitigating errors caused by Reflow's limitations. Comprehensive experiments on synthetic data and real datasets with varying resolutions show that VRFNO significantly mitigates the limitations of Reflow, achieving state-of-the-art performance in both one-step and few-step generation tasks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF",
      "index": 92,
      "title": "Scalable Dual Fingerprinting for Hierarchical Attribution of Text-to-Image Models",
      "authors": [
        "Jianwei Fei",
        "Yunshu Dai",
        "Peipeng Yu",
        "Zhe Kong",
        "Jiantao Zhou",
        "Zhihua Xia"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "traceability",
        "consumers",
        "fingerprints",
        "providers",
        "service",
        "fingerprint",
        "fingerprinting",
        "image",
        "level",
        "t2i"
      ],
      "summary": "The commercialization of generative artificial intelligence (GenAI) has led to a multi-level ecosystem involving model developers, service providers, and consumers. Thus, ensuring traceability is crucial, as service providers may violate intellectual property rights (IPR), and consumers may generate harmful content. However, existing methods are limited to single-level attribution scenarios and cannot simultaneously trace across multiple levels. To this end, we introduce a scalable dual fingerprinting method for text-to-image (T2I) models, to achieve traceability of both service providers and consumers. Specifically, we propose 2-headed Fingerprint-Informed Low-Rank Adaptation (FI-LoRA), where each head is controlled by a binary fingerprint and capable of introducing the fingerprints into generated images. In practice, one FI-LoRA head is used by the developer to assign a unique fingerprint to each service provider, while the other is made available to service providers for embedding consumer-specific fingerprints during image generation. Our method does not merely embed two fingerprints within the generated image but instead allows independent control over them at developer level and business level, enabling simultaneous traceability of businesses and consumers. Experiments show that our method applies to various image generation and editing tasks of multiple T2I models, and can achieve over 99.9% extraction accuracy for both fingerprints. Our method also demonstrates good robustness against both image-level attacks and white-box model-level attacks. We hope our work provides a unified solution for developers to implement multi-tiered traceability of their models and hierarchical control over model distribution and content generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fei_Scalable_Dual_Fingerprinting_for_Hierarchical_Attribution_of_Text-to-Image_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Scalable Dual Fingerprinting for Hierarchical Attribution of Text-to-Image Models [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Jianwei Fei , Yunshu Dai , Peipeng Yu , Zhe Kong , Jiantao Zhou , Zhihua Xia The commercialization of generative artificial intelligence (GenAI) has led to a multi-level ecosystem involving model developers, service providers, and consumers. Thus, ensuring traceability is crucial, as service providers may violate intellectual property rights (IPR), and consumers may generate harmful content. However, existing methods are limited to single-level attribution scenarios and cannot simultaneously trace across multiple levels. To this end, we introduce a scalable dual fingerprinting method for text-to-image (T2I) models, to achieve traceability of both service providers and consumers. Specifically, we propose 2-headed Fingerprint-Informed Low-Rank Adaptation (FI-LoRA), where each head is controlled by a binary fingerprint and capable of introducing the fingerprints into generated images. In practice, one FI-LoRA head is used by the developer to assign a unique fingerprint to each service provider, while the other is made available to service providers for embedding consumer-specific fingerprints during image generation. Our method does not merely embed two fingerprints within the generated image but instead allows independent control over them at developer level and business level, enabling simultaneous traceability of businesses and consumers. Experiments show that our method applies to various image generation and editing tasks of multiple T2I models, and can achieve over 99.9% extraction accuracy for both fingerprints. Our method also demonstrates good robustness against both image-level attacks and white-box model-level attacks. We hope our work provides a unified solution for developers to implement multi-tiered traceability of their models and hierarchical control over model distribution and content generation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF",
      "index": 93,
      "title": "SummDiff: Generative Modeling of Video Summarization with Diffusion",
      "authors": [
        "Kwanseok Kim",
        "Jaehoon Hahm",
        "Sumin Kim",
        "Jinhwan Sul",
        "Byunghak Kim",
        "Joonseok Lee"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "summdiff",
        "summaries",
        "video",
        "summarization",
        "subjectivity",
        "task",
        "diffusion",
        "regressed",
        "innate",
        "multiple"
      ],
      "summary": "Video summarization is a task of shortening a video by choosing a subset of frames while preserving its essential moments. Despite the innate subjectivity of the task, previous works have deterministically regressed to an averaged frame score over multiple raters, ignoring the inherent subjectivity of what constitutes a \"good\" summary. We propose a novel problem formulation by framing video summarization as a conditional generation task, allowing a model to learn the distribution of good summaries and to generate multiple plausible summaries that better reflect varying human perspectives. Adopting diffusion models for the first time in video summarization, our proposed method, SummDiff, dynamically adapts to visual contexts and generates multiple candidate summaries conditioned on the input video. Extensive experiments demonstrate that SummDiff not only achieves the state-of-the-art performance on various benchmarks but also produces summaries that closely align with individual annotator preferences. Moreover, we provide a deeper insight with novel metrics from an analysis of the knapsack, which is an important last step of generating summaries but has been overlooked in evaluation.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion_ICCV_2025_paper.html",
          "/venue/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_SummDiff_Generative_Modeling_of_Video_Summarization_with_Diffusion_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "SummDiff: Generative Modeling of Video Summarization with Diffusion [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Kwanseok Kim , Jaehoon Hahm , Sumin Kim , Jinhwan Sul , Byunghak Kim , Joonseok Lee Video summarization is a task of shortening a video by choosing a subset of frames while preserving its essential moments. Despite the innate subjectivity of the task, previous works have deterministically regressed to an averaged frame score over multiple raters, ignoring the inherent subjectivity of what constitutes a \"good\" summary. We propose a novel problem formulation by framing video summarization as a conditional generation task, allowing a model to learn the distribution of good summaries and to generate multiple plausible summaries that better reflect varying human perspectives. Adopting diffusion models for the first time in video summarization, our proposed method, SummDiff, dynamically adapts to visual contexts and generates multiple candidate summaries conditioned on the input video. Extensive experiments demonstrate that SummDiff not only achieves the state-of-the-art performance on various benchmarks but also produces summaries that closely align with individual annotator preferences. Moreover, we provide a deeper insight with novel metrics from an analysis of the knapsack, which is an important last step of generating summaries but has been overlooked in evaluation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF",
      "index": 94,
      "title": "IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models",
      "authors": [
        "Khaled Abud",
        "Sergey Lavrushkin",
        "Alexey Kirillov",
        "Dmitriy Vatolin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "iqa",
        "quality",
        "adapter",
        "images",
        "diffusion",
        "image",
        "generation",
        "conditioning",
        "models",
        "assessment"
      ],
      "summary": "Diffusion-based models have recently revolutionized image generation, achieving unprecedented levels of fidelity. However, consistent generation of high-quality images remains challenging partly due to the lack of conditioning mechanisms for perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation. We show that diffusion models can learn complex qualitative relationships from both IQA models' outputs and internal activations. First, we experiment with gradient-based guidance to optimize image quality directly and show this method has limited generalizability. To address this, we introduce IQA-Adapter, a novel framework that conditions generation on target quality levels by learning the implicit relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter can shift the distribution of generated images towards a higher-quality subdomain, and, inversely, it can be used as a degradation model, generating progressively more distorted images when provided with a lower-quality signal. Under high-quality condition, IQA-Adapter achieves up to a 10% improvement across multiple objective metrics, as confirmed by a user preference study, while preserving generative diversity and content. Furthermore, we extend IQA-Adapter to a reference-based conditioning scenario, utilizing the rich activation space of IQA models to transfer highly specific, content-agnostic qualitative features between images.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Abud_IQA-Adapter_Exploring_Knowledge_Transfer_from_Image_Quality_Assessment_to_Diffusion-based_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "IQA-Adapter: Exploring Knowledge Transfer from Image Quality Assessment to Diffusion-based Generative Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Khaled Abud , Sergey Lavrushkin , Alexey Kirillov , Dmitriy Vatolin Diffusion-based models have recently revolutionized image generation, achieving unprecedented levels of fidelity. However, consistent generation of high-quality images remains challenging partly due to the lack of conditioning mechanisms for perceptual quality. In this work, we propose methods to integrate image quality assessment (IQA) models into diffusion-based generators, enabling quality-aware image generation. We show that diffusion models can learn complex qualitative relationships from both IQA models' outputs and internal activations. First, we experiment with gradient-based guidance to optimize image quality directly and show this method has limited generalizability. To address this, we introduce IQA-Adapter, a novel framework that conditions generation on target quality levels by learning the implicit relationship between images and quality scores. When conditioned on high target quality, IQA-Adapter can shift the distribution of generated images towards a higher-quality subdomain, and, inversely, it can be used as a degradation model, generating progressively more distorted images when provided with a lower-quality signal. Under high-quality condition, IQA-Adapter achieves up to a 10% improvement across multiple objective metrics, as confirmed by a user preference study, while preserving generative diversity and content. Furthermore, we extend IQA-Adapter to a reference-based conditioning scenario, utilizing the rich activation space of IQA models to transfer highly specific, content-agnostic qualitative features between images. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF",
      "index": 95,
      "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
      "authors": [
        "Runze Zhang",
        "Guoguang Du",
        "Xiaochuan Li",
        "Qi Jia",
        "Liang Jin",
        "Lu Liu",
        "Jingjing Wang",
        "Cong Xu",
        "Zhenhua Guo",
        "Yaqian Zhao",
        "Xiaoli Gong",
        "Rengang Li",
        "Baoyu Fan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dropletvideo",
        "camera",
        "spatio",
        "video",
        "temporal",
        "consistency",
        "generation",
        "plot",
        "movements",
        "integral"
      ],
      "summary": "Spatio-temporal consistency is a critical topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a camera-movement description after a prompt without constraining its outcomes. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to model development. Initially, we constructed DropletVideo-10M, which comprises 10 million videos that feature dynamic camera motion and object actions. With an average length of 206 words, the captions offer detailed accounts of camera movements. Following this, we developed the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The work has been open-sourced: https://dropletx.github.io/.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent_ICCV_2025_paper.html",
          "/venue/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_DropletVideo_A_Dataset_and_Approach_to_Explore_Integral_Spatio-Temporal_Consistent_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Runze Zhang , Guoguang Du , Xiaochuan Li , Qi Jia , Liang Jin , Lu Liu , Jingjing Wang , Cong Xu , Zhenhua Guo , Yaqian Zhao , Xiaoli Gong , Rengang Li , Baoyu Fan Spatio-temporal consistency is a critical topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a camera-movement description after a prompt without constraining its outcomes. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to model development. Initially, we constructed DropletVideo-10M, which comprises 10 million videos that feature dynamic camera motion and object actions. With an average length of 206 words, the captions offer detailed accounts of camera movements. Following this, we developed the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The work has been open-sourced: https://dropletx.github.io/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF",
      "index": 96,
      "title": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation",
      "authors": [
        "Jimyeong Kim",
        "Jungwon Park",
        "Yeji Song",
        "Nojun Kwak",
        "Wonjong Rhee"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "editing",
        "reflow",
        "mid",
        "rectified",
        "text",
        "step",
        "reflex",
        "real",
        "image",
        "images"
      ],
      "summary": "Rectified Flow text-to-image models surpass diffusion models in image quality and text alignment, but adapting ReFlow for real-image editing remains challenging. We propose a new real-image editing method for ReFlow by analyzing the intermediate representations of multimodal transformer blocks and identifying three key features. To extract these features from real images with sufficient structural preservation, we leverage mid-step latent, which is inverted only up to the mid-step. We then adapt attention during injection to improve editability and enhance alignment to the target text. Our method is training-free, requires no user-provided mask, and can be applied even without a source prompt. Extensive experiments on two benchmarks with nine baselines demonstrate its superior performance over prior methods, further validated by human evaluations confirming a strong user preference for our approach.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_ReFlex_Text-Guided_Editing_of_Real_Images_in_Rectified_Flow_via_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Jimyeong Kim , Jungwon Park , Yeji Song , Nojun Kwak , Wonjong Rhee Rectified Flow text-to-image models surpass diffusion models in image quality and text alignment, but adapting ReFlow for real-image editing remains challenging. We propose a new real-image editing method for ReFlow by analyzing the intermediate representations of multimodal transformer blocks and identifying three key features. To extract these features from real images with sufficient structural preservation, we leverage mid-step latent, which is inverted only up to the mid-step. We then adapt attention during injection to improve editability and enhance alignment to the target text. Our method is training-free, requires no user-provided mask, and can be applied even without a source prompt. Extensive experiments on two benchmarks with nine baselines demonstrate its superior performance over prior methods, further validated by human evaluations confirming a strong user preference for our approach. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF",
      "index": 97,
      "title": "Magic Insert: Style-Aware Drag-and-Drop",
      "authors": [
        "Nataniel Ruiz",
        "Yuanzhen Li",
        "Neal Wadhwa",
        "Yael Pritch",
        "Michael Rubinstein",
        "David E. Jacobs",
        "Shlomi Fruchter"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "style",
        "drag",
        "aware",
        "drop",
        "insertion",
        "insert",
        "magic",
        "personalization",
        "subjectplop",
        "domain"
      ],
      "summary": "We present Magic Insert, a method to drag-and-drop subjects from a user-provided image into a target image of a different style in a plausible manner while matching the style of the target image. This work formalizes our version of the problem of style-aware drag-and-drop and proposes to tackle it by decomposing it into two sub-problems: style-aware personalization and realistic object insertion in stylized images. For style-aware personalization, we cast our method as a weight-and-text-embedding finetuning method with inference-time module-targeted style injection. For subject insertion, we propose Bootstrapped Domain Adaption (BDA) to adapt a domain-specific photorealistic object insertion model to the domain of diverse artistic styles. Overall, the method significantly outperforms traditional and state-of-the-art approaches that struggle with quality, subject fidelity and harmonious stylization. Finally, we present a new dataset, SubjectPlop, to facilitate evaluation and future progress in this area.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ruiz_Magic_Insert_Style-Aware_Drag-and-Drop_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Magic Insert: Style-Aware Drag-and-Drop [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Nataniel Ruiz , Yuanzhen Li , Neal Wadhwa , Yael Pritch , Michael Rubinstein , David E. Jacobs , Shlomi Fruchter We present Magic Insert, a method to drag-and-drop subjects from a user-provided image into a target image of a different style in a plausible manner while matching the style of the target image. This work formalizes our version of the problem of style-aware drag-and-drop and proposes to tackle it by decomposing it into two sub-problems: style-aware personalization and realistic object insertion in stylized images. For style-aware personalization, we cast our method as a weight-and-text-embedding finetuning method with inference-time module-targeted style injection. For subject insertion, we propose Bootstrapped Domain Adaption (BDA) to adapt a domain-specific photorealistic object insertion model to the domain of diverse artistic styles. Overall, the method significantly outperforms traditional and state-of-the-art approaches that struggle with quality, subject fidelity and harmonious stylization. Finally, we present a new dataset, SubjectPlop, to facilitate evaluation and future progress in this area. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF",
      "index": 98,
      "title": "Outlier-Aware Post-Training Quantization for Image Super-Resolution",
      "authors": [
        "Hailing Wang",
        "Jianglin Lu",
        "Yitian Zhang",
        "Yun Fu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "quantization",
        "ptq",
        "qat",
        "outlier",
        "aware",
        "region",
        "performance",
        "outliers",
        "post",
        "super"
      ],
      "summary": "Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75x speedup.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Outlier-Aware_Post-Training_Quantization_for_Image_Super-Resolution_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Outlier-Aware Post-Training Quantization for Image Super-Resolution [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Hailing Wang , Jianglin Lu , Yitian Zhang , Yun Fu Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75x speedup. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF",
      "index": 99,
      "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle",
      "authors": [
        "Junchao Huang",
        "Xinting Hu",
        "Shaoshuai Shi",
        "Zhuotao Tian",
        "Li Jiang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "edit360",
        "editing",
        "edits",
        "assets",
        "view",
        "anchor",
        "diffusion",
        "multi",
        "modifications",
        "views"
      ],
      "summary": "Recent advances in diffusion models have significantly improved image generation and editing, but extending these capabilities to 3D assets remains challenging, especially for fine-grained edits that require multi-view consistency. Existing methods typically restrict editing to predetermined viewing angles, severely limiting their flexibility and practical applications.We introduce Edit360, a tuning-free framework that extends 2D modifications to multi-view consistent 3D editing. Built upon video diffusion models, Edit360 enables user-specific editing from arbitrary viewpoints while ensuring structural coherence across all views. The framework selects anchor views for 2D modifications and propagates edits across the entire 360-degree range. To achieve this, Edit360 introduces a novel Anchor-View Editing Propagation mechanism, which effectively aligns and merges multi-view information within the latent and attention spaces of diffusion models. The resulting edited multi-view sequences facilitate the reconstruction of high-quality 3D assets, enabling customizable 3D content creation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Edit360_2D_Image_Edits_to_3D_Assets_from_Any_Angle_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Edit360: 2D Image Edits to 3D Assets from Any Angle [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Junchao Huang , Xinting Hu , Shaoshuai Shi , Zhuotao Tian , Li Jiang Recent advances in diffusion models have significantly improved image generation and editing, but extending these capabilities to 3D assets remains challenging, especially for fine-grained edits that require multi-view consistency. Existing methods typically restrict editing to predetermined viewing angles, severely limiting their flexibility and practical applications.We introduce Edit360, a tuning-free framework that extends 2D modifications to multi-view consistent 3D editing. Built upon video diffusion models, Edit360 enables user-specific editing from arbitrary viewpoints while ensuring structural coherence across all views. The framework selects anchor views for 2D modifications and propagates edits across the entire 360-degree range. To achieve this, Edit360 introduces a novel Anchor-View Editing Propagation mechanism, which effectively aligns and merges multi-view information within the latent and attention spaces of diffusion models. The resulting edited multi-view sequences facilitate the reconstruction of high-quality 3D assets, enabling customizable 3D content creation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF",
      "index": 100,
      "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
      "authors": [
        "Yanzuo Lu",
        "Yuxi Ren",
        "Xin Xia",
        "Shanchuan Lin",
        "Xing Wang",
        "Xuefeng Xiao",
        "Andy J. Ma",
        "Xiaohua Xie",
        "Jian-Huang Lai"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "distillation",
        "dmd2",
        "sd3",
        "adversarial",
        "adm",
        "score",
        "step",
        "discriminators",
        "matching",
        "pre"
      ],
      "summary": "Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators.Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications.To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner.In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces.Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage.By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time.Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_Adversarial_Distribution_Matching_for_Diffusion_Distillation_Towards_Efficient_Image_and_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Yanzuo Lu , Yuxi Ren , Xin Xia , Shanchuan Lin , Xing Wang , Xuefeng Xiao , Andy J. Ma , Xiaohua Xie , Jian-Huang Lai Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators.Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications.To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner.In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces.Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage.By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time.Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF",
      "index": 101,
      "title": "From Image to Video: An Empirical Study of Diffusion Representations",
      "authors": [
        "Pedro Vlez",
        "Luisa F. Polana",
        "Yi Yang",
        "Chuhan Zhang",
        "Rishabh Kabra",
        "Anurag Arnab",
        "Mehdi S. M. Sajjadi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "video",
        "image",
        "diffusion",
        "generation",
        "representations",
        "walt",
        "visual",
        "understanding",
        "objectives",
        "uncharted"
      ],
      "summary": "Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis.This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we analyze the performance of latent image and video diffusion representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. For the most informative comparison, we utilize the same model architecture, WALT, across image and video generation objectives. Our results show that video generation pre-training consistently outperforms its image counterpart, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.html",
          "/venue/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Velez_From_Image_to_Video_An_Empirical_Study_of_Diffusion_Representations_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "From Image to Video: An Empirical Study of Diffusion Representations [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Pedro Vlez , Luisa F. Polana , Yi Yang , Chuhan Zhang , Rishabh Kabra , Anurag Arnab , Mehdi S. M. Sajjadi Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis.This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we analyze the performance of latent image and video diffusion representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. For the most informative comparison, we utilize the same model architecture, WALT, across image and video generation objectives. Our results show that video generation pre-training consistently outperforms its image counterpart, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF",
      "index": 102,
      "title": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?",
      "authors": [
        "Ryan Ramos",
        "Vladan Stojni",
        "Giorgos Kordopatis-Zilos",
        "Yuta Nakashima",
        "Giorgos Tolias",
        "Noa Garcia"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "acquisition",
        "visual",
        "encoders",
        "corruptions",
        "traces",
        "semantic",
        "ryan",
        "caesar",
        "ramos",
        "clip"
      ],
      "summary": "Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions. We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ramos_Processing_and_acquisition_traces_in_visual_encoders_What_does_CLIP_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera? [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ryan Ramos , Vladan Stojni , Giorgos Kordopatis-Zilos , Yuta Nakashima , Giorgos Tolias , Noa Garcia Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions. We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF",
      "index": 103,
      "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation",
      "authors": [
        "Yuhan Li",
        "Xianfeng Tan",
        "Wenxiang Shang",
        "Yubo Wu",
        "Jian Wang",
        "Xuanhong Chen",
        "Yi Zhang",
        "Hangcheng Zhu",
        "Bingbing Ni"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ragdiffusion",
        "clothing",
        "faithful",
        "hallucinations",
        "generation",
        "garment",
        "texture",
        "rag",
        "structure",
        "slle"
      ],
      "summary": "Standard clothing asset generation involves restoring forward-facing flat-lay garment images displayed on a clear background by extracting clothing information from diverse real-world contexts, which presents significant challenges due to highly standardized structure sampling distributions and clothing semantic absence in complex scenarios. Existing models have limited spatial perception, often exhibiting structural hallucinations and texture distortion in this high-specification generative task. To address this issue, we propose a novel Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance structure determinacy and mitigate hallucinations by assimilating knowledge from language models and external databases. RAGDiffusion consists of two processes: (1) Retrieval-based structure aggregation, which employs contrastive learning and a Structure Locally Linear Embedding (SLLE) to derive global structure and spatial landmarks, providing both soft and hard guidance to counteract structural ambiguities; and (2) Omni-level faithful garment generation, which introduces a coarse-to-fine texture alignment that ensures fidelity in pattern and detail components within the diffusing. Extensive experiments on challenging real-world datasets demonstrate that RAGDiffusion synthesizes structurally and texture-faithful clothing assets with significant performance improvements, representing a pioneering effort in high-specification faithful generation with RAG to confront intrinsic hallucinations and enhance fidelity.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_RAGDiffusion_Faithful_Cloth_Generation_via_External_Knowledge_Assimilation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yuhan Li , Xianfeng Tan , Wenxiang Shang , Yubo Wu , Jian Wang , Xuanhong Chen , Yi Zhang , Hangcheng Zhu , Bingbing Ni Standard clothing asset generation involves restoring forward-facing flat-lay garment images displayed on a clear background by extracting clothing information from diverse real-world contexts, which presents significant challenges due to highly standardized structure sampling distributions and clothing semantic absence in complex scenarios. Existing models have limited spatial perception, often exhibiting structural hallucinations and texture distortion in this high-specification generative task. To address this issue, we propose a novel Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance structure determinacy and mitigate hallucinations by assimilating knowledge from language models and external databases. RAGDiffusion consists of two processes: (1) Retrieval-based structure aggregation, which employs contrastive learning and a Structure Locally Linear Embedding (SLLE) to derive global structure and spatial landmarks, providing both soft and hard guidance to counteract structural ambiguities; and (2) Omni-level faithful garment generation, which introduces a coarse-to-fine texture alignment that ensures fidelity in pattern and detail components within the diffusing. Extensive experiments on challenging real-world datasets demonstrate that RAGDiffusion synthesizes structurally and texture-faithful clothing assets with significant performance improvements, representing a pioneering effort in high-specification faithful generation with RAG to confront intrinsic hallucinations and enhance fidelity. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF",
      "index": 104,
      "title": "ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning",
      "authors": [
        "Jongseo Lee",
        "Kyungho Bae",
        "Kyle Min",
        "Gyeong-Moon Park",
        "Jinwoo Choi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "episodic",
        "memory",
        "temporally",
        "essential",
        "vcil",
        "incremental",
        "semantic",
        "ucf",
        "video",
        "101"
      ],
      "summary": "In this work, we tackle the problem of video class-incremental learning (VCIL). Many existing VCIL methods mitigate catastrophic forgetting by rehearsal training with a few temporally dense samples stored in episodic memory, which is memory-inefficient. Alternatively, some methods store temporally sparse samples, sacrificing essential temporal information and thereby resulting in inferior performance. To address this trade-off between memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL consists of episodic memory for storing temporally sparse features and semantic memory for storing general knowledge represented by learnable prompts. We introduce a novel memory retrieval (MR) module that integrates episodic memory and semantic prompts through cross-attention, enabling the retrieval of temporally dense features from temporally sparse features. We rigorously validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced memory, ESSENTIAL achieves favorable performance on the benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning_ICCV_2025_paper.html",
          "/venue/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_ESSENTIAL_Episodic_and_Semantic_Memory_Integration_for_Video_Class-Incremental_Learning_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning [PDF ] [Copy] [Kimi ] [REL] Authors : Jongseo Lee , Kyungho Bae , Kyle Min , Gyeong-Moon Park , Jinwoo Choi In this work, we tackle the problem of video class-incremental learning (VCIL). Many existing VCIL methods mitigate catastrophic forgetting by rehearsal training with a few temporally dense samples stored in episodic memory, which is memory-inefficient. Alternatively, some methods store temporally sparse samples, sacrificing essential temporal information and thereby resulting in inferior performance. To address this trade-off between memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL consists of episodic memory for storing temporally sparse features and semantic memory for storing general knowledge represented by learnable prompts. We introduce a novel memory retrieval (MR) module that integrates episodic memory and semantic prompts through cross-attention, enabling the retrieval of temporally dense features from temporally sparse features. We rigorously validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced memory, ESSENTIAL achieves favorable performance on the benchmarks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF",
      "index": 105,
      "title": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation",
      "authors": [
        "Ruoyu Wang",
        "Huayang Huang",
        "Ye Zhu",
        "Olga Russakovsky",
        "Yu Wu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "noisequery",
        "noise",
        "generation",
        "t2i",
        "guidance",
        "implicit",
        "goal",
        "silent",
        "text",
        "assistant"
      ],
      "summary": "In this work, we introduce NoiseQuery as a novel method for enhanced noise initialization in versatile goal-driven text-to-image (T2I) generation. Specifically, we propose to leverage an aligned Gaussian noise as implicit guidance to complement explicit user-defined inputs, such as text prompts, for better generation quality and controllability. Unlike existing noise optimization methods designed for specific models, our approach is grounded in a fundamental examination of the generic finite-step noise scheduler design in diffusion formulation, allowing better generalization across different diffusion-based architectures in a tuning-free manner. This model-agnostic nature allows us to construct a reusable noise library compatible with multiple T2I models and enhancement techniques, serving as a foundational layer for more effective generation. Extensive experiments demonstrate that NoiseQuery enables fine-grained control and yields significant performance boosts not only over high-level semantics but also over low-level visual attributes, which are typically difficult to specify through text alone, with seamless integration into current workflows with minimal computational overhead.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_The_Silent_Assistant_NoiseQuery_as_Implicit_Guidance_for_Goal-Driven_Image_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation [PDF ] [Copy] [Kimi ] [REL] Authors : Ruoyu Wang , Huayang Huang , Ye Zhu , Olga Russakovsky , Yu Wu In this work, we introduce NoiseQuery as a novel method for enhanced noise initialization in versatile goal-driven text-to-image (T2I) generation. Specifically, we propose to leverage an aligned Gaussian noise as implicit guidance to complement explicit user-defined inputs, such as text prompts, for better generation quality and controllability. Unlike existing noise optimization methods designed for specific models, our approach is grounded in a fundamental examination of the generic finite-step noise scheduler design in diffusion formulation, allowing better generalization across different diffusion-based architectures in a tuning-free manner. This model-agnostic nature allows us to construct a reusable noise library compatible with multiple T2I models and enhancement techniques, serving as a foundational layer for more effective generation. Extensive experiments demonstrate that NoiseQuery enables fine-grained control and yields significant performance boosts not only over high-level semantics but also over low-level visual attributes, which are typically difficult to specify through text alone, with seamless integration into current workflows with minimal computational overhead. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF",
      "index": 106,
      "title": "GFPack++: Attention-Driven Gradient Fields for Optimizing 2D Irregular Packing",
      "authors": [
        "Tianyang Xue",
        "Lin Lu",
        "Yang Liu",
        "Mingdong Wu",
        "Hao Dong",
        "Yanbin Zhang",
        "Renmin Han",
        "Baoquan Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gfpack",
        "packing",
        "irregular",
        "gradient",
        "utilization",
        "feasibility",
        "attention",
        "rotation",
        "timhsue",
        "approaches"
      ],
      "summary": "2D irregular packing is a classic combinatorial optimization problem with various applications, such as material utilization and texture atlas generation. Due to its NP-hard nature, conventional numerical approaches typically encounter slow convergence and high computational costs. Previous research GFPack introduced a generative method for gradient-based packing, providing early evidence of its feasibility but faced limitations such as insufficient rotation support, poor boundary adaptability, and high overlap ratios. In this paper, we propose GFPack++, a deeply investigated framework that adopts attention-based geometry and relation encoding, enabling more comprehensive modeling of complex packing relationships. We further design a constrained gradient and a weighting function to enhance both the feasibility of the produced solutions and the learning effectiveness. Experimental results on multiple datasets demonstrate that GFPack++ achieves higher space utilization, supports continuous rotation, generalizes well to arbitrary boundaries, and infers orders of magnitude faster than previous approaches. Codes for this paper are at https://github.com/TimHsue/GFPack-pp.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_GFPack_Attention-Driven_Gradient_Fields_for_Optimizing_2D_Irregular_Packing_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "GFPack++: Attention-Driven Gradient Fields for Optimizing 2D Irregular Packing [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Tianyang Xue , Lin Lu , Yang Liu , Mingdong Wu , Hao Dong , Yanbin Zhang , Renmin Han , Baoquan Chen 2D irregular packing is a classic combinatorial optimization problem with various applications, such as material utilization and texture atlas generation. Due to its NP-hard nature, conventional numerical approaches typically encounter slow convergence and high computational costs. Previous research GFPack introduced a generative method for gradient-based packing, providing early evidence of its feasibility but faced limitations such as insufficient rotation support, poor boundary adaptability, and high overlap ratios. In this paper, we propose GFPack++, a deeply investigated framework that adopts attention-based geometry and relation encoding, enabling more comprehensive modeling of complex packing relationships. We further design a constrained gradient and a weighting function to enhance both the feasibility of the produced solutions and the learning effectiveness. Experimental results on multiple datasets demonstrate that GFPack++ achieves higher space utilization, supports continuous rotation, generalizes well to arbitrary boundaries, and infers orders of magnitude faster than previous approaches. Codes for this paper are at https://github.com/TimHsue/GFPack-pp. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF",
      "index": 107,
      "title": "Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation",
      "authors": [
        "Tuna Han Salih Meral",
        "Enis Simsar",
        "Federico Tombari",
        "Pinar Yanardag"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lora",
        "clora",
        "dog",
        "generation",
        "image",
        "concepts",
        "cat",
        "models",
        "attention",
        "loras"
      ],
      "summary": "Low-Rank Adaptation (LoRA) has emerged as a powerful and popular technique for personalization, enabling efficient adaptation of pre-trained image generation models for specific tasks without comprehensive retraining. While employing individual pre-trained LoRA models excels at representing single concepts, such as those representing a specific dog or a cat, utilizing multiple LoRA models to capture a variety of concepts in a single image still poses a significant challenge. Existing methods often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). We introduce CLoRA, a training-free approach that addresses these limitations by updating the attention maps of multiple LoRA models at test-time, and leveraging the attention maps to create semantic masks for fusing latent representations. This enables the generation of composite images that accurately reflect the characteristics of each LoRA. Our comprehensive qualitative and quantitative evaluations demonstrate that CLoRA significantly outperforms existing methods in multi-concept image generation using LoRAs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Meral_Contrastive_Test-Time_Composition_of_Multiple_LoRA_Models_for_Image_Generation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Tuna Han Salih Meral , Enis Simsar , Federico Tombari , Pinar Yanardag Low-Rank Adaptation (LoRA) has emerged as a powerful and popular technique for personalization, enabling efficient adaptation of pre-trained image generation models for specific tasks without comprehensive retraining. While employing individual pre-trained LoRA models excels at representing single concepts, such as those representing a specific dog or a cat, utilizing multiple LoRA models to capture a variety of concepts in a single image still poses a significant challenge. Existing methods often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). We introduce CLoRA, a training-free approach that addresses these limitations by updating the attention maps of multiple LoRA models at test-time, and leveraging the attention maps to create semantic masks for fusing latent representations. This enables the generation of composite images that accurately reflect the characteristics of each LoRA. Our comprehensive qualitative and quantitative evaluations demonstrate that CLoRA significantly outperforms existing methods in multi-concept image generation using LoRAs. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF",
      "index": 108,
      "title": "CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching",
      "authors": [
        "Zizhuo Li",
        "Yifan Lu",
        "Linfeng Tang",
        "Shihua Zhang",
        "Jiayi Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "covisibility",
        "comatch",
        "subpixel",
        "covisible",
        "tokens",
        "bilateral",
        "level",
        "zizhuoli",
        "dense",
        "matching"
      ],
      "summary": "This prospective study proposes CoMatch, a novel semi-dense image matcher with dynamic covisibility awareness and bilateral subpixel accuracy. Firstly, observing that modeling context interaction over the entire coarse feature map elicits highly redundant computation due to the neighboring representation similarity of tokens, a covisibility-guided token condenser is introduced to adaptively aggregate tokens in light of their covisibility scores that are dynamically estimated, thereby ensuring computational efficiency while improving the representational capacity of aggregated tokens simultaneously. Secondly, considering that feature interaction with non-covisible areas is distracting, which may degrade feature distinctiveness, a covisibility-assisted attention mechanism is deployed to selectively suppress irrelevant message broadcast from non-covisible reduced tokens, resulting in robust and compact attention to relevant rather than all ones. Thirdly, we find that at the fine-level stage, current methods adjust only the target view's keypoints to subpixel level, while those in the source view remain restricted at the coarse level and thus not informative enough, detrimental to keypoint location-sensitive usages. A simple yet potent fine correlation module is developed to refine matching candidates in both source and target views to subpixel level, attaining attractive performance improvement. Thorough experimentation across an array of public benchmarks affirms CoMatch's promising accuracy, efficiency, and generalizability. Code is available at https://github.com/ZizhuoLi/CoMatch.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_CoMatch_Dynamic_Covisibility-Aware_Transformer_for_Bilateral_Subpixel-Level_Semi-Dense_Image_Matching_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zizhuo Li , Yifan Lu , Linfeng Tang , Shihua Zhang , Jiayi Ma This prospective study proposes CoMatch, a novel semi-dense image matcher with dynamic covisibility awareness and bilateral subpixel accuracy. Firstly, observing that modeling context interaction over the entire coarse feature map elicits highly redundant computation due to the neighboring representation similarity of tokens, a covisibility-guided token condenser is introduced to adaptively aggregate tokens in light of their covisibility scores that are dynamically estimated, thereby ensuring computational efficiency while improving the representational capacity of aggregated tokens simultaneously. Secondly, considering that feature interaction with non-covisible areas is distracting, which may degrade feature distinctiveness, a covisibility-assisted attention mechanism is deployed to selectively suppress irrelevant message broadcast from non-covisible reduced tokens, resulting in robust and compact attention to relevant rather than all ones. Thirdly, we find that at the fine-level stage, current methods adjust only the target view's keypoints to subpixel level, while those in the source view remain restricted at the coarse level and thus not informative enough, detrimental to keypoint location-sensitive usages. A simple yet potent fine correlation module is developed to refine matching candidates in both source and target views to subpixel level, attaining attractive performance improvement. Thorough experimentation across an array of public benchmarks affirms CoMatch's promising accuracy, efficiency, and generalizability. Code is available at https://github.com/ZizhuoLi/CoMatch. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF",
      "index": 109,
      "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing",
      "authors": [
        "Aniruddha Bala",
        "Rohit Chowdhury",
        "Rohan Jaiswal",
        "Siddharth Roheda"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "editing",
        "malicious",
        "dct",
        "image",
        "purification",
        "jpeg",
        "adversarial",
        "shield",
        "images",
        "noise"
      ],
      "summary": "Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Bala_DCT-Shield_A_Robust_Frequency_Domain_Defense_against_Malicious_Image_Editing_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Aniruddha Bala , Rohit Chowdhury , Rohan Jaiswal , Siddharth Roheda Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF",
      "index": 110,
      "title": "Bi-Level Optimization for Self-Supervised AI-Generated Face Detection",
      "authors": [
        "Mian Zou",
        "Nan Zhong",
        "Baosheng Yu",
        "Yibing Zhan",
        "Kede Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "face",
        "exif",
        "generated",
        "supervised",
        "photographic",
        "pretext",
        "faces",
        "tags",
        "self",
        "serving"
      ],
      "summary": "AI-generated face detectors trained via supervised learning typically rely on synthesized images from specific generators, limiting their generalization to emerging generative techniques. To overcome this limitation, we introduce a self-supervised method based on bi-level optimization. In the inner loop, we pretrain a vision encoder only on photographic face images using a set of linearly weighted pretext tasks: classification of categorical exchangeable image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of artificial face manipulations. The outer loop then optimizes the relative weights of these pretext tasks to enhance the coarse-grained detection of manipulated faces, serving as a proxy task for identifying AI-generated faces. In doing so, it aligns self-supervised learning more closely with the ultimate goal of AI-generated face detection. Once pretrained, the encoder remains fixed, and AI-generated faces are detected either as anomalies under a Gaussian mixture model fitted to photographic face features or by a lightweight two-layer perceptron serving as a binary classifier. Extensive experiments demonstrate that our detectors significantly outperform existing approaches in both one-class and binary classification settings, exhibiting strong generalization to unseen generators.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zou_Bi-Level_Optimization_for_Self-Supervised_AI-Generated_Face_Detection_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Bi-Level Optimization for Self-Supervised AI-Generated Face Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Mian Zou , Nan Zhong , Baosheng Yu , Yibing Zhan , Kede Ma AI-generated face detectors trained via supervised learning typically rely on synthesized images from specific generators, limiting their generalization to emerging generative techniques. To overcome this limitation, we introduce a self-supervised method based on bi-level optimization. In the inner loop, we pretrain a vision encoder only on photographic face images using a set of linearly weighted pretext tasks: classification of categorical exchangeable image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of artificial face manipulations. The outer loop then optimizes the relative weights of these pretext tasks to enhance the coarse-grained detection of manipulated faces, serving as a proxy task for identifying AI-generated faces. In doing so, it aligns self-supervised learning more closely with the ultimate goal of AI-generated face detection. Once pretrained, the encoder remains fixed, and AI-generated faces are detected either as anomalies under a Gaussian mixture model fitted to photographic face features or by a lightweight two-layer perceptron serving as a binary classifier. Extensive experiments demonstrate that our detectors significantly outperform existing approaches in both one-class and binary classification settings, exhibiting strong generalization to unseen generators. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF",
      "index": 111,
      "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing",
      "authors": [
        "Etai Sella",
        "Noam Atia",
        "Ron Mokady",
        "Hadar Averbuch-Elor"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "shape",
        "editing",
        "shapes",
        "localized",
        "edits",
        "edited",
        "blending",
        "inpainting",
        "innacurate",
        "blended"
      ],
      "summary": "Natural language offers a highly intuitive interface for enabling localized fine-grained edits of 3D shapes. However, prior works face challenges in preserving global coherence while locally modifying the input 3D shape. In this work, we introduce an inpainting-based framework for editing shapes represented as point clouds. Our approach leverages foundation 3D diffusion models for achieving localized shape edits, adding structural guidance in the form of a partial conditional shape, ensuring that other regions correctly preserve the shape's identity. Furthermore, to encourage identity preservation also within the local edited region, we propose an inference-time coordinate blending algorithm which balances reconstruction of the full shape with inpainting at a progression of noise levels during the inference process. Our coordinate blending algorithm seamlessly blends the original shape with its edited version, enabling a fine-grained editing of 3D shapes, all while circumventing the need for computationally expensive and often innacurate inversion. Extensive experiments show that our method outperforms alternative techniques across a wide range of metrics that evaluate both fidelity to the original shape and also adherence to the textual description. We will release our code and trained models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Sella_Blended_Point_Cloud_Diffusion_for_Localized_Text-guided_Shape_Editing_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing [PDF ] [Copy] [Kimi ] [REL] Authors : Etai Sella , Noam Atia , Ron Mokady , Hadar Averbuch-Elor Natural language offers a highly intuitive interface for enabling localized fine-grained edits of 3D shapes. However, prior works face challenges in preserving global coherence while locally modifying the input 3D shape. In this work, we introduce an inpainting-based framework for editing shapes represented as point clouds. Our approach leverages foundation 3D diffusion models for achieving localized shape edits, adding structural guidance in the form of a partial conditional shape, ensuring that other regions correctly preserve the shape's identity. Furthermore, to encourage identity preservation also within the local edited region, we propose an inference-time coordinate blending algorithm which balances reconstruction of the full shape with inpainting at a progression of noise levels during the inference process. Our coordinate blending algorithm seamlessly blends the original shape with its edited version, enabling a fine-grained editing of 3D shapes, all while circumventing the need for computationally expensive and often innacurate inversion. Extensive experiments show that our method outperforms alternative techniques across a wide range of metrics that evaluate both fidelity to the original shape and also adherence to the textual description. We will release our code and trained models. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF",
      "index": 112,
      "title": "DLF: Extreme Image Compression with Dual-generative Latent Fusion",
      "authors": [
        "Naifu Xue",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dlf",
        "generative",
        "branch",
        "latent",
        "compressing",
        "fidelity",
        "clic2020",
        "illm",
        "extreme",
        "compression"
      ],
      "summary": "Recent studies in extreme image compression have achieved remarkable performance by compressing the tokens from generative tokenizers. However, these methods often prioritize clustering common semantics within the dataset, while overlooking the diverse details of individual objects. Consequently, this results in suboptimal reconstruction fidelity, especially at low bitrates. To address this issue, we introduce a Dual-generative Latent Fusion (DLF) paradigm. DLF decomposes the latent into semantic and detail elements, compressing them through two distinct branches. The semantic branch clusters high-level information into compact tokens, while the detail branch encodes perceptually critical details to enhance the overall fidelity. Additionally, we propose a cross-branch interactive design to reduce redundancy between the two branches, thereby minimizing the overall bit cost. Experimental results demonstrate the impressive reconstruction quality of DLF even below 0.01 bits per pixel (bpp). On the CLIC2020 test set, our method achieves bitrate savings of up to 27.93% on LPIPS and 53.55% on DISTS compared to MS-ILLM. Furthermore, DLF surpasses recent diffusion-based codecs in visual fidelity while maintaining a comparable level of generative realism. Project: https://dlfcodec.github.io/",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xue_DLF_Extreme_Image_Compression_with_Dual-generative_Latent_Fusion_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "DLF: Extreme Image Compression with Dual-generative Latent Fusion [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Naifu Xue , Zhaoyang Jia , Jiahao Li , Bin Li , Yuan Zhang , Yan Lu Recent studies in extreme image compression have achieved remarkable performance by compressing the tokens from generative tokenizers. However, these methods often prioritize clustering common semantics within the dataset, while overlooking the diverse details of individual objects. Consequently, this results in suboptimal reconstruction fidelity, especially at low bitrates. To address this issue, we introduce a Dual-generative Latent Fusion (DLF) paradigm. DLF decomposes the latent into semantic and detail elements, compressing them through two distinct branches. The semantic branch clusters high-level information into compact tokens, while the detail branch encodes perceptually critical details to enhance the overall fidelity. Additionally, we propose a cross-branch interactive design to reduce redundancy between the two branches, thereby minimizing the overall bit cost. Experimental results demonstrate the impressive reconstruction quality of DLF even below 0.01 bits per pixel (bpp). On the CLIC2020 test set, our method achieves bitrate savings of up to 27.93% on LPIPS and 53.55% on DISTS compared to MS-ILLM. Furthermore, DLF surpasses recent diffusion-based codecs in visual fidelity while maintaining a comparable level of generative realism. Project: https://dlfcodec.github.io/ Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF",
      "index": 113,
      "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification",
      "authors": [
        "Wenkui Yang",
        "Jie Cao",
        "Junxian Duan",
        "Ran He"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "purification",
        "customization",
        "protective",
        "antipure",
        "guidance",
        "perturbation",
        "imperceptible",
        "diffusion",
        "workflow",
        "resistant"
      ],
      "summary": "Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the \"purification-customization\" workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model's influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model's denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Towards_Robust_Defense_against_Customization_via_Protective_Perturbation_Resistant_to_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 2
      },
      "raw_excerpt": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification [PDF ] [Copy] [Kimi 2 ] [REL] Authors : Wenkui Yang , Jie Cao , Junxian Duan , Ran He Diffusion models like Stable Diffusion have become prominent in visual synthesis tasks due to their powerful customization capabilities, which also introduce significant security risks, including deepfakes and copyright infringement. In response, a class of methods known as protective perturbation emerged, which mitigates image misuse by injecting imperceptible adversarial noise. However, purification can remove protective perturbations, thereby exposing images again to the risk of malicious forgery. In this work, we formalize the anti-purification task, highlighting challenges that hinder existing approaches, and propose a simple diagnostic protective perturbation named AntiPure. AntiPure exposes vulnerabilities of purification within the \"purification-customization\" workflow, owing to two guidance mechanisms: 1) Patch-wise Frequency Guidance, which reduces the model's influence over high-frequency components in the purified image, and 2) Erroneous Timestep Guidance, which disrupts the model's denoising strategy across different timesteps. With additional guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings, achieving effective post-customization distortion. Experiments show that, as a stress test for purification, AntiPure achieves minimal perceptual discrepancy and maximal distortion, outperforming other protective perturbation methods within the purification-customization workflow. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF",
      "index": 114,
      "title": "On the Provable Importance of Gradients for Autonomous Language-Assisted Image Clustering",
      "authors": [
        "Bo Peng",
        "Jie Lu",
        "Guangquan Zhang",
        "Zhen Fang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gradnorm",
        "laic",
        "clustering",
        "nouns",
        "gradients",
        "assisted",
        "strategies",
        "filtering",
        "rigorous",
        "positiveness"
      ],
      "summary": "This paper investigates the recently emerged problem of Language-assisted Image Clustering (LaIC), where textual semantics are leveraged to improve the discriminability of visual representations to facilitate image clustering. Due to the unavailability of true class names, one of core challenges of LaIC lies in how to filter positive nouns, i.e., those semantically close to the images of interest, from unlabeled wild corpus data. Existing filtering strategies are predominantly based on the off-the-shelf feature space learned by CLIP; however, despite being intuitive, these strategies lack a rigorous theoretical foundation. To fill this gap, we propose a novel gradient-based framework, termed as GradNorm, which is theoretically guaranteed and shows strong empirical performance. In particular, we measure the positiveness of each noun based on the magnitude of gradients back-propagated from the cross-entropy between the predicted target distribution and the softmax output. Theoretically, we provide a rigorous error bound to quantify the separability of positive nouns by GradNorm and prove that GradNorm naturally subsumes existing filtering strategies as extremely special cases of itself. Empirically, extensive experiments show that GradNorm achieves the state-of-the-art clustering performance on various benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Peng_On_the_Provable_Importance_of_Gradients_for_Autonomous_Language-Assisted_Image_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "On the Provable Importance of Gradients for Autonomous Language-Assisted Image Clustering [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Bo Peng , Jie Lu , Guangquan Zhang , Zhen Fang This paper investigates the recently emerged problem of Language-assisted Image Clustering (LaIC), where textual semantics are leveraged to improve the discriminability of visual representations to facilitate image clustering. Due to the unavailability of true class names, one of core challenges of LaIC lies in how to filter positive nouns, i.e., those semantically close to the images of interest, from unlabeled wild corpus data. Existing filtering strategies are predominantly based on the off-the-shelf feature space learned by CLIP; however, despite being intuitive, these strategies lack a rigorous theoretical foundation. To fill this gap, we propose a novel gradient-based framework, termed as GradNorm, which is theoretically guaranteed and shows strong empirical performance. In particular, we measure the positiveness of each noun based on the magnitude of gradients back-propagated from the cross-entropy between the predicted target distribution and the softmax output. Theoretically, we provide a rigorous error bound to quantify the separability of positive nouns by GradNorm and prove that GradNorm naturally subsumes existing filtering strategies as extremely special cases of itself. Empirically, extensive experiments show that GradNorm achieves the state-of-the-art clustering performance on various benchmarks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF",
      "index": 115,
      "title": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning",
      "authors": [
        "Kuniaki Saito",
        "Donghyun Kim",
        "Kwanyong Park",
        "Atsushi Hashimoto",
        "Yoshitaka Ushiku"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "captionsmiths",
        "caption",
        "captioning",
        "language",
        "descriptiveness",
        "flexibly",
        "pattern",
        "captions",
        "controlling",
        "smoothly"
      ],
      "summary": "An image captioning model flexibly switching its language pattern, e.g., descriptiveness and length, should be useful since it can be applied to diverse applications. However, despite the dramatic improvement in generative vision-language models, fine-grained control over the properties of generated captions is not easy due to two reasons: (i) existing models are not given the properties as a condition during training and (ii) existing models cannot smoothly transition its language pattern from one state to the other. Given this challenge, we propose a new approach, CaptionSmiths, to acquire a single captioning model that can handle diverse language patterns. First, our approach quantifies three properties of each caption, length, descriptiveness, and uniqueness of a word, as continuous scalar values, without human annotation. Given the values, we represent the conditioning via interpolation between two endpoint vectors corresponding to the extreme states, e.g., one for a very short caption and one for a very long caption. Empirical results demonstrate that the resulting model can smoothly change the properties of the output captions and show higher lexical alignment than baselines. For instance, CaptionSmiths reduces the error in controlling caption length by 506% despite better lexical alignment. Code will be available on https://github.com/omronsinicx/captionsmiths.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Saito_CaptionSmiths_Flexibly_Controlling_Language_Pattern_in_Image_Captioning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning [PDF ] [Copy] [Kimi ] [REL] Authors : Kuniaki Saito , Donghyun Kim , Kwanyong Park , Atsushi Hashimoto , Yoshitaka Ushiku An image captioning model flexibly switching its language pattern, e.g., descriptiveness and length, should be useful since it can be applied to diverse applications. However, despite the dramatic improvement in generative vision-language models, fine-grained control over the properties of generated captions is not easy due to two reasons: (i) existing models are not given the properties as a condition during training and (ii) existing models cannot smoothly transition its language pattern from one state to the other. Given this challenge, we propose a new approach, CaptionSmiths, to acquire a single captioning model that can handle diverse language patterns. First, our approach quantifies three properties of each caption, length, descriptiveness, and uniqueness of a word, as continuous scalar values, without human annotation. Given the values, we represent the conditioning via interpolation between two endpoint vectors corresponding to the extreme states, e.g., one for a very short caption and one for a very long caption. Empirical results demonstrate that the resulting model can smoothly change the properties of the output captions and show higher lexical alignment than baselines. For instance, CaptionSmiths reduces the error in controlling caption length by 506% despite better lexical alignment. Code will be available on https://github.com/omronsinicx/captionsmiths. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF",
      "index": 116,
      "title": "Visual Test-time Scaling for GUI Agent Grounding",
      "authors": [
        "Tiange Luo",
        "Lajanugen Logeswaran",
        "Justin Johnson",
        "Honglak Lee"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "72b",
        "regionfocus",
        "screenspot",
        "grounding",
        "qwen2",
        "gui",
        "visual",
        "test",
        "scaling",
        "pro"
      ],
      "summary": "We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+% on Screenspot-pro and 24+% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS-72B and Qwen2.5-VL-72B, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code is publicly available at https://github.com/tiangeluo/RegionFocus.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Luo_Visual_Test-time_Scaling_for_GUI_Agent_Grounding_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Visual Test-time Scaling for GUI Agent Grounding [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Tiange Luo , Lajanugen Logeswaran , Justin Johnson , Honglak Lee We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+% on Screenspot-pro and 24+% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS-72B and Qwen2.5-VL-72B, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code is publicly available at https://github.com/tiangeluo/RegionFocus. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF",
      "index": 117,
      "title": "GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology",
      "authors": [
        "Saarthak Kapse",
        "Pushpak Pati",
        "Srikar Yellapragada",
        "Srijan Das",
        "Rajarsi R. Gupta",
        "Joel Saltz",
        "Dimitris Samaras",
        "Prateek Prasanna"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gecko",
        "wsi",
        "pretraining",
        "mil",
        "concept",
        "gigapixel",
        "modalities",
        "contrastive",
        "patch",
        "wsis"
      ],
      "summary": "Pretraining a Multiple Instance Learning (MIL) aggregator enables the derivation of Whole Slide Image (WSI)-level embeddings from patch-level representations without supervision. While recent multimodal MIL pretraining approaches leveraging auxiliary modalities have demonstrated performance gains over unimodal WSI pretraining, the acquisition of these additional modalities necessitates extensive clinical profiling. This requirement increases costs and limits scalability in existing WSI datasets lacking such paired modalities. To address this, we propose Gigapixel Vision-Concept Knowledge Contrastive pretraining (GECKO), which aligns WSIs with a Concept Prior derived from the available WSIs. First, we derive an inherently interpretable concept prior by computing the similarity between each WSI patch and textual descriptions of predefined pathology concepts. GECKO then employs a dual-branch MIL network: one branch aggregates patch embeddings into a WSI-level deep embedding, while the other aggregates the concept prior to a corresponding WSI-level concept embedding. Both aggregated embeddings are aligned using a contrastive objective, thereby pretraining the entire dual-branch MIL model. Moreover, when auxiliary modalities such as transcriptomics data are available, GECKO seamlessly integrates them. Across five diverse tasks, GECKO consistently outperforms prior unimodal and multimodal pretraining approaches while also delivering clinically meaningful interpretability that bridges the gap between computational models and pathology expertise. Code is made available at github.com/bmi-imaginelab/GECKO",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kapse_GECKO_Gigapixel_Vision-Concept_Contrastive_Pretraining_in_Histopathology_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "GECKO: Gigapixel Vision-Concept Contrastive Pretraining in Histopathology [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Saarthak Kapse , Pushpak Pati , Srikar Yellapragada , Srijan Das , Rajarsi R. Gupta , Joel Saltz , Dimitris Samaras , Prateek Prasanna Pretraining a Multiple Instance Learning (MIL) aggregator enables the derivation of Whole Slide Image (WSI)-level embeddings from patch-level representations without supervision. While recent multimodal MIL pretraining approaches leveraging auxiliary modalities have demonstrated performance gains over unimodal WSI pretraining, the acquisition of these additional modalities necessitates extensive clinical profiling. This requirement increases costs and limits scalability in existing WSI datasets lacking such paired modalities. To address this, we propose Gigapixel Vision-Concept Knowledge Contrastive pretraining (GECKO), which aligns WSIs with a Concept Prior derived from the available WSIs. First, we derive an inherently interpretable concept prior by computing the similarity between each WSI patch and textual descriptions of predefined pathology concepts. GECKO then employs a dual-branch MIL network: one branch aggregates patch embeddings into a WSI-level deep embedding, while the other aggregates the concept prior to a corresponding WSI-level concept embedding. Both aggregated embeddings are aligned using a contrastive objective, thereby pretraining the entire dual-branch MIL model. Moreover, when auxiliary modalities such as transcriptomics data are available, GECKO seamlessly integrates them. Across five diverse tasks, GECKO consistently outperforms prior unimodal and multimodal pretraining approaches while also delivering clinically meaningful interpretability that bridges the gap between computational models and pathology expertise. Code is made available at github.com/bmi-imaginelab/GECKO Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF",
      "index": 118,
      "title": "Test-time Adaptation for Foundation Medical Segmentation Model Without Parametric Updates",
      "authors": [
        "Kecheng Chen",
        "Xinyu Luo",
        "Tiexin Qin",
        "Jie Liu",
        "Hui Liu",
        "Victor Ho Fun Lee",
        "Hong Yan",
        "Haoliang Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "medsam",
        "segmentation",
        "foundation",
        "updates",
        "adaptation",
        "medical",
        "parametric",
        "lesions",
        "catastrophic",
        "forgetting"
      ],
      "summary": "Foundation medical segmentation models, with MedSAM being the most popular, have achieved promising performance across organs and lesions. However, MedSAM still suffers from compromised performance on specific lesions with intricate structures and appearance, as well as bounding box prompt-induced perturbations. Although current test-time adaptation (TTA) methods for medical image segmentation may tackle this issue, partial (e.g., batch normalization) or whole parametric updates restrict their effectiveness due to limited update signals or catastrophic forgetting in large models. Meanwhile, these approaches ignore the computational complexity during adaptation, which is particularly significant for modern foundation models. To this end, our theoretical analyses reveal that directly refining image embeddings is feasible to approach the same goal as parametric updates under the MedSAM architecture, which enables us to realize high computational efficiency and segmentation performance without the risk of catastrophic forgetting. Under this framework, we propose to encourage maximizing factorized conditional probabilities of the posterior prediction probability using a proposed distribution-approximated latent conditional random field loss combined with an entropy minimization loss. Experiments show that we achieve about 3% Dice score improvements across three datasets while reducing computational complexity by over 7 times.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Test-time_Adaptation_for_Foundation_Medical_Segmentation_Model_Without_Parametric_Updates_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Test-time Adaptation for Foundation Medical Segmentation Model Without Parametric Updates [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Kecheng Chen , Xinyu Luo , Tiexin Qin , Jie Liu , Hui Liu , Victor Ho Fun Lee , Hong Yan , Haoliang Li Foundation medical segmentation models, with MedSAM being the most popular, have achieved promising performance across organs and lesions. However, MedSAM still suffers from compromised performance on specific lesions with intricate structures and appearance, as well as bounding box prompt-induced perturbations. Although current test-time adaptation (TTA) methods for medical image segmentation may tackle this issue, partial (e.g., batch normalization) or whole parametric updates restrict their effectiveness due to limited update signals or catastrophic forgetting in large models. Meanwhile, these approaches ignore the computational complexity during adaptation, which is particularly significant for modern foundation models. To this end, our theoretical analyses reveal that directly refining image embeddings is feasible to approach the same goal as parametric updates under the MedSAM architecture, which enables us to realize high computational efficiency and segmentation performance without the risk of catastrophic forgetting. Under this framework, we propose to encourage maximizing factorized conditional probabilities of the posterior prediction probability using a proposed distribution-approximated latent conditional random field loss combined with an entropy minimization loss. Experiments show that we achieve about 3% Dice score improvements across three datasets while reducing computational complexity by over 7 times. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF",
      "index": 119,
      "title": "Feature Purification Matters: Suppressing Outlier Propagation for Training-Free Open-Vocabulary Semantic Segmentation",
      "authors": [
        "Shuo Jin",
        "Siyue Yu",
        "Bingfeng Zhang",
        "Mingjie Sun",
        "Yi Dong",
        "Jimin Xiao"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "segmentation",
        "outliers",
        "feature",
        "outlier",
        "vocabulary",
        "attention",
        "semantic",
        "sfp",
        "purification",
        "propagated"
      ],
      "summary": "Training-free open-vocabulary semantic segmentation has advanced with vision-language models like CLIP, which exhibit strong zero-shot abilities. However, CLIP's attention mechanism often wrongly emphasises specific image tokens, namely outliers, which results in irrelevant over-activation. Existing approaches struggle with these outliers that arise in intermediate layers and propagate through the model, ultimately degrading spatial perception. In this paper, we propose a Self-adaptive Feature Purifier framework (SFP) to suppress propagated outliers and enhance semantic representations for open-vocabulary semantic segmentation. Specifically, based on an in-depth analysis of attention responses between image and class tokens, we design a self-adaptive outlier mitigator to detect and mitigate outliers at each layer for propagated feature purification. In addition, we introduce a semantic-aware attention enhancer to augment attention intensity in semantically relevant regions, which strengthens the purified feature to focus on objects. Further, we introduce a hierarchical attention integrator to aggregate multi-layer attention maps to refine spatially coherent feature representations for final segmentation. Our proposed SFP enables robust outlier suppression and object-centric feature representation, leading to a more precise segmentation. Extensive experiments show that our method achieves state-of-the-art performance and surpasses existing methods by an average of 4.6% mIoU on eight segmentation benchmarks. The code will be released.",
      "session": null,
      "time": null,
      "links": {
        "supp": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.html",
          "/venue/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Feature Purification Matters: Suppressing Outlier Propagation for Training-Free Open-Vocabulary Semantic Segmentation [PDF ] [Copy] [Kimi ] [REL] Authors : Shuo Jin , Siyue Yu , Bingfeng Zhang , Mingjie Sun , Yi Dong , Jimin Xiao Training-free open-vocabulary semantic segmentation has advanced with vision-language models like CLIP, which exhibit strong zero-shot abilities. However, CLIP's attention mechanism often wrongly emphasises specific image tokens, namely outliers, which results in irrelevant over-activation. Existing approaches struggle with these outliers that arise in intermediate layers and propagate through the model, ultimately degrading spatial perception. In this paper, we propose a Self-adaptive Feature Purifier framework (SFP) to suppress propagated outliers and enhance semantic representations for open-vocabulary semantic segmentation. Specifically, based on an in-depth analysis of attention responses between image and class tokens, we design a self-adaptive outlier mitigator to detect and mitigate outliers at each layer for propagated feature purification. In addition, we introduce a semantic-aware attention enhancer to augment attention intensity in semantically relevant regions, which strengthens the purified feature to focus on objects. Further, we introduce a hierarchical attention integrator to aggregate multi-layer attention maps to refine spatially coherent feature representations for final segmentation. Our proposed SFP enables robust outlier suppression and object-centric feature representation, leading to a more precise segmentation. Extensive experiments show that our method achieves state-of-the-art performance and surpasses existing methods by an average of 4.6% mIoU on eight segmentation benchmarks. The code will be released. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF",
      "index": 120,
      "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search",
      "authors": [
        "Giyeol Kim",
        "Sooyoung Yang",
        "Jihyong Oh",
        "Myungjoo Kang",
        "Chanho Eom"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "person",
        "diffusion",
        "diffps",
        "search",
        "suboptimal",
        "sfan",
        "uncropped",
        "prw",
        "prior",
        "knowledge"
      ],
      "summary": "Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Leveraging_Prior_Knowledge_of_Diffusion_Model_for_Person_Search_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Leveraging Prior Knowledge of Diffusion Model for Person Search [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Giyeol Kim , Sooyoung Yang , Jihyong Oh , Myungjoo Kang , Chanho Eom Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF",
      "index": 121,
      "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
      "authors": [
        "Zeren Jiang",
        "Chuanxia Zheng",
        "Iro Laina",
        "Diane Larlus",
        "Andrea Vedaldi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "geo4d",
        "video",
        "reconstruction",
        "leveraging",
        "modalities",
        "geometric",
        "repurpose",
        "generators",
        "dynamic",
        "trained"
      ],
      "summary": "We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic priors captured by large-scale pre-trained video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, disparity, and ray maps. We propose a new multi-modal alignment algorithm to align and fuse these modalities, as well as a sliding window approach at inference time, thus enabling robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction_ICCV_2025_paper.html",
          "/venue/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Geo4D_Leveraging_Video_Generators_for_Geometric_4D_Scene_Reconstruction_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Zeren Jiang , Chuanxia Zheng , Iro Laina , Diane Larlus , Andrea Vedaldi We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic priors captured by large-scale pre-trained video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, disparity, and ray maps. We propose a new multi-modal alignment algorithm to align and fuse these modalities, as well as a sliding window approach at inference time, thus enabling robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF",
      "index": 122,
      "title": "RhythmGuassian: Repurposing Generalizable Gaussian Model For Remote Physiological Measurement",
      "authors": [
        "Hao Lu",
        "Yuting Zhang",
        "Jiaqi Tang",
        "Bowen Fu",
        "Wenhang Ge",
        "Wei Wei",
        "Kaishun Wu",
        "Yingcong Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "rppg",
        "chroma",
        "gaussian",
        "rhythmguassian",
        "physiological",
        "ggm",
        "motion",
        "generalizable",
        "remote",
        "signals"
      ],
      "summary": "Remote Photoplethysmography (rPPG) enables non-contact extraction of physiological signals, providing significant advantages in medical monitoring, emotion recognition, and face anti-spoofing. However, the extraction of reliable rPPG signals is hindered by motion variations in real-world environments, leading to entanglement issue. To address the challenge, we employ the Generalizable Gaussian Model (GGM) to disentangle geometry and chroma components with 4D Gaussian representations. Employing the GGM for robust rPPG estimation is non-trivial. Firstly, there are no camera parameters in the dataset, resulting in the inability to render video from 4D Gaussian. The \"4D virtual camera\" is proposed to construct extra Gaussian parameters to describe view and motion changes, giving the ability to render video with the fixed virtual camera parameters. Further, the chroma component is still not explicitly decoupled in 4D Gaussian representation. Explicit motion modeling (EMM) is designed to decouple the motion variation in an unsupervised manner. Explicit chroma modeling (ECM) is tailored to decouple specular, physiological, and noise signals, respectively. To validate our approach, we expand existing rPPG datasets to include various motion and illumination interference scenarios, demonstrating the effectiveness of our method in real-world settings. The code is available at https://github.com/LuPaoPao/RhythmGuassian.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lu_RhythmGuassian_Repurposing_Generalizable_Gaussian_Model_For_Remote_Physiological_Measurement_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "RhythmGuassian: Repurposing Generalizable Gaussian Model For Remote Physiological Measurement [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hao Lu , Yuting Zhang , Jiaqi Tang , Bowen Fu , Wenhang Ge , Wei Wei , Kaishun Wu , Yingcong Chen Remote Photoplethysmography (rPPG) enables non-contact extraction of physiological signals, providing significant advantages in medical monitoring, emotion recognition, and face anti-spoofing. However, the extraction of reliable rPPG signals is hindered by motion variations in real-world environments, leading to entanglement issue. To address the challenge, we employ the Generalizable Gaussian Model (GGM) to disentangle geometry and chroma components with 4D Gaussian representations. Employing the GGM for robust rPPG estimation is non-trivial. Firstly, there are no camera parameters in the dataset, resulting in the inability to render video from 4D Gaussian. The \"4D virtual camera\" is proposed to construct extra Gaussian parameters to describe view and motion changes, giving the ability to render video with the fixed virtual camera parameters. Further, the chroma component is still not explicitly decoupled in 4D Gaussian representation. Explicit motion modeling (EMM) is designed to decouple the motion variation in an unsupervised manner. Explicit chroma modeling (ECM) is tailored to decouple specular, physiological, and noise signals, respectively. To validate our approach, we expand existing rPPG datasets to include various motion and illumination interference scenarios, demonstrating the effectiveness of our method in real-world settings. The code is available at https://github.com/LuPaoPao/RhythmGuassian. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF",
      "index": 123,
      "title": "On the Recovery of Cameras from Fundamental Matrices",
      "authors": [
        "Rakshith Madhavan",
        "Federica Arrigoni"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "cameras",
        "matrices",
        "fundamental",
        "viewing",
        "recovery",
        "represent",
        "noisy",
        "uncalibrated",
        "graph",
        "available"
      ],
      "summary": "The viewing graph is a compact tool to encode the geometry of multiple views: nodes represent uncalibrated cameras and edges represent fundamental matrices (when available). Most research focuses on theoretical analyses, exploring for which viewing graphs it is possible (in principle) to retrieve cameras from fundamental matrices, in the sense that the problem admits a unique solution for noiseless data. However, the practical task of recovering cameras from noisy fundamental matrices is still open, as available methods are limited to special graphs (such as those covered by triplets). In this paper, we develop the first method that can deal with the recovery of cameras from noisy fundamental matrices in a general viewing graph. Experimental results demonstrate the promise of the proposed approach on a variety of synthetic and real scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Madhavan_On_the_Recovery_of_Cameras_from_Fundamental_Matrices_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "On the Recovery of Cameras from Fundamental Matrices [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Rakshith Madhavan , Federica Arrigoni The viewing graph is a compact tool to encode the geometry of multiple views: nodes represent uncalibrated cameras and edges represent fundamental matrices (when available). Most research focuses on theoretical analyses, exploring for which viewing graphs it is possible (in principle) to retrieve cameras from fundamental matrices, in the sense that the problem admits a unique solution for noiseless data. However, the practical task of recovering cameras from noisy fundamental matrices is still open, as available methods are limited to special graphs (such as those covered by triplets). In this paper, we develop the first method that can deal with the recovery of cameras from noisy fundamental matrices in a general viewing graph. Experimental results demonstrate the promise of the proposed approach on a variety of synthetic and real scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF",
      "index": 124,
      "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
      "authors": [
        "Haiwen Diao",
        "Xiaotong Li",
        "Yufeng Cui",
        "Yueze Wang",
        "Haoge Deng",
        "Ting Pan",
        "Wenxuan Wang",
        "Huchuan Lu",
        "Xinlong Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "vlms",
        "encoder",
        "evev2",
        "vision",
        "free",
        "language",
        "baaivision",
        "modalities",
        "excavating",
        "improved"
      ],
      "summary": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Diao_EVEv2_Improved_Baselines_for_Encoder-Free_Vision-Language_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haiwen Diao , Xiaotong Li , Yufeng Cui , Yueze Wang , Haoge Deng , Ting Pan , Wenxuan Wang , Huchuan Lu , Xinlong Wang Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF",
      "index": 125,
      "title": "Few-Shot Pattern Detection via Template Matching and Regression",
      "authors": [
        "Eunchan Jo",
        "Dahyun Kang",
        "Sanghyun Kim",
        "Yunseon Choi",
        "Minsu Cho"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "fscd",
        "exemplars",
        "template",
        "rpine",
        "shot",
        "pattern",
        "matching",
        "regression",
        "object",
        "dubbed"
      ],
      "summary": "We address the problem of few-shot pattern detection, which aims to detect all instances of a given pattern, typically represented by a few exemplars, from an input image. Although similar problems have been studied in few-shot object counting and detection (FSCD), previous methods and their benchmarks have narrowed patterns of interest to object categories and often fail to localize non-object patterns. In this work, we propose a simple yet effective detector based on template matching and regression, dubbed TMR. While previous FSCD methods typically represent target exemplars as spatially collapsed prototypes and lose structural information, we revisit classic template matching and regression. It effectively preserves and leverages the spatial layout of exemplars through a minimalistic structure with a small number of learnable convolutional or projection layers on top of a frozen backbone. We also introduce a new dataset, dubbed RPINE, which covers a wider range of patterns than existing object-centric datasets. Our method outperforms the state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jo_Few-Shot_Pattern_Detection_via_Template_Matching_and_Regression_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Few-Shot Pattern Detection via Template Matching and Regression [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Eunchan Jo , Dahyun Kang , Sanghyun Kim , Yunseon Choi , Minsu Cho We address the problem of few-shot pattern detection, which aims to detect all instances of a given pattern, typically represented by a few exemplars, from an input image. Although similar problems have been studied in few-shot object counting and detection (FSCD), previous methods and their benchmarks have narrowed patterns of interest to object categories and often fail to localize non-object patterns. In this work, we propose a simple yet effective detector based on template matching and regression, dubbed TMR. While previous FSCD methods typically represent target exemplars as spatially collapsed prototypes and lose structural information, we revisit classic template matching and regression. It effectively preserves and leverages the spatial layout of exemplars through a minimalistic structure with a small number of learnable convolutional or projection layers on top of a frozen backbone. We also introduce a new dataset, dubbed RPINE, which covers a wider range of patterns than existing object-centric datasets. Our method outperforms the state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF",
      "index": 126,
      "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling",
      "authors": [
        "Trong Thang Pham",
        "Akash Awasthi",
        "Saba Khan",
        "Esteban Duran Marti",
        "Tien-Phat Nguyen",
        "Khoa Vo",
        "Minh Tran",
        "Son Nguyen",
        "Cuong Tran",
        "Yuki Ikebe",
        "Anh Totti Nguyen",
        "Anh Nguyen",
        "Zhigang Deng",
        "Carol C. Wu",
        "Hien Nguyen",
        "Ngan Le"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "scanpath",
        "scangaze",
        "gaze",
        "eye",
        "searcher",
        "volumes",
        "volumetric",
        "dataset",
        "publicly",
        "radiologist"
      ],
      "summary": "Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. However, CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes. To address these challenges, we present the first publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences, overcoming the limitations of current scanpath predictors that only handle 2D inputs. Since deep learning models benefit from a pretraining step, we develop a pipeline that converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher. Through both qualitative and quantitative evaluations on CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a comprehensive assessment framework for 3D scanpath prediction in medical imaging.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling [PDF ] [Copy] [Kimi ] [REL] Authors : Trong Thang Pham , Akash Awasthi , Saba Khan , Esteban Duran Marti , Tien-Phat Nguyen , Khoa Vo , Minh Tran , Son Nguyen , Cuong Tran , Yuki Ikebe , Anh Totti Nguyen , Anh Nguyen , Zhigang Deng , Carol C. Wu , Hien Nguyen , Ngan Le Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. However, CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes. To address these challenges, we present the first publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences, overcoming the limitations of current scanpath predictors that only handle 2D inputs. Since deep learning models benefit from a pretraining step, we develop a pipeline that converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher. Through both qualitative and quantitative evaluations on CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a comprehensive assessment framework for 3D scanpath prediction in medical imaging. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF",
      "index": 127,
      "title": "WeaveSeg: Iterative Contrast-weaving and Spectral Feature-refining for Nuclei Instance Segmentation",
      "authors": [
        "Jiajia Li",
        "Huisi Wu",
        "Jing Qin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "weaveseg",
        "weaving",
        "nuclei",
        "adaptive",
        "contrast",
        "feature",
        "iterative",
        "segmentation",
        "spectral",
        "ambiguous"
      ],
      "summary": "histopathology images is a fundamental task in computational pathology. It is also a very challenging task due to complex nuclei morphologies, ambiguous boundaries, and staining variations. Existing methods often struggle to precisely delineate overlapping nuclei and handle class imbalance. We introduce WeaveSeg, a novel deep learning model for nuclei instance segmentation that significantly improves segmentation performance via synergistic integration of adaptive spectral feature refinement and iterative contrast-weaving. WeaveSeg features an adaptive spectral detail refinement (SAR) module for multi-scale feature enhancement via adaptive frequency component fusion, and an iterative contrast-weaving (ICW) module that progressively refines features through integrating contrastive attention, decoupled semantic context, and adaptive gating. Furthermore, we introduce a specialized uncertainty loss to explicitly model ambiguous regions, and a novel local contrast-based self-adaptive adjustment mechanism to accommodate dynamic feature distributions. Extensive experiments on MoNuSeg and CoNSeP demonstrate WeaveSeg's SOTA performance over existing models. Code will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_WeaveSeg_Iterative_Contrast-weaving_and_Spectral_Feature-refining_for_Nuclei_Instance_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "WeaveSeg: Iterative Contrast-weaving and Spectral Feature-refining for Nuclei Instance Segmentation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jiajia Li , Huisi Wu , Jing Qin histopathology images is a fundamental task in computational pathology. It is also a very challenging task due to complex nuclei morphologies, ambiguous boundaries, and staining variations. Existing methods often struggle to precisely delineate overlapping nuclei and handle class imbalance. We introduce WeaveSeg, a novel deep learning model for nuclei instance segmentation that significantly improves segmentation performance via synergistic integration of adaptive spectral feature refinement and iterative contrast-weaving. WeaveSeg features an adaptive spectral detail refinement (SAR) module for multi-scale feature enhancement via adaptive frequency component fusion, and an iterative contrast-weaving (ICW) module that progressively refines features through integrating contrastive attention, decoupled semantic context, and adaptive gating. Furthermore, we introduce a specialized uncertainty loss to explicitly model ambiguous regions, and a novel local contrast-based self-adaptive adjustment mechanism to accommodate dynamic feature distributions. Extensive experiments on MoNuSeg and CoNSeP demonstrate WeaveSeg's SOTA performance over existing models. Code will be publicly available. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF",
      "index": 128,
      "title": "Modeling Saliency Dataset Bias",
      "authors": [
        "Matthias Kmmerer",
        "Harneet Singh Khanuja",
        "Matthias Bethge"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "saliency",
        "dataset",
        "bias",
        "datasets",
        "freeview",
        "mit300",
        "cat2000",
        "gap",
        "adapting",
        "tuebingen"
      ],
      "summary": "Recent advances in image-based saliency prediction are approaching gold standard performance levels on existing benchmarks. Despite this success, we show that predicting fixations across multiple saliency datasets remains challenging due to dataset bias. We find a significant performance drop (around 40%) when models trained on one dataset are applied to another. Surprisingly, increasing dataset diversity does not resolve this inter-dataset gap, with close to 60% attributed to dataset-specific biases. To address this remaining generalization gap, we propose a novel architecture extending a mostly dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters that govern interpretable mechanisms such as multi-scale structure, center bias, and fixation spread. Adapting only these parameters to new data accounts for more than 75% of the generalization gap, with a large fraction of the improvement achieved with as few as 50 samples. Our model sets a new state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark (MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from unrelated datasets, but with a substantial boost when adapting to the respective training datasets. The model also provides valuable insights into spatial saliency properties, revealing complex multi-scale effects that combine both absolute and relative sizes.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kummerer_Modeling_Saliency_Dataset_Bias_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kummerer_Modeling_Saliency_Dataset_Bias@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kummerer_Modeling_Saliency_Dataset_Bias_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Modeling Saliency Dataset Bias [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Matthias Kmmerer , Harneet Singh Khanuja , Matthias Bethge Recent advances in image-based saliency prediction are approaching gold standard performance levels on existing benchmarks. Despite this success, we show that predicting fixations across multiple saliency datasets remains challenging due to dataset bias. We find a significant performance drop (around 40%) when models trained on one dataset are applied to another. Surprisingly, increasing dataset diversity does not resolve this inter-dataset gap, with close to 60% attributed to dataset-specific biases. To address this remaining generalization gap, we propose a novel architecture extending a mostly dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific parameters that govern interpretable mechanisms such as multi-scale structure, center bias, and fixation spread. Adapting only these parameters to new data accounts for more than 75% of the generalization gap, with a large fraction of the improvement achieved with as few as 50 samples. Our model sets a new state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark (MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from unrelated datasets, but with a substantial boost when adapting to the respective training datasets. The model also provides valuable insights into spatial saliency properties, revealing complex multi-scale effects that combine both absolute and relative sizes. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF",
      "index": 129,
      "title": "Interpretable point cloud classification using multiple instance learning",
      "authors": [
        "Matt De Vries",
        "Reed Naidoo",
        "Olga Fourkioti",
        "Lucas G. Dent",
        "Nathan Curry",
        "Chris Dunsby",
        "Chris Bakal"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "pointmil",
        "interpretable",
        "biomedical",
        "classification",
        "cloud",
        "drug",
        "point",
        "instance",
        "interpretability",
        "macc"
      ],
      "summary": "Understanding 3D cell shape is crucial in biomedical research, where morphology serves as a key indicator of disease, cellular state, and drug response. However, many existing 3D point cloud classification models lack interpretability, limiting their utility for extracting biologically meaningful insights. In this work, we unify standard point cloud backbones and feature aggregation strategies within a Multiple Instance Learning (MIL) framework to enable inherently interpretable classification. Our approach, PointMIL, improves classification performance while providing fine-grained point-level explanations without relying on post hoc analysis. We demonstrate state-of-the-art mACC (97.3%) and F1 (97.5%) in the IntrA biomedical dataset and evaluate the interpretability using quantitative and qualitative metrics. Additionally, we introduce ATLAS-1, a novel dataset of drug-treated 3D cancer cells, and use it to show how PointMIL captures fine-grained morphological effects of chemical treatments. Beyond biomedical applications, PointMIL generalises to standard benchmarks such as ModelNet40 and ScanObjectNN, offering interpretable 3D object recognition across domains",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/De_Vries_Interpretable_point_cloud_classification_using_multiple_instance_learning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Interpretable point cloud classification using multiple instance learning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Matt De Vries , Reed Naidoo , Olga Fourkioti , Lucas G. Dent , Nathan Curry , Chris Dunsby , Chris Bakal Understanding 3D cell shape is crucial in biomedical research, where morphology serves as a key indicator of disease, cellular state, and drug response. However, many existing 3D point cloud classification models lack interpretability, limiting their utility for extracting biologically meaningful insights. In this work, we unify standard point cloud backbones and feature aggregation strategies within a Multiple Instance Learning (MIL) framework to enable inherently interpretable classification. Our approach, PointMIL, improves classification performance while providing fine-grained point-level explanations without relying on post hoc analysis. We demonstrate state-of-the-art mACC (97.3%) and F1 (97.5%) in the IntrA biomedical dataset and evaluate the interpretability using quantitative and qualitative metrics. Additionally, we introduce ATLAS-1, a novel dataset of drug-treated 3D cancer cells, and use it to show how PointMIL captures fine-grained morphological effects of chemical treatments. Beyond biomedical applications, PointMIL generalises to standard benchmarks such as ModelNet40 and ScanObjectNN, offering interpretable 3D object recognition across domains Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF",
      "index": 130,
      "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval",
      "authors": [
        "Dohwan Ko",
        "Ji Soo Lee",
        "Minhyuk Choi",
        "Zihang Meng",
        "Hyunwoo J. Kim"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "blim",
        "candidate",
        "video",
        "retrieval",
        "cpn",
        "text",
        "query",
        "mlvlab",
        "likelihood",
        "mllms"
      ],
      "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate given a video (or text) query from large-scale online databases. Recent work leverages multi-modal large language models (MLLMs) to improve retrieval, especially for long or complex query-candidate pairs. However, we observe that the naive application of MLLMs, i.e., retrieval based on candidate likelihood, introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query. To this end, we propose a novel retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from a given video as well as video features from a given text. Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks, our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. Our in-depth analysis across various multi-modal tasks beyond retrieval highlights the broad applicability of CPN which enhances visual understanding by reducing reliance on textual priors. Code is available at \\href https://github.com/mlvlab/BLiM https://github.com/mlvlab/BLiM .",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video_ICCV_2025_paper.html",
          "/venue/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ko_Bidirectional_Likelihood_Estimation_with_Multi-Modal_Large_Language_Models_for_Text-Video_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Dohwan Ko , Ji Soo Lee , Minhyuk Choi , Zihang Meng , Hyunwoo J. Kim Text-Video Retrieval aims to find the most relevant text (or video) candidate given a video (or text) query from large-scale online databases. Recent work leverages multi-modal large language models (MLLMs) to improve retrieval, especially for long or complex query-candidate pairs. However, we observe that the naive application of MLLMs, i.e., retrieval based on candidate likelihood, introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query. To this end, we propose a novel retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from a given video as well as video features from a given text. Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks, our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. Our in-depth analysis across various multi-modal tasks beyond retrieval highlights the broad applicability of CPN which enhances visual understanding by reducing reliance on textual priors. Code is available at \\href https://github.com/mlvlab/BLiM https://github.com/mlvlab/BLiM . Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF",
      "index": 131,
      "title": "Progressive Test Time Energy Adaptation for Medical Image Segmentation",
      "authors": [
        "Xiaoran Zhang",
        "Byung-Woo Hong",
        "Hyoungseob Park",
        "Daniel H. Pak",
        "Anne-Marie Rickmann",
        "Lawrence H. Staib",
        "James S. Duncan",
        "Alex Wong"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "voldemort108x",
        "segmentation",
        "energy",
        "pttea",
        "test",
        "seg",
        "progressive",
        "adaptation",
        "medical",
        "distribution"
      ],
      "summary": "We propose a model-agnostic, progressive test-time energy adaptation approach for medical image segmentation. Maintaining model performance across diverse medical datasets is challenging, as distribution shifts arise from inconsistent imaging protocols and patient variations. Unlike domain adaptation methods that require multiple passes through target data--impractical in clinical settings--our approach adapts pretrained models progressively as they process test data. Our method leverages a shape energy model trained on source data, which assigns an energy score at the patch level to segmentation maps: low energy represents in-distribution (accurate) shapes, while high energy signals out-of-distribution (erroneous) predictions. By minimizing this energy score at test time, we refine the segmentation model to align with the target distribution. To validate effectiveness and adaptability, we evaluated our framework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets spanning cardiac, spinal cord, and lung segmentation. We consistently outperform baselines both quantitatively and qualitatively. Project page is available at: \\href https://voldemort108x.github.io/pttea_seg/ https://voldemort108x.github.io/pttea_seg/",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Progressive_Test_Time_Energy_Adaptation_for_Medical_Image_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Progressive Test Time Energy Adaptation for Medical Image Segmentation [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Xiaoran Zhang , Byung-Woo Hong , Hyoungseob Park , Daniel H. Pak , Anne-Marie Rickmann , Lawrence H. Staib , James S. Duncan , Alex Wong We propose a model-agnostic, progressive test-time energy adaptation approach for medical image segmentation. Maintaining model performance across diverse medical datasets is challenging, as distribution shifts arise from inconsistent imaging protocols and patient variations. Unlike domain adaptation methods that require multiple passes through target data--impractical in clinical settings--our approach adapts pretrained models progressively as they process test data. Our method leverages a shape energy model trained on source data, which assigns an energy score at the patch level to segmentation maps: low energy represents in-distribution (accurate) shapes, while high energy signals out-of-distribution (erroneous) predictions. By minimizing this energy score at test time, we refine the segmentation model to align with the target distribution. To validate effectiveness and adaptability, we evaluated our framework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets spanning cardiac, spinal cord, and lung segmentation. We consistently outperform baselines both quantitatively and qualitatively. Project page is available at: \\href https://voldemort108x.github.io/pttea_seg/ https://voldemort108x.github.io/pttea_seg/ Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF",
      "index": 132,
      "title": "WINS: Winograd Structured Pruning for Fast Winograd Convolution",
      "authors": [
        "Cheonjun Park",
        "Hyun Jae Oh",
        "Mincheol Park",
        "Hyunchan Moon",
        "Minsik Kim",
        "Suhyun Kim",
        "Myung Kuk Yoon",
        "Won Woo Ro"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "winograd",
        "wins",
        "pruning",
        "convolution",
        "structured",
        "gpus",
        "inference",
        "25x",
        "alongside",
        "inefficient"
      ],
      "summary": "Recent GPUs leverage Winograd convolution and structured pruning to significantly accelerate inference. First, Winograd convolution is theoretically 2.25x faster than standard convolution. Second, structured pruning reduces inference time without additional overhead as the pruning ratio increases. However, applying conventional structured pruning alongside Winograd convolution is inefficient. Existing structured pruning methods, which do not account for how GPUs process Winograd convolution, require large pruning unit sizes, leading to significant information loss. In this paper, we propose Winograd Structured Pruning (WINS), the first approach to employ optimized structured pruning for Winograd convolution. WINS is designed based on an in-depth analysis of Winograd convolution's computational characteristics on GPUs. Additionally, we introduce two variants, WINS-B and WINS-AB, which further enhance performance. Experimental results show that WINS-AB achieves up to 2.8x practical speedup in baseline inference on GPUs while preserving the accuracy of ResNet-18 on ImageNet.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Park_WINS_Winograd_Structured_Pruning_for_Fast_Winograd_Convolution_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "WINS: Winograd Structured Pruning for Fast Winograd Convolution [PDF ] [Copy] [Kimi ] [REL] Authors : Cheonjun Park , Hyun Jae Oh , Mincheol Park , Hyunchan Moon , Minsik Kim , Suhyun Kim , Myung Kuk Yoon , Won Woo Ro Recent GPUs leverage Winograd convolution and structured pruning to significantly accelerate inference. First, Winograd convolution is theoretically 2.25x faster than standard convolution. Second, structured pruning reduces inference time without additional overhead as the pruning ratio increases. However, applying conventional structured pruning alongside Winograd convolution is inefficient. Existing structured pruning methods, which do not account for how GPUs process Winograd convolution, require large pruning unit sizes, leading to significant information loss. In this paper, we propose Winograd Structured Pruning (WINS), the first approach to employ optimized structured pruning for Winograd convolution. WINS is designed based on an in-depth analysis of Winograd convolution's computational characteristics on GPUs. Additionally, we introduce two variants, WINS-B and WINS-AB, which further enhance performance. Experimental results show that WINS-AB achieves up to 2.8x practical speedup in baseline inference on GPUs while preserving the accuracy of ResNet-18 on ImageNet. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF",
      "index": 133,
      "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines",
      "authors": [
        "Jiayuan Chen",
        "Thai-Hoang Pham",
        "Yuanlong Wang",
        "Ping Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "cell",
        "novo",
        "lines",
        "profiling",
        "microscopy",
        "biological",
        "specific",
        "perturbation",
        "rxrx",
        "rxrx19a"
      ],
      "summary": "High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for de novo cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to de novo cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for de novo cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Integrating_Biological_Knowledge_for_Robust_Microscopy_Image_Profiling_on_De_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines [PDF ] [Copy] [Kimi ] [REL] Authors : Jiayuan Chen , Thai-Hoang Pham , Yuanlong Wang , Ping Zhang High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for de novo cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to de novo cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for de novo cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF",
      "index": 134,
      "title": "LVBench: An Extreme Long Video Understanding Benchmark",
      "authors": [
        "Weihan Wang",
        "Zehai He",
        "Wenyi Hong",
        "Yean Cheng",
        "Xiaohan Zhang",
        "Ji Qi",
        "Ming Ding",
        "Xiaotao Gu",
        "Shiyu Huang",
        "Bin Xu",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lvbench",
        "comprehension",
        "video",
        "long",
        "understanding",
        "multimodal",
        "videos",
        "benchmark",
        "commentary",
        "underperform"
      ],
      "summary": "Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark_ICCV_2025_paper.html",
          "/venue/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LVBench_An_Extreme_Long_Video_Understanding_Benchmark_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "LVBench: An Extreme Long Video Understanding Benchmark [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Weihan Wang , Zehai He , Wenyi Hong , Yean Cheng , Xiaohan Zhang , Ji Qi , Ming Ding , Xiaotao Gu , Shiyu Huang , Bin Xu , Yuxiao Dong , Jie Tang Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF",
      "index": 135,
      "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation",
      "authors": [
        "Hao Tang",
        "Zhiqing Guo",
        "Liejun Wang",
        "Chao Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mpnet",
        "similarity",
        "dmw",
        "memory",
        "medical",
        "segmentation",
        "prior",
        "sim",
        "grandmother",
        "macaques"
      ],
      "summary": "In recent years, it has been found that \"grandmother cells\" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_Similarity_Memory_Prior_is_All_You_Need_for_Medical_Image_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 5
      },
      "raw_excerpt": "Similarity Memory Prior is All You Need for Medical Image Segmentation [PDF 7 ] [Copy] [Kimi 5 ] [REL] Authors : Hao Tang , Zhiqing Guo , Liejun Wang , Chao Liu In recent years, it has been found that \"grandmother cells\" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF",
      "index": 136,
      "title": "Cross-Architecture Distillation Made Simple with Redundancy Suppression",
      "authors": [
        "Weijia Zhang",
        "Yuehao Liu",
        "Wu Ran",
        "Chao Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "architecture",
        "distillation",
        "ofa",
        "suppression",
        "cross",
        "rsd",
        "redundancy",
        "redundant",
        "simple",
        "knowledge"
      ],
      "summary": "We describe a simple method for cross-architecture knowledge distillation, where the knowledge transfer is cast into a redundant information suppression formulation. Existing methods introduce sophisticated modules, architecture-tailored designs, and excessive parameters, which impair their efficiency and applicability. We propose to extract the architecture-agnostic knowledge in heterogeneous representations by reducing the redundant architecture-exclusive information. To this end, we present a simple redundancy suppression distillation (RSD) loss, which comprises cross-architecture invariance maximisation and feature decorrelation objectives. To prevent the student from entirely losing its architecture-specific capabilities, we further design a lightweight module that decouples the RSD objective from the student's internal representations. Our method is devoid of the architecture-specific designs and complex operations in the pioneering method of OFA. It outperforms OFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their parameter overhead, which highlights its potential as a simple and strong baseline to the cross-architecture distillation community.",
      "session": null,
      "time": null,
      "links": {
        "supp": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression_ICCV_2025_paper.html",
          "/venue/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Cross-Architecture_Distillation_Made_Simple_with_Redundancy_Suppression_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Cross-Architecture Distillation Made Simple with Redundancy Suppression [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Weijia Zhang , Yuehao Liu , Wu Ran , Chao Ma We describe a simple method for cross-architecture knowledge distillation, where the knowledge transfer is cast into a redundant information suppression formulation. Existing methods introduce sophisticated modules, architecture-tailored designs, and excessive parameters, which impair their efficiency and applicability. We propose to extract the architecture-agnostic knowledge in heterogeneous representations by reducing the redundant architecture-exclusive information. To this end, we present a simple redundancy suppression distillation (RSD) loss, which comprises cross-architecture invariance maximisation and feature decorrelation objectives. To prevent the student from entirely losing its architecture-specific capabilities, we further design a lightweight module that decouples the RSD objective from the student's internal representations. Our method is devoid of the architecture-specific designs and complex operations in the pioneering method of OFA. It outperforms OFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their parameter overhead, which highlights its potential as a simple and strong baseline to the cross-architecture distillation community. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF",
      "index": 137,
      "title": "Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation",
      "authors": [
        "Sebastian Schmidt",
        "Julius Koerner",
        "Dominik Fuchsgruber",
        "Stefano Gasperini",
        "Federico Tombari",
        "Stephan Gnnemann"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "p2f",
        "segmentation",
        "panoptic",
        "oodis",
        "prior2former",
        "evidential",
        "mask",
        "ood",
        "classes",
        "transformers"
      ],
      "summary": "In panoptic segmentation, individual instances must be separated within semantic classes. As state-of-the-art methods rely on a pre-defined set of classes, they struggle with novel categories and out-of-distribution (OOD) data. This is particularly problematic in safety-critical applications, such as autonomous driving, where reliability in unseen scenarios is essential. We address the gap between outstanding benchmark performance and reliability by proposing Prior2Former (P2F), the first approach for segmentation vision transformers rooted in evidential learning. P2F extends the mask vision transformer architecture by incorporating a Beta prior for computing model uncertainty in pixel-wise binary mask assignments. This design enables high-quality uncertainty estimation that effectively detects novel and OOD objects, enabling state-of-the-art anomaly instance segmentation and open-world panoptic segmentation. Unlike most segmentation models addressing unknown classes, P2F operates without access to OOD data samples or contrastive training on void (i.e., unlabeled) classes, making it highly applicable in real-world scenarios where such prior information is unavailable. Additionally, P2F can be flexibly applied to anomaly instance and panoptic segmentation. Through comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan, and OoDIS datasets, P2F demonstrates state-of-the-art performance. Especially in OoDIS, P2F ranks first in its category.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Schmidt_Prior2Former_-_Evidential_Modeling_of_Mask_Transformers_for_Assumption-Free_Open-World_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Prior2Former - Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Sebastian Schmidt , Julius Koerner , Dominik Fuchsgruber , Stefano Gasperini , Federico Tombari , Stephan Gnnemann In panoptic segmentation, individual instances must be separated within semantic classes. As state-of-the-art methods rely on a pre-defined set of classes, they struggle with novel categories and out-of-distribution (OOD) data. This is particularly problematic in safety-critical applications, such as autonomous driving, where reliability in unseen scenarios is essential. We address the gap between outstanding benchmark performance and reliability by proposing Prior2Former (P2F), the first approach for segmentation vision transformers rooted in evidential learning. P2F extends the mask vision transformer architecture by incorporating a Beta prior for computing model uncertainty in pixel-wise binary mask assignments. This design enables high-quality uncertainty estimation that effectively detects novel and OOD objects, enabling state-of-the-art anomaly instance segmentation and open-world panoptic segmentation. Unlike most segmentation models addressing unknown classes, P2F operates without access to OOD data samples or contrastive training on void (i.e., unlabeled) classes, making it highly applicable in real-world scenarios where such prior information is unavailable. Additionally, P2F can be flexibly applied to anomaly instance and panoptic segmentation. Through comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan, and OoDIS datasets, P2F demonstrates state-of-the-art performance. Especially in OoDIS, P2F ranks first in its category. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF",
      "index": 138,
      "title": "OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation",
      "authors": [
        "Ding Zhong",
        "Xu Zheng",
        "Chenfei Liao",
        "Yuanhuiyi Lyu",
        "Jialei Chen",
        "Shengyang Wu",
        "Linfeng Zhang",
        "Xuming Hu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sam2",
        "omnisam",
        "fov",
        "circ",
        "panoramic",
        "segmentation",
        "anything",
        "pinhole",
        "semantic",
        "uparrow"
      ],
      "summary": "Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to 360^\\circ domain, the significant field-of-view (FoV) gap between pinhole (70^\\circ x70^\\circ) and panoramic images (180^\\circ x360^\\circ) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2's memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods by large margins, e.g., 79.06% (10.22%\\uparrow) on SPin8-to-SPan8, 62.46% (6.58%\\uparrow) on CS13-to-DP13.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_OmniSAM_Omnidirectional_Segment_Anything_Model_for_UDA_in_Panoramic_Semantic_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic Semantic Segmentation [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Ding Zhong , Xu Zheng , Chenfei Liao , Yuanhuiyi Lyu , Jialei Chen , Shengyang Wu , Linfeng Zhang , Xuming Hu Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to 360^\\circ domain, the significant field-of-view (FoV) gap between pinhole (70^\\circ x70^\\circ) and panoramic images (180^\\circ x360^\\circ) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2's memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods by large margins, e.g., 79.06% (10.22%\\uparrow) on SPin8-to-SPan8, 62.46% (6.58%\\uparrow) on CS13-to-DP13. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF",
      "index": 139,
      "title": "Token-Efficient VLM: High-Resolution Image Understanding via Dynamic Region Proposal",
      "authors": [
        "Yitong Jiang",
        "Jinwei Gu",
        "Tianfan Xue",
        "Ka Chun Cheung",
        "Pavlo Molchanov",
        "Hongxu Yin",
        "Sifei Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "token",
        "teva",
        "vlm",
        "patch",
        "vision",
        "details",
        "dynamic",
        "efficient",
        "language",
        "visual"
      ],
      "summary": "Vision-Language Models (VLMs) excel at visual understanding by leveraging pretrained image encoders to generate visual tokens. However, they struggle with high-resolution images and zoomed-in regions due to the computational burden and token redundancy of uniform patch-based processing, often leading to the loss of critical details. To address these challenges, we propose Token-Efficient Vision Language Model (TEVA), a novel framework that detects key regions and applies dynamic patch sampling to efficiently capture fine-grained details while preserving global context. Our approach first identifies subject-oriented regions using an adaptive detection strategy. Then, a dynamic patch sampling mechanism selects and arranges patches at varying scales, ensuring efficient processing without increasing token count. Extensive experiments demonstrate that Token-Efficient Vision Language Model (TEVA) significantly enhances VLM performance in handling visual details, seamlessly integrating with various decoders and LLMs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_Token-Efficient_VLM_High-Resolution_Image_Understanding_via_Dynamic_Region_Proposal_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Token-Efficient VLM: High-Resolution Image Understanding via Dynamic Region Proposal [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Yitong Jiang , Jinwei Gu , Tianfan Xue , Ka Chun Cheung , Pavlo Molchanov , Hongxu Yin , Sifei Liu Vision-Language Models (VLMs) excel at visual understanding by leveraging pretrained image encoders to generate visual tokens. However, they struggle with high-resolution images and zoomed-in regions due to the computational burden and token redundancy of uniform patch-based processing, often leading to the loss of critical details. To address these challenges, we propose Token-Efficient Vision Language Model (TEVA), a novel framework that detects key regions and applies dynamic patch sampling to efficiently capture fine-grained details while preserving global context. Our approach first identifies subject-oriented regions using an adaptive detection strategy. Then, a dynamic patch sampling mechanism selects and arranges patches at varying scales, ensuring efficient processing without increasing token count. Extensive experiments demonstrate that Token-Efficient Vision Language Model (TEVA) significantly enhances VLM performance in handling visual details, seamlessly integrating with various decoders and LLMs. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF",
      "index": 140,
      "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
      "authors": [
        "Shraman Pramanick",
        "Effrosyni Mavroudi",
        "Yale Song",
        "Rama Chellappa",
        "Lorenzo Torresani",
        "Triantafyllos Afouras"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "grounding",
        "queries",
        "video",
        "enriched",
        "temporal",
        "multimodal",
        "grounded",
        "llms",
        "vtg",
        "language"
      ],
      "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.html",
          "/venue/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Pramanick_Enrich_and_Detect_Video_Temporal_Grounding_with_Multimodal_LLMs_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Shraman Pramanick , Effrosyni Mavroudi , Yale Song , Rama Chellappa , Lorenzo Torresani , Triantafyllos Afouras We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF",
      "index": 141,
      "title": "Emulating Self-attention with Convolution for Efficient Image Super-Resolution",
      "authors": [
        "Dongheon Lee",
        "Seokju Yun",
        "Youngmin Ro"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "attention",
        "self",
        "convattn",
        "emulating",
        "module",
        "transformers",
        "esc",
        "memory",
        "representational",
        "psnr"
      ],
      "summary": "In this paper, we tackle the high computational overhead of Transformers for efficient image super-resolution (SR). Motivated by the observations of self-attention's inter-layer repetition, we introduce a convolutionized self-attention module named Convolutional Attention (ConvAttn) that emulates self-attention's long-range modeling capability and instance-dependent weighting with a single shared large kernel and dynamic kernels. By utilizing the ConvAttn module, we significantly reduce the reliance on self-attention and its involved memory-bound operations while maintaining the representational capability of Transformers. Furthermore, we overcome the challenge of integrating flash attention into the lightweight SR regime, effectively mitigating self-attention's inherent memory bottleneck. We scale up the window size to 32x32 with flash attention rather than proposing an intricate self-attention module, significantly improving PSNR by 0.31dB on Urban100x2 while reducing latency and memory usage by 16xand 12.2x. Building on these approaches, our proposed network, termed Emulating Self-attention with Convolution (ESC), notably improves PSNR by 0.27 dB on Urban100x4 compared to HiT-SRF, reducing the latency and memory usage by 3.7xand 6.2x, respectively. Extensive experiments demonstrate that our ESC maintains the ability for long-range modeling, data scalability, and the representational power of Transformers despite most self-attention being replaced by the ConvAttn module.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Emulating_Self-attention_with_Convolution_for_Efficient_Image_Super-Resolution_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": null
      },
      "raw_excerpt": "Emulating Self-attention with Convolution for Efficient Image Super-Resolution [PDF 5 ] [Copy] [Kimi ] [REL] Authors : Dongheon Lee , Seokju Yun , Youngmin Ro In this paper, we tackle the high computational overhead of Transformers for efficient image super-resolution (SR). Motivated by the observations of self-attention's inter-layer repetition, we introduce a convolutionized self-attention module named Convolutional Attention (ConvAttn) that emulates self-attention's long-range modeling capability and instance-dependent weighting with a single shared large kernel and dynamic kernels. By utilizing the ConvAttn module, we significantly reduce the reliance on self-attention and its involved memory-bound operations while maintaining the representational capability of Transformers. Furthermore, we overcome the challenge of integrating flash attention into the lightweight SR regime, effectively mitigating self-attention's inherent memory bottleneck. We scale up the window size to 32x32 with flash attention rather than proposing an intricate self-attention module, significantly improving PSNR by 0.31dB on Urban100x2 while reducing latency and memory usage by 16xand 12.2x. Building on these approaches, our proposed network, termed Emulating Self-attention with Convolution (ESC), notably improves PSNR by 0.27 dB on Urban100x4 compared to HiT-SRF, reducing the latency and memory usage by 3.7xand 6.2x, respectively. Extensive experiments demonstrate that our ESC maintains the ability for long-range modeling, data scalability, and the representational power of Transformers despite most self-attention being replaced by the ConvAttn module. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF",
      "index": 142,
      "title": "Inverse Image-Based Rendering for Light Field Generation from Single Images",
      "authors": [
        "Hyunjun Jung",
        "Hae-Gon Jeon"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "rendering",
        "light",
        "image",
        "view",
        "inverse",
        "generation",
        "images",
        "contents",
        "novel",
        "specialized"
      ],
      "summary": "A concept of light-fields computed from multiple view images on regular grids has proven its benefit for scene representations, and supported realistic renderings of novel views and photographic effects such as refocusing and shallow depth of field. In spite of its effectiveness of light flow computations, obtaining light fields requires either computational costs or specialized devices like a bulky camera setup and a specialized microlens array. In an effort to broaden its benefit and applicability, in this paper, we propose a novel view synthesis method for light field generation from only single images, named inverse image-based rendering. Unlike previous attempts to implicitly rebuild 3D geometry or to explicitly represent objective scenes, our method reconstructs light flows in a space from image pixels, which behaves in the opposite way to image-based rendering. To accomplish this, we design a neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our neural renderer first stores the light flow of source rays from the input image, then computes the relationships among them through cross-attention, and finally predicts the color of the target ray based on these relationships. After the rendering pipeline generates the first novel view from a single input image, the generated out-of-view contents are updated to the set of source rays, and this procedure is iteratively performed while ensuring the consistent generation of occluded contents. We demonstrate that our inverse image-based rendering works well with various challenging datasets without any retraining or finetuning after once trained on synthetic dataset. In addition, our method outperforms relevant state-of-the-art novel view synthesis methods.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jung_Inverse_Image-Based_Rendering_for_Light_Field_Generation_from_Single_Images_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Inverse Image-Based Rendering for Light Field Generation from Single Images [PDF ] [Copy] [Kimi ] [REL] Authors : Hyunjun Jung , Hae-Gon Jeon A concept of light-fields computed from multiple view images on regular grids has proven its benefit for scene representations, and supported realistic renderings of novel views and photographic effects such as refocusing and shallow depth of field. In spite of its effectiveness of light flow computations, obtaining light fields requires either computational costs or specialized devices like a bulky camera setup and a specialized microlens array. In an effort to broaden its benefit and applicability, in this paper, we propose a novel view synthesis method for light field generation from only single images, named inverse image-based rendering. Unlike previous attempts to implicitly rebuild 3D geometry or to explicitly represent objective scenes, our method reconstructs light flows in a space from image pixels, which behaves in the opposite way to image-based rendering. To accomplish this, we design a neural rendering pipeline to render a target ray in an arbitrary viewpoint. Our neural renderer first stores the light flow of source rays from the input image, then computes the relationships among them through cross-attention, and finally predicts the color of the target ray based on these relationships. After the rendering pipeline generates the first novel view from a single input image, the generated out-of-view contents are updated to the set of source rays, and this procedure is iteratively performed while ensuring the consistent generation of occluded contents. We demonstrate that our inverse image-based rendering works well with various challenging datasets without any retraining or finetuning after once trained on synthetic dataset. In addition, our method outperforms relevant state-of-the-art novel view synthesis methods. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF",
      "index": 143,
      "title": "RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters",
      "authors": [
        "Xiaolin Liu",
        "Tianyi Zhou",
        "Hongbo Kang",
        "Jian Ma",
        "Ziwen Wang",
        "Jing Huang",
        "Wenguo Weng",
        "Yu-Kun Lai",
        "Kun Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "evacuation",
        "crowd",
        "sdm",
        "rescue",
        "simulation",
        "terrain",
        "motor",
        "likun",
        "personalized",
        "tju"
      ],
      "summary": "Crowd evacuation simulation is critical for enhancing public safety, and demanded for realistic virtual environments. Current mainstream evacuation models overlook the complex human behaviors that occur during evacuation, such as pedestrian collisions, interpersonal interactions, and variations in behavior influenced by terrain types or individual body shapes. This results in the failure to accurately simulate the escape of people in the real world. In this paper, aligned with the sensory-decision-motor (SDM) flow of the human brain, we propose a real-time 3D crowd evacuation simulation framework that integrates a 3D-adaptive SFM (Social Force Model) Decision Mechanism and a Personalized Gait Control Motor. This framework allows multiple agents to move in parallel and is suitable for various scenarios, with dynamic crowd awareness. Additionally, we introduce Part-level Force Visualization to assist in evacuation analysis. Experimental results demonstrate that our framework supports dynamic trajectory planning and personalized behavior for each agent throughout the evacuation process, and is compatible with uneven terrain. Visually, our method generates evacuation results that are more realistic and plausible, providing enhanced insights for crowd simulation. The code is available at http://cic.tju.edu.cn/faculty/likun/projects/RESCUE.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_RESCUE_Crowd_Evacuation_Simulation_via_Controlling_SDM-United_Characters_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Xiaolin Liu , Tianyi Zhou , Hongbo Kang , Jian Ma , Ziwen Wang , Jing Huang , Wenguo Weng , Yu-Kun Lai , Kun Li Crowd evacuation simulation is critical for enhancing public safety, and demanded for realistic virtual environments. Current mainstream evacuation models overlook the complex human behaviors that occur during evacuation, such as pedestrian collisions, interpersonal interactions, and variations in behavior influenced by terrain types or individual body shapes. This results in the failure to accurately simulate the escape of people in the real world. In this paper, aligned with the sensory-decision-motor (SDM) flow of the human brain, we propose a real-time 3D crowd evacuation simulation framework that integrates a 3D-adaptive SFM (Social Force Model) Decision Mechanism and a Personalized Gait Control Motor. This framework allows multiple agents to move in parallel and is suitable for various scenarios, with dynamic crowd awareness. Additionally, we introduce Part-level Force Visualization to assist in evacuation analysis. Experimental results demonstrate that our framework supports dynamic trajectory planning and personalized behavior for each agent throughout the evacuation process, and is compatible with uneven terrain. Visually, our method generates evacuation results that are more realistic and plausible, providing enhanced insights for crowd simulation. The code is available at http://cic.tju.edu.cn/faculty/likun/projects/RESCUE. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF",
      "index": 144,
      "title": "PRM: Photometric Stereo based Large Reconstruction Model",
      "authors": [
        "Wenhang Ge",
        "Jiantao Lin",
        "Guibao Shen",
        "Jiawei Feng",
        "Tao Hu",
        "Xinli Xu",
        "Ying-Cong Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "prm",
        "reconstruction",
        "photometric",
        "rendering",
        "images",
        "pbr",
        "stereo",
        "specular",
        "lighting",
        "cues"
      ],
      "summary": "We propose PRM, a novel photometric stereo based large reconstruction model to reconstruct high-quality meshes with fine-grained details. Previous large reconstruction models typically prepare training images under fixed and simple lighting, offering minimal photometric cues for precise reconstruction. Furthermore, images containing specular surfaces are treated as out-of-distribution samples, resulting in degraded reconstruction quality. To handle these challenges, PRM renders images by varying materials and lighting, which not only improves the local details by providing rich photometric cues but also increases the model's robustness to variations in the appearance of input images. To offer enhanced flexibility, we incorporate a real-time physically-based rendering (PBR) method and mesh rasterization for ground-truth rendering. By using an explicit mesh as 3D representation, PRM ensures the application of differentiable PBR for predicted rendering. This approach models specular color more accurately for images with varying materials and illumination than previous neural rendering methods and supports multiple supervisions for geometry optimization. Extensive experiments demonstrate that PRM significantly outperforms other models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ge_PRM_Photometric_Stereo_based_Large_Reconstruction_Model_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "PRM: Photometric Stereo based Large Reconstruction Model [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Wenhang Ge , Jiantao Lin , Guibao Shen , Jiawei Feng , Tao Hu , Xinli Xu , Ying-Cong Chen We propose PRM, a novel photometric stereo based large reconstruction model to reconstruct high-quality meshes with fine-grained details. Previous large reconstruction models typically prepare training images under fixed and simple lighting, offering minimal photometric cues for precise reconstruction. Furthermore, images containing specular surfaces are treated as out-of-distribution samples, resulting in degraded reconstruction quality. To handle these challenges, PRM renders images by varying materials and lighting, which not only improves the local details by providing rich photometric cues but also increases the model's robustness to variations in the appearance of input images. To offer enhanced flexibility, we incorporate a real-time physically-based rendering (PBR) method and mesh rasterization for ground-truth rendering. By using an explicit mesh as 3D representation, PRM ensures the application of differentiable PBR for predicted rendering. This approach models specular color more accurately for images with varying materials and illumination than previous neural rendering methods and supports multiple supervisions for geometry optimization. Extensive experiments demonstrate that PRM significantly outperforms other models. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF",
      "index": 145,
      "title": "Benchmarking Egocentric Visual-Inertial SLAM at City Scale",
      "authors": [
        "Anusha Krishnan",
        "Shaohui Liu",
        "Paul-Edouard Sarlin",
        "Oscar Gentilhomme",
        "David Caruso",
        "Maurizio Monge",
        "Richard Newcombe",
        "Jakob Engel",
        "Marc Pollefeys"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "slam",
        "egocentric",
        "city",
        "inertial",
        "visual",
        "challenges",
        "ethz",
        "sensors",
        "trajectories",
        "lamaria"
      ],
      "summary": "Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at www.lamaria.ethz.ch.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Krishnan_Benchmarking_Egocentric_Visual-Inertial_SLAM_at_City_Scale_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Benchmarking Egocentric Visual-Inertial SLAM at City Scale [PDF ] [Copy] [Kimi ] [REL] Authors : Anusha Krishnan , Shaohui Liu , Paul-Edouard Sarlin , Oscar Gentilhomme , David Caruso , Maurizio Monge , Richard Newcombe , Jakob Engel , Marc Pollefeys Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at www.lamaria.ethz.ch. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF",
      "index": 146,
      "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians",
      "authors": [
        "Shenxing Wei",
        "Jinxi Li",
        "Yafei Yang",
        "Siyuan Zhou",
        "Bo Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "raylet",
        "gaussians",
        "rayletdf",
        "surface",
        "generalizable",
        "reconstruction",
        "clouds",
        "distance",
        "point",
        "3dgs"
      ],
      "summary": "In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named **RayletDF**, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_RayletDF_Raylet_Distance_Fields_for_Generalizable_3D_Surface_Reconstruction_from_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Shenxing Wei , Jinxi Li , Yafei Yang , Siyuan Zhou , Bo Yang In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named **RayletDF**, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF",
      "index": 147,
      "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity",
      "authors": [
        "Yida Wang",
        "Xueyang Zhang",
        "Kun Zhan",
        "Peng Jia",
        "Xianpeng Lang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "hineus",
        "eikonal",
        "surface",
        "radiance",
        "fidelity",
        "reflective",
        "constraints",
        "reflection",
        "rendering",
        "urban"
      ],
      "summary": "Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate SotA performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting. Project hosted at https://wangyida.github.io/posts/hineus, where the urban and vehicle reconstruction related modules are excluded from open-sourced codes due to legal concerns.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_HiNeuS_High-fidelity_Neural_Surface_Mitigating_Low-texture_and_Reflective_Ambiguity_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yida Wang , Xueyang Zhang , Kun Zhan , Peng Jia , Xianpeng Lang Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate SotA performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting. Project hosted at https://wangyida.github.io/posts/hineus, where the urban and vehicle reconstruction related modules are excluded from open-sourced codes due to legal concerns. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF",
      "index": 148,
      "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
      "authors": [
        "Jianing Zhang",
        "Jiayi Zhu",
        "Feiyu Ji",
        "Xiaokang Yang",
        "Xiaoyun Yuan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "degradation",
        "metalens",
        "photography",
        "tunable",
        "multipath",
        "fidelity",
        "svda",
        "modeled",
        "optical",
        "diffusion"
      ],
      "summary": "Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Degradation-Modeled_Multipath_Diffusion_for_Tunable_Metalens_Photography_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Jianing Zhang , Jiayi Zhu , Feiyu Ji , Xiaokang Yang , Xiaoyun Yuan Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF",
      "index": 149,
      "title": "MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy",
      "authors": [
        "Wuyang Li",
        "Wentao Pan",
        "Xiaoyuan Liu",
        "Zhendong Luo",
        "Chenxin Li",
        "Hengyu Liu",
        "Din Ping Tsai",
        "Mu Ku Chen",
        "Yixuan Yuan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "metascope",
        "metalens",
        "endoscopy",
        "optics",
        "micro",
        "informed",
        "optical",
        "driven",
        "chromatic",
        "ultra"
      ],
      "summary": "Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_MetaScope_Optics-Driven_Neural_Network_for_Ultra-Micro_Metalens_Endoscopy_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Wuyang Li , Wentao Pan , Xiaoyuan Liu , Zhendong Luo , Chenxin Li , Hengyu Liu , Din Ping Tsai , Mu Ku Chen , Yixuan Yuan Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF",
      "index": 150,
      "title": "Discontinuity-aware Normal Integration for Generic Central Camera Models",
      "authors": [
        "Francesco Milano",
        "Manuel Lpez-Antequera",
        "Naina Dhingra",
        "Roland Siegwart",
        "Robert Thiel"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "integration",
        "normal",
        "normals",
        "generic",
        "surface",
        "central",
        "discontinuities",
        "camera",
        "photometric",
        "cameras"
      ],
      "summary": "Recovering a 3D surface from its surface normal map, a problem known as normal integration, is a key component for photometric shape reconstruction techniques such as shape-from-shading and photometric stereo. The vast majority of existing approaches for normal integration handle only implicitly the presence of depth discontinuities and are limited to orthographic or ideal pinhole cameras. In this paper, we propose a novel formulation that allows modeling discontinuities explicitly and handling generic central cameras. Our key idea is based on a local planarity assumption, that we model through constraints between surface normals and ray directions. Compared to existing methods, our approach more accurately approximates the relation between depth and surface normals, achieves state-of-the-art results on the standard normal integration benchmark, and is the first to directly handle generic central camera models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Milano_Discontinuity-aware_Normal_Integration_for_Generic_Central_Camera_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Discontinuity-aware Normal Integration for Generic Central Camera Models [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Francesco Milano , Manuel Lpez-Antequera , Naina Dhingra , Roland Siegwart , Robert Thiel Recovering a 3D surface from its surface normal map, a problem known as normal integration, is a key component for photometric shape reconstruction techniques such as shape-from-shading and photometric stereo. The vast majority of existing approaches for normal integration handle only implicitly the presence of depth discontinuities and are limited to orthographic or ideal pinhole cameras. In this paper, we propose a novel formulation that allows modeling discontinuities explicitly and handling generic central cameras. Our key idea is based on a local planarity assumption, that we model through constraints between surface normals and ray directions. Compared to existing methods, our approach more accurately approximates the relation between depth and surface normals, achieves state-of-the-art results on the standard normal integration benchmark, and is the first to directly handle generic central camera models. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF",
      "index": 151,
      "title": "EDM: Efficient Deep Feature Matching",
      "authors": [
        "Xi Li",
        "Tong Rao",
        "Cihui Pan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "edm",
        "matching",
        "feature",
        "efficiency",
        "accuracy",
        "efficient",
        "deep",
        "subpixel",
        "chicleee",
        "level"
      ],
      "summary": "Recent feature matching methods have achieved remarkable performance but lack efficiency consideration. In this paper, we revisit the mainstream detector-free matching pipeline and improve all its stages considering both accuracy and efficiency. We propose an Efficient Deep feature Matching network, EDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level features. Then we present a Correlation Injection Module that conducts feature transformation on high-level deep features, and progressively injects feature correlations from global to local for efficient multi-scale feature aggregation, improving both speed and performance. In the refinement stage, a novel lightweight bidirectional axis-based regression head is designed to directly predict subpixel-level correspondences from latent features, avoiding the significant computational cost of explicitly locating keypoints on high-resolution local feature heatmaps. Moreover, effective selection strategies are introduced to enhance matching accuracy. Extensive experiments show that our EDM achieves competitive matching accuracy on various benchmarks and exhibits excellent efficiency, offering valuable best practices for real-world applications. The code is available at https://github.com/chicleee/EDM.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_EDM_Efficient_Deep_Feature_Matching_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_EDM_Efficient_Deep_Feature_Matching@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_EDM_Efficient_Deep_Feature_Matching_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "EDM: Efficient Deep Feature Matching [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Xi Li , Tong Rao , Cihui Pan Recent feature matching methods have achieved remarkable performance but lack efficiency consideration. In this paper, we revisit the mainstream detector-free matching pipeline and improve all its stages considering both accuracy and efficiency. We propose an Efficient Deep feature Matching network, EDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level features. Then we present a Correlation Injection Module that conducts feature transformation on high-level deep features, and progressively injects feature correlations from global to local for efficient multi-scale feature aggregation, improving both speed and performance. In the refinement stage, a novel lightweight bidirectional axis-based regression head is designed to directly predict subpixel-level correspondences from latent features, avoiding the significant computational cost of explicitly locating keypoints on high-resolution local feature heatmaps. Moreover, effective selection strategies are introduced to enhance matching accuracy. Extensive experiments show that our EDM achieves competitive matching accuracy on various benchmarks and exhibits excellent efficiency, offering valuable best practices for real-world applications. The code is available at https://github.com/chicleee/EDM. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF",
      "index": 152,
      "title": "MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion",
      "authors": [
        "Zebin He",
        "Mingxin Yang",
        "Shuhui Yang",
        "Yixuan Tang",
        "Tao Wang",
        "Kaihao Zhang",
        "Guanying Chen",
        "Yuhong Liu",
        "Jie Jiang",
        "Chunchao Guo",
        "Wenhan Luo"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "pbr",
        "material",
        "materialmvp",
        "illumination",
        "textures",
        "albedo",
        "generation",
        "lighting",
        "multi",
        "invariant"
      ],
      "summary": "Physically-based rendering (PBR) has become a cornerstone in modern computer graphics, enabling realistic material representation and lighting interactions in 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model for generating PBR textures from 3D meshes and image prompts, addressing key challenges in multi-view material synthesis. Our approach leverages Reference Attention to extract and encode informative latent from the input reference images, enabling intuitive and controllable texture generation. We also introduce a Consistency-Regularized Training strategy to enforce stability across varying viewpoints and illumination conditions, ensuring illumination-invariant and geometrically consistent results. Additionally, we propose Dual-Channel Material Generation, which separately optimizes albedo and metallic-roughness (MR) textures while maintaining precise spatial alignment with the input images through Multi-Channel Aligned Attention. Learnable material embeddings are further integrated to capture the distinct properties of albedo and MR. Experimental results demonstrate that our model generates PBR textures with realistic behavior across diverse lighting scenarios, outperforming existing methods in both consistency and quality for scalable 3D asset creation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/He_MaterialMVP_Illumination-Invariant_Material_Generation_via_Multi-view_PBR_Diffusion_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zebin He , Mingxin Yang , Shuhui Yang , Yixuan Tang , Tao Wang , Kaihao Zhang , Guanying Chen , Yuhong Liu , Jie Jiang , Chunchao Guo , Wenhan Luo Physically-based rendering (PBR) has become a cornerstone in modern computer graphics, enabling realistic material representation and lighting interactions in 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model for generating PBR textures from 3D meshes and image prompts, addressing key challenges in multi-view material synthesis. Our approach leverages Reference Attention to extract and encode informative latent from the input reference images, enabling intuitive and controllable texture generation. We also introduce a Consistency-Regularized Training strategy to enforce stability across varying viewpoints and illumination conditions, ensuring illumination-invariant and geometrically consistent results. Additionally, we propose Dual-Channel Material Generation, which separately optimizes albedo and metallic-roughness (MR) textures while maintaining precise spatial alignment with the input images through Multi-Channel Aligned Attention. Learnable material embeddings are further integrated to capture the distinct properties of albedo and MR. Experimental results demonstrate that our model generates PBR textures with realistic behavior across diverse lighting scenarios, outperforming existing methods in both consistency and quality for scalable 3D asset creation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF",
      "index": 153,
      "title": "PolarAnything: Diffusion-based Polarimetric Image Synthesis",
      "authors": [
        "Kailong Zhang",
        "Youwei Lyu",
        "Heng Guo",
        "Si Li",
        "Zhanyu Ma",
        "Boxin Shi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "polarization",
        "polaranything",
        "images",
        "photorealistic",
        "mitsuba",
        "diffusion",
        "synthesizing",
        "polarimetric",
        "image",
        "pbr"
      ],
      "summary": "Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization images. The existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_PolarAnything_Diffusion-based_Polarimetric_Image_Synthesis_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "PolarAnything: Diffusion-based Polarimetric Image Synthesis [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Kailong Zhang , Youwei Lyu , Heng Guo , Si Li , Zhanyu Ma , Boxin Shi Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization images. The existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF",
      "index": 154,
      "title": "Spatio-Spectral Pattern Illumination for Direct and Indirect Separation from a Single Hyperspectral Image",
      "authors": [
        "Shin Ishihara",
        "Imari Sato"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "hyperspectral",
        "illumination",
        "indirect",
        "separation",
        "direct",
        "pattern",
        "spatio",
        "single",
        "spectral",
        "appearance"
      ],
      "summary": "Hyperspectral imaging has proven effective for appearance inspection because it can identify material compositions and reveal hidden features. Similarly, direct/indirect separation provides essential information about surface appearance and internal conditions, including layer structures and scattering behaviors. This paper presents a novel illumination system incorporating dispersive optics to unify both advantages for scene analysis. In general, achieving distinct direct/indirect separation requires multiple images with varying patterns. In a hyperspectral scenario, using a hyperspectral camera or tunable filters extends exposure and measurement times, hindering practical application. Our proposed system enables the illumination of a wavelength-dependent, spatially shifted pattern. With proper consideration of reflectance differences, we demonstrate that robust separation of direct and indirect components for each wavelength can be achieved using a single hyperspectral image acquired under our single spatio-spectral pattern illumination.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ishihara_Spatio-Spectral_Pattern_Illumination_for_Direct_and_Indirect_Separation_from_a_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Spatio-Spectral Pattern Illumination for Direct and Indirect Separation from a Single Hyperspectral Image [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Shin Ishihara , Imari Sato Hyperspectral imaging has proven effective for appearance inspection because it can identify material compositions and reveal hidden features. Similarly, direct/indirect separation provides essential information about surface appearance and internal conditions, including layer structures and scattering behaviors. This paper presents a novel illumination system incorporating dispersive optics to unify both advantages for scene analysis. In general, achieving distinct direct/indirect separation requires multiple images with varying patterns. In a hyperspectral scenario, using a hyperspectral camera or tunable filters extends exposure and measurement times, hindering practical application. Our proposed system enables the illumination of a wavelength-dependent, spatially shifted pattern. With proper consideration of reflectance differences, we demonstrate that robust separation of direct and indirect components for each wavelength can be achieved using a single hyperspectral image acquired under our single spatio-spectral pattern illumination. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF",
      "index": 155,
      "title": "Explaining Human Preferences via Metrics for Structured 3D Reconstruction",
      "authors": [
        "Jack Langerman",
        "Denys Rozumnyi",
        "Yuzhong Huang",
        "Dmytro Mishkin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "metrics",
        "preferences",
        "structured",
        "metric",
        "expert",
        "wireframe",
        "human",
        "uttered",
        "lord",
        "modelers"
      ],
      "summary": "\"What cannot be measured cannot be improved\" while likely never uttered by Lord Kelvin, summarizes effectively the driving force behind this work. This paper presents a detailed discussion of automated metrics for evaluating structured 3D reconstructions. Pitfalls of each metric are discussed, and an analysis through the lens of expert 3D modelers' preferences is presented. A set of systematic \"unit tests\" are proposed to empirically verify desirable properties, and context aware recommendations regarding which metric to use depending on application are provided. Finally, a learned metric distilled from human expert judgments is proposed and analyzed. The source code is available at https://github.com/s23dr/ wireframe-metrics-iccv2025.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Langerman_Explaining_Human_Preferences_via_Metrics_for_Structured_3D_Reconstruction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Explaining Human Preferences via Metrics for Structured 3D Reconstruction [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jack Langerman , Denys Rozumnyi , Yuzhong Huang , Dmytro Mishkin \"What cannot be measured cannot be improved\" while likely never uttered by Lord Kelvin, summarizes effectively the driving force behind this work. This paper presents a detailed discussion of automated metrics for evaluating structured 3D reconstructions. Pitfalls of each metric are discussed, and an analysis through the lens of expert 3D modelers' preferences is presented. A set of systematic \"unit tests\" are proposed to empirically verify desirable properties, and context aware recommendations regarding which metric to use depending on application are provided. Finally, a learned metric distilled from human expert judgments is proposed and analyzed. The source code is available at https://github.com/s23dr/ wireframe-metrics-iccv2025. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF",
      "index": 156,
      "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception",
      "authors": [
        "Jiaru Zhong",
        "Jiahao Wang",
        "Jiahui Xu",
        "Xiaofan Li",
        "Zaiqing Nie",
        "Haibao Yu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "cooptrack",
        "cooperative",
        "perception",
        "end",
        "instance",
        "association",
        "v2x",
        "seq",
        "sequential",
        "amota"
      ],
      "summary": "Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0% mAP and 32.8% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_CoopTrack_Exploring_End-to-End_Learning_for_Efficient_Cooperative_Sequential_Perception_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jiaru Zhong , Jiahao Wang , Jiahui Xu , Xiaofan Li , Zaiqing Nie , Haibao Yu Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0% mAP and 32.8% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF",
      "index": 157,
      "title": "Inverse 3D Microscopy Rendering for Cell Shape Inference with Active Mesh",
      "authors": [
        "Sacha Ichbiah",
        "Anshuman Sinha",
        "Fabrice Delbary",
        "Herv Turlier"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "deltamic",
        "microscopy",
        "shape",
        "contour",
        "inverse",
        "mesh",
        "inference",
        "active",
        "cell",
        "differentiable"
      ],
      "summary": "Traditional methods for biological shape inference, such as deep learning (DL) and active contour models, face important limitations in 3D. DL approaches require large annotated datasets, which are often impractical to obtain, while active contour methods depend on carefully tuned heuristics for intensity attraction and shape regularization. We introduce deltaMic, a novel differentiable 3D renderer for fluorescence microscopy that formulates shape inference as an inverse problem. By leveraging differentiable convolutions, deltaMic simulates the image formation process, integrating a parameterized point spread function (PSF) with a triangle mesh-based representation of biological structures. Unlike DL- or contour-based segmentation, deltaMic directly optimizes both shape and optical parameters to align synthetic and real microscopy images, removing the need for large datasets or sample-specific fine-tuning. To ensure scalability, we implement a GPU-accelerated Fourier transform for triangle meshes along with narrow-band spectral filtering. We show that deltaMic accurately reconstructs cell geometries from both synthetic and diverse experimental 3D microscopy data, while remaining robust to noise and initialization. This establishes a new physics-informed framework for biophysical image analysis and inverse modeling.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ichbiah_Inverse_3D_Microscopy_Rendering_for_Cell_Shape_Inference_with_Active_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Inverse 3D Microscopy Rendering for Cell Shape Inference with Active Mesh [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Sacha Ichbiah , Anshuman Sinha , Fabrice Delbary , Herv Turlier Traditional methods for biological shape inference, such as deep learning (DL) and active contour models, face important limitations in 3D. DL approaches require large annotated datasets, which are often impractical to obtain, while active contour methods depend on carefully tuned heuristics for intensity attraction and shape regularization. We introduce deltaMic, a novel differentiable 3D renderer for fluorescence microscopy that formulates shape inference as an inverse problem. By leveraging differentiable convolutions, deltaMic simulates the image formation process, integrating a parameterized point spread function (PSF) with a triangle mesh-based representation of biological structures. Unlike DL- or contour-based segmentation, deltaMic directly optimizes both shape and optical parameters to align synthetic and real microscopy images, removing the need for large datasets or sample-specific fine-tuning. To ensure scalability, we implement a GPU-accelerated Fourier transform for triangle meshes along with narrow-band spectral filtering. We show that deltaMic accurately reconstructs cell geometries from both synthetic and diverse experimental 3D microscopy data, while remaining robust to noise and initialization. This establishes a new physics-informed framework for biophysical image analysis and inverse modeling. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF",
      "index": 158,
      "title": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching",
      "authors": [
        "Xiangzeng Liu",
        "Chi Wang",
        "Guanglu Shi",
        "Xiaodong Zhang",
        "Qiguang Miao",
        "Miao Fan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sgad",
        "matching",
        "82s",
        "area",
        "descriptor",
        "containment",
        "a2pm",
        "51s",
        "aware",
        "accuracy"
      ],
      "summary": "Local feature matching remains a fundamental challenge in computer vision. Recent Area to Point Matching (A2PM) methods have improved matching accuracy. However, existing research based on this framework relies on inefficient pixel-level comparisons and complex graph matching that limit scalability. In this work, we introduce the Semantic and Geometric-aware Descriptor Network (SGAD), which fundamentally rethinks area-based matching by generating highly discriminative area descriptors that enable direct matching without complex graph optimization. This approach significantly improves both accuracy and efficiency of area matching. We further improve the performance of area matching through a novel supervision strategy that decomposes the area matching task into classification and ranking subtasks. Finally, we introduce the Hierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping areas by analyzing containment graphs. SGAD demonstrates remarkable performance gains, reducing runtime by 60x(0.82s vs. 60.23s) compared to MESA. Extensive evaluations show consistent improvements across multiple point matchers: SGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy (0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA delivers +7.39% AUC@5^\\circ in indoor pose estimation, establishing a new state-of-the-art.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_SGAD_Semantic_and_Geometric-aware_Descriptor_for_Local_Feature_Matching_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Xiangzeng Liu , Chi Wang , Guanglu Shi , Xiaodong Zhang , Qiguang Miao , Miao Fan Local feature matching remains a fundamental challenge in computer vision. Recent Area to Point Matching (A2PM) methods have improved matching accuracy. However, existing research based on this framework relies on inefficient pixel-level comparisons and complex graph matching that limit scalability. In this work, we introduce the Semantic and Geometric-aware Descriptor Network (SGAD), which fundamentally rethinks area-based matching by generating highly discriminative area descriptors that enable direct matching without complex graph optimization. This approach significantly improves both accuracy and efficiency of area matching. We further improve the performance of area matching through a novel supervision strategy that decomposes the area matching task into classification and ranking subtasks. Finally, we introduce the Hierarchical Containment Redundancy Filter (HCRF) to eliminate overlapping areas by analyzing containment graphs. SGAD demonstrates remarkable performance gains, reducing runtime by 60x(0.82s vs. 60.23s) compared to MESA. Extensive evaluations show consistent improvements across multiple point matchers: SGAD+LoFTR reduces runtime compared to DKM, while achieving higher accuracy (0.82s vs. 1.51s, 65.98 vs. 61.11) in outdoor pose estimation, and SGAD+ROMA delivers +7.39% AUC@5^\\circ in indoor pose estimation, establishing a new state-of-the-art. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF",
      "index": 159,
      "title": "Planar Affine Rectification from Local Change of Scale and Orientation",
      "authors": [
        "Yuval Nissan",
        "Marc Pollefeys",
        "Daniel Barath"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "rectification",
        "orientations",
        "local",
        "orientation",
        "constraint",
        "affine",
        "scales",
        "facades",
        "projective",
        "linear"
      ],
      "summary": "We propose a method for affine rectification of an image plane by leveraging changes in local scales and orientations under projective distortion. Specifically, we derive a novel linear constraint that directly relates pairs of points with orientations to the parameters of a projective transformation. This constraint is combined with an existing linear constraint on local scales, leading to highly robust rectification. The method reduces to solving a system of linear equations, enabling an efficient algebraic least-squares solution. It requires only two local scales and two local orientations, which can be extracted from, e.g., SIFT features. Unlike prior approaches, our method does not impose restrictions on individual features, does not require class segmentation, and makes no assumptions about feature interrelations. It is compatible with any feature detector that provides local scale or orientation. Furthermore, combining scaled and oriented points with line segments yields a highly robust algorithm that outperforms baselines. Extensive experiments show the effectiveness of our approach on real-world images, including repetitive patterns, building facades, and text-based content.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Nissan_Planar_Affine_Rectification_from_Local_Change_of_Scale_and_Orientation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Planar Affine Rectification from Local Change of Scale and Orientation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yuval Nissan , Marc Pollefeys , Daniel Barath We propose a method for affine rectification of an image plane by leveraging changes in local scales and orientations under projective distortion. Specifically, we derive a novel linear constraint that directly relates pairs of points with orientations to the parameters of a projective transformation. This constraint is combined with an existing linear constraint on local scales, leading to highly robust rectification. The method reduces to solving a system of linear equations, enabling an efficient algebraic least-squares solution. It requires only two local scales and two local orientations, which can be extracted from, e.g., SIFT features. Unlike prior approaches, our method does not impose restrictions on individual features, does not require class segmentation, and makes no assumptions about feature interrelations. It is compatible with any feature detector that provides local scale or orientation. Furthermore, combining scaled and oriented points with line segments yields a highly robust algorithm that outperforms baselines. Extensive experiments show the effectiveness of our approach on real-world images, including repetitive patterns, building facades, and text-based content. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF",
      "index": 160,
      "title": "Thermal Polarimetric Multi-view Stereo",
      "authors": [
        "Takahiro Kushida",
        "Kenichiro Tanaka"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "polarimetric",
        "polarization",
        "thermal",
        "lwir",
        "stereo",
        "view",
        "translucent",
        "detailed",
        "ambiguities",
        "reconstructs"
      ],
      "summary": "This paper introduces a novel method for detailed 3D shape reconstruction utilizing thermal polarization cues. Unlike state-of-the-art methods, the proposed approach is independent of illumination and material properties. In this paper, we formulate a general theory of polarization observation and show that long-wave infrared (LWIR) polarimetric imaging is free from the ambiguities that affect visible polarization analyses. Subsequently, we propose a method for recovering detailed 3D shapes using multi-view thermal polarimetric images. Experimental results demonstrate that our approach effectively reconstructs fine details in transparent, translucent, and heterogeneous objects, outperforming existing techniques.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kushida_Thermal_Polarimetric_Multi-view_Stereo_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kushida_Thermal_Polarimetric_Multi-view_Stereo@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kushida_Thermal_Polarimetric_Multi-view_Stereo_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Thermal Polarimetric Multi-view Stereo [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Takahiro Kushida , Kenichiro Tanaka This paper introduces a novel method for detailed 3D shape reconstruction utilizing thermal polarization cues. Unlike state-of-the-art methods, the proposed approach is independent of illumination and material properties. In this paper, we formulate a general theory of polarization observation and show that long-wave infrared (LWIR) polarimetric imaging is free from the ambiguities that affect visible polarization analyses. Subsequently, we propose a method for recovering detailed 3D shapes using multi-view thermal polarimetric images. Experimental results demonstrate that our approach effectively reconstructs fine details in transparent, translucent, and heterogeneous objects, outperforming existing techniques. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF",
      "index": 161,
      "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering",
      "authors": [
        "Michael Steiner",
        "Thomas Khler",
        "Lukas Radl",
        "Felix Windisch",
        "Dieter Schmalstieg",
        "Markus Steinberger"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gaussians",
        "aliasing",
        "popping",
        "rendering",
        "artifact",
        "3dgs",
        "rasterization",
        "artifacts",
        "aaa",
        "aliased"
      ],
      "summary": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction, it still faces challenges such as aliasing, projection artifacts, and view inconsistencies, primarily due to the simplification of treating splats as 2D entities. We argue that incorporating full 3D evaluation of Gaussians throughout the 3DGS pipeline can effectively address these issues while preserving rasterization efficiency. Specifically, we introduce an adaptive 3D smoothing filter to mitigate aliasing and present a stable view-space bounding method that eliminates popping artifacts when Gaussians extend beyond the view frustum. Furthermore, we promote tile-based culling to 3D with screen-space planes, accelerating rendering and reducing sorting costs for hierarchical rasterization. Our method achieves state-of-the-art quality on in-distribution evaluation sets and significantly outperforms other approaches for out-of-distribution views. Our qualitative evaluations further demonstrate the effective removal of aliasing, distortions, and popping artifacts, ensuring real-time, artifact-free rendering.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Steiner_AAA-Gaussians_Anti-Aliased_and_Artifact-Free_3D_Gaussian_Rendering_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Michael Steiner , Thomas Khler , Lukas Radl , Felix Windisch , Dieter Schmalstieg , Markus Steinberger Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction, it still faces challenges such as aliasing, projection artifacts, and view inconsistencies, primarily due to the simplification of treating splats as 2D entities. We argue that incorporating full 3D evaluation of Gaussians throughout the 3DGS pipeline can effectively address these issues while preserving rasterization efficiency. Specifically, we introduce an adaptive 3D smoothing filter to mitigate aliasing and present a stable view-space bounding method that eliminates popping artifacts when Gaussians extend beyond the view frustum. Furthermore, we promote tile-based culling to 3D with screen-space planes, accelerating rendering and reducing sorting costs for hierarchical rasterization. Our method achieves state-of-the-art quality on in-distribution evaluation sets and significantly outperforms other approaches for out-of-distribution views. Our qualitative evaluations further demonstrate the effective removal of aliasing, distortions, and popping artifacts, ensuring real-time, artifact-free rendering. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF",
      "index": 162,
      "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video",
      "authors": [
        "David Stotko",
        "Reinhard Klein"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "monocular",
        "fabrics",
        "reconstruction",
        "video",
        "appearance",
        "saft",
        "differentiable",
        "rendering",
        "rgb",
        "template"
      ],
      "summary": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Stotko_SAFT_Shape_and_Appearance_of_Fabrics_from_Template_via_Differentiable_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video [PDF 1 ] [Copy] [Kimi ] [REL] Authors : David Stotko , Reinhard Klein The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF",
      "index": 163,
      "title": "BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment",
      "authors": [
        "Tongfan Guan",
        "Jiaxin Guo",
        "Chen Wang",
        "Yun-Hui Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "omnidepth",
        "monocular",
        "stereo",
        "alignment",
        "ambiguities",
        "reflective",
        "bridgedepth",
        "surfaces",
        "aeolusguan",
        "reasoning"
      ],
      "summary": "Monocular and stereo depth estimation offer complementary strengths: monocular methods capture rich contextual priors but lack geometric precision, while stereo approaches leverage epipolar geometry yet struggle with ambiguities such as reflective or textureless surfaces. Despite post-hoc synergies, these paradigms remain largely disjoint in practice. We introduce OmniDepth, a unified framework that bridges both through iterative bidirectional alignment of their latent representations. At its core, a novel cross-attentive alignment mechanism dynamically synchronizes monocular contextual cues with stereo hypothesis representations during stereo reasoning. This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by injecting monocular structure priors while refining monocular depth with stereo geometry within a single network.Extensive experiments demonstrate state-of-the-art results: OmniDepth reduces zero-shot generalization error by \\!>\\!40% on Middlebury and ETH3D, while addressing longstanding failures on transparent and reflective surfaces. By harmonizing multi-view geometry with monocular context, OmniDepth enables robust 3D perception that transcends modality-specific limitations. Codes available at https://github.com/aeolusguan/OmniDepth.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Guan_BridgeDepth_Bridging_Monocular_and_Stereo_Reasoning_with_Latent_Alignment_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "BridgeDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Tongfan Guan , Jiaxin Guo , Chen Wang , Yun-Hui Liu Monocular and stereo depth estimation offer complementary strengths: monocular methods capture rich contextual priors but lack geometric precision, while stereo approaches leverage epipolar geometry yet struggle with ambiguities such as reflective or textureless surfaces. Despite post-hoc synergies, these paradigms remain largely disjoint in practice. We introduce OmniDepth, a unified framework that bridges both through iterative bidirectional alignment of their latent representations. At its core, a novel cross-attentive alignment mechanism dynamically synchronizes monocular contextual cues with stereo hypothesis representations during stereo reasoning. This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by injecting monocular structure priors while refining monocular depth with stereo geometry within a single network.Extensive experiments demonstrate state-of-the-art results: OmniDepth reduces zero-shot generalization error by \\!>\\!40% on Middlebury and ETH3D, while addressing longstanding failures on transparent and reflective surfaces. By harmonizing multi-view geometry with monocular context, OmniDepth enables robust 3D perception that transcends modality-specific limitations. Codes available at https://github.com/aeolusguan/OmniDepth. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF",
      "index": 164,
      "title": "FlowR: Flowing from Sparse to Dense 3D Reconstructions",
      "authors": [
        "Tobias Fischer",
        "Samuel Rota Bul",
        "Yung-Hsu Yang",
        "Nikhil Keetha",
        "Lorenzo Porzi",
        "Norman Mller",
        "Katja Schwarz",
        "Jonathon Luiten",
        "Marc Pollefeys",
        "Peter Kontschieder"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "reconstructions",
        "nvs",
        "views",
        "dense",
        "flowr",
        "quality",
        "captures",
        "renderings",
        "sparse",
        "view"
      ],
      "summary": "3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of applications like Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These models typically rely on a noise-to-data generative process conditioned only on a handful of reference input views, leading to hallucinations, inconsistent generation results, and subsequent reconstruction artifacts. Instead, we propose a multi-view, flow matching model that learns a flow to directly connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with consistent, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fischer_FlowR_Flowing_from_Sparse_to_Dense_3D_Reconstructions_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "FlowR: Flowing from Sparse to Dense 3D Reconstructions [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Tobias Fischer , Samuel Rota Bul , Yung-Hsu Yang , Nikhil Keetha , Lorenzo Porzi , Norman Mller , Katja Schwarz , Jonathon Luiten , Marc Pollefeys , Peter Kontschieder 3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of applications like Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These models typically rely on a noise-to-data generative process conditioned only on a handful of reference input views, leading to hallucinations, inconsistent generation results, and subsequent reconstruction artifacts. Instead, we propose a multi-view, flow matching model that learns a flow to directly connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with consistent, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF",
      "index": 165,
      "title": "NeuFrameQ: Neural Frame Fields for Scalable and Generalizable Anisotropic Quadrangulation",
      "authors": [
        "Ying-Tian Liu",
        "Jiajun Li",
        "Yu-Tao Liu",
        "Xin Yu",
        "Yuan-Chen Guo",
        "Yan-Pei Cao",
        "Ding Liang",
        "Ariel Shamir",
        "Song-Hai Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "quadrangulation",
        "neuframeq",
        "quad",
        "meshes",
        "frame",
        "mesh",
        "generalizable",
        "scalable",
        "quality",
        "field"
      ],
      "summary": "Quad meshes play a crucial role in computer graphics applications, yet automatically generating high-quality quad meshes remains challenging. Traditional quadrangulation approaches rely on local geometric features and manual constraints, often producing suboptimal mesh layouts that fail to capture global shape semantics. We introduce NeuFrameQ, a novel learning-based framework for scalable and generalizable mesh quadrangulation via frame field prediction. We first create a large-scale dataset of high-quality quad meshes with various shapes to serve as priors of domain knowledge. Empowered by this dataset, we employ a connectivity-agnostic learning approach that operates on point clouds with normals, enabling robust processing of complex geometries. By decomposing frame field prediction into direction regression and magnitude estimation tasks, we effectively handle the ill-posed nature in frame field estimation. We also employ the polyvector representation and attention mechanism in both tasks to handle the inherent ambiguities in frame field representation. Extensive experiments demonstrate that NeuFrameQ produces high-quality quad meshes with superior semantic alignment, also for geometries derived from neural fields. Our method significantly advances the state of the art in automatic quad mesh generation, bridging the gap between neural content creation and production-ready geometric assets.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_NeuFrameQ_Neural_Frame_Fields_for_Scalable_and_Generalizable_Anisotropic_Quadrangulation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "NeuFrameQ: Neural Frame Fields for Scalable and Generalizable Anisotropic Quadrangulation [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Ying-Tian Liu , Jiajun Li , Yu-Tao Liu , Xin Yu , Yuan-Chen Guo , Yan-Pei Cao , Ding Liang , Ariel Shamir , Song-Hai Zhang Quad meshes play a crucial role in computer graphics applications, yet automatically generating high-quality quad meshes remains challenging. Traditional quadrangulation approaches rely on local geometric features and manual constraints, often producing suboptimal mesh layouts that fail to capture global shape semantics. We introduce NeuFrameQ, a novel learning-based framework for scalable and generalizable mesh quadrangulation via frame field prediction. We first create a large-scale dataset of high-quality quad meshes with various shapes to serve as priors of domain knowledge. Empowered by this dataset, we employ a connectivity-agnostic learning approach that operates on point clouds with normals, enabling robust processing of complex geometries. By decomposing frame field prediction into direction regression and magnitude estimation tasks, we effectively handle the ill-posed nature in frame field estimation. We also employ the polyvector representation and attention mechanism in both tasks to handle the inherent ambiguities in frame field representation. Extensive experiments demonstrate that NeuFrameQ produces high-quality quad meshes with superior semantic alignment, also for geometries derived from neural fields. Our method significantly advances the state of the art in automatic quad mesh generation, bridging the gap between neural content creation and production-ready geometric assets. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF",
      "index": 166,
      "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement",
      "authors": [
        "Yang Yang",
        "Dongni Mao",
        "Hiroaki Santo",
        "Yasuyuki Matsushita",
        "Fumio Okura"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "leaf",
        "neuraleaf",
        "shapes",
        "leaves",
        "parametric",
        "deformation",
        "skinning",
        "plant",
        "modeling",
        "deformations"
      ],
      "summary": "We develop a neural parametric model for 3D plant leaves for modeling and reconstruction of plants that are essential for agriculture and computer graphics. While parametric modeling has been actively studied for human and animal shapes, plant leaves present unique challenges due to their diverse shapes and flexible deformation, making common approaches inapplicable. To this problem, we introduce a learning-based parametric model, NeuraLeaf, disentangling the leaves' geometry into their 2D base shapes and 3D deformations. Since the base shapes represent flattened 2D leaves, it allows learning from rich sources of 2D leaf image datasets, and also has the advantage of simultaneously learning texture aligned with the geometry. To model the 3D deformation, we propose a novel skeleton-free skinning model and a newly captured 3D leaf dataset called DeformLeaf. We establish a parametric deformation space by converting the sample-wise skinning parameters into a compact latent representation, allowing for flexible and efficient modeling of leaf deformations. We show that NeuraLeaf successfully generates a wide range of leaf shapes with deformation, resulting in accurate model fitting to 3D observations like depth maps and point clouds. Our implementation and datasets will be released upon acceptance.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_NeuraLeaf_Neural_Parametric_Leaf_Models_with_Shape_and_Deformation_Disentanglement_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yang Yang , Dongni Mao , Hiroaki Santo , Yasuyuki Matsushita , Fumio Okura We develop a neural parametric model for 3D plant leaves for modeling and reconstruction of plants that are essential for agriculture and computer graphics. While parametric modeling has been actively studied for human and animal shapes, plant leaves present unique challenges due to their diverse shapes and flexible deformation, making common approaches inapplicable. To this problem, we introduce a learning-based parametric model, NeuraLeaf, disentangling the leaves' geometry into their 2D base shapes and 3D deformations. Since the base shapes represent flattened 2D leaves, it allows learning from rich sources of 2D leaf image datasets, and also has the advantage of simultaneously learning texture aligned with the geometry. To model the 3D deformation, we propose a novel skeleton-free skinning model and a newly captured 3D leaf dataset called DeformLeaf. We establish a parametric deformation space by converting the sample-wise skinning parameters into a compact latent representation, allowing for flexible and efficient modeling of leaf deformations. We show that NeuraLeaf successfully generates a wide range of leaf shapes with deformation, resulting in accurate model fitting to 3D observations like depth maps and point clouds. Our implementation and datasets will be released upon acceptance. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF",
      "index": 167,
      "title": "Stochastic Gradient Estimation for Higher-Order Differentiable Rendering",
      "authors": [
        "Zican Wang",
        "Michael Fischer",
        "Tobias Ritschel"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "rendering",
        "differentials",
        "gradient",
        "higher",
        "order",
        "hessians",
        "rasterization",
        "differentiable",
        "optimizers",
        "hessian"
      ],
      "summary": "We derive methods to compute higher order differentials (Hessians and Hessian-vector products) of the rendering operator. Our approach is based on importance sampling of a convolution that represents the differentials of rendering parameters and shows to be applicable to both rasterization and path tracing. We demonstrate that this information improves convergence when used in higher-order optimizers such as Newton or Conjugate Gradient relative to a gradient descent baseline in several inverse rendering tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Stochastic_Gradient_Estimation_for_Higher-Order_Differentiable_Rendering_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Stochastic Gradient Estimation for Higher-Order Differentiable Rendering [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Zican Wang , Michael Fischer , Tobias Ritschel We derive methods to compute higher order differentials (Hessians and Hessian-vector products) of the rendering operator. Our approach is based on importance sampling of a convolution that represents the differentials of rendering parameters and shows to be applicable to both rasterization and path tracing. We demonstrate that this information improves convergence when used in higher-order optimizers such as Newton or Conjugate Gradient relative to a gradient descent baseline in several inverse rendering tasks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF",
      "index": 168,
      "title": "HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Models",
      "authors": [
        "Yiwen Chen",
        "Hieu T. Nguyen",
        "Vikram Voleti",
        "Varun Jampani",
        "Huaizu Jiang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "housecrafter",
        "floorplan",
        "images",
        "diffusion",
        "locations",
        "floorplans",
        "house",
        "scenes",
        "scene",
        "rgb"
      ],
      "summary": "We introduce HouseCrafter, a novel approach that can lift a 2D floorplan into a complete large 3D indoor scene (e.g., a house). Our key insight is to adapt a 2D diffusion model, which is trained on web-scale images, to generate consistent multi-view color (RGB) and depth (D) images across different locations of the scene. Specifically, the RGB-D images are generated autoregressively in batches along sampled locations derived from the floorplan. At each step, the diffusion model conditions on previously generated images to produce new images at nearby locations. The global floorplan and attention design in the diffusion model ensures the consistency of the generated images, from which a 3D scene can be reconstructed. Through extensive evaluation on the 3D-FRONT dataset, we demonstrate that HouseCrafter can generate high-quality house-scale 3D scenes. Ablation studies also validate the effectiveness of different design choices. We will release our code and model weights.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_HouseCrafter_Lifting_Floorplans_to_3D_Scenes_with_2D_Diffusion_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Models [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Yiwen Chen , Hieu T. Nguyen , Vikram Voleti , Varun Jampani , Huaizu Jiang We introduce HouseCrafter, a novel approach that can lift a 2D floorplan into a complete large 3D indoor scene (e.g., a house). Our key insight is to adapt a 2D diffusion model, which is trained on web-scale images, to generate consistent multi-view color (RGB) and depth (D) images across different locations of the scene. Specifically, the RGB-D images are generated autoregressively in batches along sampled locations derived from the floorplan. At each step, the diffusion model conditions on previously generated images to produce new images at nearby locations. The global floorplan and attention design in the diffusion model ensures the consistency of the generated images, from which a 3D scene can be reconstructed. Through extensive evaluation on the 3D-FRONT dataset, we demonstrate that HouseCrafter can generate high-quality house-scale 3D scenes. Ablation studies also validate the effectiveness of different design choices. We will release our code and model weights. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF",
      "index": 169,
      "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning",
      "authors": [
        "Giwon Lee",
        "Wooseong Jeong",
        "Daehee Park",
        "Jaewoo Jeong",
        "Kuk-Jin Yoon"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "planning",
        "immp",
        "domain",
        "merged",
        "motion",
        "diverse",
        "datasets",
        "target",
        "interactions",
        "merging"
      ],
      "summary": "Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lee_Interaction-Merged_Motion_Planning_Effectively_Leveraging_Diverse_Motion_Datasets_for_Robust_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning [PDF ] [Copy] [Kimi ] [REL] Authors : Giwon Lee , Wooseong Jeong , Daehee Park , Jaewoo Jeong , Kuk-Jin Yoon Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF",
      "index": 170,
      "title": "Lidar Waveforms are Worth 40x128x33 Words",
      "authors": [
        "Dominik Scheuble",
        "Hanno Holzhter",
        "Steven Peters",
        "Mario Bijelic",
        "Felix Heide"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lidar",
        "waveforms",
        "dsp",
        "automotive",
        "fog",
        "40x128x33",
        "clouds",
        "waveform",
        "32cm",
        "conventional"
      ],
      "summary": "Lidar has become crucial for autonomous driving, providing high-resolution 3D scans that are key for accurate scene understanding. To this end, lidar sensors measure the time-resolved full waveforms from the returning laser light, which a subsequent digital signal processor (DSP) converts to point clouds by identifying peaks in the waveform. Conventional automotive lidar DSPs process each waveform individually, ignoring potentially valuable context from neighboring waveforms. As a result, lidar point clouds are prone to artifacts from low signal-to-noise ratio (SNR) regions, highly reflective objects, and environmental conditions like fog. While leveraging neighboring waveforms is investigated extensively in transient imaging, applications remain limited to scientific or experimental hardware. In this work, we propose a learned DSP that directly processes full waveforms using a transformer architecture, leveraging features from adjacent waveforms to generate high-fidelity multi-echo point clouds. To assess our method, we capture data in real-world driving scenarios and a weather chamber with a conventional automotive lidar. Trained on synthetic and real data, the method improves Chamfer distance by 32cm and 20cm compared to conventional peak finding and existing transient imaging approaches, respectively. This translates to maximum range improvements of up to 17m in fog and 14m in nominal real-world conditions.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Lidar Waveforms are Worth 40x128x33 Words [PDF ] [Copy] [Kimi ] [REL] Authors : Dominik Scheuble , Hanno Holzhter , Steven Peters , Mario Bijelic , Felix Heide Lidar has become crucial for autonomous driving, providing high-resolution 3D scans that are key for accurate scene understanding. To this end, lidar sensors measure the time-resolved full waveforms from the returning laser light, which a subsequent digital signal processor (DSP) converts to point clouds by identifying peaks in the waveform. Conventional automotive lidar DSPs process each waveform individually, ignoring potentially valuable context from neighboring waveforms. As a result, lidar point clouds are prone to artifacts from low signal-to-noise ratio (SNR) regions, highly reflective objects, and environmental conditions like fog. While leveraging neighboring waveforms is investigated extensively in transient imaging, applications remain limited to scientific or experimental hardware. In this work, we propose a learned DSP that directly processes full waveforms using a transformer architecture, leveraging features from adjacent waveforms to generate high-fidelity multi-echo point clouds. To assess our method, we capture data in real-world driving scenarios and a weather chamber with a conventional automotive lidar. Trained on synthetic and real data, the method improves Chamfer distance by 32cm and 20cm compared to conventional peak finding and existing transient imaging approaches, respectively. This translates to maximum range improvements of up to 17m in fog and 14m in nominal real-world conditions. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF",
      "index": 171,
      "title": "LBM: Latent Bridge Matching for Fast Image-to-Image Translation",
      "authors": [
        "Clment Chadebec",
        "Onur Tasar",
        "Sanjeev Sreetharan",
        "Benjamin Aubin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lbm",
        "image",
        "bridge",
        "relighting",
        "latent",
        "matching",
        "translation",
        "tasks",
        "fast",
        "object"
      ],
      "summary": "In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chadebec_LBM_Latent_Bridge_Matching_for_Fast_Image-to-Image_Translation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "LBM: Latent Bridge Matching for Fast Image-to-Image Translation [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Clment Chadebec , Onur Tasar , Sanjeev Sreetharan , Benjamin Aubin In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF",
      "index": 172,
      "title": "Super Resolved Imaging with Adaptive Optics",
      "authors": [
        "Robin Swanson",
        "Esther Y. H. Lin",
        "Masen Lamb",
        "Suresh Sivanandam",
        "Kiriakos N. Kutulakos"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "distortions",
        "optics",
        "wavefront",
        "telescope",
        "super",
        "fov",
        "tradeoff",
        "resolved",
        "telescopes",
        "mirror"
      ],
      "summary": "Astronomical telescopes suffer from a tradeoff between field-of-view (FoV) and image resolution: increasing the FoV leads to an optical field that is under-sampled by the science camera. This work presents a novel computational imaging approach to overcome this tradeoff by leveraging the existing adaptive optics (AO) systems in modern ground-based telescopes. Our key idea is to use the AO system's deformable mirror to apply a series of learned, precisely controlled distortions to the optical wavefront, producing a sequence of images that exhibit distinct, high-frequency, sub-pixel shifts. These images can then be jointly upsampled to yield the final super-resolved image. Crucially, we show this can be done while simultaneously maintaining the core AO operation --- correcting for the unknown and rapidly changing wavefront distortions caused by Earth's atmosphere. To achieve this, we incorporate end-to-end optimization of both the induced mirror distortions and the upsampling algorithm, such that telescope-specific optics and temporal statistics of atmospheric wavefront distortions are accounted for. Our experimental results with a hardware prototype, as well as simulations, demonstrate significant SNR improvements of up to 12 dB over non-AO super-resolution baselines, using only existing telescope optics and no hardware modifications. Moreover, by using a precise bench-top replica of a complete telescope and AO system, we show that our methodology can be readily transferred to an operational telescope.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Swanson_Super_Resolved_Imaging_with_Adaptive_Optics_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Super Resolved Imaging with Adaptive Optics [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Robin Swanson , Esther Y. H. Lin , Masen Lamb , Suresh Sivanandam , Kiriakos N. Kutulakos Astronomical telescopes suffer from a tradeoff between field-of-view (FoV) and image resolution: increasing the FoV leads to an optical field that is under-sampled by the science camera. This work presents a novel computational imaging approach to overcome this tradeoff by leveraging the existing adaptive optics (AO) systems in modern ground-based telescopes. Our key idea is to use the AO system's deformable mirror to apply a series of learned, precisely controlled distortions to the optical wavefront, producing a sequence of images that exhibit distinct, high-frequency, sub-pixel shifts. These images can then be jointly upsampled to yield the final super-resolved image. Crucially, we show this can be done while simultaneously maintaining the core AO operation --- correcting for the unknown and rapidly changing wavefront distortions caused by Earth's atmosphere. To achieve this, we incorporate end-to-end optimization of both the induced mirror distortions and the upsampling algorithm, such that telescope-specific optics and temporal statistics of atmospheric wavefront distortions are accounted for. Our experimental results with a hardware prototype, as well as simulations, demonstrate significant SNR improvements of up to 12 dB over non-AO super-resolution baselines, using only existing telescope optics and no hardware modifications. Moreover, by using a precise bench-top replica of a complete telescope and AO system, we show that our methodology can be readily transferred to an operational telescope. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF",
      "index": 173,
      "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization",
      "authors": [
        "Ju-Hyeon Nam",
        "Dong-Hyun Moon",
        "Sang-Chul Lee"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "m2sformer",
        "forgery",
        "difficulty",
        "localization",
        "multi",
        "subtle",
        "attention",
        "forgeries",
        "guidance",
        "tampering"
      ],
      "summary": "Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map--a curvature metric indicating the difficulty of forgery localization--which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains. Our M2SFormer code is available in Github Link.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Nam_M2SFormer_Multi-Spectral_and_Multi-Scale_Attention_with_Edge-Aware_Difficulty_Guidance_for_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Ju-Hyeon Nam , Dong-Hyun Moon , Sang-Chul Lee Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map--a curvature metric indicating the difficulty of forgery localization--which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains. Our M2SFormer code is available in Github Link. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF",
      "index": 174,
      "title": "ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives",
      "authors": [
        "Yuqian Fu",
        "Runze Wang",
        "Bin Ren",
        "Guolei Sun",
        "Biao Gong",
        "Yanwei Fu",
        "Danda Pani Paudel",
        "Xuanjing Huang",
        "Luc Van Gool"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ego",
        "objectrelator",
        "exo",
        "object",
        "centric",
        "psalm",
        "view",
        "cross",
        "segmentation",
        "mcfuse"
      ],
      "summary": "Bridging the gap between ego-centric and exo-centric views has been a long-standing question in computer vision. In this paper, we focus on the emerging Ego-Exo object correspondence task, which aims to understand object relations across ego-exo perspectives through segmentation. While numerous segmentation models have been proposed, most operate on a single image (view), making them impractical for cross-view scenarios. PSALM, a recently proposed segmentation method, stands out as a notable exception with its demonstrated zero-shot ability on this task. However, due to the drastic viewpoint change between ego and exo, PSALM fails to accurately locate and segment objects, especially in complex backgrounds or when object appearances change significantly. To address these issues, we propose ObjectRelator, a novel approach featuring two key modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language as an additional cue, integrating both visual masks and textual descriptions to improve object localization and prevent incorrect associations. XObjAlign enforces cross-view consistency through self-supervised alignment, enhancing robustness to object appearance variations. Extensive experiments demonstrate ObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and HANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art performance. Code is available at: http://yuqianfu.com/ObjectRelator.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fu_ObjectRelator_Enabling_Cross-View_Object_Relation_Understanding_Across_Ego-Centric_and_Exo-Centric_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Yuqian Fu , Runze Wang , Bin Ren , Guolei Sun , Biao Gong , Yanwei Fu , Danda Pani Paudel , Xuanjing Huang , Luc Van Gool Bridging the gap between ego-centric and exo-centric views has been a long-standing question in computer vision. In this paper, we focus on the emerging Ego-Exo object correspondence task, which aims to understand object relations across ego-exo perspectives through segmentation. While numerous segmentation models have been proposed, most operate on a single image (view), making them impractical for cross-view scenarios. PSALM, a recently proposed segmentation method, stands out as a notable exception with its demonstrated zero-shot ability on this task. However, due to the drastic viewpoint change between ego and exo, PSALM fails to accurately locate and segment objects, especially in complex backgrounds or when object appearances change significantly. To address these issues, we propose ObjectRelator, a novel approach featuring two key modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language as an additional cue, integrating both visual masks and textual descriptions to improve object localization and prevent incorrect associations. XObjAlign enforces cross-view consistency through self-supervised alignment, enhancing robustness to object appearance variations. Extensive experiments demonstrate ObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and HANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art performance. Code is available at: http://yuqianfu.com/ObjectRelator. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF",
      "index": 175,
      "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts",
      "authors": [
        "Zixuan Hu",
        "Dongxiao Li",
        "Xinzhu Ma",
        "Shixiang Tang",
        "Xiaotong Li",
        "Wenhan Yang",
        "Ling-Yu Duan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "uncertainty",
        "m3od",
        "duo",
        "tta",
        "shifts",
        "semantic",
        "dual",
        "monocular",
        "optimization",
        "spatial"
      ],
      "summary": "Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel conjugate loss, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types. The source code is available at https://github.com/hzcar/DUO.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_Adaptive_Dual_Uncertainty_Optimization_Boosting_Monocular_3D_Object_Detection_under_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts [PDF ] [Copy] [Kimi ] [REL] Authors : Zixuan Hu , Dongxiao Li , Xinzhu Ma , Shixiang Tang , Xiaotong Li , Wenhan Yang , Ling-Yu Duan Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel conjugate loss, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types. The source code is available at https://github.com/hzcar/DUO. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF",
      "index": 176,
      "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation",
      "authors": [
        "Junsong Chen",
        "Shuchen Xue",
        "Yuyang Zhao",
        "Jincheng Yu",
        "Sayak Paul",
        "Junyu Chen",
        "Han Cai",
        "Song Han",
        "Enze Xie"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sana",
        "sprint",
        "distillation",
        "t2i",
        "scm",
        "ladd",
        "geneval",
        "step",
        "h100",
        "controlnet"
      ],
      "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4.We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in just 1 step -- outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024x1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_SANA-Sprint_One-Step_Diffusion_with_Continuous-Time_Consistency_Distillation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Junsong Chen , Shuchen Xue , Yuyang Zhao , Jincheng Yu , Sayak Paul , Junyu Chen , Han Cai , Song Han , Enze Xie This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4.We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in just 1 step -- outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024x1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF",
      "index": 177,
      "title": "Beyond Losses Reweighting: Empowering Multi-Task Learning via the Generalization Perspective",
      "authors": [
        "Hoang Phan",
        "Lam Tran",
        "Quyen Tran",
        "Ngoc Tran",
        "Tuan Truong",
        "Qi Lei",
        "Nhat Ho",
        "Dinh Phung",
        "Trung Le"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mtl",
        "task",
        "gradient",
        "conflicts",
        "weight",
        "generalization",
        "reweighting",
        "empowering",
        "harmonizes",
        "perturbation"
      ],
      "summary": "Multi-task learning (MTL) trains deep neural networks to optimize several objectives simultaneously using a shared backbone, which leads to reduced computational costs, improved data efficiency, and enhanced performance through cross-task knowledge sharing. Although recent gradient manipulation techniques seek a common descent direction to benefit all tasks, conventional empirical loss minimization still leaves models prone to overfitting and gradient conflicts. To address this, we introduce a novel MTL framework that leverages weight perturbation to regulate gradient norms. thus improve generalization. By carefully modulating weight perturbations, our approach harmonizes task-specific gradients, reducing conflicts and encouraging more robust learning across tasks. Theoretical insights reveal that controlling the gradient norm through weight perturbation directly contributes to better generalization. Extensive experiments across diverse applications demonstrate that our method significantly outperforms existing gradient-based MTL techniques in terms of task performance and overall model robustness.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Phan_Beyond_Losses_Reweighting_Empowering_Multi-Task_Learning_via_the_Generalization_Perspective_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Beyond Losses Reweighting: Empowering Multi-Task Learning via the Generalization Perspective [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hoang Phan , Lam Tran , Quyen Tran , Ngoc Tran , Tuan Truong , Qi Lei , Nhat Ho , Dinh Phung , Trung Le Multi-task learning (MTL) trains deep neural networks to optimize several objectives simultaneously using a shared backbone, which leads to reduced computational costs, improved data efficiency, and enhanced performance through cross-task knowledge sharing. Although recent gradient manipulation techniques seek a common descent direction to benefit all tasks, conventional empirical loss minimization still leaves models prone to overfitting and gradient conflicts. To address this, we introduce a novel MTL framework that leverages weight perturbation to regulate gradient norms. thus improve generalization. By carefully modulating weight perturbations, our approach harmonizes task-specific gradients, reducing conflicts and encouraging more robust learning across tasks. Theoretical insights reveal that controlling the gradient norm through weight perturbation directly contributes to better generalization. Extensive experiments across diverse applications demonstrate that our method significantly outperforms existing gradient-based MTL techniques in terms of task performance and overall model robustness. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF",
      "index": 178,
      "title": "Consensus-Driven Active Model Selection",
      "authors": [
        "Justin Kay",
        "Grant Van Horn",
        "Subhransu Maji",
        "Daniel Sheldon",
        "Sara Beery"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "coda",
        "selection",
        "consensus",
        "model",
        "active",
        "candidate",
        "justinkay",
        "best",
        "driven",
        "data"
      ],
      "summary": "The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a validation dataset---a costly and time-intensive process. We propose a method for active model selection, using predictions from candidate models to prioritize the labeling of test data points that efficiently differentiate the best candidate. Our method, CODA, performs consensus-driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilistic framework. The framework uses the consensus and disagreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to update beliefs about which model is best as more information is collected. We validate our approach by curating a collection of 26 benchmark tasks capturing a range of model selection scenarios.CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art. We will make our code and data public. Code and data are available at https://github.com/justinkay/coda.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kay_Consensus-Driven_Active_Model_Selection_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kay_Consensus-Driven_Active_Model_Selection@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kay_Consensus-Driven_Active_Model_Selection_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Consensus-Driven Active Model Selection [PDF ] [Copy] [Kimi ] [REL] Authors : Justin Kay , Grant Van Horn , Subhransu Maji , Daniel Sheldon , Sara Beery The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a validation dataset---a costly and time-intensive process. We propose a method for active model selection, using predictions from candidate models to prioritize the labeling of test data points that efficiently differentiate the best candidate. Our method, CODA, performs consensus-driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilistic framework. The framework uses the consensus and disagreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to update beliefs about which model is best as more information is collected. We validate our approach by curating a collection of 26 benchmark tasks capturing a range of model selection scenarios.CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art. We will make our code and data public. Code and data are available at https://github.com/justinkay/coda. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF",
      "index": 179,
      "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors",
      "authors": [
        "Zheyuan Zhang",
        "Weihao Tang",
        "Hong Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "causalnet",
        "frame",
        "key",
        "mer",
        "indexes",
        "accurate",
        "micro",
        "recognition",
        "expression",
        "errors"
      ],
      "summary": "Micro-expression recognition (MER) is a highly challenging task in affective computing. With the reduced-sized micro-expression (ME) input that contains key information based on key-frame indexes, key-frame-based methods have significantly improved the performance of MER. However, most of these methods focus on improving the performance with relatively accurate key-frame indexes, while ignoring the difficulty of obtaining accurate key-frame indexes and the objective existence of key-frame index errors, which impedes them from moving towards practical applications. In this paper, we propose CausalNet, a novel framework to achieve robust MER facing key-frame index errors while maintaining accurate recognition. To enhance robustness, CausalNet takes the representation of the entire ME sequence as the input. To address the information redundancy brought by the complete ME range input and maintain accurate recognition, first, the Causal Motion Position Learning Module (CMPLM) is proposed to help the model locate the muscle movement areas related to Action Units (AUs), thereby reducing the attention to other redundant areas. Second, the Causal Attention Block (CAB) is proposed to deeply learn the causal relationships between the muscle contraction and relaxation movements in MEs. Empirical experiments have demonstrated that on popular ME benchmarks, the CausalNet has achieved robust MER under different levels of key-frame index noise. Meanwhile, it has surpassed state-of-the-art (SOTA) methods on several standard MER benchmarks when using the provided annotated key-frames. Code is available at https://github.com/tony19980810/CausalNet.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Rethinking_Key-frame-based_Micro-expression_Recognition_A_Robust_and_Accurate_Framework_Against_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Zheyuan Zhang , Weihao Tang , Hong Chen Micro-expression recognition (MER) is a highly challenging task in affective computing. With the reduced-sized micro-expression (ME) input that contains key information based on key-frame indexes, key-frame-based methods have significantly improved the performance of MER. However, most of these methods focus on improving the performance with relatively accurate key-frame indexes, while ignoring the difficulty of obtaining accurate key-frame indexes and the objective existence of key-frame index errors, which impedes them from moving towards practical applications. In this paper, we propose CausalNet, a novel framework to achieve robust MER facing key-frame index errors while maintaining accurate recognition. To enhance robustness, CausalNet takes the representation of the entire ME sequence as the input. To address the information redundancy brought by the complete ME range input and maintain accurate recognition, first, the Causal Motion Position Learning Module (CMPLM) is proposed to help the model locate the muscle movement areas related to Action Units (AUs), thereby reducing the attention to other redundant areas. Second, the Causal Attention Block (CAB) is proposed to deeply learn the causal relationships between the muscle contraction and relaxation movements in MEs. Empirical experiments have demonstrated that on popular ME benchmarks, the CausalNet has achieved robust MER under different levels of key-frame index noise. Meanwhile, it has surpassed state-of-the-art (SOTA) methods on several standard MER benchmarks when using the provided annotated key-frames. Code is available at https://github.com/tony19980810/CausalNet. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF",
      "index": 180,
      "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
      "authors": [
        "Ruining Li",
        "Chuanxia Zheng",
        "Christian Rupprecht",
        "Andrea Vedaldi"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dso",
        "generator",
        "feedback",
        "objects",
        "dpo",
        "dro",
        "aligning",
        "optimization",
        "generators",
        "simulation"
      ],
      "summary": "Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO)---a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_DSO_Aligning_3D_Generators_with_Simulation_Feedback_for_Physical_Soundness_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ruining Li , Chuanxia Zheng , Christian Rupprecht , Andrea Vedaldi Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO)---a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF",
      "index": 181,
      "title": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation",
      "authors": [
        "Ziyu Zhu",
        "Xilin Wang",
        "Yixuan Li",
        "Zhuofan Zhang",
        "Xiaojian Ma",
        "Yixin Chen",
        "Baoxiong Jia",
        "Wei Liang",
        "Qian Yu",
        "Zhidong Deng",
        "Siyuan Huang",
        "Qing Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "grounding",
        "embodied",
        "mtu3d",
        "navigation",
        "exploration",
        "understand",
        "bridging",
        "language",
        "move",
        "vision"
      ],
      "summary": "Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce Move to Understand (MTU3D), a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploration that represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines Vision-Language-Exploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14%, 23%, 9%, and 2% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. MTU3D's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. Additionally, we deploy it on a real robot to demonstrate its effectiveness in handling real-world data. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Move_to_Understand_a_3D_Scene_Bridging_Visual_Grounding_and_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Ziyu Zhu , Xilin Wang , Yixuan Li , Zhuofan Zhang , Xiaojian Ma , Yixin Chen , Baoxiong Jia , Wei Liang , Qian Yu , Zhidong Deng , Siyuan Huang , Qing Li Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce Move to Understand (MTU3D), a unified framework that integrates active perception with 3D vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploration that represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines Vision-Language-Exploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14%, 23%, 9%, and 2% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. MTU3D's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. Additionally, we deploy it on a real robot to demonstrate its effectiveness in handling real-world data. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_Video_Motion_Graphs@ICCV2025@CVF",
      "index": 182,
      "title": "Video Motion Graphs",
      "authors": [
        "Haiyang Liu",
        "Zhan Xu",
        "Fa-Ting Hong",
        "Hsin-Ping Huang",
        "Yi Zhou",
        "Yang Zhou"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "video",
        "motion",
        "interpolation",
        "hminterp",
        "frame",
        "graphs",
        "human",
        "videos",
        "frames",
        "vfi"
      ],
      "summary": "We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Our Video Motion Graphs outperforms existing generative- and retrieval-based methods for human motion video generation. Our codes and pretrained models are public available.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Video_Motion_Graphs_ICCV_2025_paper.html",
          "/venue/Liu_Video_Motion_Graphs@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Video_Motion_Graphs_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Liu_Video_Motion_Graphs@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Video_Motion_Graphs_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Video Motion Graphs [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Haiyang Liu , Zhan Xu , Fa-Ting Hong , Hsin-Ping Huang , Yi Zhou , Yang Zhou We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Our Video Motion Graphs outperforms existing generative- and retrieval-based methods for human motion video generation. Our codes and pretrained models are public available. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF",
      "index": 183,
      "title": "GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting",
      "authors": [
        "Xiaobao Wei",
        "Peng Chen",
        "Guangyu Li",
        "Ming Lu",
        "Hui Chen",
        "Feng Tian"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "gaze",
        "gazegaussian",
        "3dgs",
        "redirection",
        "eye",
        "splatting",
        "nerf",
        "ucwxb",
        "fidelity",
        "generalization"
      ],
      "summary": "Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: https://ucwxb.github.io/GazeGaussian",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_GazeGaussian_High-Fidelity_Gaze_Redirection_with_3D_Gaussian_Splatting_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Xiaobao Wei , Peng Chen , Guangyu Li , Ming Lu , Hui Chen , Feng Tian Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: https://ucwxb.github.io/GazeGaussian Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF",
      "index": 184,
      "title": "Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features",
      "authors": [
        "Liying Yang",
        "Chen Liu",
        "Zhenwei Zhu",
        "Ajian Liu",
        "Hui Ma",
        "Jian Nong",
        "Yanyan Liang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dynamic",
        "static",
        "features",
        "ds4d",
        "regions",
        "frame",
        "decoupling",
        "video",
        "liyingcv",
        "dsfd"
      ],
      "summary": "Recently, the generation of dynamic 3D objects from a video has shown impressive results. Existing methods directly optimize Gaussians using whole information in frames. However, when dynamic regions are interwoven with static regions within frames, particularly if the static regions account for a large proportion, existing methods often overlook information in dynamic regions and are prone to overfitting on static regions. This leads to producing results with blurry textures. We consider that decoupling dynamic-static features to enhance dynamic representations can alleviate this issue. Thus, we propose a dynamic-static feature decoupling module (DSFD). Along temporal axes, it regards the regions of current frame features that possess significant differences relative to reference frame features as dynamic features. Conversely, the remaining parts are the static features. Then, we acquire decoupled features driven by dynamic features and current frame features. Moreover, to further enhance the dynamic representation of decoupled features from different viewpoints and ensure accurate motion prediction, we design a temporal-spatial similarity fusion module (TSSF). Along spatial axes, it adaptively selects similar information of dynamic regions. Hinging on the above, we construct a novel approach, DS4D. Experimental results verify our method achieves state-of-the-art (SOTA) results in video-to-4D. In addition, the experiments on a real-world scenario dataset demonstrate its effectiveness on the 4D scene. Project page: https://github.com/LiyingCV/DS4D.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling_ICCV_2025_paper.html",
          "/venue/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Not_All_Frame_Features_Are_Equal_Video-to-4D_Generation_via_Decoupling_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Liying Yang , Chen Liu , Zhenwei Zhu , Ajian Liu , Hui Ma , Jian Nong , Yanyan Liang Recently, the generation of dynamic 3D objects from a video has shown impressive results. Existing methods directly optimize Gaussians using whole information in frames. However, when dynamic regions are interwoven with static regions within frames, particularly if the static regions account for a large proportion, existing methods often overlook information in dynamic regions and are prone to overfitting on static regions. This leads to producing results with blurry textures. We consider that decoupling dynamic-static features to enhance dynamic representations can alleviate this issue. Thus, we propose a dynamic-static feature decoupling module (DSFD). Along temporal axes, it regards the regions of current frame features that possess significant differences relative to reference frame features as dynamic features. Conversely, the remaining parts are the static features. Then, we acquire decoupled features driven by dynamic features and current frame features. Moreover, to further enhance the dynamic representation of decoupled features from different viewpoints and ensure accurate motion prediction, we design a temporal-spatial similarity fusion module (TSSF). Along spatial axes, it adaptively selects similar information of dynamic regions. Hinging on the above, we construct a novel approach, DS4D. Experimental results verify our method achieves state-of-the-art (SOTA) results in video-to-4D. In addition, the experiments on a real-world scenario dataset demonstrate its effectiveness on the 4D scene. Project page: https://github.com/LiyingCV/DS4D. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF",
      "index": 185,
      "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
      "authors": [
        "Ranran Huang",
        "Krystian Mikolajczyk"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "spfsplat",
        "pose",
        "splatting",
        "gaussian",
        "poses",
        "primitives",
        "feed",
        "sparse",
        "view",
        "unposed"
      ],
      "summary": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_No_Pose_at_All_Self-Supervised_Pose-Free_3D_Gaussian_Splatting_from_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Ranran Huang , Krystian Mikolajczyk We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF",
      "index": 186,
      "title": "A Unified Interpretation of Training-Time Out-of-Distribution Detection",
      "authors": [
        "Xu Cheng",
        "Xin Jiang",
        "Zechao Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ood",
        "interactions",
        "samples",
        "detection",
        "training",
        "dnns",
        "distribution",
        "unified",
        "methods",
        "time"
      ],
      "summary": "This paper explains training-time out-of-distribution (OOD) detection from a novel view, i.e., interactions between different input variables of deep neural networks (DNNs). Specifically, we provide a unified understanding of the effectiveness of current training-time OOD detection methods, i.e., DNNs trained with these methods all encode more complex interactions for inference than those trained without training-time methods, which contributes to their superior OOD detection performance. We further conduct thorough empirical analyses and verify that complex interactions play a primary role in OOD detection, by developing a simple-yet-efficient method to force the DNN to learn interactions of specific complexities and evaluate the change of OOD detection performances. Besides, we also use interactions to investigate why near-OOD samples are more difficult to distinguish from in-distribution (ID) samples than far-OOD samples, mainly because compared to far-OOD samples, the distribution of interactions in near-OOD samples is more similar to that of ID samples. Moreover, we discover that training-time OOD detection methods can effectively decrease such similarities.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Cheng_A_Unified_Interpretation_of_Training-Time_Out-of-Distribution_Detection_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "A Unified Interpretation of Training-Time Out-of-Distribution Detection [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Xu Cheng , Xin Jiang , Zechao Li This paper explains training-time out-of-distribution (OOD) detection from a novel view, i.e., interactions between different input variables of deep neural networks (DNNs). Specifically, we provide a unified understanding of the effectiveness of current training-time OOD detection methods, i.e., DNNs trained with these methods all encode more complex interactions for inference than those trained without training-time methods, which contributes to their superior OOD detection performance. We further conduct thorough empirical analyses and verify that complex interactions play a primary role in OOD detection, by developing a simple-yet-efficient method to force the DNN to learn interactions of specific complexities and evaluate the change of OOD detection performances. Besides, we also use interactions to investigate why near-OOD samples are more difficult to distinguish from in-distribution (ID) samples than far-OOD samples, mainly because compared to far-OOD samples, the distribution of interactions in near-OOD samples is more similar to that of ID samples. Moreover, we discover that training-time OOD detection methods can effectively decrease such similarities. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF",
      "index": 187,
      "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective",
      "authors": [
        "Zongheng Tang",
        "Yi Liu",
        "Yifan Sun",
        "Yulu Gao",
        "Jinyu Chen",
        "Runsheng Xu",
        "Si Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "perception",
        "collaborative",
        "fusion",
        "unified",
        "temporal",
        "transmission",
        "spatio",
        "feature",
        "agents",
        "simultanesouly"
      ],
      "summary": "Collaborative perception shares information among different agents and helps solving problems that individual agents may face, e.g., occlusions and small sensing range. Prior methods usually separate the multi-agent fusion and multi-time fusion into two consecutive steps. In contrast, this paper proposes an efficient collaborative perception that aggregates the observations from different agents (space) and different times into a unified spatio-temporal space simultanesouly. The unified spatio-temporal space brings two benefits, i.e., efficient feature transmission and superior feature fusion. 1) Efficient feature transmission: each static object yields a single observation in the spatial temporal space, and thus only requires transmission only once (whereas prior methods re-transmit all the object features multiple times). 2) superior feature fusion: merging the multi-agent and multi-time fusion into a unified spatial-temporal aggregation enables a more holistic perspective, thereby enhancing perception performance in challenging scenarios. Consequently, our Collaborative perception with Spatio-temporal Transformer (CoST) gains improvement in both efficiency and accuracy. Notably, CoST is not tied to any specific method and is compatible with a majority of previous methods, enhancing their accuracy while reducing the transmission bandwidth.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Tang_CoST_Efficient_Collaborative_Perception_From_Unified_Spatiotemporal_Perspective_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective [PDF ] [Copy] [Kimi ] [REL] Authors : Zongheng Tang , Yi Liu , Yifan Sun , Yulu Gao , Jinyu Chen , Runsheng Xu , Si Liu Collaborative perception shares information among different agents and helps solving problems that individual agents may face, e.g., occlusions and small sensing range. Prior methods usually separate the multi-agent fusion and multi-time fusion into two consecutive steps. In contrast, this paper proposes an efficient collaborative perception that aggregates the observations from different agents (space) and different times into a unified spatio-temporal space simultanesouly. The unified spatio-temporal space brings two benefits, i.e., efficient feature transmission and superior feature fusion. 1) Efficient feature transmission: each static object yields a single observation in the spatial temporal space, and thus only requires transmission only once (whereas prior methods re-transmit all the object features multiple times). 2) superior feature fusion: merging the multi-agent and multi-time fusion into a unified spatial-temporal aggregation enables a more holistic perspective, thereby enhancing perception performance in challenging scenarios. Consequently, our Collaborative perception with Spatio-temporal Transformer (CoST) gains improvement in both efficiency and accuracy. Notably, CoST is not tied to any specific method and is compatible with a majority of previous methods, enhancing their accuracy while reducing the transmission bandwidth. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF",
      "index": 188,
      "title": "Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset",
      "authors": [
        "Ruofei Wang",
        "Peiqi Duan",
        "Boxin Shi",
        "Renjie Wan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "event",
        "unauthorized",
        "unlearnable",
        "uevs",
        "asynchronous",
        "safeguarding",
        "noise",
        "prevent",
        "streams",
        "minimizing"
      ],
      "summary": "With more event datasets being released online, safeguarding the event dataset against unauthorized usage has become a serious concern for data owners. Unlearnable Examples are proposed to prevent the unauthorized exploitation of image datasets. However, it's unclear how to create unlearnable asynchronous event streams to prevent event misuse. In this work, we propose the first unlearnable event stream generation method to prevent unauthorized training from event datasets. A new form of asynchronous event error-minimizing noise is proposed to perturb event streams, tricking the unauthorized model into learning embedded noise instead of realistic features. To be compatible with the sparse event, a projection strategy is presented to sparsify the noise to render our unlearnable event streams (UEvs). Extensive experiments demonstrate that our method effectively protects event data from unauthorized exploitation, while preserving their utility for legitimate use. We hope our UEvs contribute to the advancement of secure and trustworthy event dataset sharing. Code is available at: https://github.com/rfww/uevs.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Asynchronous_Event_Error-Minimizing_Noise_for_Safeguarding_Event_Dataset_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset [PDF ] [Copy] [Kimi ] [REL] Authors : Ruofei Wang , Peiqi Duan , Boxin Shi , Renjie Wan With more event datasets being released online, safeguarding the event dataset against unauthorized usage has become a serious concern for data owners. Unlearnable Examples are proposed to prevent the unauthorized exploitation of image datasets. However, it's unclear how to create unlearnable asynchronous event streams to prevent event misuse. In this work, we propose the first unlearnable event stream generation method to prevent unauthorized training from event datasets. A new form of asynchronous event error-minimizing noise is proposed to perturb event streams, tricking the unauthorized model into learning embedded noise instead of realistic features. To be compatible with the sparse event, a projection strategy is presented to sparsify the noise to render our unlearnable event streams (UEvs). Extensive experiments demonstrate that our method effectively protects event data from unauthorized exploitation, while preserving their utility for legitimate use. We hope our UEvs contribute to the advancement of secure and trustworthy event dataset sharing. Code is available at: https://github.com/rfww/uevs. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF",
      "index": 189,
      "title": "Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation",
      "authors": [
        "Shengfang Zhai",
        "Jiajun Li",
        "Yue Liu",
        "Huanran Chen",
        "Zhihua Tian",
        "Wenjie Qu",
        "Qingni Shen",
        "Ruoxi Jia",
        "Yinpeng Dong",
        "Jiaheng Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "t2i",
        "backdoor",
        "navit2i",
        "activation",
        "defense",
        "neuron",
        "backdoors",
        "variation",
        "input",
        "text"
      ],
      "summary": "In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high-quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhai_Efficient_Input-level_Backdoor_Defense_on_Text-to-Image_Synthesis_via_Neuron_Activation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Shengfang Zhai , Jiajun Li , Yue Liu , Huanran Chen , Zhihua Tian , Wenjie Qu , Qingni Shen , Ruoxi Jia , Yinpeng Dong , Jiaheng Zhang In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high-quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF",
      "index": 190,
      "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory",
      "authors": [
        "Runjia Li",
        "Philip Torr",
        "Andrea Vedaldi",
        "Tomas Jakab"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "vmem",
        "views",
        "scene",
        "surfel",
        "memory",
        "video",
        "indexed",
        "past",
        "generators",
        "surfels"
      ],
      "summary": "We propose a novel memory module for building video generators capable of interactively exploring environments. Previous approaches have achieved similar results either by out-painting 2D views of a scene while incrementally reconstructing its 3D geometry--which quickly accumulates errors--or by using video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a memory module that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost required to use all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory_ICCV_2025_paper.html",
          "/venue/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_VMem_Consistent_Interactive_Video_Scene_Generation_with_Surfel-Indexed_View_Memory_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Runjia Li , Philip Torr , Andrea Vedaldi , Tomas Jakab We propose a novel memory module for building video generators capable of interactively exploring environments. Previous approaches have achieved similar results either by out-painting 2D views of a scene while incrementally reconstructing its 3D geometry--which quickly accumulates errors--or by using video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a memory module that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost required to use all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF",
      "index": 191,
      "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models",
      "authors": [
        "Gaojie Lin",
        "Jianwen Jiang",
        "Jiaqi Yang",
        "Zerong Zheng",
        "Chao Liang",
        "Yuan Zhang",
        "Jingtuo Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "omnihuman",
        "human",
        "animation",
        "end",
        "driven",
        "audio",
        "supports",
        "talking",
        "generation",
        "portrait"
      ],
      "summary": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals).",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lin_OmniHuman-1_Rethinking_the_Scaling-Up_of_One-Stage_Conditioned_Human_Animation_Models_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Gaojie Lin , Jianwen Jiang , Jiaqi Yang , Zerong Zheng , Chao Liang , Yuan Zhang , Jingtuo Liu End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF",
      "index": 192,
      "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection",
      "authors": [
        "Jinglun Li",
        "Kaixun Jiang",
        "Zhaoyu Chen",
        "Bo Lin",
        "Yao Tang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ood",
        "samples",
        "synood",
        "ind",
        "boundary",
        "mllms",
        "clip",
        "models",
        "foundation",
        "fpr95"
      ],
      "summary": "Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD/OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by 11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Synthesizing_Near-Boundary_OOD_Samples_for_Out-of-Distribution_Detection_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jinglun Li , Kaixun Jiang , Zhaoyu Chen , Bo Lin , Yao Tang , Weifeng Ge , Wenqiang Zhang Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD/OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by 11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF",
      "index": 193,
      "title": "CountSE: Soft Exemplar Open-set Object Counting",
      "authors": [
        "Shuai Liu",
        "Peng Zhang",
        "Shiwei Zhang",
        "Wei Ke"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "countse",
        "exemplar",
        "counting",
        "guided",
        "exemplars",
        "shot",
        "text",
        "object",
        "zero",
        "methods"
      ],
      "summary": "Open-set counting is garnering increasing attention due to its capability to enumerate objects of arbitrary category. It can be generally categorized into two methodologies: text-guided zero-shot counting methods and exemplar-guided few-shot counting methods. Previous text-guided zero-shot methods only provide limited object information through text, resulting in poor performance. Besides, though exemplar-guided few-shot approaches gain better results, they rely heavily on manually annotated visual exemplars, resulting in low efficiency and high labor intensity. Therefore, we propose CountSE, which simultaneously achieves high efficiency and high performance. CountSE is a new text-guided zero-shot object counting algorithm that generates multiple precise soft exemplars at different scales to enhance counting models driven solely by semantics. Specifically, to obtain richer object information and address the diversity in object scales, we introduce Semantic-guided Exemplar Selection, a module that generates candidate soft exemplars at various scales and selects those with high similarity scores. Then, to ensure accuracy and representativeness, Clustering-based Exemplar Filtering is introduced to refine the candidate exemplars by effectively eliminating inaccurate exemplars through clustering analysis. In the text-guided zero-shot setting, CountSE outperforms all state-of-the-art methods on the FSC-147 benchmark by at least 15%. Additionally, experiments on two other widely used datasets demonstrate that CountSE significantly outperforms all previous text-guided zero-shot counting methods and is competitive with the most advanced exemplar-guided few-shot methods. Codes will be available. Code is available at https://github.com/pppppz22/CountSE.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_CountSE_Soft_Exemplar_Open-set_Object_Counting_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "CountSE: Soft Exemplar Open-set Object Counting [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Shuai Liu , Peng Zhang , Shiwei Zhang , Wei Ke Open-set counting is garnering increasing attention due to its capability to enumerate objects of arbitrary category. It can be generally categorized into two methodologies: text-guided zero-shot counting methods and exemplar-guided few-shot counting methods. Previous text-guided zero-shot methods only provide limited object information through text, resulting in poor performance. Besides, though exemplar-guided few-shot approaches gain better results, they rely heavily on manually annotated visual exemplars, resulting in low efficiency and high labor intensity. Therefore, we propose CountSE, which simultaneously achieves high efficiency and high performance. CountSE is a new text-guided zero-shot object counting algorithm that generates multiple precise soft exemplars at different scales to enhance counting models driven solely by semantics. Specifically, to obtain richer object information and address the diversity in object scales, we introduce Semantic-guided Exemplar Selection, a module that generates candidate soft exemplars at various scales and selects those with high similarity scores. Then, to ensure accuracy and representativeness, Clustering-based Exemplar Filtering is introduced to refine the candidate exemplars by effectively eliminating inaccurate exemplars through clustering analysis. In the text-guided zero-shot setting, CountSE outperforms all state-of-the-art methods on the FSC-147 benchmark by at least 15%. Additionally, experiments on two other widely used datasets demonstrate that CountSE significantly outperforms all previous text-guided zero-shot counting methods and is competitive with the most advanced exemplar-guided few-shot methods. Codes will be available. Code is available at https://github.com/pppppz22/CountSE. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF",
      "index": 194,
      "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery",
      "authors": [
        "Shubhendu Jena",
        "Amine Ouasfi",
        "Mae Younes",
        "Adnane Boukhayma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sparse",
        "unposed",
        "2dgs",
        "reconstruction",
        "splatting",
        "sparfels",
        "radiance",
        "splatted",
        "view",
        "shape"
      ],
      "summary": "We present a method for Sparse view reconstruction with surface element splatting that runs within 2 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate stat-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view Benchmarks based on established multi-view datasets.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jena_Sparfels_Fast_Reconstruction_from_Sparse_Unposed_Imagery_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Shubhendu Jena , Amine Ouasfi , Mae Younes , Adnane Boukhayma We present a method for Sparse view reconstruction with surface element splatting that runs within 2 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate stat-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view Benchmarks based on established multi-view datasets. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF",
      "index": 195,
      "title": "Underwater Visual SLAM with Depth Uncertainty and Medium Modeling",
      "authors": [
        "Rui Liu",
        "Sheng Fan",
        "Wenguan Wang",
        "Yi Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "slam",
        "underwater",
        "medium",
        "duv",
        "visual",
        "uncertainty",
        "depth",
        "geometric",
        "modeling",
        "mapping"
      ],
      "summary": "Underwater visual simultaneous localization and mapping (SLAM) faces critical challenges in light attenuation and degraded geometric consistency. Despite recent advances of visual SLAM in indoor and urban scenes, these approaches typically assume a clear medium and neglect medium-light interactions, leading to performance degradation in underwater environments. To overcome these limitations, we propose DUV-SLAM, a dense underwater visual SLAM framework that integrates uncertainty-aware geometry estimation with physics-inspired neural scattering modeling. Our method introduces two core innovations: i) depth uncertainty quantification derived from differentiable bundle adjustment, which propagates geometric confidence to guide mapping optimization; and ii) a neural-Gaussian hybrid representation that combines adaptive 3D Gaussians for underwater reconstruction with a neural field capturing wavelength-dependent medium properties, optimized using a combination of photometric, geometric, and distribution losses. Experiments on synthetic and real-world datasets demonstrate that DUV-SLAM achieves high-quality monocular reconstruction while maintaining real-time efficiency and robust tracking accuracy.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Underwater_Visual_SLAM_with_Depth_Uncertainty_and_Medium_Modeling_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Underwater Visual SLAM with Depth Uncertainty and Medium Modeling [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Rui Liu , Sheng Fan , Wenguan Wang , Yi Yang Underwater visual simultaneous localization and mapping (SLAM) faces critical challenges in light attenuation and degraded geometric consistency. Despite recent advances of visual SLAM in indoor and urban scenes, these approaches typically assume a clear medium and neglect medium-light interactions, leading to performance degradation in underwater environments. To overcome these limitations, we propose DUV-SLAM, a dense underwater visual SLAM framework that integrates uncertainty-aware geometry estimation with physics-inspired neural scattering modeling. Our method introduces two core innovations: i) depth uncertainty quantification derived from differentiable bundle adjustment, which propagates geometric confidence to guide mapping optimization; and ii) a neural-Gaussian hybrid representation that combines adaptive 3D Gaussians for underwater reconstruction with a neural field capturing wavelength-dependent medium properties, optimized using a combination of photometric, geometric, and distribution losses. Experiments on synthetic and real-world datasets demonstrate that DUV-SLAM achieves high-quality monocular reconstruction while maintaining real-time efficiency and robust tracking accuracy. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF",
      "index": 196,
      "title": "Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training",
      "authors": [
        "Qiaosi Yi",
        "Shuai Li",
        "Rongyuan Wu",
        "Lingchen Sun",
        "Yuhui Wu",
        "Lei Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "vae",
        "tvt",
        "unet",
        "fine",
        "resolution",
        "trained",
        "joyies",
        "transfer",
        "image",
        "super"
      ],
      "summary": "Impressive results on real-world image super-resolution (Real-ISR) have been achieved by employing pre-trained stable diffusion (SD) models. However, one critical issue of such methods lies in their poor reconstruction of image fine structures, such as small characters and textures, due to the aggressive resolution reduction of the VAE (e.g., 8xdownsampling) in the SD model. One solution is to employ a VAE with a lower downsampling rate for diffusion; however, adapting its latent features with the pre-trained UNet while mitigating the increased computational cost poses new challenges. To address these issues, we propose a Transfer VAE Training (TVT) strategy to transfer the 8xdownsampled VAE into a 4xone while adapting to the pre-trained UNet. Specifically, we first train a 4xdecoder based on the output features of the original VAE encoder, then train a 4xencoder while keeping the newly trained decoder fixed. Such a TVT strategy aligns the new encoder-decoder pair with the original VAE latent space while enhancing image fine details. Additionally, we introduce a compact VAE and compute-efficient UNet by optimizing their network architectures, reducing the computational cost while capturing high-resolution fine-scale features. Experimental results demonstrate that our TVT method significantly improves fine-structure preservation, which is often compromised by other SD-based methods, while requiring fewer FLOPs than state-of-the-art one-step diffusion models. The official code can be found at https://github.com/Joyies/TVT.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yi_Fine-structure_Preserved_Real-world_Image_Super-resolution_via_Transfer_VAE_Training_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": null
      },
      "raw_excerpt": "Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training [PDF 8 ] [Copy] [Kimi ] [REL] Authors : Qiaosi Yi , Shuai Li , Rongyuan Wu , Lingchen Sun , Yuhui Wu , Lei Zhang Impressive results on real-world image super-resolution (Real-ISR) have been achieved by employing pre-trained stable diffusion (SD) models. However, one critical issue of such methods lies in their poor reconstruction of image fine structures, such as small characters and textures, due to the aggressive resolution reduction of the VAE (e.g., 8xdownsampling) in the SD model. One solution is to employ a VAE with a lower downsampling rate for diffusion; however, adapting its latent features with the pre-trained UNet while mitigating the increased computational cost poses new challenges. To address these issues, we propose a Transfer VAE Training (TVT) strategy to transfer the 8xdownsampled VAE into a 4xone while adapting to the pre-trained UNet. Specifically, we first train a 4xdecoder based on the output features of the original VAE encoder, then train a 4xencoder while keeping the newly trained decoder fixed. Such a TVT strategy aligns the new encoder-decoder pair with the original VAE latent space while enhancing image fine details. Additionally, we introduce a compact VAE and compute-efficient UNet by optimizing their network architectures, reducing the computational cost while capturing high-resolution fine-scale features. Experimental results demonstrate that our TVT method significantly improves fine-structure preservation, which is often compromised by other SD-based methods, while requiring fewer FLOPs than state-of-the-art one-step diffusion models. The official code can be found at https://github.com/Joyies/TVT. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF",
      "index": 197,
      "title": "Rectifying Magnitude Neglect in Linear Attention",
      "authors": [
        "Qihang Fan",
        "Huaibo Huang",
        "Yuang Ai",
        "Ran He"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "attention",
        "mala",
        "softmax",
        "linear",
        "magnitude",
        "exhibits",
        "query",
        "score",
        "rectifying",
        "complexity"
      ],
      "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query(Q or \\phi(Q)). The absence of magnitude information prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose **Magnitude-Aware Linear Attention** (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. As a result, MALA surpasses Softmax Attention in performance while maintaining only linear complexity. We build Magnitude-Aware Vision Transformer (MAViT) based on MALA, achieving **84.7%** accuracy on ImageNet-1K with only **27M** parameters and **4.6G** flops, without using any additional data or labels. It also exhibits excellent inference efficiency. This result highlights the strong potential of MALA.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Fan_Rectifying_Magnitude_Neglect_in_Linear_Attention_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Rectifying Magnitude Neglect in Linear Attention [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Qihang Fan , Huaibo Huang , Yuang Ai , Ran He As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query(Q or \\phi(Q)). The absence of magnitude information prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose **Magnitude-Aware Linear Attention** (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. As a result, MALA surpasses Softmax Attention in performance while maintaining only linear complexity. We build Magnitude-Aware Vision Transformer (MAViT) based on MALA, achieving **84.7%** accuracy on ImageNet-1K with only **27M** parameters and **4.6G** flops, without using any additional data or labels. It also exhibits excellent inference efficiency. This result highlights the strong potential of MALA. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF",
      "index": 198,
      "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder",
      "authors": [
        "Wonwoong Cho",
        "Yan-Ying Chen",
        "Matthew Klenk",
        "David I. Inouye",
        "Yanxia Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "adapter",
        "att",
        "attributes",
        "t2i",
        "control",
        "autoencoder",
        "multiple",
        "domain",
        "diffusion",
        "precise"
      ],
      "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the **Attribute (Att) Adapter**, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning.We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world.Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Cho_Att-Adapter_A_Robust_and_Precise_Domain-Specific_Multi-Attributes_T2I_Diffusion_Adapter_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Wonwoong Cho , Yan-Ying Chen , Matthew Klenk , David I. Inouye , Yanxia Zhang Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the **Attribute (Att) Adapter**, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning.We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world.Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF",
      "index": 199,
      "title": "DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning",
      "authors": [
        "Ziqi Gao",
        "Qiufu Li",
        "Linlin Shen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dap",
        "mae",
        "cloud",
        "point",
        "domain",
        "masked",
        "autoencoder",
        "domains",
        "pre",
        "training"
      ],
      "summary": "Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus. The code will be released at https://github.com/CVI-SZU/DAP-MAE",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_DAP-MAE_Domain-Adaptive_Point_Cloud_Masked_Autoencoder_for_Effective_Cross-Domain_Learning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Ziqi Gao , Qiufu Li , Linlin Shen Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus. The code will be released at https://github.com/CVI-SZU/DAP-MAE Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF",
      "index": 200,
      "title": "PanoLlama: Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs",
      "authors": [
        "Teng Zhou",
        "Xiaoyu Zhang",
        "Yongchuan Tang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "pig",
        "panollama",
        "endless",
        "token",
        "coherent",
        "panoramic",
        "crop",
        "var",
        "panoramas",
        "paradigm"
      ],
      "summary": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhou_PanoLlama_Generating_Endless_and_Coherent_Panoramas_with_Next-Token-Prediction_LLMs_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "PanoLlama: Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Teng Zhou , Xiaoyu Zhang , Yongchuan Tang Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF",
      "index": 201,
      "title": "DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection",
      "authors": [
        "Hongwei Yu",
        "Xinlong Ding",
        "Jiawei Li",
        "Jinlong Wang",
        "Yudong Zhang",
        "Rongquan Wang",
        "Huimin Ma",
        "Jiansheng Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "backdoor",
        "adversarial",
        "diffusion",
        "attacks",
        "dadet",
        "anomaly",
        "deviate",
        "coco",
        "safeguarding",
        "detection"
      ],
      "summary": "While image conditional diffusion models demonstrate impressive generation capabilities, they exhibit high vulnerability when facing backdoor and adversarial attacks. In this paper, we define a scenario named diffusion anomaly where the generated results of a reverse process under attack deviate significantly from the normal ones. By analyzing the underlying formation mechanism of the diffusion anomaly, we reveal how perturbations are amplified during the reverse process and accumulated in the results. Based on the analysis, we reveal the phenomena of divergence and homogeneity, which cause the diffusion process to deviate significantly from the normal process and to decline in diversity. Leveraging these two phenomena, we propose a method named Diffusion Anomaly Detection (DADet) to effectively detect both backdoor and adversarial attacks. Extensive experiments demonstrate that our proposal achieves excellent defense performance against backdoor and adversarial attacks. Specifically, for the backdoor attack detection, our method achieves an F1 score of 99% on different datasets, including MS COCO and CIFAR-10. For the detection of adversarial samples, the F1 score exceeds 84% across three adversarial attacks and two different tasks, evaluated on the MS COCO and Places365 datasets, respectively.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Hongwei Yu , Xinlong Ding , Jiawei Li , Jinlong Wang , Yudong Zhang , Rongquan Wang , Huimin Ma , Jiansheng Chen While image conditional diffusion models demonstrate impressive generation capabilities, they exhibit high vulnerability when facing backdoor and adversarial attacks. In this paper, we define a scenario named diffusion anomaly where the generated results of a reverse process under attack deviate significantly from the normal ones. By analyzing the underlying formation mechanism of the diffusion anomaly, we reveal how perturbations are amplified during the reverse process and accumulated in the results. Based on the analysis, we reveal the phenomena of divergence and homogeneity, which cause the diffusion process to deviate significantly from the normal process and to decline in diversity. Leveraging these two phenomena, we propose a method named Diffusion Anomaly Detection (DADet) to effectively detect both backdoor and adversarial attacks. Extensive experiments demonstrate that our proposal achieves excellent defense performance against backdoor and adversarial attacks. Specifically, for the backdoor attack detection, our method achieves an F1 score of 99% on different datasets, including MS COCO and CIFAR-10. For the detection of adversarial samples, the F1 score exceeds 84% across three adversarial attacks and two different tasks, evaluated on the MS COCO and Places365 datasets, respectively. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF",
      "index": 202,
      "title": "ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation",
      "authors": [
        "Xiwei Xuan",
        "Ziquan Deng",
        "Kwan-Liu Ma"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "ovs",
        "reme",
        "vocabulary",
        "training",
        "quality",
        "data",
        "centric",
        "reference",
        "free",
        "segmentation"
      ],
      "summary": "Training-free open-vocabulary semantic segmentation (OVS) aims to segment images given a set of arbitrary textual categories without costly model fine-tuning. Existing solutions often explore attention mechanisms of pre-trained models, such as CLIP, or generate synthetic data and design complex retrieval processes to perform OVS. However, their performance is limited by the capability of reliant models or the suboptimal quality of reference sets. In this work, we investigate the largely overlooked data quality problem for this challenging dense scene understanding task, and identify that a high-quality reference set can significantly benefit training-free OVS. With this observation, we introduce a data-quality-oriented framework, comprising a data pipeline to construct a reference set with well-paired segment-text embeddings and a simple similarity-based retrieval to unveil the essential effect of data. Remarkably, extensive evaluations on ten benchmark datasets demonstrate that our method outperforms all existing training-free OVS approaches, highlighting the importance of data-centric design for advancing OVS without training.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xuan_ReME_A_Data-Centric_Framework_for_Training-Free_Open-Vocabulary_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "ReME: A Data-Centric Framework for Training-Free Open-Vocabulary Segmentation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Xiwei Xuan , Ziquan Deng , Kwan-Liu Ma Training-free open-vocabulary semantic segmentation (OVS) aims to segment images given a set of arbitrary textual categories without costly model fine-tuning. Existing solutions often explore attention mechanisms of pre-trained models, such as CLIP, or generate synthetic data and design complex retrieval processes to perform OVS. However, their performance is limited by the capability of reliant models or the suboptimal quality of reference sets. In this work, we investigate the largely overlooked data quality problem for this challenging dense scene understanding task, and identify that a high-quality reference set can significantly benefit training-free OVS. With this observation, we introduce a data-quality-oriented framework, comprising a data pipeline to construct a reference set with well-paired segment-text embeddings and a simple similarity-based retrieval to unveil the essential effect of data. Remarkably, extensive evaluations on ten benchmark datasets demonstrate that our method outperforms all existing training-free OVS approaches, highlighting the importance of data-centric design for advancing OVS without training. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF",
      "index": 203,
      "title": "CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance",
      "authors": [
        "Peiqi Chen",
        "Lei Yu",
        "Yi Wan",
        "Yingying Pei",
        "Xinyi Liu",
        "Yongxiang Yao",
        "Yingying Zhang",
        "Lixiang Ru",
        "Liheng Zhong",
        "Jingdong Chen",
        "Ming Yang",
        "Yongjun Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "casp",
        "pipeline",
        "feature",
        "matching",
        "cascaded",
        "priors",
        "guidance",
        "dense",
        "correspondence",
        "matches"
      ],
      "summary": "Semi-dense feature matching methods have shown strong performance in challenging scenarios. However, the existing pipeline relies on a global search across the entire feature map to establish coarse matches, limiting further improvements in accuracy and efficiency. Motivated by this limitation, we propose a novel pipeline, CasP, which leverages cascaded correspondence priors for guidance. Specifically, the matching stage is decomposed into two progressive phases, bridged by a region-based selective cross-attention mechanism designed to enhance feature discriminability. In the second phase, one-to-one matches are determined by restricting the search range to the one-to-many prior areas identified in the first phase. Additionally, this pipeline benefits from incorporating high-level features, which helps reduce the computational costs of low-level feature extraction. The acceleration gains of CasP increase with higher resolution, and our lite model achieves a speedup of ~2.2xat a resolution of 1152 compared to the most efficient method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority in geometric estimation, particularly with impressive cross-domain generalization. These advantages highlight its potential for latency-sensitive and high-robustness applications, such as SLAM and UAV systems.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_CasP_Improving_Semi-Dense_Feature_Matching_Pipeline_Leveraging_Cascaded_Correspondence_Priors_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Peiqi Chen , Lei Yu , Yi Wan , Yingying Pei , Xinyi Liu , Yongxiang Yao , Yingying Zhang , Lixiang Ru , Liheng Zhong , Jingdong Chen , Ming Yang , Yongjun Zhang Semi-dense feature matching methods have shown strong performance in challenging scenarios. However, the existing pipeline relies on a global search across the entire feature map to establish coarse matches, limiting further improvements in accuracy and efficiency. Motivated by this limitation, we propose a novel pipeline, CasP, which leverages cascaded correspondence priors for guidance. Specifically, the matching stage is decomposed into two progressive phases, bridged by a region-based selective cross-attention mechanism designed to enhance feature discriminability. In the second phase, one-to-one matches are determined by restricting the search range to the one-to-many prior areas identified in the first phase. Additionally, this pipeline benefits from incorporating high-level features, which helps reduce the computational costs of low-level feature extraction. The acceleration gains of CasP increase with higher resolution, and our lite model achieves a speedup of ~2.2xat a resolution of 1152 compared to the most efficient method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority in geometric estimation, particularly with impressive cross-domain generalization. These advantages highlight its potential for latency-sensitive and high-robustness applications, such as SLAM and UAV systems. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF",
      "index": 204,
      "title": "PLMP - Point-Line Minimal Problems for Projective SfM",
      "authors": [
        "Kim Kiehn",
        "Albin Ahlbck",
        "Kathln Kohn"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "minimal",
        "problems",
        "plmp",
        "sfm",
        "cameras",
        "subarrangements",
        "underconstrained",
        "lines",
        "pinhole",
        "291"
      ],
      "summary": "We completely classify all minimal problems for Structure-from-Motion (SfM) where arrangements of points and lines are fully observed by multiple uncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have unique solutions and can thus be solved linearly.Two of the linear problems allow an arbitrary number of views, while all other minimal problems have at most 9 cameras. All minimal problems have at most 7 points and at most 12 lines. We compute the number of solutions of each minimal problem, as this gives a measurement of the problem's intrinsic difficulty, and find that these number are relatively low (e.g., when comparing with minimal problems for calibrated cameras). Finally, by exploring stabilizer subgroups of subarrangements, we develop a geometric and systematic way to 1) factorize minimal problems into smaller problems, 2) identify minimal problems in underconstrained problems, and 3) formally prove non-minimality.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kiehn_PLMP_-_Point-Line_Minimal_Problems_for_Projective_SfM_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "PLMP - Point-Line Minimal Problems for Projective SfM [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Kim Kiehn , Albin Ahlbck , Kathln Kohn We completely classify all minimal problems for Structure-from-Motion (SfM) where arrangements of points and lines are fully observed by multiple uncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have unique solutions and can thus be solved linearly.Two of the linear problems allow an arbitrary number of views, while all other minimal problems have at most 9 cameras. All minimal problems have at most 7 points and at most 12 lines. We compute the number of solutions of each minimal problem, as this gives a measurement of the problem's intrinsic difficulty, and find that these number are relatively low (e.g., when comparing with minimal problems for calibrated cameras). Finally, by exploring stabilizer subgroups of subarrangements, we develop a geometric and systematic way to 1) factorize minimal problems into smaller problems, 2) identify minimal problems in underconstrained problems, and 3) formally prove non-minimality. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF",
      "index": 205,
      "title": "TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging",
      "authors": [
        "Qinglei Cao",
        "Ziyao Tang",
        "Xiaoqin Tang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "reconstruction",
        "implicit",
        "tpg",
        "target",
        "sparse",
        "prior",
        "view",
        "voxel",
        "inr",
        "nerp"
      ],
      "summary": "X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available upon request.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Cao_TPG-INR_Target_Prior-Guided_Implicit_3D_CT_Reconstruction_for_Enhanced_Sparse-view_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Qinglei Cao , Ziyao Tang , Xiaoqin Tang X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available upon request. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF",
      "index": 206,
      "title": "UniDxMD: Towards Unified Representation for Cross-Modal Unsupervised Domain Adaptation in 3D Semantic Segmentation",
      "authors": [
        "Zhengyin Liang",
        "Hui Yin",
        "Min Liang",
        "Qianqian Du",
        "Ying Yang",
        "Hua Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "cross",
        "modal",
        "semantic",
        "unidxmd",
        "uda",
        "unified",
        "adaptation",
        "segmentation",
        "domain",
        "representation"
      ],
      "summary": "Modality or domain distribution shifts pose formidable challenges in 3D semantic segmentation. Existing methods predominantly address either cross-modal or cross-domain adaptation in isolation, leading to insufficient exploration of semantic associations and complementary features in heterogeneous data. To bridge this gap, we present UniDxMD, a unified representation method for cross-modal unsupervised domain adaptation (UDA) in 3D semantic segmentation that simultaneously tackles both cross-modal and cross-domain adaptation objectives. Our core insight is deriving a unified discrete representation from heterogeneous data to mitigate distribution shifts, inspired by vector quantization. Specifically, we propose a differentiable, cluster-based soft quantization mechanism (CSQM) that maps heterogeneous data (spanning modalities and domains) into a shared discrete latent space. Then, we introduce latent space regularization (LSR), leveraging joint prototypes that satisfy semantic relation consistency as learnable anchors to enhance the compactness and semantic discriminability of the discrete latent space. Our method paves the way for advancing cross-modal UDA in 3D semantic segmentation towards the unified representation. Extensive results across four challenging cross-modal UDA scenarios demonstrate the superiority of our method.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_UniDxMD_Towards_Unified_Representation_for_Cross-Modal_Unsupervised_Domain_Adaptation_in_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "UniDxMD: Towards Unified Representation for Cross-Modal Unsupervised Domain Adaptation in 3D Semantic Segmentation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Zhengyin Liang , Hui Yin , Min Liang , Qianqian Du , Ying Yang , Hua Huang Modality or domain distribution shifts pose formidable challenges in 3D semantic segmentation. Existing methods predominantly address either cross-modal or cross-domain adaptation in isolation, leading to insufficient exploration of semantic associations and complementary features in heterogeneous data. To bridge this gap, we present UniDxMD, a unified representation method for cross-modal unsupervised domain adaptation (UDA) in 3D semantic segmentation that simultaneously tackles both cross-modal and cross-domain adaptation objectives. Our core insight is deriving a unified discrete representation from heterogeneous data to mitigate distribution shifts, inspired by vector quantization. Specifically, we propose a differentiable, cluster-based soft quantization mechanism (CSQM) that maps heterogeneous data (spanning modalities and domains) into a shared discrete latent space. Then, we introduce latent space regularization (LSR), leveraging joint prototypes that satisfy semantic relation consistency as learnable anchors to enhance the compactness and semantic discriminability of the discrete latent space. Our method paves the way for advancing cross-modal UDA in 3D semantic segmentation towards the unified representation. Extensive results across four challenging cross-modal UDA scenarios demonstrate the superiority of our method. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF",
      "index": 207,
      "title": "ZIM: Zero-Shot Image Matting for Anything",
      "authors": [
        "Beomyoung Kim",
        "Chanyong Shin",
        "Joonhyun Jeong",
        "Hyungsik Jung",
        "Se-Yun Lee",
        "Sewhan Chun",
        "Dong-Hyun Hwang",
        "Joonsang Yu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "zim",
        "matte",
        "matting",
        "shot",
        "zero",
        "masks",
        "anything",
        "segmentation",
        "sam",
        "precise"
      ],
      "summary": "The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D segmentation. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://naver-ai.github.io/ZIM.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_ZIM_Zero-Shot_Image_Matting_for_Anything_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "ZIM: Zero-Shot Image Matting for Anything [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Beomyoung Kim , Chanyong Shin , Joonhyun Jeong , Hyungsik Jung , Se-Yun Lee , Sewhan Chun , Dong-Hyun Hwang , Joonsang Yu The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D segmentation. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://naver-ai.github.io/ZIM. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF",
      "index": 208,
      "title": "Backdoor Mitigation by Distance-Driven Detoxification",
      "authors": [
        "Shaokui Wei",
        "Jiayin Liu",
        "Hongyuan Zha"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "backdoor",
        "detoxification",
        "defense",
        "poisoned",
        "backdoors",
        "attacks",
        "detoxify",
        "sota",
        "distance",
        "driven"
      ],
      "summary": "Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model's departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wei_Backdoor_Mitigation_by_Distance-Driven_Detoxification_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Backdoor Mitigation by Distance-Driven Detoxification [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Shaokui Wei , Jiayin Liu , Hongyuan Zha Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model's departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF",
      "index": 209,
      "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
      "authors": [
        "Fangwei Zhong",
        "Kui Wu",
        "Churan Wang",
        "Hao Chen",
        "Hai Ci",
        "Zhoujun Li",
        "Yizhou Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "unrealzoo",
        "embodied",
        "worlds",
        "photo",
        "unrealcv",
        "realistic",
        "virtual",
        "enriching",
        "world",
        "navigation"
      ],
      "summary": "We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects.UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhong_UnrealZoo_Enriching_Photo-realistic_Virtual_Worlds_for_Embodied_AI_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Fangwei Zhong , Kui Wu , Churan Wang , Hao Chen , Hai Ci , Zhoujun Li , Yizhou Wang We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects.UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF",
      "index": 210,
      "title": "Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection",
      "authors": [
        "Hanshi Wang",
        "Jin Gao",
        "Weiming Hu",
        "Zhipeng Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "height",
        "lidar",
        "mamba",
        "fusion",
        "modal",
        "mambafusion",
        "global",
        "autolab",
        "fidelity",
        "dense"
      ],
      "summary": "We present the first work demonstrating that a pure Mamba block can achieve efficient Dense Global Fusion, meanwhile guaranteeing top performance for camera-LiDAR multi-modal 3D object detection. Our motivation stems from the observation that existing fusion strategies are constrained by their inability to simultaneously achieve efficiency, long-range modeling, and retaining complete scene information. Inspired by recent advances in state-space models (SSMs) and linear attention, we leverage their linear complexity and long-range modeling capabilities to address these challenges. However, this is non-trivial since our experiments reveal that simply adopting efficient linear-complexity methods does not necessarily yield improvements and may even degrade performance. We attribute this degradation to the loss of height information during multi-modal alignment, leading to deviations in sequence order. To resolve this, we propose height-fidelity LiDAR encoding that preserves precise height information through voxel compression in continuous space, thereby enhancing camera-LiDAR alignment. Subsequently, we introduce the Hybrid Mamba Block, which leverages the enriched height-informed features to conduct local and global contextual learning. By integrating these components, our method achieves state-of-the-art performance with the top-tire NDS score of 75.0 on the nuScenes validation benchmark, even surpassing methods that utilize high-resolution inputs. Meanwhile, our method maintains efficiency, achieving faster inference speed than most recent state-of-the-art methods. Code is available at https://github.com/AutoLab-SAI-SJTU/MambaFusion",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Height-Fidelity_Dense_Global_Fusion_for_Multi-modal_3D_Object_Detection_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Hanshi Wang , Jin Gao , Weiming Hu , Zhipeng Zhang We present the first work demonstrating that a pure Mamba block can achieve efficient Dense Global Fusion, meanwhile guaranteeing top performance for camera-LiDAR multi-modal 3D object detection. Our motivation stems from the observation that existing fusion strategies are constrained by their inability to simultaneously achieve efficiency, long-range modeling, and retaining complete scene information. Inspired by recent advances in state-space models (SSMs) and linear attention, we leverage their linear complexity and long-range modeling capabilities to address these challenges. However, this is non-trivial since our experiments reveal that simply adopting efficient linear-complexity methods does not necessarily yield improvements and may even degrade performance. We attribute this degradation to the loss of height information during multi-modal alignment, leading to deviations in sequence order. To resolve this, we propose height-fidelity LiDAR encoding that preserves precise height information through voxel compression in continuous space, thereby enhancing camera-LiDAR alignment. Subsequently, we introduce the Hybrid Mamba Block, which leverages the enriched height-informed features to conduct local and global contextual learning. By integrating these components, our method achieves state-of-the-art performance with the top-tire NDS score of 75.0 on the nuScenes validation benchmark, even surpassing methods that utilize high-resolution inputs. Meanwhile, our method maintains efficiency, achieving faster inference speed than most recent state-of-the-art methods. Code is available at https://github.com/AutoLab-SAI-SJTU/MambaFusion Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF",
      "index": 211,
      "title": "SMSTracker: Tri-path Score Mask Sigma Fusion for Multi-Modal Tracking",
      "authors": [
        "Sixian Chan",
        "Zedong Li",
        "Wenhao Li",
        "Shijian Lu",
        "Chunhua Shen",
        "Xiaoqin Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "smstracker",
        "modal",
        "tri",
        "rgb",
        "fusion",
        "mask",
        "path",
        "score",
        "sigma",
        "tracking"
      ],
      "summary": "Multi-modal object tracking has emerged as a significant research focus in computer vision due to its robustness in complex environments, such as exposure variations, blur, and occlusions. Despite existing studies integrating supplementary modal information into pre-trained RGB trackers through visual prompt mechanisms, this approach exhibits a critical limitation: it inherently prioritizes RGB information as the dominant modality, thereby underutilizing the complementary information of alternative modalities. To address this fundamental limitation, we present SMSTracker, an innovative tri-path score mask sigma fusion framework for multi-modal tracking, including three key modules. Firstly, we design a tri-path Score Mask Fusion (SMF) module to evaluate and quantify the reliability of each modality, allowing optimal exploitation of complementary features between modalities. Secondly, we introduce a pioneering Sigma Interaction (SGI) module to facilitate a sophisticated fusion of modal features across tri-branches. Furthermore, we advance a Drop Key Fine-tuning (DKF) strategy to address the inherent challenge of unequal data contribution in multi-modal learning scenarios, thereby enhancing the model's capacity for comprehensive multi-modal information processing. Finally, extensive experiments on RGB+Thermal, RGB+Depth, and RGB+Event datasets demonstrate the significant performance improvements achieved by SMSTracker over existing state-of-the-art methods. Code and model are available at https://github.com/Leezed525/SMSTracker.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chan_SMSTracker_Tri-path_Score_Mask_Sigma_Fusion_for_Multi-Modal_Tracking_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "SMSTracker: Tri-path Score Mask Sigma Fusion for Multi-Modal Tracking [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Sixian Chan , Zedong Li , Wenhao Li , Shijian Lu , Chunhua Shen , Xiaoqin Zhang Multi-modal object tracking has emerged as a significant research focus in computer vision due to its robustness in complex environments, such as exposure variations, blur, and occlusions. Despite existing studies integrating supplementary modal information into pre-trained RGB trackers through visual prompt mechanisms, this approach exhibits a critical limitation: it inherently prioritizes RGB information as the dominant modality, thereby underutilizing the complementary information of alternative modalities. To address this fundamental limitation, we present SMSTracker, an innovative tri-path score mask sigma fusion framework for multi-modal tracking, including three key modules. Firstly, we design a tri-path Score Mask Fusion (SMF) module to evaluate and quantify the reliability of each modality, allowing optimal exploitation of complementary features between modalities. Secondly, we introduce a pioneering Sigma Interaction (SGI) module to facilitate a sophisticated fusion of modal features across tri-branches. Furthermore, we advance a Drop Key Fine-tuning (DKF) strategy to address the inherent challenge of unequal data contribution in multi-modal learning scenarios, thereby enhancing the model's capacity for comprehensive multi-modal information processing. Finally, extensive experiments on RGB+Thermal, RGB+Depth, and RGB+Event datasets demonstrate the significant performance improvements achieved by SMSTracker over existing state-of-the-art methods. Code and model are available at https://github.com/Leezed525/SMSTracker. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF",
      "index": 212,
      "title": "Two Losses, One Goal: Balancing Conflict Gradients for Semi-supervised Semantic Segmentation",
      "authors": [
        "Rui Sun",
        "Huayu Mai",
        "Wangkai Li",
        "Yujia Chen",
        "Yuan Wang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "pos",
        "supervised",
        "objectives",
        "conflict",
        "segmentation",
        "gradients",
        "semi",
        "descent",
        "gradient",
        "meo"
      ],
      "summary": "Semi-supervised semantic segmentation has attracted considerable attention as it alleviates the need for extensive pixel-level annotations. However, existing methods often overlook the potential optimization conflict between supervised and unsupervised learning objectives, leading to suboptimal performance. In this paper, we identify this under-explored issue and propose a novel Pareto Optimization Strategy (POS) to tackle it. POS aims to find a descent gradient direction that benefits both learning objectives, thereby facilitating model training. By dynamically assigning weights to the gradients at each iteration based on the model's learning status, POS effectively reconciles the intrinsic tension between the two objectives. Furthermore, we analyze POS from the perspective of gradient descent in random batch sampling and propose the Magnitude Enhancement Operation (MEO) to further unleash its potential by considering both direction and magnitude during gradient integration. Extensive experiments on challenging benchmarks demonstrate that integrating POS into existing semi-supervised segmentation methods yields consistent improvements across different data splits and architectures (CNN, Transformer), showcasing its effectiveness.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Sun_Two_Losses_One_Goal_Balancing_Conflict_Gradients_for_Semi-supervised_Semantic_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "Two Losses, One Goal: Balancing Conflict Gradients for Semi-supervised Semantic Segmentation [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Rui Sun , Huayu Mai , Wangkai Li , Yujia Chen , Yuan Wang Semi-supervised semantic segmentation has attracted considerable attention as it alleviates the need for extensive pixel-level annotations. However, existing methods often overlook the potential optimization conflict between supervised and unsupervised learning objectives, leading to suboptimal performance. In this paper, we identify this under-explored issue and propose a novel Pareto Optimization Strategy (POS) to tackle it. POS aims to find a descent gradient direction that benefits both learning objectives, thereby facilitating model training. By dynamically assigning weights to the gradients at each iteration based on the model's learning status, POS effectively reconciles the intrinsic tension between the two objectives. Furthermore, we analyze POS from the perspective of gradient descent in random batch sampling and propose the Magnitude Enhancement Operation (MEO) to further unleash its potential by considering both direction and magnitude during gradient integration. Extensive experiments on challenging benchmarks demonstrate that integrating POS into existing semi-supervised segmentation methods yields consistent improvements across different data splits and architectures (CNN, Transformer), showcasing its effectiveness. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF",
      "index": 213,
      "title": "Region-based Cluster Discrimination for Visual Representation Learning",
      "authors": [
        "Yin Xie",
        "Kaicheng Yang",
        "Xiang An",
        "Kun Wu",
        "Yongle Zhao",
        "Weimo Deng",
        "Zimin Ran",
        "Yumeng Wang",
        "Ziyong Feng",
        "Roy Miles",
        "Ismail Elezi",
        "Jiankang Deng"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "region",
        "ocr",
        "discrimination",
        "visual",
        "rice",
        "cluster",
        "siglip",
        "deepglint",
        "tasks",
        "language"
      ],
      "summary": "Learning visual representations is foundational for a broad spectrum of downstream tasks. Although recent vision-language contrastive models, such as CLIP and SigLIP, have achieved impressive zero-shot performance via large-scale vision-language alignment, their reliance on global representations constrains their effectiveness for dense prediction tasks, such as grounding, OCR, and segmentation. To address this gap, we introduce Region-Aware Cluster Discrimination (RICE), a novel method that enhances region-level visual and OCR capabilities. We first construct a billion-scale candidate region dataset and propose a Region Transformer layer to extract rich regional semantics. We further design a unified region cluster discrimination loss that jointly supports object and OCR learning within a single classification framework, enabling efficient and scalable distributed training on large-scale data. Extensive experiments show that RICE consistently outperforms previous methods on tasks, including segmentation, dense detection, and visual perception for Multimodal Large Language Models (MLLMs). The pre-trained models have been released at https://github.com/deepglint/MVT.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xie_Region-based_Cluster_Discrimination_for_Visual_Representation_Learning_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Region-based Cluster Discrimination for Visual Representation Learning [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Yin Xie , Kaicheng Yang , Xiang An , Kun Wu , Yongle Zhao , Weimo Deng , Zimin Ran , Yumeng Wang , Ziyong Feng , Roy Miles , Ismail Elezi , Jiankang Deng Learning visual representations is foundational for a broad spectrum of downstream tasks. Although recent vision-language contrastive models, such as CLIP and SigLIP, have achieved impressive zero-shot performance via large-scale vision-language alignment, their reliance on global representations constrains their effectiveness for dense prediction tasks, such as grounding, OCR, and segmentation. To address this gap, we introduce Region-Aware Cluster Discrimination (RICE), a novel method that enhances region-level visual and OCR capabilities. We first construct a billion-scale candidate region dataset and propose a Region Transformer layer to extract rich regional semantics. We further design a unified region cluster discrimination loss that jointly supports object and OCR learning within a single classification framework, enabling efficient and scalable distributed training on large-scale data. Extensive experiments show that RICE consistently outperforms previous methods on tasks, including segmentation, dense detection, and visual perception for Multimodal Large Language Models (MLLMs). The pre-trained models have been released at https://github.com/deepglint/MVT. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF",
      "index": 214,
      "title": "Shape of Motion: 4D Reconstruction from a Single Video",
      "authors": [
        "Qianqian Wang",
        "Vickie Ye",
        "Hang Gao",
        "Weijia Zeng",
        "Jake Austin",
        "Zhengqi Li",
        "Angjoo Kanazawa"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "motion",
        "monocular",
        "scenes",
        "dynamic",
        "scene",
        "reconstruction",
        "bases",
        "casually",
        "long",
        "rigidly"
      ],
      "summary": "Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. We introduce a method for reconstructing generic dynamic scenes, featuring explicit, persistent 3D motion trajectories in the world coordinate frame, from casually captured monocular videos.We tackle the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE(3) motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we take advantage of off-the-shelf data-driven priors such as monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video_ICCV_2025_paper.html",
          "/venue/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Shape_of_Motion_4D_Reconstruction_from_a_Single_Video_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Shape of Motion: 4D Reconstruction from a Single Video [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Qianqian Wang , Vickie Ye , Hang Gao , Weijia Zeng , Jake Austin , Zhengqi Li , Angjoo Kanazawa Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. We introduce a method for reconstructing generic dynamic scenes, featuring explicit, persistent 3D motion trajectories in the world coordinate frame, from casually captured monocular videos.We tackle the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE(3) motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we take advantage of off-the-shelf data-driven priors such as monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF",
      "index": 215,
      "title": "FPEM: Face Prior Enhanced Facial Attractiveness Prediction for Live Videos with Face Retouching",
      "authors": [
        "Hui Li",
        "Xiaoyu Ren",
        "Hongjiu Yu",
        "Ying Chen",
        "Kai Li",
        "L Wang",
        "Xiongkuo Min",
        "Huiyu Duan",
        "Guangtao Zhai",
        "Xu Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "fap",
        "fpem",
        "attractiveness",
        "facial",
        "live",
        "livebeauty",
        "retouching",
        "face",
        "dataset",
        "videos"
      ],
      "summary": "Facial attractiveness prediction (FAP) has long been an important computer vision task, which could be widely applied in live videos with facial retouching. However, previous FAP datasets are either small or closed-source. Moreover, the corresponding FAP models exhibit limited generalization and adaptation ability.To overcome these limitations, we introduce the first large-scale FAP dataset LiveBeauty specifically designed for live video scenarios wherein face images may be real-time processed for aesthetics purposes.10,000 face images are collected directly from a live streaming platform, with 200,000 corresponding attractiveness annotations obtained from a well-devised subjective experiment, making LiveBeauty the largest open-access FAP dataset. Based on the built dataset, a novel FAP method named Facial Prior Enhanced Multi-modal model (FPEM) is proposed to measure the attractiveness of facial images.Extensive experiments conducted on both LiveBeauty and other open-source FAP datasets demonstrate that our proposed method achieves state-of-the-art performance. The dataset will be released at https://github.com/Estella-LH/FPEM.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.html",
          "/venue/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_FPEM_Face_Prior_Enhanced_Facial_Attractiveness_Prediction_for_Live_Videos_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "FPEM: Face Prior Enhanced Facial Attractiveness Prediction for Live Videos with Face Retouching [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Hui Li , Xiaoyu Ren , Hongjiu Yu , Ying Chen , Kai Li , L Wang , Xiongkuo Min , Huiyu Duan , Guangtao Zhai , Xu Liu Facial attractiveness prediction (FAP) has long been an important computer vision task, which could be widely applied in live videos with facial retouching. However, previous FAP datasets are either small or closed-source. Moreover, the corresponding FAP models exhibit limited generalization and adaptation ability.To overcome these limitations, we introduce the first large-scale FAP dataset LiveBeauty specifically designed for live video scenarios wherein face images may be real-time processed for aesthetics purposes.10,000 face images are collected directly from a live streaming platform, with 200,000 corresponding attractiveness annotations obtained from a well-devised subjective experiment, making LiveBeauty the largest open-access FAP dataset. Based on the built dataset, a novel FAP method named Facial Prior Enhanced Multi-modal model (FPEM) is proposed to measure the attractiveness of facial images.Extensive experiments conducted on both LiveBeauty and other open-source FAP datasets demonstrate that our proposed method achieves state-of-the-art performance. The dataset will be released at https://github.com/Estella-LH/FPEM. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF",
      "index": 216,
      "title": "Dataset Distillation via Vision-Language Category Prototype",
      "authors": [
        "Yawen Zou",
        "Guang Li",
        "Duo Su",
        "Zi Wang",
        "Jun Yu",
        "Chao Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "distillation",
        "dataset",
        "vision",
        "language",
        "prototypes",
        "text",
        "prototype",
        "category",
        "datasets",
        "information"
      ],
      "summary": "Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source vision-language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zou_Dataset_Distillation_via_Vision-Language_Category_Prototype_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Dataset Distillation via Vision-Language Category Prototype [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Yawen Zou , Guang Li , Duo Su , Zi Wang , Jun Yu , Chao Zhang Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source vision-language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF",
      "index": 217,
      "title": "A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions",
      "authors": [
        "Youliang Zhang",
        "Ronghui Li",
        "Yachao Zhang",
        "Liang Pan",
        "Jingbo Wang",
        "Yebin Liu",
        "Xiu Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "motion",
        "motions",
        "wild",
        "difficulty",
        "video",
        "flawed",
        "plug",
        "capture",
        "module",
        "physical"
      ],
      "summary": "Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions; and propose a physics-based motion transfer module (PTM), which employs a prior injected pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture, which also excels in motion generation tasks. Finally, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets. Our project page is : https://physicalmotionrestoration.github.io/",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_A_Plug-and-Play_Physical_Motion_Restoration_Approach_for_In-the-Wild_High-Difficulty_Motions_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Youliang Zhang , Ronghui Li , Yachao Zhang , Liang Pan , Jingbo Wang , Yebin Liu , Xiu Li Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions; and propose a physics-based motion transfer module (PTM), which employs a prior injected pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture, which also excels in motion generation tasks. Finally, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public datasets. Our project page is : https://physicalmotionrestoration.github.io/ Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF",
      "index": 218,
      "title": "Stereo Any Video: Temporally Consistent Stereo Matching",
      "authors": [
        "Junpeng Jing",
        "Weixun Luo",
        "Ye Mao",
        "Krystian Mikolajczyk"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "stereo",
        "video",
        "matching",
        "temporally",
        "temporal",
        "consistent",
        "disparities",
        "upsampling",
        "monocular",
        "architectural"
      ],
      "summary": "This paper introduces Stereo Any Video, a powerful framework for video stereo matching. It can estimate spatially accurate and temporally consistent disparities without relying on auxiliary information such as camera poses or optical flow. The strong capability is driven by rich priors from monocular video depth models, which are integrated with convolutional features to produce stable representations. To further enhance performance, key architectural innovations are introduced: all-to-all-pairs correlation, which constructs smooth and robust matching cost volumes, and temporal convex upsampling, which improves temporal coherence. These components collectively ensure robustness, accuracy, and temporal consistency, setting a new standard in video stereo matching. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple datasets both qualitatively and quantitatively in zero-shot settings, as well as strong generalization to real-world indoor and outdoor scenarios. Code and models will be publicly released.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching_ICCV_2025_paper.html",
          "/venue/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jing_Stereo_Any_Video_Temporally_Consistent_Stereo_Matching_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Stereo Any Video: Temporally Consistent Stereo Matching [PDF ] [Copy] [Kimi ] [REL] Authors : Junpeng Jing , Weixun Luo , Ye Mao , Krystian Mikolajczyk This paper introduces Stereo Any Video, a powerful framework for video stereo matching. It can estimate spatially accurate and temporally consistent disparities without relying on auxiliary information such as camera poses or optical flow. The strong capability is driven by rich priors from monocular video depth models, which are integrated with convolutional features to produce stable representations. To further enhance performance, key architectural innovations are introduced: all-to-all-pairs correlation, which constructs smooth and robust matching cost volumes, and temporal convex upsampling, which improves temporal coherence. These components collectively ensure robustness, accuracy, and temporal consistency, setting a new standard in video stereo matching. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple datasets both qualitatively and quantitatively in zero-shot settings, as well as strong generalization to real-world indoor and outdoor scenarios. Code and models will be publicly released. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF",
      "index": 219,
      "title": "Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration",
      "authors": [
        "Baoyou Chen",
        "Ce Liu",
        "Weihao Yuan",
        "Zilong Dong",
        "Siyu Zhu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "restoration",
        "video",
        "codebook",
        "facial",
        "variational",
        "face",
        "dirichlet",
        "constrained",
        "pretrained",
        "quality"
      ],
      "summary": "Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration_ICCV_2025_paper.html",
          "/venue/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chen_Dirichlet-Constrained_Variational_Codebook_Learning_for_Temporally_Coherent_Video_Face_Restoration_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Baoyou Chen , Ce Liu , Weihao Yuan , Zilong Dong , Siyu Zhu Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF",
      "index": 220,
      "title": "When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation",
      "authors": [
        "Pan Liu",
        "Jinshi Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "csl",
        "confidence",
        "pseudo",
        "label",
        "predictions",
        "discarding",
        "selection",
        "semantic",
        "segmentation",
        "revisiting"
      ],
      "summary": "While significant advances exist in pseudo-label generation for semi-supervised semantic segmentation, pseudo-label selection remains understudied. Existing methods typically use fixed confidence thresholds to retain high-confidence predictions as pseudo-labels. However, these methods cannot cope with network overconfidence tendency, where correct and incorrect predictions overlap significantly in high-confidence regions, making separation challenging and amplifying model cognitive bias. Meanwhile, the direct discarding of low-confidence predictions disrupts spatial-semantic continuity, causing critical context loss. We propose Confidence Separable Learning (CSL) to address these limitations. CSL formulates pseudo-label selection as a convex optimization problem within the confidence distribution feature space, establishing sample-specific decision boundaries to distinguish reliable from unreliable predictions. Additionally, CSL introduces random masking of reliable pixels to guide the network in learning contextual relationships from low-reliability regions, thereby mitigating the adverse effects of discarding uncertain predictions. Extensive experimental results on the Pascal, Cityscapes, and COCO benchmarks show that CSL performs favorably against state-of-the-art methods. Code and model weights are available at: https://github.com/PanLiuCSU/CSL.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_When_Confidence_Fails_Revisiting_Pseudo-Label_Selection_in_Semi-supervised_Semantic_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Pan Liu , Jinshi Liu While significant advances exist in pseudo-label generation for semi-supervised semantic segmentation, pseudo-label selection remains understudied. Existing methods typically use fixed confidence thresholds to retain high-confidence predictions as pseudo-labels. However, these methods cannot cope with network overconfidence tendency, where correct and incorrect predictions overlap significantly in high-confidence regions, making separation challenging and amplifying model cognitive bias. Meanwhile, the direct discarding of low-confidence predictions disrupts spatial-semantic continuity, causing critical context loss. We propose Confidence Separable Learning (CSL) to address these limitations. CSL formulates pseudo-label selection as a convex optimization problem within the confidence distribution feature space, establishing sample-specific decision boundaries to distinguish reliable from unreliable predictions. Additionally, CSL introduces random masking of reliable pixels to guide the network in learning contextual relationships from low-reliability regions, thereby mitigating the adverse effects of discarding uncertain predictions. Extensive experimental results on the Pascal, Cityscapes, and COCO benchmarks show that CSL performs favorably against state-of-the-art methods. Code and model weights are available at: https://github.com/PanLiuCSU/CSL. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF",
      "index": 221,
      "title": "Scendi Score: Prompt-Aware Diversity Evaluation via Schur Complement of CLIP Embeddings",
      "authors": [
        "Azim Ospanov",
        "Mohammad Jalali",
        "Farzan Farnia"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "scendi",
        "text",
        "prompt",
        "clip",
        "diversity",
        "schur",
        "score",
        "complement",
        "image",
        "embeddings"
      ],
      "summary": "The use of CLIP embeddings to assess the fidelity of samples produced by text-to-image generative models has been extensively explored in the literature. While the widely adopted CLIPScore, derived from the cosine similarity of text and image embeddings, effectively measures the alignment of a generated image, it does not quantify the diversity of images generated by a text-to-image model. In this work, we extend the application of CLIP embeddings to quantify and interpret the intrinsic diversity of text-to-image models, which are responsible for generating diverse images from similar text prompts, which we refer to as prompt-aware diversity. To achieve this, we propose a decomposition of the CLIP-based kernel covariance matrix of image data into text-based and non-text-based components. Using the Schur complement of the joint image-text kernel covariance matrix, we perform this decomposition and define the matrix-based entropy of the decomposed component as the Schur Complement ENtopy DIversity (Scendi) score, as a measure of the prompt-aware diversity for prompt-guided generative models. Additionally, we discuss the application of the Schur complement-based decomposition to nullify the influence of a given prompt on the CLIP embedding of an image, enabling focus or defocus of the embedded vectors on specific objects. We present several numerical results that apply our proposed Scendi score to evaluate text-to-image and LLM (text-to-text) models. Our numerical results indicate the success of the Scendi score in capturing the intrinsic diversity of prompt-guided generative models. The codebase is available at https://github.com/aziksh-ospanov/scendi-score.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ospanov_Scendi_Score_Prompt-Aware_Diversity_Evaluation_via_Schur_Complement_of_CLIP_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Scendi Score: Prompt-Aware Diversity Evaluation via Schur Complement of CLIP Embeddings [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Azim Ospanov , Mohammad Jalali , Farzan Farnia The use of CLIP embeddings to assess the fidelity of samples produced by text-to-image generative models has been extensively explored in the literature. While the widely adopted CLIPScore, derived from the cosine similarity of text and image embeddings, effectively measures the alignment of a generated image, it does not quantify the diversity of images generated by a text-to-image model. In this work, we extend the application of CLIP embeddings to quantify and interpret the intrinsic diversity of text-to-image models, which are responsible for generating diverse images from similar text prompts, which we refer to as prompt-aware diversity. To achieve this, we propose a decomposition of the CLIP-based kernel covariance matrix of image data into text-based and non-text-based components. Using the Schur complement of the joint image-text kernel covariance matrix, we perform this decomposition and define the matrix-based entropy of the decomposed component as the Schur Complement ENtopy DIversity (Scendi) score, as a measure of the prompt-aware diversity for prompt-guided generative models. Additionally, we discuss the application of the Schur complement-based decomposition to nullify the influence of a given prompt on the CLIP embedding of an image, enabling focus or defocus of the embedded vectors on specific objects. We present several numerical results that apply our proposed Scendi score to evaluate text-to-image and LLM (text-to-text) models. Our numerical results indicate the success of the Scendi score in capturing the intrinsic diversity of prompt-guided generative models. The codebase is available at https://github.com/aziksh-ospanov/scendi-score. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF",
      "index": 222,
      "title": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding",
      "authors": [
        "Ta Duc Huy",
        "Duy Anh Huynh",
        "Yutong Xie",
        "Yuankai Qi",
        "Qi Chen",
        "Phi Le Nguyen",
        "Sen Kim Tran",
        "Son Lam Phung",
        "Anton van den Hengel",
        "Zhibin Liao",
        "Minh-Son To",
        "Johan W. Verjans",
        "Vu Minh Hieu Phan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "disease",
        "grounding",
        "tokens",
        "dap",
        "regions",
        "visual",
        "medical",
        "seeing",
        "rethinking",
        "textual"
      ],
      "summary": "Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huy_Seeing_the_Trees_for_the_Forest_Rethinking_Weakly-Supervised_Medical_Visual_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Ta Duc Huy , Duy Anh Huynh , Yutong Xie , Yuankai Qi , Qi Chen , Phi Le Nguyen , Sen Kim Tran , Son Lam Phung , Anton van den Hengel , Zhibin Liao , Minh-Son To , Johan W. Verjans , Vu Minh Hieu Phan Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF",
      "index": 223,
      "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts",
      "authors": [
        "Yun Wang",
        "Longguang Wang",
        "Chenghao Zhang",
        "Yongjian Zhang",
        "Zhanjie Zhang",
        "Ao Ma",
        "Chenyou Fan",
        "Tin Lun Lam",
        "Junjie Hu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "moe",
        "stereo",
        "vfms",
        "matching",
        "experts",
        "smoestereo",
        "lora",
        "robustness",
        "mixture",
        "domain"
      ],
      "summary": "Recently, learning-based stereo matching networks have advanced significantly.However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets.Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge.To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules.SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction.Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at https://github.com/cocowy1/SMoE-Stereo.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Learning_Robust_Stereo_Matching_in_the_Wild_with_Selective_Mixture-of-Experts_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Yun Wang , Longguang Wang , Chenghao Zhang , Yongjian Zhang , Zhanjie Zhang , Ao Ma , Chenyou Fan , Tin Lun Lam , Junjie Hu Recently, learning-based stereo matching networks have advanced significantly.However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets.Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge.To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules.SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction.Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at https://github.com/cocowy1/SMoE-Stereo. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF",
      "index": 224,
      "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
      "authors": [
        "Linzhan Mou",
        "Jiahui Lei",
        "Chen Wang",
        "Lingjie Liu",
        "Kostas Daniilidis"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dimo",
        "motion",
        "motions",
        "diverse",
        "latent",
        "embed",
        "generation",
        "arbitrary",
        "objects",
        "shared"
      ],
      "summary": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Mou_DIMO_Diverse_3D_Motion_Generation_for_Arbitrary_Objects_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Linzhan Mou , Jiahui Lei , Chen Wang , Lingjie Liu , Kostas Daniilidis We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF",
      "index": 225,
      "title": "Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation",
      "authors": [
        "Guanyi Qin",
        "Ziyue Wang",
        "Daiyun Shen",
        "Haofeng Liu",
        "Hantao Zhou",
        "Junde Wu",
        "Runze Hu",
        "Yueming Jin"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "object",
        "video",
        "segmentation",
        "refinement",
        "oasis",
        "davis",
        "boundary",
        "structure",
        "svos",
        "youtubevos"
      ],
      "summary": "Given an object mask, Semi-supervised Video Object Segmentation (SVOS) technique aims to track and segment the object across video frames, serving as a fundamental task in computer vision. Although recent memory-based methods demonstrate potential, they often struggle with scenes involving occlusion, particularly in handling object interactions and high feature similarity. To address these issues and meet the real-time processing requirements of downstream applications, in this paper, we propose a novel bOundary Amendment video object Segmentation method with Inherent Structure refinement, hereby named OASIS. Specifically, a lightweight structure refinement module is proposed to enhance segmentation accuracy. With the fusion of rough edge priors captured by the Canny filter and stored object features, the module can generate an object-level structure map and refine the representations by highlighting boundary features. Evidential learning for uncertainty estimation is introduced to further address challenges in occluded regions. The proposed method, OASIS, maintains an efficient design, yet extensive experiments on challenging benchmarks demonstrate its superior performance and competitive inference speed compared to other state-of-the-art methods, i.e., achieving the F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6 (vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive speed of 48 FPS on DAVIS.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation_ICCV_2025_paper.html",
          "/venue/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Structure_Matters_Revisiting_Boundary_Refinement_in_Video_Object_Segmentation_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Guanyi Qin , Ziyue Wang , Daiyun Shen , Haofeng Liu , Hantao Zhou , Junde Wu , Runze Hu , Yueming Jin Given an object mask, Semi-supervised Video Object Segmentation (SVOS) technique aims to track and segment the object across video frames, serving as a fundamental task in computer vision. Although recent memory-based methods demonstrate potential, they often struggle with scenes involving occlusion, particularly in handling object interactions and high feature similarity. To address these issues and meet the real-time processing requirements of downstream applications, in this paper, we propose a novel bOundary Amendment video object Segmentation method with Inherent Structure refinement, hereby named OASIS. Specifically, a lightweight structure refinement module is proposed to enhance segmentation accuracy. With the fusion of rough edge priors captured by the Canny filter and stored object features, the module can generate an object-level structure map and refine the representations by highlighting boundary features. Evidential learning for uncertainty estimation is introduced to further address challenges in occluded regions. The proposed method, OASIS, maintains an efficient design, yet extensive experiments on challenging benchmarks demonstrate its superior performance and competitive inference speed compared to other state-of-the-art methods, i.e., achieving the F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6 (vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive speed of 48 FPS on DAVIS. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF",
      "index": 226,
      "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking",
      "authors": [
        "Eyad Alshami",
        "Shashank Agnihotri",
        "Bernt Schiele",
        "Margret Keuper"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "aim",
        "amending",
        "interpretability",
        "masking",
        "genuine",
        "spurious",
        "features",
        "self",
        "supervised",
        "inherent"
      ],
      "summary": "It has been observed that deep neural networks (DNNs) often use both genuine as well as spurious features.In this work, we propose \"Amending Inherent Interpretability via Self-Supervised Masking\" (AIM), a simple yet surprisingly effective method that promotes the network's utilization of genuine features over spurious alternatives without requiring additional annotations.In particular, AIM uses features at multiple encoding stages to guide a self-supervised, sample-specific feature-masking process. As a result, AIM allows training well-performing and inherently interpretable models that faithfully summarize the decision process.When tested on challenging datasets designed to assess reliance on spurious features and out-of-domain generalization, AIM networks demonstrate significant dual benefits: Evaluations show that AIM improves interpretability, as measured by the Energy Pointing Game (EPG) score, by ~6-37%, while simultaneously enhancing accuracy by ~10-40%. These impressive performance gains are further validated on the standard in-domain CUB-200 dataset for fine-grained classification. The results provide compelling evidence supporting our hypothesis that AIM finds genuine and meaningful features that directly contribute to its improved human interpretability.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Alshami_AIM_Amending_Inherent_Interpretability_via_Self-Supervised_Masking_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "AIM: Amending Inherent Interpretability via Self-Supervised Masking [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Eyad Alshami , Shashank Agnihotri , Bernt Schiele , Margret Keuper It has been observed that deep neural networks (DNNs) often use both genuine as well as spurious features.In this work, we propose \"Amending Inherent Interpretability via Self-Supervised Masking\" (AIM), a simple yet surprisingly effective method that promotes the network's utilization of genuine features over spurious alternatives without requiring additional annotations.In particular, AIM uses features at multiple encoding stages to guide a self-supervised, sample-specific feature-masking process. As a result, AIM allows training well-performing and inherently interpretable models that faithfully summarize the decision process.When tested on challenging datasets designed to assess reliance on spurious features and out-of-domain generalization, AIM networks demonstrate significant dual benefits: Evaluations show that AIM improves interpretability, as measured by the Energy Pointing Game (EPG) score, by ~6-37%, while simultaneously enhancing accuracy by ~10-40%. These impressive performance gains are further validated on the standard in-domain CUB-200 dataset for fine-grained classification. The results provide compelling evidence supporting our hypothesis that AIM finds genuine and meaningful features that directly contribute to its improved human interpretability. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF",
      "index": 227,
      "title": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation",
      "authors": [
        "Wentao Hu",
        "Shunkai Li",
        "Ziqiao Peng",
        "Haoxian Zhang",
        "Fan Shi",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Di Zhang",
        "Hui Tian"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "talking",
        "ggtalker",
        "priors",
        "head",
        "generalizable",
        "adaptation",
        "systhesis",
        "identity",
        "heads",
        "audio"
      ],
      "summary": "Creating high-quality, generalizable speech-driven 3D talking heads remains a persistent challenge. Previous methods achieve satisfactory results for fixed viewpoints and small-scale audio variations, but they struggle with large head rotations and out-of-distribution (OOD) audio. Moreover, they are constrained by the need for time-consuming, identity-specific training. We believe the core issue lies in the lack of sufficient 3D priors, which limits the extrapolation capabilities of synthesized talking heads. To address this, we propose GGTalker, which synthesizes talking heads through a combination of generalizable priors and identity-specific adaptation. We introduce a two-stage Prior-Adaptation training strategy to learn Gaussian head priors and adapt to individual characteristics. We train Audio-Expression and Expression-Visual priors to capture the universal patterns of lip movements and the general distribution of head textures. During the Customized Adaptation, individual speaking styles and texture details are precisely modeled. Additionally, we introduce a color MLP to generate fine-grained, motion-aligned textures and a Body Inpainter to blend rendered results with the background, producing indistinguishable, photorealistic video frames. Comprehensive experiments show that GGTalker achieves state-of-the-art performance in rendering quality, 3D consistency, lip-sync accuracy, and training efficiency.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Hu_GGTalker_Talking_Head_Systhesis_with_Generalizable_Gaussian_Priors_and_Identity-Specific_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Wentao Hu , Shunkai Li , Ziqiao Peng , Haoxian Zhang , Fan Shi , Xiaoqiang Liu , Pengfei Wan , Di Zhang , Hui Tian Creating high-quality, generalizable speech-driven 3D talking heads remains a persistent challenge. Previous methods achieve satisfactory results for fixed viewpoints and small-scale audio variations, but they struggle with large head rotations and out-of-distribution (OOD) audio. Moreover, they are constrained by the need for time-consuming, identity-specific training. We believe the core issue lies in the lack of sufficient 3D priors, which limits the extrapolation capabilities of synthesized talking heads. To address this, we propose GGTalker, which synthesizes talking heads through a combination of generalizable priors and identity-specific adaptation. We introduce a two-stage Prior-Adaptation training strategy to learn Gaussian head priors and adapt to individual characteristics. We train Audio-Expression and Expression-Visual priors to capture the universal patterns of lip movements and the general distribution of head textures. During the Customized Adaptation, individual speaking styles and texture details are precisely modeled. Additionally, we introduce a color MLP to generate fine-grained, motion-aligned textures and a Body Inpainter to blend rendered results with the background, producing indistinguishable, photorealistic video frames. Comprehensive experiments show that GGTalker achieves state-of-the-art performance in rendering quality, 3D consistency, lip-sync accuracy, and training efficiency. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF",
      "index": 228,
      "title": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds",
      "authors": [
        "Jizong Peng",
        "Tze Ho Elden Tse",
        "Kai Xu",
        "Wenchao Gao",
        "Angela Yao"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "3dgs",
        "sfm",
        "splatting",
        "reconstruction",
        "camera",
        "clouds",
        "optimization",
        "initialization",
        "restricts",
        "coarsely"
      ],
      "summary": "3D Gaussian Splatting (3DGS) is a powerful reconstruction technique; however, it requires initialization from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Peng_A_Constrained_Optimization_Approach_for_Gaussian_Splatting_from_Coarsely-posed_Images_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jizong Peng , Tze Ho Elden Tse , Kai Xu , Wenchao Gao , Angela Yao 3D Gaussian Splatting (3DGS) is a powerful reconstruction technique; however, it requires initialization from accurate camera poses and high-fidelity point clouds. Typically, the initialization is taken from Structure-from-Motion (SfM) algorithms; however, SfM is time-consuming and restricts the application of 3DGS in real-world scenarios and large-scale scene reconstruction. We introduce a constrained optimization method for simultaneous camera pose estimation and 3D reconstruction that does not require SfM support. Core to our approach is decomposing a camera pose into a sequence of camera-to-(device-)center and (device-)center-to-world optimizations. To facilitate, we propose two optimization constraints conditioned to the sensitivity of each parameter group and restricts each parameter's search space. In addition, as we learn the scene geometry directly from the noisy point clouds, we propose geometric constraints to improve the reconstruction quality. Experiments demonstrate that the proposed method significantly outperforms the existing (multi-modal) 3DGS baseline and methods supplemented by COLMAP on both our collected dataset and two public benchmarks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF",
      "index": 229,
      "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting",
      "authors": [
        "Xingyu Miao",
        "Haoran Duan",
        "Quanhao Qian",
        "Jiuniu Wang",
        "Yang Long",
        "Ling Shao",
        "Deli Zhao",
        "Ran Xu",
        "Gongjie Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "spatial",
        "intelligence",
        "imagery",
        "scalable",
        "scale",
        "objects365",
        "data",
        "lifting",
        "pipeline",
        "camera"
      ],
      "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations -- including point clouds, camera poses, depth maps, and pseudo-RGBD -- via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Miao_Towards_Scalable_Spatial_Intelligence_via_2D-to-3D_Data_Lifting_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Xingyu Miao , Haoran Duan , Quanhao Qian , Jiuniu Wang , Yang Long , Ling Shao , Deli Zhao , Ran Xu , Gongjie Zhang Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations -- including point clouds, camera poses, depth maps, and pseudo-RGBD -- via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF",
      "index": 230,
      "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting",
      "authors": [
        "Seunggeun Chi",
        "Enna Sachdeva",
        "Pin-Hao Huang",
        "Kwonjoon Lee"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "hoi",
        "amodal",
        "regional",
        "inpainting",
        "human",
        "contact",
        "occlusions",
        "completions",
        "completion",
        "object"
      ],
      "summary": "Amodal completion, the task of inferring the complete appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, including pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios due to their limited understanding of HOI. To address this challenge, we propose a novel approach that leverages physical prior knowledge alongside a specialized multi-regional inpainting technique tailored for HOI. By incorporating physical constraints derived from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to reside, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method employs customized denoising strategies across these regions within a diffusion model, thereby enhancing the accuracy and realism of generated completions in both shape and visual detail. Experimental results demonstrate that our approach substantially outperforms existing methods in HOI scenarios, advancing machine perception toward a more human-like understanding of dynamic environments. Furthermore, we show that our pipeline remains robust even without ground-truth contact annotations, broadening its applicability to tasks such as 3D reconstruction and novel view/pose synthesis.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Chi_Contact-Aware_Amodal_Completion_for_Human-Object_Interaction_via_Multi-Regional_Inpainting_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting [PDF ] [Copy] [Kimi ] [REL] Authors : Seunggeun Chi , Enna Sachdeva , Pin-Hao Huang , Kwonjoon Lee Amodal completion, the task of inferring the complete appearance of objects despite partial occlusions, is crucial for understanding complex human-object interactions (HOI) in computer vision and robotics. Existing methods, including pre-trained diffusion models, often struggle to generate plausible completions in dynamic scenarios due to their limited understanding of HOI. To address this challenge, we propose a novel approach that leverages physical prior knowledge alongside a specialized multi-regional inpainting technique tailored for HOI. By incorporating physical constraints derived from human topology and contact information, we define two distinct regions: the primary region, where occluded object parts are most likely to reside, and the secondary region, where occlusions are less probable. Our multi-regional inpainting method employs customized denoising strategies across these regions within a diffusion model, thereby enhancing the accuracy and realism of generated completions in both shape and visual detail. Experimental results demonstrate that our approach substantially outperforms existing methods in HOI scenarios, advancing machine perception toward a more human-like understanding of dynamic environments. Furthermore, we show that our pipeline remains robust even without ground-truth contact annotations, broadening its applicability to tasks such as 3D reconstruction and novel view/pose synthesis. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF",
      "index": 231,
      "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior",
      "authors": [
        "Haoran Wang",
        "Bo Zhao",
        "Jinghui Wang",
        "Hanzhang Wang",
        "Huan Yang",
        "Wei Ji",
        "Hao Liu",
        "Xinyan Xiao"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sega",
        "layout",
        "stepwise",
        "planning",
        "coarse",
        "content",
        "aware",
        "reasoning",
        "brucew91",
        "generation"
      ],
      "summary": "In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_SEGA_A_Stepwise_Evolution_Paradigm_for_Content-Aware_Layout_Generation_with_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Haoran Wang , Bo Zhao , Jinghui Wang , Hanzhang Wang , Huan Yang , Wei Ji , Hao Liu , Xinyan Xiao In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF",
      "index": 232,
      "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology",
      "authors": [
        "Siyuan Yan",
        "Ming Hu",
        "Yiwen Jiang",
        "Xieji Li",
        "Hao Fei",
        "Philipp Tschandl",
        "Harald Kittler",
        "Zongyuan Ge"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "derm1m",
        "clinical",
        "dermatology",
        "skin",
        "dermlip",
        "dataset",
        "vision",
        "ontology",
        "medical",
        "shot"
      ],
      "summary": "The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M's potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code are available at https://github.com/SiyuanYan1/Derm1M.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Siyuan Yan , Ming Hu , Yiwen Jiang , Xieji Li , Hao Fei , Philipp Tschandl , Harald Kittler , Zongyuan Ge The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M's potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code are available at https://github.com/SiyuanYan1/Derm1M. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF",
      "index": 233,
      "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them",
      "authors": [
        "Zhongdao Wang",
        "Guodongfang Zhao",
        "Jingjing Ren",
        "Bailan Feng",
        "Shifeng Zhang",
        "Wenbo Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "turbovsr",
        "1080p",
        "video",
        "super",
        "resolution",
        "upscalers",
        "vsr",
        "fantastic",
        "frame",
        "diffusion"
      ],
      "summary": "Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32x32x8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648x2048) image SR show surprising fine details.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them_ICCV_2025_paper.html",
          "/venue/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_TurboVSR_Fantastic_Video_Upscalers_and_Where_to_Find_Them_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "TurboVSR: Fantastic Video Upscalers and Where to Find Them [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Zhongdao Wang , Guodongfang Zhao , Jingjing Ren , Bailan Feng , Shifeng Zhang , Wenbo Li Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32x32x8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648x2048) image SR show surprising fine details. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF",
      "index": 234,
      "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
      "authors": [
        "Xinlong Ding",
        "Hongwei Yu",
        "Jiawei Li",
        "Feifan Li",
        "Yu Shang",
        "Bochao Zou",
        "Huimin Ma",
        "Jiansheng Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "kaleidoscopic",
        "pose",
        "attack",
        "estimation",
        "segments",
        "textures",
        "fold",
        "background",
        "viewpoints",
        "kba"
      ],
      "summary": "Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to a significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Ding_Kaleidoscopic_Background_Attack_Disrupting_Pose_Estimation_with_Multi-Fold_Radial_Symmetry_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Xinlong Ding , Hongwei Yu , Jiawei Li , Feifan Li , Yu Shang , Bochao Zou , Huimin Ma , Jiansheng Chen Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to a significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF",
      "index": 235,
      "title": "Rethinking DPO-style Diffusion Aligning Frameworks",
      "authors": [
        "Xun Wu",
        "Shaohan Huang",
        "Lingjie Jiang",
        "Furu Wei"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dpo",
        "rdpo",
        "preference",
        "dispreferred",
        "step",
        "diffusion",
        "aligning",
        "wise",
        "alignment",
        "rethinking"
      ],
      "summary": "Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference. However, We identify two potential risks for existing DPO algorithms: First, current DPO methods for estimating the rewards of step-wise intermediate samples are biased, leading to inaccurate preference ordering for step-wise optimization. Second, existing DPO methods may inadvertently increase the sampling probabilities of dispreferred samples, potentially introducing application risks. To address these issues, we propose Revised Direct Preference Optimization (RDPO), a simple but effective step-wise DPO-based text-to-image diffusion model alignment method. By designing a more theoretically grounded and efficient intermediate-step reward estimation and introducing an additional regularization terms to constrain the sampling probability of dispreferred samples, RDPO can achieve more effective and stable text-to-image alignment performance. Our experiments on two datasets, with base models including Stable Diffusion v1.5 and SDXL, demonstrate that RDPO can effectively learn and construct reward signals for each step of the model, improving alignment performance while ensuring better generalization.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Rethinking_DPO-style_Diffusion_Aligning_Frameworks_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Rethinking DPO-style Diffusion Aligning Frameworks [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Xun Wu , Shaohan Huang , Lingjie Jiang , Furu Wei Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference. However, We identify two potential risks for existing DPO algorithms: First, current DPO methods for estimating the rewards of step-wise intermediate samples are biased, leading to inaccurate preference ordering for step-wise optimization. Second, existing DPO methods may inadvertently increase the sampling probabilities of dispreferred samples, potentially introducing application risks. To address these issues, we propose Revised Direct Preference Optimization (RDPO), a simple but effective step-wise DPO-based text-to-image diffusion model alignment method. By designing a more theoretically grounded and efficient intermediate-step reward estimation and introducing an additional regularization terms to constrain the sampling probability of dispreferred samples, RDPO can achieve more effective and stable text-to-image alignment performance. Our experiments on two datasets, with base models including Stable Diffusion v1.5 and SDXL, demonstrate that RDPO can effectively learn and construct reward signals for each step of the model, improving alignment performance while ensuring better generalization. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF",
      "index": 236,
      "title": "Ensemble Foreground Management for Unsupervised Object Discovery",
      "authors": [
        "Ziling Wu",
        "Armaghan Moemeni",
        "Praminda Caleb-Solly"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "foreground",
        "uod",
        "unioncut",
        "discovery",
        "unionseg",
        "union",
        "objects",
        "object",
        "ensemble",
        "unsupervised"
      ],
      "summary": "Unsupervised object discovery (UOD) aims to detect and segment objects in 2D images without handcrafted annotations. Recent progress in self-supervised representation learning has led to some success in UOD algorithms. However, the absence of ground truth provides existing UOD methods with two challenges: 1) determining if a discovered region is foreground or background, and 2) knowing how many objects remain undiscovered. To address these two problems, previous solutions rely on foreground priors to distinguish if the discovered region is foreground, and conduct one or fixed iterations of discovery. However, the existing foreground priors are heuristic and not always robust, and a fixed number of discoveries leads to under or over-segmentation, since the number of objects in images varies. This paper introduces UnionCut, a robust and well-grounded foreground prior based on min-cut and ensemble methods that detects the union of foreground areas of an image, allowing UOD algorithms to identify foreground objects and stop discovery once the majority of the foreground union in the image is segmented. In addition, we propose UnionSeg, a distilled transformer of UnionCut that outputs the foreground union more efficiently and accurately. Our experiments show that by combining with UnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increase in the performance of single object discovery, saliency detection and self-supervised instance segmentation on various benchmarks. The code is available at https://github.com/YFaris/UnionCut.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wu_Ensemble_Foreground_Management_for_Unsupervised_Object_Discovery_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Ensemble Foreground Management for Unsupervised Object Discovery [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Ziling Wu , Armaghan Moemeni , Praminda Caleb-Solly Unsupervised object discovery (UOD) aims to detect and segment objects in 2D images without handcrafted annotations. Recent progress in self-supervised representation learning has led to some success in UOD algorithms. However, the absence of ground truth provides existing UOD methods with two challenges: 1) determining if a discovered region is foreground or background, and 2) knowing how many objects remain undiscovered. To address these two problems, previous solutions rely on foreground priors to distinguish if the discovered region is foreground, and conduct one or fixed iterations of discovery. However, the existing foreground priors are heuristic and not always robust, and a fixed number of discoveries leads to under or over-segmentation, since the number of objects in images varies. This paper introduces UnionCut, a robust and well-grounded foreground prior based on min-cut and ensemble methods that detects the union of foreground areas of an image, allowing UOD algorithms to identify foreground objects and stop discovery once the majority of the foreground union in the image is segmented. In addition, we propose UnionSeg, a distilled transformer of UnionCut that outputs the foreground union more efficiently and accurately. Our experiments show that by combining with UnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increase in the performance of single object discovery, saliency detection and self-supervised instance segmentation on various benchmarks. The code is available at https://github.com/YFaris/UnionCut. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF",
      "index": 237,
      "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
      "authors": [
        "Liming Jiang",
        "Qing Yan",
        "Yumin Jia",
        "Zichuan Liu",
        "Hao Kang",
        "Xin Lu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "infu",
        "identity",
        "infiniteyou",
        "dits",
        "recrafting",
        "flexible",
        "image",
        "generation",
        "spms",
        "ameliorates"
      ],
      "summary": "Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Jiang_InfiniteYou_Flexible_Photo_Recrafting_While_Preserving_Your_Identity_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity [PDF ] [Copy] [Kimi ] [REL] Authors : Liming Jiang , Qing Yan , Yumin Jia , Zichuan Liu , Hao Kang , Xin Lu Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF",
      "index": 238,
      "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer",
      "authors": [
        "Weixian Lei",
        "Jiacong Wang",
        "Haochen Wang",
        "Xiangtai Li",
        "Jun Hao Liew",
        "Jiashi Feng",
        "Zilong Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sail",
        "mllms",
        "vision",
        "scalability",
        "vit",
        "transformer",
        "modular",
        "language",
        "multimodal",
        "visual"
      ],
      "summary": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal rotary position embedding to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Lei_The_Scalability_of_Simplicity_Empirical_Analysis_of_Vision-Language_Learning_with_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Weixian Lei , Jiacong Wang , Haochen Wang , Xiangtai Li , Jun Hao Liew , Jiashi Feng , Zilong Huang This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal rotary position embedding to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF",
      "index": 239,
      "title": "CounterPC: Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds",
      "authors": [
        "Feng Yang",
        "Yichao Cao",
        "Xiu Su",
        "Dan Niu",
        "Xuanpeng Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "counterfactual",
        "domain",
        "counterpc",
        "realignment",
        "adaptation",
        "feature",
        "category",
        "clouds",
        "alignment",
        "graspnetpc"
      ],
      "summary": "Understanding real-world 3D point clouds is challenging due to domain shifts, causing geometric variations like density changes, noise, and occlusions. The key challenge is disentangling domain-invariant semantics from domain-specific geometric variations, as point clouds exhibit local inconsistency and global redundancy, making direct alignment ineffective. To address this, we propose CounterPC, a counterfactual intervention-based domain adaptation framework, which formulates domain adaptation within a causal latent space, identifying category-discriminative features entangled with intra-class geometric variation confounders. Through counterfactual interventions, we generate counterfactual target samples that retain domain-specific characteristics while improving class separation, mitigating domain bias for optimal feature transfer. To achieve this, we introduce two key modules: i) Joint Distribution Alignment, which leverages 3D foundation models (3D-FMs) and a self-supervised autoregressive generative prediction task to unify feature alignment, and ii) Counterfactual Feature Realignment, which employs Optimal Transport (OT) to align category-relevant and category-irrelevant feature distributions, ensuring robust sample-level adaptation while preserving domain and category properties. CounterPC outperforms state-of-the-art methods on PointDA and GraspNetPC-10, achieving accuracy improvements of 4.7 and 3.6, respectively. Code and pre-trained weights will be publicly released.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CounterPC_Counterfactual_Feature_Realignment_for_Unsupervised_Domain_Adaptation_on_Point_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "CounterPC: Counterfactual Feature Realignment for Unsupervised Domain Adaptation on Point Clouds [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Feng Yang , Yichao Cao , Xiu Su , Dan Niu , Xuanpeng Li Understanding real-world 3D point clouds is challenging due to domain shifts, causing geometric variations like density changes, noise, and occlusions. The key challenge is disentangling domain-invariant semantics from domain-specific geometric variations, as point clouds exhibit local inconsistency and global redundancy, making direct alignment ineffective. To address this, we propose CounterPC, a counterfactual intervention-based domain adaptation framework, which formulates domain adaptation within a causal latent space, identifying category-discriminative features entangled with intra-class geometric variation confounders. Through counterfactual interventions, we generate counterfactual target samples that retain domain-specific characteristics while improving class separation, mitigating domain bias for optimal feature transfer. To achieve this, we introduce two key modules: i) Joint Distribution Alignment, which leverages 3D foundation models (3D-FMs) and a self-supervised autoregressive generative prediction task to unify feature alignment, and ii) Counterfactual Feature Realignment, which employs Optimal Transport (OT) to align category-relevant and category-irrelevant feature distributions, ensuring robust sample-level adaptation while preserving domain and category properties. CounterPC outperforms state-of-the-art methods on PointDA and GraspNetPC-10, achieving accuracy improvements of 4.7 and 3.6, respectively. Code and pre-trained weights will be publicly released. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF",
      "index": 240,
      "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
      "authors": [
        "Muhammad Danish",
        "Muhammad Akhtar Munir",
        "Syed Roshaan Ali Shah",
        "Kartik Kuckreja",
        "Fahad Shahbaz Khan",
        "Paolo Fraccaro",
        "Alexandre Lacoste",
        "Salman Khan"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "geospatial",
        "geobench",
        "vlms",
        "vlm",
        "challenges",
        "object",
        "tasks",
        "onevision",
        "vision",
        "specific"
      ],
      "summary": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications.Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management.Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery.To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales.We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Danish_GEOBench-VLM_Benchmarking_Vision-Language_Models_for_Geospatial_Tasks_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Muhammad Danish , Muhammad Akhtar Munir , Syed Roshaan Ali Shah , Kartik Kuckreja , Fahad Shahbaz Khan , Paolo Fraccaro , Alexandre Lacoste , Salman Khan While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications.Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management.Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery.To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales.We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark will be publicly available. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF",
      "index": 241,
      "title": "Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions",
      "authors": [
        "Tommaso Galliena",
        "Tommaso Apicella",
        "Stefano Rosa",
        "Pietro Morerio",
        "Alessio Del Bue",
        "Lorenzo Natale"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "captioning",
        "caption",
        "pseudo",
        "embodied",
        "captions",
        "fine",
        "image",
        "coherent",
        "tune",
        "consensus"
      ],
      "summary": "We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Galliena_Embodied_Image_Captioning_Self-supervised_Learning_Agents_for_Spatially_Coherent_Image_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Tommaso Galliena , Tommaso Apicella , Stefano Rosa , Pietro Morerio , Alessio Del Bue , Lorenzo Natale We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF",
      "index": 242,
      "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
      "authors": [
        "Hengrui Kang",
        "Siwei Wen",
        "Zichen Wen",
        "Junyan Ye",
        "Weijia Li",
        "Peilin Feng",
        "Baichuan Zhou",
        "Bin Wang",
        "Dahua Lin",
        "Linfeng Zhang",
        "Conghui He"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "legion",
        "artifact",
        "synthetic",
        "image",
        "synthscars",
        "annotations",
        "detection",
        "explain",
        "textual",
        "images"
      ],
      "summary": "The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. More information about LEGION can be found at https://opendatalab.github.io/LEGION.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Kang_LEGION_Learning_to_Ground_and_Explain_for_Synthetic_Image_Detection_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "LEGION: Learning to Ground and Explain for Synthetic Image Detection [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Hengrui Kang , Siwei Wen , Zichen Wen , Junyan Ye , Weijia Li , Peilin Feng , Baichuan Zhou , Bin Wang , Dahua Lin , Linfeng Zhang , Conghui He The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. More information about LEGION can be found at https://opendatalab.github.io/LEGION. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF",
      "index": 243,
      "title": "Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person Anomaly Search",
      "authors": [
        "Shuyu Yang",
        "Yaxiong Wang",
        "Li Zhu",
        "Zhedong Zheng"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "text",
        "pab",
        "person",
        "walking",
        "anomaly",
        "pose",
        "benchmark",
        "search",
        "image",
        "anomalies"
      ],
      "summary": "Text-based person search aims to retrieve specific individuals across camera networks using natural language descriptions. However, current benchmarks often exhibit biases towards common actions like walking or standing, neglecting the critical need for identifying abnormal behaviors in real-world scenarios. To meet such demands, we propose a new task, text-based person anomaly search, locating pedestrians engaged in both routine or anomalous activities via text. To enable the training and evaluation of this new task, we construct a large-scale image-text Pedestrian Anomaly Behavior (PAB) benchmark, featuring a broad spectrum of actions, e.g., running, performing, playing soccer, and the corresponding anomalies, e.g., lying, being hit, and falling of the same identity. The training set of PAB comprises 1,013,605 synthesized image-text pairs of both normalities and anomalies, while the test set includes 1,978 real-world image-text pairs. To validate the potential of PAB, we introduce a cross-modal pose-aware framework, which integrates human pose patterns with identity-based hard negative pair sampling. Extensive experiments on the proposed benchmark show that synthetic training data facilitates the fine-grained behavior retrieval, and the proposed pose-aware method arrives at 84.93% recall@1 accuracy, surpassing other competitive methods.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_Beyond_Walking_A_Large-Scale_Image-Text_Benchmark_for_Text-based_Person_Anomaly_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person Anomaly Search [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Shuyu Yang , Yaxiong Wang , Li Zhu , Zhedong Zheng Text-based person search aims to retrieve specific individuals across camera networks using natural language descriptions. However, current benchmarks often exhibit biases towards common actions like walking or standing, neglecting the critical need for identifying abnormal behaviors in real-world scenarios. To meet such demands, we propose a new task, text-based person anomaly search, locating pedestrians engaged in both routine or anomalous activities via text. To enable the training and evaluation of this new task, we construct a large-scale image-text Pedestrian Anomaly Behavior (PAB) benchmark, featuring a broad spectrum of actions, e.g., running, performing, playing soccer, and the corresponding anomalies, e.g., lying, being hit, and falling of the same identity. The training set of PAB comprises 1,013,605 synthesized image-text pairs of both normalities and anomalies, while the test set includes 1,978 real-world image-text pairs. To validate the potential of PAB, we introduce a cross-modal pose-aware framework, which integrates human pose patterns with identity-based hard negative pair sampling. Extensive experiments on the proposed benchmark show that synthetic training data facilitates the fine-grained behavior retrieval, and the proposed pose-aware method arrives at 84.93% recall@1 accuracy, surpassing other competitive methods. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF",
      "index": 244,
      "title": "Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation",
      "authors": [
        "Fan Li",
        "Xuanbin Wang",
        "Xuan Wang",
        "Zhaoxiang Zhang",
        "Yuelei Xu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "scene",
        "clip",
        "diffusion",
        "vocabulary",
        "proxy",
        "dedos",
        "denoising",
        "queries",
        "segmentation",
        "images"
      ],
      "summary": "Recently, open-vocabulary semantic segmentation has garnered growing attention. Most current methods leverage vision-language models like CLIP to recognize unseen categories through their zero-shot capabilities. However, CLIP struggles to establish potential spatial dependencies among scene objects due to its holistic pre-training objective, causing sub-optimal results. In this paper, we propose a DEnoising learning framework based on the Diffusion model for Open-vocabulary semantic Segmentation, called DEDOS, which is aimed at constructing the scene skeleton. Motivation stems from the fact that diffusion models incorporate not only the visual appearance of objects but also embed rich scene spatial priors. Our core idea is to view images as labels embedded with \"noise\"--non-essential details for perceptual tasks--and to disentangle the intrinsic scene prior from the diffusion feature during the denoising process of the images. Specifically, to fully harness the scene prior knowledge of the diffusion model, we introduce learnable proxy queries during the denoising process. Meanwhile, we leverage the robustness of CLIP features to texture shifts as supervision, guiding proxy queries to focus on constructing the scene skeleton and avoiding interference from texture information in the diffusion feature space. Finally, we enhance spatial understanding within CLIP features using proxy queries, which also serve as an interface for multi-level interaction between text and visual modalities. Extensive experiments validate the effectiveness of our method, experimental results on five standard benchmarks have shown that DEDOS achieves state-of-the-art performance. We will make the code publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Li_Images_as_Noisy_Labels_Unleashing_the_Potential_of_the_Diffusion_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "Images as Noisy Labels: Unleashing the Potential of the Diffusion Model for Open-Vocabulary Semantic Segmentation [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Fan Li , Xuanbin Wang , Xuan Wang , Zhaoxiang Zhang , Yuelei Xu Recently, open-vocabulary semantic segmentation has garnered growing attention. Most current methods leverage vision-language models like CLIP to recognize unseen categories through their zero-shot capabilities. However, CLIP struggles to establish potential spatial dependencies among scene objects due to its holistic pre-training objective, causing sub-optimal results. In this paper, we propose a DEnoising learning framework based on the Diffusion model for Open-vocabulary semantic Segmentation, called DEDOS, which is aimed at constructing the scene skeleton. Motivation stems from the fact that diffusion models incorporate not only the visual appearance of objects but also embed rich scene spatial priors. Our core idea is to view images as labels embedded with \"noise\"--non-essential details for perceptual tasks--and to disentangle the intrinsic scene prior from the diffusion feature during the denoising process of the images. Specifically, to fully harness the scene prior knowledge of the diffusion model, we introduce learnable proxy queries during the denoising process. Meanwhile, we leverage the robustness of CLIP features to texture shifts as supervision, guiding proxy queries to focus on constructing the scene skeleton and avoiding interference from texture information in the diffusion feature space. Finally, we enhance spatial understanding within CLIP features using proxy queries, which also serve as an interface for multi-level interaction between text and visual modalities. Extensive experiments validate the effectiveness of our method, experimental results on five standard benchmarks have shown that DEDOS achieves state-of-the-art performance. We will make the code publicly available. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF",
      "index": 245,
      "title": "ResidualViT for Efficient Temporally Dense Video Encoding",
      "authors": [
        "Mattia Soldan",
        "Fabian Caba Heilbron",
        "Bernard Ghanem",
        "Josef Sivic",
        "Bryan Russell"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "temporally",
        "residualvit",
        "temporal",
        "dense",
        "foundation",
        "video",
        "tasks",
        "frame",
        "frames",
        "features"
      ],
      "summary": "Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require \"temporally dense\" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding_ICCV_2025_paper.html",
          "/venue/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Soldan_ResidualViT_for_Efficient_Temporally_Dense_Video_Encoding_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ResidualViT for Efficient Temporally Dense Video Encoding [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Mattia Soldan , Fabian Caba Heilbron , Bernard Ghanem , Josef Sivic , Bryan Russell Several video understanding tasks, such as natural language temporal video grounding, temporal activity localization, and audio description generation, require \"temporally dense\" reasoning over frames sampled at high temporal resolution. However, computing frame-level features for these tasks is computationally expensive given the temporal resolution requirements. In this paper, we make three contributions to reduce the cost of computing features for temporally dense tasks. First, we introduce a vision transformer (ViT) architecture, dubbed ResidualViT, that leverages the large temporal redundancy in videos to efficiently compute temporally dense frame-level features. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module that enhances processing speed by selectively discarding temporally redundant information while reusing weights of a pretrained foundation model. Second, we propose a lightweight distillation strategy to approximate the frame-level features of the original foundation model. Finally, we evaluate our approach across four tasks and five datasets, in both zero-shot and fully supervised settings, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while closely approximating the accuracy of the original foundation model. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF",
      "index": 246,
      "title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes",
      "authors": [
        "Xinjie Zhang",
        "Zhening Liu",
        "Yifan Zhang",
        "Xingtong Ge",
        "Dailan He",
        "Tongda Xu",
        "Yan Wang",
        "Zehong Lin",
        "Shuicheng Yan",
        "Jun Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "4dgs",
        "gaussian",
        "gaussians",
        "mega",
        "splatting",
        "memory",
        "scenes",
        "storage",
        "color",
        "dynamic"
      ],
      "summary": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190xand 125xon the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field. Code is available at https://github.com/Xinjie-Q/MEGA.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_MEGA_Memory-Efficient_4D_Gaussian_Splatting_for_Dynamic_Scenes_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Xinjie Zhang , Zhening Liu , Yifan Zhang , Xingtong Ge , Dailan He , Tongda Xu , Yan Wang , Zehong Lin , Shuicheng Yan , Jun Zhang 4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190xand 125xon the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field. Code is available at https://github.com/Xinjie-Q/MEGA. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF",
      "index": 247,
      "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance",
      "authors": [
        "Huu-Phu Do",
        "Yu-Wei Chen",
        "Yi-Cheng Liao",
        "Chi-Wei Hsiao",
        "Han-Yang Wang",
        "Wei-Chen Chiu",
        "Ching-Chun Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "dynfacerestore",
        "guidance",
        "fidelity",
        "restoration",
        "blind",
        "blurry",
        "face",
        "diffusion",
        "timesteps",
        "detail"
      ],
      "summary": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Do_DynFaceRestore_Balancing_Fidelity_and_Quality_in_Diffusion-Guided_Blind_Face_Restoration_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Huu-Phu Do , Yu-Wei Chen , Yi-Cheng Liao , Chi-Wei Hsiao , Han-Yang Wang , Wei-Chen Chiu , Ching-Chun Huang Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF",
      "index": 248,
      "title": "Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment",
      "authors": [
        "Lijie Liu",
        "Tianxiang Ma",
        "Bingchuan Li",
        "Zhuowei Chen",
        "Jiawei Liu",
        "Gen Li",
        "Siyu Zhou",
        "Qian He",
        "Xinglong Wu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "video",
        "subject",
        "generation",
        "text",
        "consistent",
        "phantom",
        "modal",
        "image",
        "alignment",
        "cross"
      ],
      "summary": "The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent videos following textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single- and multi-subject references.Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. The proposed method achieves perfect subject-consistent video generation while addressing issues of image content leakage and multi-subject confusion.Evaluation results indicate that our method outperforms other state-of-the-art closed-source commercial solutions.In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment_ICCV_2025_paper.html",
          "/venue/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Phantom_Subject-Consistent_Video_Generation_via_Cross-Modal_Alignment_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment [PDF ] [Copy] [Kimi ] [REL] Authors : Lijie Liu , Tianxiang Ma , Bingchuan Li , Zhuowei Chen , Jiawei Liu , Gen Li , Siyu Zhou , Qian He , Xinglong Wu The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent videos following textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single- and multi-subject references.Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. The proposed method achieves perfect subject-consistent video generation while addressing issues of image content leakage and multi-subject confusion.Evaluation results indicate that our method outperforms other state-of-the-art closed-source commercial solutions.In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF",
      "index": 249,
      "title": "Sliced Wasserstein Bridge for Open-Vocabulary Video Instance Segmentation",
      "authors": [
        "Zheyun Qin",
        "Deng Yu",
        "Chuanchen Luo",
        "Zhumin Chen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "sliced",
        "wasserstein",
        "vocabulary",
        "open",
        "video",
        "instance",
        "segmentation",
        "alignments",
        "bridging",
        "weighting"
      ],
      "summary": "In recent years, researchers have explored the task of open-vocabulary video instance segmentation, which aims to identify, track, and segment any instance within an open set of categories. The core challenge of Open-Vocabulary VIS lies in solving the cross-domain alignment problem, including spatial-temporal and text-visual domain alignments. Existing methods have made progress but still face shortcomings in addressing these alignments, especially due to data heterogeneity. Inspired by metric learning, we propose an innovative Sliced Wasserstein Bridging Learning Framework. This framework utilizes the Sliced Wasserstein distance as the core tool for metric learning, effectively bridging the four domains involved in the task. Our innovations are threefold: (1) Domain Alignment: By mapping features from different domains into a unified metric space, our method maintains temporal consistency and learns intrinsic consistent features between modalities, improving the fusion of text and visual information. (2) Weighting Mechanism: We introduce an importance weighting mechanism to enhance the discriminative ability of our method when dealing with imbalanced or significantly different data. (3) High Efficiency: Our method inherits the computational efficiency of the Sliced Wasserstein distance, allowing for online processing of large-scale video data while maintaining segmentation accuracy. Through extensive experimental evaluations, we have validated the robustness of our concept and the effectiveness of our framework.",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.html",
          "/venue/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Qin_Sliced_Wasserstein_Bridge_for_Open-Vocabulary_Video_Instance_Segmentation_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "Sliced Wasserstein Bridge for Open-Vocabulary Video Instance Segmentation [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zheyun Qin , Deng Yu , Chuanchen Luo , Zhumin Chen In recent years, researchers have explored the task of open-vocabulary video instance segmentation, which aims to identify, track, and segment any instance within an open set of categories. The core challenge of Open-Vocabulary VIS lies in solving the cross-domain alignment problem, including spatial-temporal and text-visual domain alignments. Existing methods have made progress but still face shortcomings in addressing these alignments, especially due to data heterogeneity. Inspired by metric learning, we propose an innovative Sliced Wasserstein Bridging Learning Framework. This framework utilizes the Sliced Wasserstein distance as the core tool for metric learning, effectively bridging the four domains involved in the task. Our innovations are threefold: (1) Domain Alignment: By mapping features from different domains into a unified metric space, our method maintains temporal consistency and learns intrinsic consistent features between modalities, improving the fusion of text and visual information. (2) Weighting Mechanism: We introduce an importance weighting mechanism to enhance the discriminative ability of our method when dealing with imbalanced or significantly different data. (3) High Efficiency: Our method inherits the computational efficiency of the Sliced Wasserstein distance, allowing for online processing of large-scale video data while maintaining segmentation accuracy. Through extensive experimental evaluations, we have validated the robustness of our concept and the effectiveness of our framework. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF",
      "index": 250,
      "title": "Self-Calibrating Gaussian Splatting for Large Field-of-View Reconstruction",
      "authors": [
        "Youming Deng",
        "Wenqi Xian",
        "Guandao Yang",
        "Leonidas Guibas",
        "Gordon Wetzstein",
        "Steve Marschner",
        "Paul Debevec"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "splatting",
        "fov",
        "lenses",
        "fisheye",
        "lens",
        "reconstruction",
        "self",
        "gaussian",
        "view",
        "calibrating"
      ],
      "summary": "Large field-of-view (FOV) cameras can simplify and accelerate scene capture because they provide complete coverage with fewer views. However, existing reconstruction pipelines fail to take full advantage of large-FOV input data because they convert input views to perspective images, resulting in stretching that prevents the use of the full image. Additionally, they calibrate lenses using models that do not accurately fit real fisheye lenses in the periphery. We present a new reconstruction pipeline based on Gaussian Splatting that uses a flexible lens model and supports fields of view approaching 180 degrees. We represent lens distortion with a hybrid neural field based on an Invertible ResNet and use a cubemap to render wide-FOV images while retaining the efficiency of the Gaussian Splatting pipeline. Our system jointly optimizes lens distortion, camera intrinsics, camera poses, and scene representations using a loss measured directly against the original input pixels. We present extensive experiments on both synthetic and real-world scenes, demonstrating that our model accurately fits real-world fisheye lenses and that our end-to-end self-calibration approach provides higher-quality reconstructions than existing methods. More details and videos can be found at the project page: https://denghilbert.github.io/self-cali/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Deng_Self-Calibrating_Gaussian_Splatting_for_Large_Field-of-View_Reconstruction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "Self-Calibrating Gaussian Splatting for Large Field-of-View Reconstruction [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Youming Deng , Wenqi Xian , Guandao Yang , Leonidas Guibas , Gordon Wetzstein , Steve Marschner , Paul Debevec Large field-of-view (FOV) cameras can simplify and accelerate scene capture because they provide complete coverage with fewer views. However, existing reconstruction pipelines fail to take full advantage of large-FOV input data because they convert input views to perspective images, resulting in stretching that prevents the use of the full image. Additionally, they calibrate lenses using models that do not accurately fit real fisheye lenses in the periphery. We present a new reconstruction pipeline based on Gaussian Splatting that uses a flexible lens model and supports fields of view approaching 180 degrees. We represent lens distortion with a hybrid neural field based on an Invertible ResNet and use a cubemap to render wide-FOV images while retaining the efficiency of the Gaussian Splatting pipeline. Our system jointly optimizes lens distortion, camera intrinsics, camera poses, and scene representations using a loss measured directly against the original input pixels. We present extensive experiments on both synthetic and real-world scenes, demonstrating that our model accurately fits real-world fisheye lenses and that our end-to-end self-calibration approach provides higher-quality reconstructions than existing methods. More details and videos can be found at the project page: https://denghilbert.github.io/self-cali/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF",
      "index": 251,
      "title": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face Recognition",
      "authors": [
        "Jinghan You",
        "Shanglin Li",
        "Yuanrui Sun",
        "Jiangchuan Wei",
        "Mingyu Guo",
        "Chao Feng",
        "Jiao Ran"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lvface",
        "face",
        "pco",
        "recognition",
        "ncs",
        "progressive",
        "uniface",
        "topofr",
        "cluster",
        "vit"
      ],
      "summary": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling, yet remain underexplored in face recognition (FR) where CNNs still dominate. We identify a critical bottleneck: CNN-inspired training paradigms fail to unlock ViT's potential, leading to suboptimal performance and convergence instability.To address this challenge, we propose LVFace, a ViT-based FR model that integrates Progressive Cluster Optimization (PCO) to achieve superior results. Specifically, PCO sequentially applies negative class sub-sampling (NCS) for robust and fast feature alignment from random initialization, feature expectation penalties for centroid stabilization, performing cluster boundary refinement through full-batch training without NCS constraints. LVFace establishes a new state-of-the-art face recognition baseline, surpassing leading approaches such as UniFace and TopoFR across multiple benchmarks. Extensive experiments demonstrate that LVFace delivers consistent performance gains, while exhibiting scalability to large-scale datasets and compatibility with mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV 2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its efficacy in real-world scenarios. Project is available at https://github.com/bytedance/LVFace.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/You_LVFace_Progressive_Cluster_Optimization_for_Large_Vision_Models_in_Face_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face Recognition [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jinghan You , Shanglin Li , Yuanrui Sun , Jiangchuan Wei , Mingyu Guo , Chao Feng , Jiao Ran Vision Transformers (ViTs) have revolutionized large-scale visual modeling, yet remain underexplored in face recognition (FR) where CNNs still dominate. We identify a critical bottleneck: CNN-inspired training paradigms fail to unlock ViT's potential, leading to suboptimal performance and convergence instability.To address this challenge, we propose LVFace, a ViT-based FR model that integrates Progressive Cluster Optimization (PCO) to achieve superior results. Specifically, PCO sequentially applies negative class sub-sampling (NCS) for robust and fast feature alignment from random initialization, feature expectation penalties for centroid stabilization, performing cluster boundary refinement through full-batch training without NCS constraints. LVFace establishes a new state-of-the-art face recognition baseline, surpassing leading approaches such as UniFace and TopoFR across multiple benchmarks. Extensive experiments demonstrate that LVFace delivers consistent performance gains, while exhibiting scalability to large-scale datasets and compatibility with mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV 2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its efficacy in real-world scenarios. Project is available at https://github.com/bytedance/LVFace. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF",
      "index": 252,
      "title": "SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement",
      "authors": [
        "Liwen Xiao",
        "Zhiyu Pan",
        "Zhicheng Wang",
        "Zhiguo Cao",
        "Wei Li"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "braid",
        "srefiner",
        "refinement",
        "soft",
        "trajectories",
        "trajectory",
        "agent",
        "prediction",
        "topological",
        "attention"
      ],
      "summary": "Accurate prediction of multi-agent future trajectories is crucial for autonomous driving systems to make safe and efficient decisions. Trajectory refinement has emerged as a key strategy to enhance prediction accuracy. However, existing refinement methods often overlook the topological relationships between trajectories, which are vital for improving prediction precision. Inspired by braid theory, we propose a novel trajectory refinement approach, Soft-Braid Refiner (SRefiner), guided by the soft-braid topological structure of trajectories using Soft-Braid Attention. Soft-Braid Attention captures spatio-temporal topological relationships between trajectories by considering both spatial proximity and vehicle motion states at \"soft intersection points\". Additionally, we extend this approach to model interactions between trajectories and lanes, further improving the prediction accuracy. SRefiner is a multi-iteration, multi-agent framework that iteratively refines trajectories, incorporating topological information to enhance interactions within traffic scenarios. SRefiner achieves significant performance improvements over four baseline methods across two datasets, establishing a new state-of-the-art in trajectory refinement.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Xiao_SRefiner_Soft-Braid_Attention_for_Multi-Agent_Trajectory_Refinement_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement [PDF ] [Copy] [Kimi ] [REL] Authors : Liwen Xiao , Zhiyu Pan , Zhicheng Wang , Zhiguo Cao , Wei Li Accurate prediction of multi-agent future trajectories is crucial for autonomous driving systems to make safe and efficient decisions. Trajectory refinement has emerged as a key strategy to enhance prediction accuracy. However, existing refinement methods often overlook the topological relationships between trajectories, which are vital for improving prediction precision. Inspired by braid theory, we propose a novel trajectory refinement approach, Soft-Braid Refiner (SRefiner), guided by the soft-braid topological structure of trajectories using Soft-Braid Attention. Soft-Braid Attention captures spatio-temporal topological relationships between trajectories by considering both spatial proximity and vehicle motion states at \"soft intersection points\". Additionally, we extend this approach to model interactions between trajectories and lanes, further improving the prediction accuracy. SRefiner is a multi-iteration, multi-agent framework that iteratively refines trajectories, incorporating topological information to enhance interactions within traffic scenarios. SRefiner achieves significant performance improvements over four baseline methods across two datasets, establishing a new state-of-the-art in trajectory refinement. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF",
      "index": 253,
      "title": "LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs",
      "authors": [
        "Jiarui Wang",
        "Huiyu Duan",
        "Yu Zhao",
        "Juntong Wang",
        "Guangtao Zhai",
        "Xiongkuo Min"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "lmm4lmm",
        "50k",
        "evalmi",
        "multimodal",
        "t2i",
        "image",
        "lmms",
        "generation",
        "evaluating",
        "text"
      ],
      "summary": "Recent breakthroughs in large multimodal models (LMMs) have significantly advanced both text-to-image (T2I) generation and image-to-text (I2T) interpretation. However, many generated images still suffer from issues related to perceptual quality and text-image alignment. Given the high cost and inefficiency of manual evaluation, an automatic metric that aligns with human preferences is desirable. To this end, we present EvalMi-50K, a comprehensive dataset and benchmark for evaluating large-multimodal image generation,which features (i) comprehensive tasks, encompassing 2,100 extensive prompts across 20 fine-grained task dimensions, and (ii) large-scale human-preference annotations, including 100K mean-opinion scores (MOSs) and 50K question-answering (QA) pairs annotated on 50,400 images generated from 24 T2I models.Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for evaluating large multimodal T2I generation from multiple dimensions including perceptual quality, text-image correspondence, and task-specific accuracy.Extensive experimental results show that LMM4LMM achieves state-of-the-art performance on EvalMi-50K, and exhibits strong generalization ability on other AI-generated image evaluation benchmark datasets, manifesting the generality of both the EvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be released at https://github.com/IntMeGroup/LMM4LMM.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_LMM4LMM_Benchmarking_and_Evaluating_Large-multimodal_Image_Generation_with_LMMs_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jiarui Wang , Huiyu Duan , Yu Zhao , Juntong Wang , Guangtao Zhai , Xiongkuo Min Recent breakthroughs in large multimodal models (LMMs) have significantly advanced both text-to-image (T2I) generation and image-to-text (I2T) interpretation. However, many generated images still suffer from issues related to perceptual quality and text-image alignment. Given the high cost and inefficiency of manual evaluation, an automatic metric that aligns with human preferences is desirable. To this end, we present EvalMi-50K, a comprehensive dataset and benchmark for evaluating large-multimodal image generation,which features (i) comprehensive tasks, encompassing 2,100 extensive prompts across 20 fine-grained task dimensions, and (ii) large-scale human-preference annotations, including 100K mean-opinion scores (MOSs) and 50K question-answering (QA) pairs annotated on 50,400 images generated from 24 T2I models.Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for evaluating large multimodal T2I generation from multiple dimensions including perceptual quality, text-image correspondence, and task-specific accuracy.Extensive experimental results show that LMM4LMM achieves state-of-the-art performance on EvalMi-50K, and exhibits strong generalization ability on other AI-generated image evaluation benchmark datasets, manifesting the generality of both the EvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be released at https://github.com/IntMeGroup/LMM4LMM. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Zhang_Geometry_Distributions@ICCV2025@CVF",
      "index": 254,
      "title": "Geometry Distributions",
      "authors": [
        "Biao Zhang",
        "Jing Ren",
        "Peter Wonka"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "representation",
        "geometric",
        "distributions",
        "surface",
        "watertight",
        "geometry",
        "across",
        "object",
        "textured",
        "neural"
      ],
      "summary": "Neural representations of 3D data have been widely adopted across various applications, particularly in recent work leveraging coordinate-based networks to model scalar or vector fields. However, these approaches face inherent challenges, such as handling thin structures and non-watertight geometries, which limit their flexibility and accuracy. In contrast, we propose a novel geometric data representation that models geometry as distributions-a powerful representation that makes no assumptions about surface genus, connectivity, or boundary conditions. Our approach uses diffusion models with a novel network architecture to learn surface point distributions, capturing fine-grained geometric details. We evaluate our representation qualitatively and quantitatively across various object types, demonstrating its effectiveness in achieving high geometric fidelity. Additionally, we explore applications using our representation, such as textured mesh representation, neural surface compression, dynamic object modeling, and rendering, highlighting its potential to advance 3D geometric learning.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_Geometry_Distributions_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Zhang_Geometry_Distributions@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Zhang_Geometry_Distributions_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Geometry Distributions [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Biao Zhang , Jing Ren , Peter Wonka Neural representations of 3D data have been widely adopted across various applications, particularly in recent work leveraging coordinate-based networks to model scalar or vector fields. However, these approaches face inherent challenges, such as handling thin structures and non-watertight geometries, which limit their flexibility and accuracy. In contrast, we propose a novel geometric data representation that models geometry as distributions-a powerful representation that makes no assumptions about surface genus, connectivity, or boundary conditions. Our approach uses diffusion models with a novel network architecture to learn surface point distributions, capturing fine-grained geometric details. We evaluate our representation qualitatively and quantitatively across various object types, demonstrating its effectiveness in achieving high geometric fidelity. Additionally, we explore applications using our representation, such as textured mesh representation, neural surface compression, dynamic object modeling, and rendering, highlighting its potential to advance 3D geometric learning. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF",
      "index": 255,
      "title": "SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM",
      "authors": [
        "Yannick Burkhardt",
        "Simon Schaefer",
        "Stefan Leutenegger"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "keypoint",
        "superevent",
        "event",
        "slam",
        "keypoints",
        "cameras",
        "streams",
        "based",
        "ethz",
        "mrl"
      ],
      "summary": "Event-based keypoint detection and matching holds significant potential, enabling the integration of event sensors into highly optimized Visual SLAM systems developed for frame cameras over decades of research. Unfortunately, existing approaches struggle with the motion-dependent appearance of keypoints and the complex noise prevalent in event streams, resulting in severely limited feature matching capabilities and poor performance on downstream tasks. To mitigate this problem, we propose SuperEvent, a data-driven approach to predict stable keypoints with expressive descriptors. Due to the absence of event datasets with ground truth keypoint labels, we leverage existing frame-based keypoint detectors on readily available event-aligned and synchronized gray-scale frames for self-supervision: we generate temporally sparse keypoint pseudo-labels considering that events are a product of both scene appearance and camera motion. Combined with our novel, information-rich event representation, we enable SuperEvent to effectively learn robust keypoint detection and description in event streams. Finally, we demonstrate the usefulness of SuperEvent by its integration into a modern sparse keypoint and descriptor-based SLAM framework originally developed for traditional cameras, surpassing the state-of-the-art in event-based SLAM by a wide margin. Source code is available at https://ethz-mrl.github.io/SuperEvent/.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Burkhardt_SuperEvent_Cross-Modal_Learning_of_Event-based_Keypoint_Detection_for_SLAM_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yannick Burkhardt , Simon Schaefer , Stefan Leutenegger Event-based keypoint detection and matching holds significant potential, enabling the integration of event sensors into highly optimized Visual SLAM systems developed for frame cameras over decades of research. Unfortunately, existing approaches struggle with the motion-dependent appearance of keypoints and the complex noise prevalent in event streams, resulting in severely limited feature matching capabilities and poor performance on downstream tasks. To mitigate this problem, we propose SuperEvent, a data-driven approach to predict stable keypoints with expressive descriptors. Due to the absence of event datasets with ground truth keypoint labels, we leverage existing frame-based keypoint detectors on readily available event-aligned and synchronized gray-scale frames for self-supervision: we generate temporally sparse keypoint pseudo-labels considering that events are a product of both scene appearance and camera motion. Combined with our novel, information-rich event representation, we enable SuperEvent to effectively learn robust keypoint detection and description in event streams. Finally, we demonstrate the usefulness of SuperEvent by its integration into a modern sparse keypoint and descriptor-based SLAM framework originally developed for traditional cameras, surpassing the state-of-the-art in event-based SLAM by a wide margin. Source code is available at https://ethz-mrl.github.io/SuperEvent/. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF",
      "index": 256,
      "title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction",
      "authors": [
        "Zijian Dong",
        "Longteng Duan",
        "Jie Song",
        "Michael J. Black",
        "Andreas Geiger"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "avatar",
        "moga",
        "avatars",
        "generative",
        "gaussian",
        "unseen",
        "views",
        "appearance",
        "prior",
        "ensuring"
      ],
      "summary": "We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https:// zj-dong.github.io/ MoGA/",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Dong_MoGA_3D_Generative_Avatar_Prior_for_Monocular_Gaussian_Avatar_Reconstruction_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Zijian Dong , Longteng Duan , Jie Song , Michael J. Black , Andreas Geiger We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https:// zj-dong.github.io/ MoGA/ Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF",
      "index": 257,
      "title": "ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking",
      "authors": [
        "Xiaokun Feng",
        "Shiyu Hu",
        "Xuchen Li",
        "Dailing Zhang",
        "Meiqi Wu",
        "Jing Zhang",
        "Xiaotang Chen",
        "Kaiqi Huang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "target",
        "cues",
        "atctrack",
        "context",
        "vlts",
        "textual",
        "tracking",
        "words",
        "language",
        "vision"
      ],
      "summary": "Vision-language tracking aims to locate the target object in the video sequence using a template patch and a language description provided in the initial frame. To achieve robust tracking, especially in complex long-term scenarios that reflect real-world conditions as recently highlighted by MGIT, it is essential not only to characterize the target features but also to utilize the context features related to the target. However, the visual and textual target-context cues derived from the initial prompts generally align only with the initial target state. Due to their dynamic nature, target states are constantly changing, particularly in complex long-term sequences. It is intractable for these cues to continuously guide Vision-Language Trackers (VLTs). Furthermore, for the text prompts with diverse expressions, our experiments reveal that existing VLTs struggle to discern which words pertain to the target or the context, complicating the utilization of textual cues. In this work, we present a novel tracker named ATCTrack, which can obtain multimodal cues Aligned with the dynamic target states through comprehensive Target-Context feature modeling, thereby achieving robust tracking. Specifically, (1) for the visual modality, we propose an effective temporal visual target-context modeling approach that provides the tracker with timely visual cues. (2) For the textual modality, we achieve precise target words identification solely based on textual content, and design an innovative context words calibration method to adaptively utilize auxiliary context words. (3) We conduct extensive experiments on mainstream benchmarks and ATCTrack achieves a new SOTA performance. The code and models will be released at: https://github.com/XiaokunFeng/ATCTrack.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Feng_ATCTrack_Aligning_Target-Context_Cues_with_Dynamic_Target_States_for_Robust_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": null
      },
      "raw_excerpt": "ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking [PDF ] [Copy] [Kimi ] [REL] Authors : Xiaokun Feng , Shiyu Hu , Xuchen Li , Dailing Zhang , Meiqi Wu , Jing Zhang , Xiaotang Chen , Kaiqi Huang Vision-language tracking aims to locate the target object in the video sequence using a template patch and a language description provided in the initial frame. To achieve robust tracking, especially in complex long-term scenarios that reflect real-world conditions as recently highlighted by MGIT, it is essential not only to characterize the target features but also to utilize the context features related to the target. However, the visual and textual target-context cues derived from the initial prompts generally align only with the initial target state. Due to their dynamic nature, target states are constantly changing, particularly in complex long-term sequences. It is intractable for these cues to continuously guide Vision-Language Trackers (VLTs). Furthermore, for the text prompts with diverse expressions, our experiments reveal that existing VLTs struggle to discern which words pertain to the target or the context, complicating the utilization of textual cues. In this work, we present a novel tracker named ATCTrack, which can obtain multimodal cues Aligned with the dynamic target states through comprehensive Target-Context feature modeling, thereby achieving robust tracking. Specifically, (1) for the visual modality, we propose an effective temporal visual target-context modeling approach that provides the tracker with timely visual cues. (2) For the textual modality, we achieve precise target words identification solely based on textual content, and design an innovative context words calibration method to adaptively utilize auxiliary context words. (3) We conduct extensive experiments on mainstream benchmarks and ATCTrack achieves a new SOTA performance. The code and models will be released at: https://github.com/XiaokunFeng/ATCTrack. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF",
      "index": 258,
      "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
      "authors": [
        "Shiqi Huang",
        "Shuting He",
        "Huaiyuan Qin",
        "Bihan Wen"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "remote",
        "sensing",
        "context",
        "segmentation",
        "vocabulary",
        "instance",
        "score",
        "scene",
        "matters",
        "diverse"
      ],
      "summary": "Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose SCORE (Scene Context matters in Open-vocabulary REmote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_SCORE_Scene_Context_Matters_in_Open-Vocabulary_Remote_Sensing_Instance_Segmentation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Shiqi Huang , Shuting He , Huaiyuan Qin , Bihan Wen Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose SCORE (Scene Context matters in Open-vocabulary REmote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at https://github.com/HuangShiqi128/SCORE. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF",
      "index": 259,
      "title": "Wasserstein Style Distribution Analysis and Transform for Stylized Image Generation",
      "authors": [
        "Xi Yu",
        "Xiang Gu",
        "Zhihao Shi",
        "Jian Sun"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "style",
        "stylized",
        "wsdt",
        "features",
        "wasserstein",
        "blocks",
        "attention",
        "image",
        "generation",
        "transform"
      ],
      "summary": "Large-scale text-to-image diffusion models have achieved remarkable success in image generation, thereby driving the development of stylized image generation technologies. Recent studies introduce style information by empirically replacing specific features in attention blocks with style features. However, the relationship between features and style remains unclear. In this paper, we systematically analyze the relationship between features in attention blocks and style. By quantifying the distribution discrepancy induced by style variations using the Wasserstein distance, we find that features in self-attention blocks exhibit high sensitivity to style compared to features in cross-attention blocks. Our analysis provides valuable insights into the contribution of different features to style. Based on our findings, we propose a novel Wasserstein Style Distribution Transform (WSDT) method, which generates stylized images by transforming the distribution of style-sensitive features to align with that of style features. WSDT applies channel adaptive distribution transform to ensure that information not related to the style is not introduced. Our approach is simple yet efficient, optimization-free, and can be seamlessly integrated into attention-based text-to-image diffusion models. Extensive experiments demonstrate the effectiveness of our approach in stylized image generation tasks.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Wasserstein_Style_Distribution_Analysis_and_Transform_for_Stylized_Image_Generation_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Wasserstein Style Distribution Analysis and Transform for Stylized Image Generation [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Xi Yu , Xiang Gu , Zhihao Shi , Jian Sun Large-scale text-to-image diffusion models have achieved remarkable success in image generation, thereby driving the development of stylized image generation technologies. Recent studies introduce style information by empirically replacing specific features in attention blocks with style features. However, the relationship between features and style remains unclear. In this paper, we systematically analyze the relationship between features in attention blocks and style. By quantifying the distribution discrepancy induced by style variations using the Wasserstein distance, we find that features in self-attention blocks exhibit high sensitivity to style compared to features in cross-attention blocks. Our analysis provides valuable insights into the contribution of different features to style. Based on our findings, we propose a novel Wasserstein Style Distribution Transform (WSDT) method, which generates stylized images by transforming the distribution of style-sensitive features to align with that of style features. WSDT applies channel adaptive distribution transform to ensure that information not related to the style is not introduced. Our approach is simple yet efficient, optimization-free, and can be seamlessly integrated into attention-based text-to-image diffusion models. Extensive experiments demonstrate the effectiveness of our approach in stylized image generation tasks. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF",
      "index": 260,
      "title": "Learning to Generalize without Bias for Open-Vocabulary Action Recognition",
      "authors": [
        "Yating Yu",
        "Congqi Cao",
        "Yifan Zhang",
        "Yanning Zhang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mede",
        "open",
        "vocabulary",
        "clip",
        "static",
        "video",
        "context",
        "meta",
        "learners",
        "optimization"
      ],
      "summary": "Leveraging the effective visual-text alignment and static generalizability from CLIP, recent video learners adopt CLIP initialization with further regularization or recombination for generalization in open-vocabulary action recognition in-context. However, due to the static bias of CLIP, such video learners tend to overfit on shortcut static features, thereby compromising their generalizability, especially to novel out-of-context actions. To address this issue, we introduce Open-MeDe, a novel Meta-optimization framework with static Debiasing for Open-vocabulary action recognition. From a fresh perspective of generalization, Open-MeDe adopts a meta-learning approach to improve known-to-open generalizing and image-to-video debiasing in a cost-effective manner. Specifically, Open-MeDe introduces a cross-batch meta-optimization scheme that explicitly encourages video learners to quickly generalize to arbitrary subsequent data via virtual evaluation, steering a smoother optimization landscape. In effect, the free of CLIP regularization during optimization implicitly mitigates the inherent static bias of the video meta-learner. We further apply self-ensemble over the optimization trajectory to obtain generic optimal parameters that can achieve robust generalization to both in-context and out-of-context novel data. Extensive evaluations show that Open-MeDe not only surpasses state-of-the-art regularization methods tailored for in-context open-vocabulary action recognition but also substantially excels in out-of-context scenarios.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Learning_to_Generalize_without_Bias_for_Open-Vocabulary_Action_Recognition_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Learning to Generalize without Bias for Open-Vocabulary Action Recognition [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yating Yu , Congqi Cao , Yifan Zhang , Yanning Zhang Leveraging the effective visual-text alignment and static generalizability from CLIP, recent video learners adopt CLIP initialization with further regularization or recombination for generalization in open-vocabulary action recognition in-context. However, due to the static bias of CLIP, such video learners tend to overfit on shortcut static features, thereby compromising their generalizability, especially to novel out-of-context actions. To address this issue, we introduce Open-MeDe, a novel Meta-optimization framework with static Debiasing for Open-vocabulary action recognition. From a fresh perspective of generalization, Open-MeDe adopts a meta-learning approach to improve known-to-open generalizing and image-to-video debiasing in a cost-effective manner. Specifically, Open-MeDe introduces a cross-batch meta-optimization scheme that explicitly encourages video learners to quickly generalize to arbitrary subsequent data via virtual evaluation, steering a smoother optimization landscape. In effect, the free of CLIP regularization during optimization implicitly mitigates the inherent static bias of the video meta-learner. We further apply self-ensemble over the optimization trajectory to obtain generic optimal parameters that can achieve robust generalization to both in-context and out-of-context novel data. Extensive evaluations show that Open-MeDe not only surpasses state-of-the-art regularization methods tailored for in-context open-vocabulary action recognition but also substantially excels in out-of-context scenarios. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF",
      "index": 261,
      "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
      "authors": [
        "Jonas Belouadi",
        "Eddy Ilg",
        "Margret Keuper",
        "Hideki Tanaka",
        "Masao Utiyama",
        "Raj Dabre",
        "Steffen Eger",
        "Simone Ponzetto"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "graphics",
        "programs",
        "tikzero",
        "captioned",
        "text",
        "aligned",
        "program",
        "figures",
        "captions",
        "caption"
      ],
      "summary": "Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Belouadi_TikZero_Zero-Shot_Text-Guided_Graphics_Program_Synthesis_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Jonas Belouadi , Eddy Ilg , Margret Keuper , Hideki Tanaka , Masao Utiyama , Raj Dabre , Steffen Eger , Simone Ponzetto Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available. Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF",
      "index": 262,
      "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment",
      "authors": [
        "Yachun Mi",
        "Yu Li",
        "Weicheng Meng",
        "Chaofeng Chen",
        "Chen Hui",
        "Shaohui Liu"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "mvqa",
        "usds",
        "mamba",
        "vqa",
        "resampling",
        "sampling",
        "videos",
        "distortion",
        "unified",
        "semantic"
      ],
      "summary": "The rapid growth of long-duration, high-definition videos has made efficient video quality assessment (VQA) a critical challenge. Existing research typically tackles this problem through two main strategies: reducing model parameters and resampling inputs. However, light-weight Convolution Neural Networks (CNN) and Transformers often struggle to balance efficiency with high performance due to the requirement of long-range modeling capabilities. Recently, the state-space model, particularly Mamba, has emerged as a promising alternative, offering linear complexity with respect to sequence length. Meanwhile, efficient VQA heavily depends on resampling long sequences to minimize computational costs, yet current resampling methods are often weak in preserving essential semantic information. In this work, we present MVQA, a Mamba-based model designed for efficient VQA along with a novel Unified Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch sampling from low-resolution videos and distortion patch sampling from original-resolution videos. The former captures semantically dense regions, while the latter retains critical distortion details. To prevent computation increase from dual inputs, we propose a fusion mechanism using pre-defined masks, enabling a unified sampling strategy that captures both semantic and quality information without additional computational burden. Experiments show that the proposed MVQA, equipped with USDS, achieve comparable performance to state-of-the-art methods while being 2xas fast and requiring only 1/5 GPU memory. Code is available at https://github.com/xiao-mi-d/MVQA",
      "session": null,
      "time": null,
      "links": {
        "video": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment_ICCV_2025_paper.html",
          "/venue/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment_ICCV_2025_paper.pdf"
        ],
        "venue": [
          "/venue/ICCV.2025?group=Highlight",
          "/venue/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment@ICCV2025@CVF"
        ],
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Mi_MVQA_Mamba_with_Unified_Sampling_for_Efficient_Video_Quality_Assessment_ICCV_2025_paper.html"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": null
      },
      "raw_excerpt": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment [PDF 1 ] [Copy] [Kimi ] [REL] Authors : Yachun Mi , Yu Li , Weicheng Meng , Chaofeng Chen , Chen Hui , Shaohui Liu The rapid growth of long-duration, high-definition videos has made efficient video quality assessment (VQA) a critical challenge. Existing research typically tackles this problem through two main strategies: reducing model parameters and resampling inputs. However, light-weight Convolution Neural Networks (CNN) and Transformers often struggle to balance efficiency with high performance due to the requirement of long-range modeling capabilities. Recently, the state-space model, particularly Mamba, has emerged as a promising alternative, offering linear complexity with respect to sequence length. Meanwhile, efficient VQA heavily depends on resampling long sequences to minimize computational costs, yet current resampling methods are often weak in preserving essential semantic information. In this work, we present MVQA, a Mamba-based model designed for efficient VQA along with a novel Unified Semantic and Distortion Sampling (USDS) approach. USDS combines semantic patch sampling from low-resolution videos and distortion patch sampling from original-resolution videos. The former captures semantically dense regions, while the latter retains critical distortion details. To prevent computation increase from dual inputs, we propose a fusion mechanism using pre-defined masks, enabling a unified sampling strategy that captures both semantic and quality information without additional computational burden. Experiments show that the proposed MVQA, equipped with USDS, achieve comparable performance to state-of-the-art methods while being 2xas fast and requiring only 1/5 GPU memory. Code is available at https://github.com/xiao-mi-d/MVQA Subject : ICCV.2025 - Highlight"
    },
    {
      "paper_id": "Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF",
      "index": 263,
      "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View",
      "authors": [
        "Longliang Liu",
        "Miaojie Feng",
        "Junda Cheng",
        "Jijun Xiang",
        "Xuan Zhu",
        "Xin Yang"
      ],
      "subjects": [
        "ICCV.2025 - Highlight"
      ],
      "keywords": [
        "flow",
        "panoramic",
        "optical",
        "primitive",
        "distortion",
        "orthogonal",
        "prior",
        "view",
        "distortions",
        "oddc"
      ],
      "summary": "Panoramic optical flow enables a comprehensive understanding of temporal dynamics across wide fields of view. However, severe distortions caused by sphere-to-plane projections, such as the equirectangular projection (ERP), significantly degrade the performance of conventional perspective-based optical flow methods, especially in polar regions. To address this challenge, we propose PriOr-Flow, a novel dual-branch framework that leverages the low-distortion nature of the orthogonal view to enhance optical flow estimation in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup (DCCL) operator, which jointly retrieves correlation information from both the primitive and orthogonal cost volumes, effectively mitigating distortion noise during cost volume construction. Furthermore, our Ortho-Driven Distortion Compensation (ODDC) module iteratively refines motion features of the primitive branch, further suppressing polar distortions. Extensive experiments demonstrate that PriOr-Flow is compatible with various perspective-based iterative optical flow methods and consistently achieves state-of-the-art performance on publicly available panoramic optical flow datasets, setting a new benchmark for wide-field motion estimation.",
      "session": null,
      "time": null,
      "links": {
        "detail": [
          "https://openaccess.thecvf.com/content/ICCV2025/html/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.html"
        ],
        "venue": [
          "/venue/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View@ICCV2025@CVF",
          "/venue/ICCV.2025?group=Highlight"
        ],
        "pdf": [
          "https://openaccess.thecvf.com/content/ICCV2025/papers/Liu_PriOr-Flow_Enhancing_Primitive_Panoramic_Optical_Flow_with_Orthogonal_View_ICCV_2025_paper.pdf"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 1
      },
      "raw_excerpt": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View [PDF ] [Copy] [Kimi 1 ] [REL] Authors : Longliang Liu , Miaojie Feng , Junda Cheng , Jijun Xiang , Xuan Zhu , Xin Yang Panoramic optical flow enables a comprehensive understanding of temporal dynamics across wide fields of view. However, severe distortions caused by sphere-to-plane projections, such as the equirectangular projection (ERP), significantly degrade the performance of conventional perspective-based optical flow methods, especially in polar regions. To address this challenge, we propose PriOr-Flow, a novel dual-branch framework that leverages the low-distortion nature of the orthogonal view to enhance optical flow estimation in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup (DCCL) operator, which jointly retrieves correlation information from both the primitive and orthogonal cost volumes, effectively mitigating distortion noise during cost volume construction. Furthermore, our Ortho-Driven Distortion Compensation (ODDC) module iteratively refines motion features of the primitive branch, further suppressing polar distortions. Extensive experiments demonstrate that PriOr-Flow is compatible with various perspective-based iterative optical flow methods and consistently achieves state-of-the-art performance on publicly available panoramic optical flow datasets, setting a new benchmark for wide-field motion estimation. Subject : ICCV.2025 - Highlight"
    }
  ]
}