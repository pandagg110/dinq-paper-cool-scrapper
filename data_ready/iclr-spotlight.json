{
  "source_html": "html\\iclr-spotlight.html",
  "paper_count": 373,
  "conference": "iclr",
  "year": 2025,
  "status": "spotlight",
  "papers": [
    {
      "paper_id": "TeVAZXr3yv@OpenReview",
      "index": 1,
      "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
      "authors": [
        "Sakshi",
        "Utkarsh Tyagi",
        "Sonal Kumar",
        "Ashish Seth",
        "Ramaneswaran S",
        "Oriol Nieto",
        "Ramani Duraiswami",
        "Sreyan Ghosh",
        "Dinesh Manocha"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mmau",
        "audio",
        "reasoning",
        "sounds",
        "tasks",
        "speech",
        "advanced",
        "understanding",
        "music",
        "multimodal"
      ],
      "summary": "The ability to comprehend audio—which includes speech, non-speech sounds, and music—is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challengesposed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TeVAZXr3yv"
        ],
        "venue": [
          "/venue/TeVAZXr3yv@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TeVAZXr3yv"
        ],
        "detail": [
          "https://openreview.net/forum?id=TeVAZXr3yv"
        ]
      },
      "scores": {
        "pdf": 67,
        "kimi": 54
      },
      "raw_excerpt": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark [PDF 67 ] [Copy] [Kimi 54 ] [REL] Authors : Sakshi , Utkarsh Tyagi , Sonal Kumar , Ashish Seth , Ramaneswaran S , Oriol Nieto , Ramani Duraiswami , Sreyan Ghosh , Dinesh Manocha The ability to comprehend audio—which includes speech, non-speech sounds, and music—is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challengesposed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Im2neAMlre@OpenReview",
      "index": 2,
      "title": "One slice is not enough: In search of stable conclusions in text-to-image evaluation",
      "authors": [
        "Olivia Wiles",
        "Chuhan Zhang",
        "Isabela Albuquerque",
        "Ivana Kajić",
        "Su Wang",
        "Emanuele Bugliarello",
        "Yasumasa Onoe",
        "Pinelopi Papalampidi",
        "Ira Ktena",
        "Christopher Knutsen",
        "Cyrus Rashtchian",
        "Anant Nawalgaria",
        "Jordi Pont-Tuset",
        "Aida Nematzadeh"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "t2i",
        "evaluation",
        "metrics",
        "human",
        "annotations",
        "suite",
        "slice",
        "conclusions",
        "tifa160",
        "templates"
      ],
      "summary": "While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human annotations, evaluation task) has not been systematically measured.We find that looking at only *one slice of data*, i.e. one set of capabilities or human annotations, is not enough to obtain stable conclusions that generalise to new conditions or slices when evaluating T2I models or alignment metrics. We address this by introducing an evaluation suite of > > 100K annotations across four human annotation templates that comprehensively evaluates models' capabilities across a range of methods for gathering human annotations and comparing models.In particular, we propose (1) a carefully curated set of prompts -- *Gecko2K*; (2) a statistically grounded method of comparing T2I models; and (3) how to systematically evaluate metrics under three *evaluation tasks* -- *model ordering, pair-wise instance scoring, point-wise instance scoring*.Using this evaluation suite, we evaluate a wide range of metrics and find that a metric may do better in one setting but worse in another.As a result, we introduce a new, interpretable auto-eval metric that is consistently better correlated with human ratings than such existing metrics on our evaluation suite--across different human templates and evaluation settings--and on TIFA160.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Im2neAMlre"
        ],
        "venue": [
          "/venue/Im2neAMlre@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Im2neAMlre"
        ],
        "detail": [
          "https://openreview.net/forum?id=Im2neAMlre"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 23
      },
      "raw_excerpt": "One slice is not enough: In search of stable conclusions in text-to-image evaluation [PDF 33 ] [Copy] [Kimi 23 ] [REL] Authors : Olivia Wiles , Chuhan Zhang , Isabela Albuquerque , Ivana Kajić , Su Wang , Emanuele Bugliarello , Yasumasa Onoe , Pinelopi Papalampidi , Ira Ktena , Christopher Knutsen , Cyrus Rashtchian , Anant Nawalgaria , Jordi Pont-Tuset , Aida Nematzadeh While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human annotations, evaluation task) has not been systematically measured.We find that looking at only *one slice of data*, i.e. one set of capabilities or human annotations, is not enough to obtain stable conclusions that generalise to new conditions or slices when evaluating T2I models or alignment metrics. We address this by introducing an evaluation suite of > > 100K annotations across four human annotation templates that comprehensively evaluates models' capabilities across a range of methods for gathering human annotations and comparing models.In particular, we propose (1) a carefully curated set of prompts -- *Gecko2K*; (2) a statistically grounded method of comparing T2I models; and (3) how to systematically evaluate metrics under three *evaluation tasks* -- *model ordering, pair-wise instance scoring, point-wise instance scoring*.Using this evaluation suite, we evaluate a wide range of metrics and find that a metric may do better in one setting but worse in another.As a result, we introduce a new, interpretable auto-eval metric that is consistently better correlated with human ratings than such existing metrics on our evaluation suite--across different human templates and evaluation settings--and on TIFA160. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "FPfCUJTsCn@OpenReview",
      "index": 3,
      "title": "Differentiable Integer Linear Programming",
      "authors": [
        "Zijie Geng",
        "Jie Wang",
        "Xijun Li",
        "Fangzhou Zhu",
        "Jianye HAO",
        "Bin Li",
        "Feng Wu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ilps",
        "diffilo",
        "training",
        "differentiable",
        "generating",
        "solutions",
        "erentiable",
        "nteger",
        "inear",
        "integer"
      ],
      "summary": "Machine learning (ML) techniques have shown great potential in generating high-quality solutions for integer linear programs (ILPs).However, existing methods typically rely on a *supervised learning* paradigm, leading to (1) *expensive training cost* due to repeated invocations of traditional solvers to generate training labels, and (2) *plausible yet infeasible solutions* due to the misalignment between the training objective (minimizing prediction loss) and the inference objective (generating high-quality solutions).To tackle this challenge, we propose **DiffILO** (**Diff**erentiable **I**nteger **L**inear Programming **O**ptimization), an *unsupervised learning paradigm for learning to solve ILPs*.Specifically, through a novel probabilistic modeling, DiffILO reformulates ILPs---discrete and constrained optimization problems---into continuous, differentiable (almost everywhere), and unconstrained optimization problems.This reformulation enables DiffILO to simultaneously solve ILPs and train the model via straightforward gradient descent, providing two major advantages.First, it significantly reduces the training cost, as the training process does not need the aid of traditional solvers at all.Second, it facilitates the generation of feasible and high-quality solutions, as the model *learns to solve ILPs* in an end-to-end manner, thus aligning the training and inference objectives.Experiments on commonly used ILP datasets demonstrate that DiffILO not only achieves an average training speedup of 13.2 13.2 times compared to supervised methods, but also outperforms them by generating heuristic solutions with significantly higher feasibility ratios and much better solution qualities.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FPfCUJTsCn"
        ],
        "venue": [
          "/venue/FPfCUJTsCn@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FPfCUJTsCn"
        ],
        "detail": [
          "https://openreview.net/forum?id=FPfCUJTsCn"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 22
      },
      "raw_excerpt": "Differentiable Integer Linear Programming [PDF 33 ] [Copy] [Kimi 22 ] [REL] Authors : Zijie Geng , Jie Wang , Xijun Li , Fangzhou Zhu , Jianye HAO , Bin Li , Feng Wu Machine learning (ML) techniques have shown great potential in generating high-quality solutions for integer linear programs (ILPs).However, existing methods typically rely on a *supervised learning* paradigm, leading to (1) *expensive training cost* due to repeated invocations of traditional solvers to generate training labels, and (2) *plausible yet infeasible solutions* due to the misalignment between the training objective (minimizing prediction loss) and the inference objective (generating high-quality solutions).To tackle this challenge, we propose **DiffILO** (**Diff**erentiable **I**nteger **L**inear Programming **O**ptimization), an *unsupervised learning paradigm for learning to solve ILPs*.Specifically, through a novel probabilistic modeling, DiffILO reformulates ILPs---discrete and constrained optimization problems---into continuous, differentiable (almost everywhere), and unconstrained optimization problems.This reformulation enables DiffILO to simultaneously solve ILPs and train the model via straightforward gradient descent, providing two major advantages.First, it significantly reduces the training cost, as the training process does not need the aid of traditional solvers at all.Second, it facilitates the generation of feasible and high-quality solutions, as the model *learns to solve ILPs* in an end-to-end manner, thus aligning the training and inference objectives.Experiments on commonly used ILP datasets demonstrate that DiffILO not only achieves an average training speedup of 13.2 13.2 times compared to supervised methods, but also outperforms them by generating heuristic solutions with significantly higher feasibility ratios and much better solution qualities. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "FDnZFpHmU4@OpenReview",
      "index": 4,
      "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling",
      "authors": [
        "Yuxuan YAO",
        "Han Wu",
        "Mingyang LIU",
        "Sichun Luo",
        "Xiongwei Han",
        "Jie Liu",
        "Zhijiang Guo",
        "Linqi Song"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ensembling",
        "textsc",
        "vocabulary",
        "unite",
        "union",
        "nsembling",
        "ensemble",
        "top",
        "across",
        "llm"
      ],
      "summary": "Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \\textsc{Uni}on \\textsc{T}op- k k \\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \\textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FDnZFpHmU4"
        ],
        "venue": [
          "/venue/FDnZFpHmU4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FDnZFpHmU4"
        ],
        "detail": [
          "https://openreview.net/forum?id=FDnZFpHmU4"
        ]
      },
      "scores": {
        "pdf": 26,
        "kimi": 32
      },
      "raw_excerpt": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling [PDF 26 ] [Copy] [Kimi 32 ] [REL] Authors : Yuxuan YAO , Han Wu , Mingyang LIU , Sichun Luo , Xiongwei Han , Jie Liu , Zhijiang Guo , Linqi Song Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \\textsc{Uni}on \\textsc{T}op- k k \\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \\textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "3b9SKkRAKw@OpenReview",
      "index": 5,
      "title": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models",
      "authors": [
        "Hantao Zhang",
        "Yuhe Liu",
        "Jiancheng Yang",
        "Shouhong Wan",
        "Xinyuan Wang",
        "Wei Peng",
        "Pascal Fua"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lesion",
        "lefusion",
        "diffusion",
        "synthesis",
        "control",
        "lesions",
        "swinunetr",
        "focused",
        "pathology",
        "backgrounds"
      ],
      "summary": "Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3b9SKkRAKw"
        ],
        "venue": [
          "/venue/3b9SKkRAKw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3b9SKkRAKw"
        ],
        "detail": [
          "https://openreview.net/forum?id=3b9SKkRAKw"
        ]
      },
      "scores": {
        "pdf": 34,
        "kimi": 23
      },
      "raw_excerpt": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models [PDF 34 ] [Copy] [Kimi 23 ] [REL] Authors : Hantao Zhang , Yuhe Liu , Jiancheng Yang , Shouhong Wan , Xinyuan Wang , Wei Peng , Pascal Fua Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "OwpLQrpdwE@OpenReview",
      "index": 6,
      "title": "Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels",
      "authors": [
        "Daning Huang",
        "Hanyang He",
        "John Harlim",
        "Yan Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ode",
        "solver",
        "manifolds",
        "geometrically",
        "kernel",
        "vector",
        "fields",
        "odes",
        "constrained",
        "valued"
      ],
      "summary": "We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands. To overcome this issue, we introduce a geometrically constrained operator-valued kernel that allows us to represent vector fields on tangent bundles of smooth manifolds. The construction of the kernel imposes the geometric constraints that are estimated from the data and ensures the computational feasibility for learning high dimensional systems of ODEs. Once the vector fields are estimated, e.g., by the kernel ridge regression, we need an ODE solver that guarantees the solution to stay on (or close to) the manifold. To overcome this issue, we propose a geometry-preserving ODE solver that approximates the exponential maps corresponding to the ODE solutions. We deduce a theoretical error bound for the proposed solver that guarantees the approximate solutions to lie on the manifold in the limit of large data. We verify the effectiveness of the proposed approach on high-dimensional dynamical systems, including the cavity flow problem, the beating and travelling waves in Kuramoto-Sivashinsky equations, and the reaction-diffusion dynamics.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OwpLQrpdwE"
        ],
        "venue": [
          "/venue/OwpLQrpdwE@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OwpLQrpdwE"
        ],
        "detail": [
          "https://openreview.net/forum?id=OwpLQrpdwE"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 15
      },
      "raw_excerpt": "Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels [PDF 20 ] [Copy] [Kimi 15 ] [REL] Authors : Daning Huang , Hanyang He , John Harlim , Yan Li We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands. To overcome this issue, we introduce a geometrically constrained operator-valued kernel that allows us to represent vector fields on tangent bundles of smooth manifolds. The construction of the kernel imposes the geometric constraints that are estimated from the data and ensures the computational feasibility for learning high dimensional systems of ODEs. Once the vector fields are estimated, e.g., by the kernel ridge regression, we need an ODE solver that guarantees the solution to stay on (or close to) the manifold. To overcome this issue, we propose a geometry-preserving ODE solver that approximates the exponential maps corresponding to the ODE solutions. We deduce a theoretical error bound for the proposed solver that guarantees the approximate solutions to lie on the manifold in the limit of large data. We verify the effectiveness of the proposed approach on high-dimensional dynamical systems, including the cavity flow problem, the beating and travelling waves in Kuramoto-Sivashinsky equations, and the reaction-diffusion dynamics. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "oQ4igHyh3N@OpenReview",
      "index": 7,
      "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
      "authors": [
        "Haiyang Wang",
        "Yue Fan",
        "Muhammad Ferjad Naeem",
        "Liwei Wang",
        "Yongqin Xian",
        "Jan E Lenssen",
        "Federico Tombari",
        "Bernt Schiele"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "tokenformer",
        "tokens",
        "scratch",
        "parameters",
        "transformers",
        "tokenized",
        "model",
        "scaling",
        "architectural",
        "retraining"
      ],
      "summary": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code will be available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oQ4igHyh3N"
        ],
        "venue": [
          "/venue/oQ4igHyh3N@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oQ4igHyh3N"
        ],
        "detail": [
          "https://openreview.net/forum?id=oQ4igHyh3N"
        ]
      },
      "scores": {
        "pdf": 44,
        "kimi": 31
      },
      "raw_excerpt": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters [PDF 44 ] [Copy] [Kimi 31 ] [REL] Authors : Haiyang Wang , Yue Fan , Muhammad Ferjad Naeem , Liwei Wang , Yongqin Xian , Jan E Lenssen , Federico Tombari , Bernt Schiele Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code will be available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "xaYlO03tIk@OpenReview",
      "index": 8,
      "title": "Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion",
      "authors": [
        "Kaizhe Hu",
        "Zihang Rui",
        "Yao He",
        "Yuyao Liu",
        "Pu Hua",
        "Huazhe Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "stem",
        "inversion",
        "visual",
        "imitation",
        "generalizable",
        "convergent",
        "observation",
        "diffusion",
        "settings",
        "appearance"
      ],
      "summary": "Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose ***Stem-OB*** that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. *Stem-OB* offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. *Stem-OB* shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of **22.2%** in success rates compared to the best baseline. Please refer to [this link](https://stem-ob.github.io/) for more videos and details.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xaYlO03tIk"
        ],
        "venue": [
          "/venue/xaYlO03tIk@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xaYlO03tIk"
        ],
        "detail": [
          "https://openreview.net/forum?id=xaYlO03tIk"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 9
      },
      "raw_excerpt": "Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion [PDF 11 ] [Copy] [Kimi 9 ] [REL] Authors : Kaizhe Hu , Zihang Rui , Yao He , Yuyao Liu , Pu Hua , Huazhe Xu Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose ***Stem-OB*** that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. *Stem-OB* offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. *Stem-OB* shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of **22.2%** in success rates compared to the best baseline. Please refer to [this link](https://stem-ob.github.io/) for more videos and details. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "nYpPAT4L3D@OpenReview",
      "index": 9,
      "title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding",
      "authors": [
        "Zhongyi Shui",
        "Jianpeng Zhang",
        "Weiwei Cao",
        "Sinuo Wang",
        "Ruizhe Guo",
        "Le Lu",
        "Ling Zhang",
        "Tingbo Liang",
        "Lin Yang",
        "Xianghua Ye",
        "Qi Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "fvlm",
        "anatomy",
        "medical",
        "interpretation",
        "image",
        "language",
        "diagnosis",
        "reports",
        "contrastive",
        "grained"
      ],
      "summary": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nYpPAT4L3D"
        ],
        "venue": [
          "/venue/nYpPAT4L3D@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nYpPAT4L3D"
        ],
        "detail": [
          "https://openreview.net/forum?id=nYpPAT4L3D"
        ]
      },
      "scores": {
        "pdf": 28,
        "kimi": 23
      },
      "raw_excerpt": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding [PDF 28 ] [Copy] [Kimi 23 ] [REL] Authors : Zhongyi Shui , Jianpeng Zhang , Weiwei Cao , Sinuo Wang , Ruizhe Guo , Le Lu , Ling Zhang , Tingbo Liang , Lin Yang , Xianghua Ye , Qi Zhang Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "UvfI4grcM7@OpenReview",
      "index": 10,
      "title": "Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics",
      "authors": [
        "Tianfang Zhu",
        "Dongli Hu",
        "Jiandong Zhou",
        "Kai Du",
        "Anan LI"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "barrel",
        "whisker",
        "cortex",
        "biological",
        "neural",
        "neuronal",
        "biologically",
        "constrained",
        "model",
        "brain"
      ],
      "summary": "The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an effective construction and training pipeline tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological constraints, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks.Critically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications. The code is available at https://github.com/fun0515/RSNN_bfd.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UvfI4grcM7"
        ],
        "venue": [
          "/venue/UvfI4grcM7@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UvfI4grcM7"
        ],
        "detail": [
          "https://openreview.net/forum?id=UvfI4grcM7"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 10
      },
      "raw_excerpt": "Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics [PDF 9 ] [Copy] [Kimi 10 ] [REL] Authors : Tianfang Zhu , Dongli Hu , Jiandong Zhou , Kai Du , Anan LI The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an effective construction and training pipeline tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological constraints, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks.Critically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications. The code is available at https://github.com/fun0515/RSNN_bfd. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Mn2qgIcIPS@OpenReview",
      "index": 11,
      "title": "Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs",
      "authors": [
        "Donggoo Jung",
        "Daehyun Kim",
        "Tae Hyun Kim"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "unsupervised",
        "enhancement",
        "adjustment",
        "curve",
        "light",
        "image",
        "exposure",
        "continuous",
        "low",
        "approaches"
      ],
      "summary": "Low-light image enhancement poses a significant challenge due to the limited information captured by image sensors in low-light environments. Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle. Therefore, unsupervised methods have emerged as a promising solution. In this work, we focus on the strength of curve-adjustment-based approaches to tackle unsupervised methods. The majority of existing unsupervised curve-adjustment approaches iteratively estimate higher order curve parameters to enhance the exposure of images while efficiently preserving the details of the images. However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance. To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image. The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches. Consequently, we achieve state-of-the-art performance in unsupervised low-light image enhancement across various benchmark datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Mn2qgIcIPS"
        ],
        "venue": [
          "/venue/Mn2qgIcIPS@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Mn2qgIcIPS"
        ],
        "detail": [
          "https://openreview.net/forum?id=Mn2qgIcIPS"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 7
      },
      "raw_excerpt": "Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs [PDF 19 ] [Copy] [Kimi 7 ] [REL] Authors : Donggoo Jung , Daehyun Kim , Tae Hyun Kim Low-light image enhancement poses a significant challenge due to the limited information captured by image sensors in low-light environments. Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle. Therefore, unsupervised methods have emerged as a promising solution. In this work, we focus on the strength of curve-adjustment-based approaches to tackle unsupervised methods. The majority of existing unsupervised curve-adjustment approaches iteratively estimate higher order curve parameters to enhance the exposure of images while efficiently preserving the details of the images. However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance. To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image. The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches. Consequently, we achieve state-of-the-art performance in unsupervised low-light image enhancement across various benchmark datasets. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "J9VogDTa1W@OpenReview",
      "index": 12,
      "title": "Systems with Switching Causal Relations: A Meta-Causal Perspective",
      "authors": [
        "Moritz Willig",
        "Tim Tobiasch",
        "Florian Busch",
        "Jonas Seng",
        "Devendra Singh Dhami",
        "Kristian Kersting"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "causal",
        "meta",
        "emerge",
        "qualitative",
        "behavior",
        "states",
        "relationships",
        "consolidates",
        "switching",
        "tipping"
      ],
      "summary": "Most works on causality in machine learning assume that causal relationships are governed by a constant underlying process. However, the flexibility of agents' actions or tipping point behavior in the environmental process can change the qualitative dynamics of the system. As a result, new causal relationships may emerge, while existing ones change or disappear, resulting in an altered causal graph. To analyze these qualitative changes on the causal graph, we propose the concept of *meta-causal states*, which groups classical causal models into clusters based on equivalent qualitative behavior and consolidates specific mechanism parameterizations. We demonstrate how meta-causal states can be inferred from observed agent behavior, and discuss potential methods for disentangling these states from unlabeled data. Finally, we direct our analysis toward the application of a dynamical system, demonstrating that meta-causal states can also emerge from inherent system dynamics, and thus constitute more than a context-dependent framework in which mechanisms emerge merely as a result of external factors.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=J9VogDTa1W"
        ],
        "venue": [
          "/venue/J9VogDTa1W@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=J9VogDTa1W"
        ],
        "detail": [
          "https://openreview.net/forum?id=J9VogDTa1W"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 15
      },
      "raw_excerpt": "Systems with Switching Causal Relations: A Meta-Causal Perspective [PDF 9 ] [Copy] [Kimi 15 ] [REL] Authors : Moritz Willig , Tim Tobiasch , Florian Busch , Jonas Seng , Devendra Singh Dhami , Kristian Kersting Most works on causality in machine learning assume that causal relationships are governed by a constant underlying process. However, the flexibility of agents' actions or tipping point behavior in the environmental process can change the qualitative dynamics of the system. As a result, new causal relationships may emerge, while existing ones change or disappear, resulting in an altered causal graph. To analyze these qualitative changes on the causal graph, we propose the concept of *meta-causal states*, which groups classical causal models into clusters based on equivalent qualitative behavior and consolidates specific mechanism parameterizations. We demonstrate how meta-causal states can be inferred from observed agent behavior, and discuss potential methods for disentangling these states from unlabeled data. Finally, we direct our analysis toward the application of a dynamical system, demonstrating that meta-causal states can also emerge from inherent system dynamics, and thus constitute more than a context-dependent framework in which mechanisms emerge merely as a result of external factors. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "QogcGNXJVw@OpenReview",
      "index": 13,
      "title": "The Computational Complexity of Circuit Discovery for Inner Interpretability",
      "authors": [
        "Federico Adolfi",
        "Martina G. Vilas",
        "Todd Wareham"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "queries",
        "circuit",
        "complexity",
        "interpretability",
        "hard",
        "affordances",
        "discovery",
        "tractability",
        "intractable",
        "inner"
      ],
      "summary": "Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries that capture mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons (part of, e.g., transformers). Our findings reveal a challenging complexity landscape. Many queries are intractable (NP-hard, Σ p 2 Σ 2 p -hard), remain fixed-parameter intractable (W[1]-hard) when constraining model/circuit features (e.g., depth), and are inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems (NP- vs. Σ p 2 Σ 2 p -complete) with better-understood heuristics, and prove the tractability (PTIME) or fixed-parameter tractability (FPT) of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands among existing and future architectures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QogcGNXJVw"
        ],
        "venue": [
          "/venue/QogcGNXJVw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QogcGNXJVw"
        ],
        "detail": [
          "https://openreview.net/forum?id=QogcGNXJVw"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 10
      },
      "raw_excerpt": "The Computational Complexity of Circuit Discovery for Inner Interpretability [PDF 11 ] [Copy] [Kimi 10 ] [REL] Authors : Federico Adolfi , Martina G. Vilas , Todd Wareham Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries that capture mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons (part of, e.g., transformers). Our findings reveal a challenging complexity landscape. Many queries are intractable (NP-hard, Σ p 2 Σ 2 p -hard), remain fixed-parameter intractable (W[1]-hard) when constraining model/circuit features (e.g., depth), and are inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems (NP- vs. Σ p 2 Σ 2 p -complete) with better-understood heuristics, and prove the tractability (PTIME) or fixed-parameter tractability (FPT) of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands among existing and future architectures. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "03OkC0LKDD@OpenReview",
      "index": 14,
      "title": "The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed Learning",
      "authors": [
        "Youssef Allouah",
        "Rachid Guerraoui",
        "Nirupam Gupta",
        "Ahmed Jellouli",
        "Geovani Rizk",
        "John Stephan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "clipping",
        "dgd",
        "arc",
        "robust",
        "byzantine",
        "resilient",
        "robustness",
        "gradient",
        "distributed",
        "sota"
      ],
      "summary": "Byzantine-resilient distributed machine learning seeks to achieve robust learning performance in the presence of misbehaving or adversarial workers.While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods wereproven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping.However, the currently considered staticclipping strategy exhibits mixed results: improving robustness against some attacks while being ineffective or detrimental against others.We address this gap by proposing a principled adaptive clipping strategy, termed Adaptive Robust Clipping (ARC).We show that ARC consistently enhances the empirical robustness of SOTA Robust-DGD methods, while preserving the theoretical robustness guarantees. Our analysis shows that ARC provably improves the asymptotic convergence guarantee of Robust-DGD in the case when the model is well-initialized.We validate this theoretical insight through an exhaustive set of experiments on benchmark image classification tasks.We observe that the improvement induced by ARC is more pronounced in highly heterogeneous and adversarial settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=03OkC0LKDD"
        ],
        "venue": [
          "/venue/03OkC0LKDD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=03OkC0LKDD"
        ],
        "detail": [
          "https://openreview.net/forum?id=03OkC0LKDD"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 5
      },
      "raw_excerpt": "The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed Learning [PDF 10 ] [Copy] [Kimi 5 ] [REL] Authors : Youssef Allouah , Rachid Guerraoui , Nirupam Gupta , Ahmed Jellouli , Geovani Rizk , John Stephan Byzantine-resilient distributed machine learning seeks to achieve robust learning performance in the presence of misbehaving or adversarial workers.While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods wereproven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping.However, the currently considered staticclipping strategy exhibits mixed results: improving robustness against some attacks while being ineffective or detrimental against others.We address this gap by proposing a principled adaptive clipping strategy, termed Adaptive Robust Clipping (ARC).We show that ARC consistently enhances the empirical robustness of SOTA Robust-DGD methods, while preserving the theoretical robustness guarantees. Our analysis shows that ARC provably improves the asymptotic convergence guarantee of Robust-DGD in the case when the model is well-initialized.We validate this theoretical insight through an exhaustive set of experiments on benchmark image classification tasks.We observe that the improvement induced by ARC is more pronounced in highly heterogeneous and adversarial settings. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "M7KyLjuN0A@OpenReview",
      "index": 15,
      "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
      "authors": [
        "Hengwei Bian",
        "Lingdong Kong",
        "Haozhe Xie",
        "Liang Pan",
        "Yu Qiao",
        "Ziwei Liu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hexplane",
        "dynamiccity",
        "lidar",
        "generation",
        "scenes",
        "dit",
        "feature",
        "miou",
        "dynamic",
        "06x"
      ],
      "summary": "LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic LiDAR scenes with semantics. DynamicCity mainly consists of two key models. **1)** A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel **Projection Module** to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to **12.56** mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to **7.05** mIoU gain, **2.06x** training speedup, and **70.84\\%** memory reduction). **2)** A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a **Padded Rollout Operation** is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting **versatile 4D generation applications**, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M7KyLjuN0A"
        ],
        "venue": [
          "/venue/M7KyLjuN0A@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M7KyLjuN0A"
        ],
        "detail": [
          "https://openreview.net/forum?id=M7KyLjuN0A"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 9
      },
      "raw_excerpt": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes [PDF 10 ] [Copy] [Kimi 9 ] [REL] Authors : Hengwei Bian , Lingdong Kong , Haozhe Xie , Liang Pan , Yu Qiao , Ziwei Liu LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic LiDAR scenes with semantics. DynamicCity mainly consists of two key models. **1)** A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel **Projection Module** to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to **12.56** mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to **7.05** mIoU gain, **2.06x** training speedup, and **70.84\\%** memory reduction). **2)** A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a **Padded Rollout Operation** is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting **versatile 4D generation applications**, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cTR17xl89h@OpenReview",
      "index": 16,
      "title": "BodyGen: Advancing Towards Efficient Embodiment Co-Design",
      "authors": [
        "Haofei Lu",
        "Zhe Wu",
        "Junliang Xing",
        "Jianshu Li",
        "Ruoyu Li",
        "Zhe Li",
        "Yuanchun Shi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "bodygen",
        "embodiment",
        "morphology",
        "design",
        "control",
        "reward",
        "advancing",
        "efficient",
        "towards",
        "signals"
      ],
      "summary": "Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control.We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency.To advance towards efficient embodiment co-design, we propose **BodyGen**, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average **60.03%** performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cTR17xl89h"
        ],
        "venue": [
          "/venue/cTR17xl89h@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cTR17xl89h"
        ],
        "detail": [
          "https://openreview.net/forum?id=cTR17xl89h"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 3
      },
      "raw_excerpt": "BodyGen: Advancing Towards Efficient Embodiment Co-Design [PDF 9 ] [Copy] [Kimi 3 ] [REL] Authors : Haofei Lu , Zhe Wu , Junliang Xing , Jianshu Li , Ruoyu Li , Zhe Li , Yuanchun Shi Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control.We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency.To advance towards efficient embodiment co-design, we propose **BodyGen**, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average **60.03%** performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "7BQkXXM8Fy@OpenReview",
      "index": 17,
      "title": "What Makes a Good Diffusion Planner for Decision Making?",
      "authors": [
        "Haofei Lu",
        "Dongqi Han",
        "Yifei Shen",
        "Dongsheng Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "diffusion",
        "planning",
        "planner",
        "decision",
        "components",
        "choices",
        "offline",
        "sampling",
        "good",
        "making"
      ],
      "summary": "Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7BQkXXM8Fy"
        ],
        "venue": [
          "/venue/7BQkXXM8Fy@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7BQkXXM8Fy"
        ],
        "detail": [
          "https://openreview.net/forum?id=7BQkXXM8Fy"
        ]
      },
      "scores": {
        "pdf": 25,
        "kimi": 11
      },
      "raw_excerpt": "What Makes a Good Diffusion Planner for Decision Making? [PDF 25 ] [Copy] [Kimi 11 ] [REL] Authors : Haofei Lu , Dongqi Han , Yifei Shen , Dongsheng Li Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "mOpNrrV2zH@OpenReview",
      "index": 18,
      "title": "CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph",
      "authors": [
        "Haitao Lin",
        "Guojiang Zhao",
        "Odin Zhang",
        "Yufei Huang",
        "Lirong Wu",
        "Cheng Tan",
        "Zicheng Liu",
        "Zhifeng Gao",
        "Stan Z Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cbgbench",
        "blank",
        "sbdd",
        "fill",
        "protein",
        "tasks",
        "graph",
        "novo",
        "molecule",
        "binding"
      ],
      "summary": "Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult reproducibility, and task singularity. Firstly, the absence of standardization can lead to unfair comparisons and inconclusive insights. To address this dilemma, we propose CBGBench, a comprehensive benchmark for SBDD, that unifies the task as a generative heterogeneous graph completion, analogous to fill-in-the-blank of the 3D complex binding graph. By categorizing existing methods based on their attributes, CBGBench facilitates a modular and extensible framework that implements various cutting-edge methods. Secondly, a single de novo molecule generation task can hardly reflect their capabilities. To broaden the scope, we adapt these models to a range of tasks essential in drug design, considered sub-tasks within the graph fill-in-the-blank tasks. These tasks include the generative designation of de novo molecules, linkers, fragments, scaffolds, and sidechains, all conditioned on the structures of protein pockets. Our evaluations are conducted with fairness, encompassing comprehensive perspectives on interaction, chemical properties, geometry authenticity, and substructure validity. We further provide deep insights with analysis from empirical studies. Our results indicate that there is potential for further improvements on many tasks, optimization in network architectures, and incorporation of chemical prior knowledge. To lower the barrier to entry and facilitate further developments in the field, we also provide a unified codebase (supplementary) that includes the discussed state-of-the-art models, data pre-processing, training, sampling, and evaluation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mOpNrrV2zH"
        ],
        "venue": [
          "/venue/mOpNrrV2zH@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mOpNrrV2zH"
        ],
        "detail": [
          "https://openreview.net/forum?id=mOpNrrV2zH"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 6
      },
      "raw_excerpt": "CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph [PDF 13 ] [Copy] [Kimi 6 ] [REL] Authors : Haitao Lin , Guojiang Zhao , Odin Zhang , Yufei Huang , Lirong Wu , Cheng Tan , Zicheng Liu , Zhifeng Gao , Stan Z Li Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult reproducibility, and task singularity. Firstly, the absence of standardization can lead to unfair comparisons and inconclusive insights. To address this dilemma, we propose CBGBench, a comprehensive benchmark for SBDD, that unifies the task as a generative heterogeneous graph completion, analogous to fill-in-the-blank of the 3D complex binding graph. By categorizing existing methods based on their attributes, CBGBench facilitates a modular and extensible framework that implements various cutting-edge methods. Secondly, a single de novo molecule generation task can hardly reflect their capabilities. To broaden the scope, we adapt these models to a range of tasks essential in drug design, considered sub-tasks within the graph fill-in-the-blank tasks. These tasks include the generative designation of de novo molecules, linkers, fragments, scaffolds, and sidechains, all conditioned on the structures of protein pockets. Our evaluations are conducted with fairness, encompassing comprehensive perspectives on interaction, chemical properties, geometry authenticity, and substructure validity. We further provide deep insights with analysis from empirical studies. Our results indicate that there is potential for further improvements on many tasks, optimization in network architectures, and incorporation of chemical prior knowledge. To lower the barrier to entry and facilitate further developments in the field, we also provide a unified codebase (supplementary) that includes the discussed state-of-the-art models, data pre-processing, training, sampling, and evaluation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wJv4AIt4sK@OpenReview",
      "index": 19,
      "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice",
      "authors": [
        "Simla Harma",
        "Ayan Chakraborty",
        "Elizaveta Kostenok",
        "Danila Mishin",
        "Dongho Ha",
        "Babak Falsafi",
        "Martin Jaggi",
        "Ming Liu",
        "Yunho Oh",
        "Suvinay Subramanian",
        "Amir Yazdanbakhsh"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sparsity",
        "quantization",
        "footprints",
        "compression",
        "dnns",
        "reduce",
        "methods",
        "accuracy",
        "tacitly",
        "125m"
      ],
      "summary": "The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wJv4AIt4sK"
        ],
        "venue": [
          "/venue/wJv4AIt4sK@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wJv4AIt4sK"
        ],
        "detail": [
          "https://openreview.net/forum?id=wJv4AIt4sK"
        ]
      },
      "scores": {
        "pdf": 24,
        "kimi": 12
      },
      "raw_excerpt": "Effective Interplay between Sparsity and Quantization: From Theory to Practice [PDF 24 ] [Copy] [Kimi 12 ] [REL] Authors : Simla Harma , Ayan Chakraborty , Elizaveta Kostenok , Danila Mishin , Dongho Ha , Babak Falsafi , Martin Jaggi , Ming Liu , Yunho Oh , Suvinay Subramanian , Amir Yazdanbakhsh The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "7xCSK9BLPy@OpenReview",
      "index": 20,
      "title": "Better Instruction-Following Through Minimum Bayes Risk",
      "authors": [
        "Ian Wu",
        "Patrick Fernandes",
        "Amanda Bertsch",
        "Seungone Kim",
        "Sina Pakazad",
        "Graham Neubig"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mbr",
        "decoding",
        "judges",
        "llm",
        "instruction",
        "llms",
        "bayes",
        "reference",
        "following",
        "minimum"
      ],
      "summary": "General-purpose LLM judges capable of human-level evaluation provide not only a scalable and accurate way of evaluating instruction-following LLMs but also new avenues for supervising and improving their performance. One promising way of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR) decoding, which uses a reference-based evaluator to select a high-quality output from amongst a set of candidate outputs. In the first part of this work, we explore using MBR decoding as a method for improving the test-time performance of instruction-following LLMs. We find that MBR decoding with reference-based LLM judges substantially improves over greedy decoding, best-of-N decoding with reference-free judges and MBR decoding with lexical and embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent across LLMs with up to 70B parameters, demonstrating that smaller LLM judges can be used to supervise much larger LLMs. Then, seeking to retain the improvements from MBR decoding while mitigating additional test-time costs, we explore iterative self-training on MBR-decoded outputs. We find that self-training using Direct Preference Optimisation leads to significant performance gains, such that the self-trained models with greedy decoding generally match and sometimes exceed the performance of their base models with MBR decoding.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7xCSK9BLPy"
        ],
        "venue": [
          "/venue/7xCSK9BLPy@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7xCSK9BLPy"
        ],
        "detail": [
          "https://openreview.net/forum?id=7xCSK9BLPy"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 10
      },
      "raw_excerpt": "Better Instruction-Following Through Minimum Bayes Risk [PDF 15 ] [Copy] [Kimi 10 ] [REL] Authors : Ian Wu , Patrick Fernandes , Amanda Bertsch , Seungone Kim , Sina Pakazad , Graham Neubig General-purpose LLM judges capable of human-level evaluation provide not only a scalable and accurate way of evaluating instruction-following LLMs but also new avenues for supervising and improving their performance. One promising way of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR) decoding, which uses a reference-based evaluator to select a high-quality output from amongst a set of candidate outputs. In the first part of this work, we explore using MBR decoding as a method for improving the test-time performance of instruction-following LLMs. We find that MBR decoding with reference-based LLM judges substantially improves over greedy decoding, best-of-N decoding with reference-free judges and MBR decoding with lexical and embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent across LLMs with up to 70B parameters, demonstrating that smaller LLM judges can be used to supervise much larger LLMs. Then, seeking to retain the improvements from MBR decoding while mitigating additional test-time costs, we explore iterative self-training on MBR-decoded outputs. We find that self-training using Direct Preference Optimisation leads to significant performance gains, such that the self-trained models with greedy decoding generally match and sometimes exceed the performance of their base models with MBR decoding. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qtTIP5Gjc5@OpenReview",
      "index": 21,
      "title": "Demystifying the Token Dynamics of Deep Selective State Space Models",
      "authors": [
        "Thieu Vo",
        "Duy-Tung Pham",
        "Xin Tong",
        "Tan Nguyen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mamba",
        "tokens",
        "selective",
        "scenario",
        "ssm",
        "diverge",
        "refinements",
        "convergent",
        "demystifying",
        "model"
      ],
      "summary": "Selective state space models (SSM), such as Mamba, have gained prominence for their effectiveness in modeling sequential data. Despite their outstanding empirical performance, a comprehensive theoretical understanding of deep selective SSM remains elusive, hindering their further development and adoption for applications that need high fidelity. In this paper, we investigate the dynamical properties of tokens in a pre-trained Mamba model. In particular, we derive the dynamical system governing the continuous-time limit of the Mamba model and characterize the asymptotic behavior of its solutions. In the one-dimensional case, we prove that only one of the following two scenarios happens: either all tokens converge to zero, or all tokens diverge to infinity. We provide criteria based on model parameters to determine when each scenario occurs. For the convergent scenario, we empirically verify that this scenario negatively impacts the model's performance. For the divergent scenario, we prove that different tokens will diverge to infinity at different rates, thereby contributing unequally to the updates during model training. Based on these investigations, we propose two refinements for the model: excluding the convergent scenario and reordering tokens based on their importance scores, both aimed at improving practical performance. Our experimental results validate these refinements, offering insights into enhancing Mamba's effectiveness in real-world applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qtTIP5Gjc5"
        ],
        "venue": [
          "/venue/qtTIP5Gjc5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qtTIP5Gjc5"
        ],
        "detail": [
          "https://openreview.net/forum?id=qtTIP5Gjc5"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 8
      },
      "raw_excerpt": "Demystifying the Token Dynamics of Deep Selective State Space Models [PDF 9 ] [Copy] [Kimi 8 ] [REL] Authors : Thieu Vo , Duy-Tung Pham , Xin Tong , Tan Nguyen Selective state space models (SSM), such as Mamba, have gained prominence for their effectiveness in modeling sequential data. Despite their outstanding empirical performance, a comprehensive theoretical understanding of deep selective SSM remains elusive, hindering their further development and adoption for applications that need high fidelity. In this paper, we investigate the dynamical properties of tokens in a pre-trained Mamba model. In particular, we derive the dynamical system governing the continuous-time limit of the Mamba model and characterize the asymptotic behavior of its solutions. In the one-dimensional case, we prove that only one of the following two scenarios happens: either all tokens converge to zero, or all tokens diverge to infinity. We provide criteria based on model parameters to determine when each scenario occurs. For the convergent scenario, we empirically verify that this scenario negatively impacts the model's performance. For the divergent scenario, we prove that different tokens will diverge to infinity at different rates, thereby contributing unequally to the updates during model training. Based on these investigations, we propose two refinements for the model: excluding the convergent scenario and reordering tokens based on their importance scores, both aimed at improving practical performance. Our experimental results validate these refinements, offering insights into enhancing Mamba's effectiveness in real-world applications. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IwPXYk6BV9@OpenReview",
      "index": 22,
      "title": "Enhancing Learning with Label Differential Privacy by Vector Approximation",
      "authors": [
        "Puning Zhao",
        "Jiafei Wu",
        "Zhe Liu",
        "Li Shen",
        "Zhikun Zhang",
        "Rongfei Fan",
        "Le Sun",
        "Qingming Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "label",
        "privacy",
        "vector",
        "flipping",
        "labels",
        "differential",
        "approximation",
        "privatized",
        "protects",
        "datasets"
      ],
      "summary": "Label differential privacy (DP) is a framework that protects the privacy of labels in training datasets, while the feature vectors are public. Existing approaches protect the privacy of labels by flipping them randomly, and then train a model to make the output approximate the privatized label. However, as the number of classes K increases, stronger randomization is needed, thus the performances of these methods become significantly worse. In this paper, we propose a vector approximation approach for learning with label local differential privacy, which is easy to implement and introduces little additional computational overhead. Instead of flipping each label into a single scalar, our method converts each label into a random vector with K components, whose expectations reflect class conditional probabilities. Intuitively, vector approximation retains more information than scalar labels. A brief theoretical analysis shows that the performance of our method only decays slightly with K. Finally, we conduct experiments on both synthesized and real datasets, which validate our theoretical analysis as well as the practical performance of our method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IwPXYk6BV9"
        ],
        "venue": [
          "/venue/IwPXYk6BV9@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IwPXYk6BV9"
        ],
        "detail": [
          "https://openreview.net/forum?id=IwPXYk6BV9"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 4
      },
      "raw_excerpt": "Enhancing Learning with Label Differential Privacy by Vector Approximation [PDF 7 ] [Copy] [Kimi 4 ] [REL] Authors : Puning Zhao , Jiafei Wu , Zhe Liu , Li Shen , Zhikun Zhang , Rongfei Fan , Le Sun , Qingming Li Label differential privacy (DP) is a framework that protects the privacy of labels in training datasets, while the feature vectors are public. Existing approaches protect the privacy of labels by flipping them randomly, and then train a model to make the output approximate the privatized label. However, as the number of classes K increases, stronger randomization is needed, thus the performances of these methods become significantly worse. In this paper, we propose a vector approximation approach for learning with label local differential privacy, which is easy to implement and introduces little additional computational overhead. Instead of flipping each label into a single scalar, our method converts each label into a random vector with K components, whose expectations reflect class conditional probabilities. Intuitively, vector approximation retains more information than scalar labels. A brief theoretical analysis shows that the performance of our method only decays slightly with K. Finally, we conduct experiments on both synthesized and real datasets, which validate our theoretical analysis as well as the practical performance of our method. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SgymXhOEA5@OpenReview",
      "index": 23,
      "title": "Exploring the Camera bias of Person Re-identification",
      "authors": [
        "Myungseo Song",
        "Jin-Woo Park",
        "Jong-Seok Lee"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "camera",
        "bias",
        "reid",
        "person",
        "unsupervised",
        "identification",
        "models",
        "unseen",
        "normalization",
        "biased"
      ],
      "summary": "We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SgymXhOEA5"
        ],
        "venue": [
          "/venue/SgymXhOEA5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SgymXhOEA5"
        ],
        "detail": [
          "https://openreview.net/forum?id=SgymXhOEA5"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": null
      },
      "raw_excerpt": "Exploring the Camera bias of Person Re-identification [PDF 8 ] [Copy] [Kimi ] [REL] Authors : Myungseo Song , Jin-Woo Park , Jong-Seok Lee We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "rdAbEn5DZt@OpenReview",
      "index": 24,
      "title": "Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization",
      "authors": [
        "Hansi Yang",
        "James Kwok"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ordering",
        "optimization",
        "objective",
        "sample",
        "jogba",
        "balancing",
        "sum",
        "mgda",
        "convergence",
        "objectives"
      ],
      "summary": "In finite-sum optimization problems, the sample orders for parameter updates can significantly influence the convergence rate of optimization algorithms. While numerous sample ordering techniques have been proposed in the context of single-objective optimization, the problem of sample ordering in finite-sum multi-objective optimization has not been thoroughly explored. To address this gap, we propose a sample ordering method called JoGBa, which finds the sample orders for multiple objectives by jointly performing online vector balancing on the gradients of all objectives. Our theoretical analysis demonstrates that this approach outperforms the standard baseline of random ordering and accelerates the convergence rate for the MGDA algorithm. Empirical evaluation across various datasets with different multi-objective optimization algorithms further demonstrates that JoGBa can achieve faster convergence and superior final performance than other data ordering strategies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rdAbEn5DZt"
        ],
        "venue": [
          "/venue/rdAbEn5DZt@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rdAbEn5DZt"
        ],
        "detail": [
          "https://openreview.net/forum?id=rdAbEn5DZt"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 2
      },
      "raw_excerpt": "Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization [PDF 7 ] [Copy] [Kimi 2 ] [REL] Authors : Hansi Yang , James Kwok In finite-sum optimization problems, the sample orders for parameter updates can significantly influence the convergence rate of optimization algorithms. While numerous sample ordering techniques have been proposed in the context of single-objective optimization, the problem of sample ordering in finite-sum multi-objective optimization has not been thoroughly explored. To address this gap, we propose a sample ordering method called JoGBa, which finds the sample orders for multiple objectives by jointly performing online vector balancing on the gradients of all objectives. Our theoretical analysis demonstrates that this approach outperforms the standard baseline of random ordering and accelerates the convergence rate for the MGDA algorithm. Empirical evaluation across various datasets with different multi-objective optimization algorithms further demonstrates that JoGBa can achieve faster convergence and superior final performance than other data ordering strategies. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "WcZLG8XxhD@OpenReview",
      "index": 25,
      "title": "Learning-Augmented Frequent Directions",
      "authors": [
        "Anders Aamand",
        "Justin Chen",
        "Siddharth Gollapudi",
        "Sandeep Silwal",
        "Hao WU"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "countmin",
        "augmented",
        "streaming",
        "countsketch",
        "hsu",
        "frequency",
        "learned",
        "frequent",
        "directions",
        "estimation"
      ],
      "summary": "An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in a long stream of data using only a small amount of memory. Hsu et al. develop a natural framework to combine the worst-case guarantees of popular solutions such as CountMin and CountSketch with learned predictions of high frequency elements. They demonstrate that learning the underlying structure of data can be used to yield better streaming algorithms, both in theory and practice.We simplify and generalize past work on learning-augmented frequency estimation. Our first contribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon the error of learned CountMin and learned CountSketch and achieves the state-of-the-art performance of randomized algorithms (Aamand et al., NeurIPS'23) with a simpler, deterministic algorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional generalization of frequency estimation corresponding to finding important directions (top singular vectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented variant of the Frequent Directions algorithm, extending the theoretical and empirical understanding of learned predictions to matrix streaming.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WcZLG8XxhD"
        ],
        "venue": [
          "/venue/WcZLG8XxhD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WcZLG8XxhD"
        ],
        "detail": [
          "https://openreview.net/forum?id=WcZLG8XxhD"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 5
      },
      "raw_excerpt": "Learning-Augmented Frequent Directions [PDF 6 ] [Copy] [Kimi 5 ] [REL] Authors : Anders Aamand , Justin Chen , Siddharth Gollapudi , Sandeep Silwal , Hao WU An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in a long stream of data using only a small amount of memory. Hsu et al. develop a natural framework to combine the worst-case guarantees of popular solutions such as CountMin and CountSketch with learned predictions of high frequency elements. They demonstrate that learning the underlying structure of data can be used to yield better streaming algorithms, both in theory and practice.We simplify and generalize past work on learning-augmented frequency estimation. Our first contribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon the error of learned CountMin and learned CountSketch and achieves the state-of-the-art performance of randomized algorithms (Aamand et al., NeurIPS'23) with a simpler, deterministic algorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional generalization of frequency estimation corresponding to finding important directions (top singular vectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented variant of the Frequent Directions algorithm, extending the theoretical and empirical understanding of learned predictions to matrix streaming. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "2o58Mbqkd2@OpenReview",
      "index": 26,
      "title": "The Superposition of Diffusion Models",
      "authors": [
        "Marta Skreta",
        "Lazar Atanackovic",
        "Joey Bose",
        "Alexander Tong",
        "Kirill Neklyudov"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "superdiff",
        "diffusion",
        "superposition",
        "pre",
        "trained",
        "texttt",
        "models",
        "logical",
        "cambrian",
        "painless"
      ],
      "summary": "The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed *solely through composition during inference*, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR OR and the logical AND AND . We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional *de novo* structure design of proteins.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2o58Mbqkd2"
        ],
        "venue": [
          "/venue/2o58Mbqkd2@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2o58Mbqkd2"
        ],
        "detail": [
          "https://openreview.net/forum?id=2o58Mbqkd2"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 11
      },
      "raw_excerpt": "The Superposition of Diffusion Models [PDF 18 ] [Copy] [Kimi 11 ] [REL] Authors : Marta Skreta , Lazar Atanackovic , Joey Bose , Alexander Tong , Kirill Neklyudov The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed *solely through composition during inference*, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR OR and the logical AND AND . We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional *de novo* structure design of proteins. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "xak8c9l1nu@OpenReview",
      "index": 27,
      "title": "Computational Explorations of Total Variation Distance",
      "authors": [
        "Arnab Bhattacharyya",
        "Sutanu Gayen",
        "Kuldeep S. Meel",
        "Dimitrios Myrisiotis",
        "A. Pavan",
        "N. V. Vinodchandran"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "distance",
        "mathsf",
        "variation",
        "explorations",
        "computational",
        "total",
        "underexplored",
        "distributions",
        "arbitrary",
        "alphabets"
      ],
      "summary": "We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance.First, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets.This corresponds to a special case, whereby the TV distance between the two distributions is zero.Second, we prove that unless N P ⊆ R P N P ⊆ R P it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xak8c9l1nu"
        ],
        "venue": [
          "/venue/xak8c9l1nu@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xak8c9l1nu"
        ],
        "detail": [
          "https://openreview.net/forum?id=xak8c9l1nu"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 7
      },
      "raw_excerpt": "Computational Explorations of Total Variation Distance [PDF 4 ] [Copy] [Kimi 7 ] [REL] Authors : Arnab Bhattacharyya , Sutanu Gayen , Kuldeep S. Meel , Dimitrios Myrisiotis , A. Pavan , N. V. Vinodchandran We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance.First, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets.This corresponds to a special case, whereby the TV distance between the two distributions is zero.Second, we prove that unless N P ⊆ R P N P ⊆ R P it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hJ1BaJ5ELp@OpenReview",
      "index": 28,
      "title": "Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation",
      "authors": [
        "Zhanfeng Mo",
        "Haosen Shi",
        "Sinno Pan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sfpk",
        "sparsity",
        "pruning",
        "pruner",
        "parsity",
        "neural",
        "probabilistic",
        "evolutionary",
        "mask",
        "unpruned"
      ],
      "summary": "Neural pruning aims to compress and accelerate deep neural networks by identifying the optimal subnetwork within a specified sparsity budget. In this work, we study how to gradually sparsify the unpruned dense model to the target sparsity level with a minimal performance drop. Specifically, we analyze the evolution of the population of optimal subnetworks under continuous sparsity increments from a thermodynamics perspective. We first reformulate neural pruning as an expected loss minimization problem over the mask distributions. Then, we establish an effective approximation for the sparsity evolution of the optimal mask distribution, termed the **S**parsity Evolutionary **F**okker-**P**lanck-**K**olmogorov Equation (**SFPK**), which provides closed-form, mathematically tractable guidance on distributional transitions for minimizing the expected loss under an infinitesimal sparsity increment. On top of that, we propose SFPK-pruner, a particle simulation-based probabilistic pruning method, to sample performant masks with desired sparsity from the destination distribution of SFPK. In theory, we establish the convergence guarantee for the proposed SFPK-pruner. In practice, our SFPK-pruner exhibits competitive performance across various pruning scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hJ1BaJ5ELp"
        ],
        "venue": [
          "/venue/hJ1BaJ5ELp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hJ1BaJ5ELp"
        ],
        "detail": [
          "https://openreview.net/forum?id=hJ1BaJ5ELp"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Zhanfeng Mo , Haosen Shi , Sinno Pan Neural pruning aims to compress and accelerate deep neural networks by identifying the optimal subnetwork within a specified sparsity budget. In this work, we study how to gradually sparsify the unpruned dense model to the target sparsity level with a minimal performance drop. Specifically, we analyze the evolution of the population of optimal subnetworks under continuous sparsity increments from a thermodynamics perspective. We first reformulate neural pruning as an expected loss minimization problem over the mask distributions. Then, we establish an effective approximation for the sparsity evolution of the optimal mask distribution, termed the **S**parsity Evolutionary **F**okker-**P**lanck-**K**olmogorov Equation (**SFPK**), which provides closed-form, mathematically tractable guidance on distributional transitions for minimizing the expected loss under an infinitesimal sparsity increment. On top of that, we propose SFPK-pruner, a particle simulation-based probabilistic pruning method, to sample performant masks with desired sparsity from the destination distribution of SFPK. In theory, we establish the convergence guarantee for the proposed SFPK-pruner. In practice, our SFPK-pruner exhibits competitive performance across various pruning scenarios. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "GjM61KRiTG@OpenReview",
      "index": 29,
      "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models",
      "authors": [
        "Wenxuan Zhang",
        "Philip Torr",
        "Mohamed Elhoseiny",
        "Adel Bibi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "helpfulness",
        "bfpo",
        "safety",
        "rlhf",
        "factorial",
        "human",
        "preference",
        "tuning",
        "optimization",
        "fine"
      ],
      "summary": "Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In the supervised optimization, a labeling function is used to capture global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark including comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO eliminates the need for human prompting and annotation in LLM fine-tuning while achieving the same level of safety as methods that heavily rely on human labor, with less than 10\\% of the computational resources. The training recipes and models will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GjM61KRiTG"
        ],
        "venue": [
          "/venue/GjM61KRiTG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GjM61KRiTG"
        ],
        "detail": [
          "https://openreview.net/forum?id=GjM61KRiTG"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 11
      },
      "raw_excerpt": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models [PDF 13 ] [Copy] [Kimi 11 ] [REL] Authors : Wenxuan Zhang , Philip Torr , Mohamed Elhoseiny , Adel Bibi Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In the supervised optimization, a labeling function is used to capture global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark including comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO eliminates the need for human prompting and annotation in LLM fine-tuning while achieving the same level of safety as methods that heavily rely on human labor, with less than 10\\% of the computational resources. The training recipes and models will be released. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "D042vFwJAM@OpenReview",
      "index": 30,
      "title": "Physics-aligned field reconstruction with diffusion bridge",
      "authors": [
        "Zeyu Li",
        "Hongkun Dou",
        "Shen Fang",
        "Wang Han",
        "Yue Deng",
        "Lijun Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "palsb",
        "physical",
        "reconstruction",
        "bridge",
        "aligned",
        "diffusion",
        "constraints",
        "boundary",
        "field",
        "physics"
      ],
      "summary": "The reconstruction of physical fields from sparse measurements is pivotal in both scientific research and engineering applications. Traditional methods are increasingly supplemented by deep learning models due to their efficacy in extracting features from data. However, except for the low accuracy on complex physical systems, these models often fail to comply with essential physical constraints, such as governing equations and boundary conditions. To overcome this limitation, we introduce a novel data-driven field reconstruction framework, termed the Physics-aligned Schrödinger Bridge (PalSB). This framework leverages a diffusion bridge mechanism that is specifically tailored to align with physical constraints. The PalSB approach incorporates a dual-stage training process designed to address both local reconstruction mapping and global physical principles. Additionally, a boundary-aware sampling technique is implemented to ensure adherence to physical boundary conditions. We demonstrate the effectiveness of PalSB through its application to three complex nonlinear systems: cylinder flow from Particle Image Velocimetry experiments, two-dimensional turbulence, and a reaction-diffusion system. The results reveal that PalSB not only achieves higher accuracy but also exhibits enhanced compliance with physical constraints compared to existing methods. This highlights PalSB's capability to generate high-quality representations of intricate physical interactions, showcasing its potential for advancing field reconstruction techniques. The source code can be found at https://github.com/lzy12301/PalSB.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=D042vFwJAM"
        ],
        "venue": [
          "/venue/D042vFwJAM@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=D042vFwJAM"
        ],
        "detail": [
          "https://openreview.net/forum?id=D042vFwJAM"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 5
      },
      "raw_excerpt": "Physics-aligned field reconstruction with diffusion bridge [PDF 6 ] [Copy] [Kimi 5 ] [REL] Authors : Zeyu Li , Hongkun Dou , Shen Fang , Wang Han , Yue Deng , Lijun Yang The reconstruction of physical fields from sparse measurements is pivotal in both scientific research and engineering applications. Traditional methods are increasingly supplemented by deep learning models due to their efficacy in extracting features from data. However, except for the low accuracy on complex physical systems, these models often fail to comply with essential physical constraints, such as governing equations and boundary conditions. To overcome this limitation, we introduce a novel data-driven field reconstruction framework, termed the Physics-aligned Schrödinger Bridge (PalSB). This framework leverages a diffusion bridge mechanism that is specifically tailored to align with physical constraints. The PalSB approach incorporates a dual-stage training process designed to address both local reconstruction mapping and global physical principles. Additionally, a boundary-aware sampling technique is implemented to ensure adherence to physical boundary conditions. We demonstrate the effectiveness of PalSB through its application to three complex nonlinear systems: cylinder flow from Particle Image Velocimetry experiments, two-dimensional turbulence, and a reaction-diffusion system. The results reveal that PalSB not only achieves higher accuracy but also exhibits enhanced compliance with physical constraints compared to existing methods. This highlights PalSB's capability to generate high-quality representations of intricate physical interactions, showcasing its potential for advancing field reconstruction techniques. The source code can be found at https://github.com/lzy12301/PalSB. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9WYMDgxDac@OpenReview",
      "index": 31,
      "title": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models",
      "authors": [
        "Qingni Wang",
        "Tiantian Geng",
        "Zhiyuan Wang",
        "Teng Wang",
        "Bo Fu",
        "Feng Zheng"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "tron",
        "ended",
        "risk",
        "mllms",
        "prediction",
        "sets",
        "multimodal",
        "levels",
        "language",
        "isk"
      ],
      "summary": "Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce *TRON*, a **t**wo-step framework for **r**isk c**o**ntrol and assessme**n**t, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. *TRON* comprises two main components: (1) a novel conformal score to **sample** response sets of minimum size, and (2) a nonconformity score to **identify** high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that *TRON* achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9WYMDgxDac"
        ],
        "venue": [
          "/venue/9WYMDgxDac@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9WYMDgxDac"
        ],
        "detail": [
          "https://openreview.net/forum?id=9WYMDgxDac"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 12
      },
      "raw_excerpt": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models [PDF 10 ] [Copy] [Kimi 12 ] [REL] Authors : Qingni Wang , Tiantian Geng , Zhiyuan Wang , Teng Wang , Bo Fu , Feng Zheng Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce *TRON*, a **t**wo-step framework for **r**isk c**o**ntrol and assessme**n**t, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. *TRON* comprises two main components: (1) a novel conformal score to **sample** response sets of minimum size, and (2) a nonconformity score to **identify** high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that *TRON* achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "xGs7Ch3Vyo@OpenReview",
      "index": 32,
      "title": "Better autoregressive regression with LLMs",
      "authors": [
        "Michal Lukasik",
        "Zhao Meng",
        "Harikrishna Narasimhan",
        "Aditya Krishna Menon",
        "Yin-Wen Chang",
        "Felix Yu",
        "Sanjiv Kumar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "regression",
        "llms",
        "raft",
        "autoregressive",
        "language",
        "fine",
        "perplexity",
        "loss",
        "finetuning",
        "principled"
      ],
      "summary": "Large language models (LLMs) have proven successful on many machine learning tasks,including those that do not involve language generation. In specific, LLMs have been shown to be effective in solving regression, where the targets are real-numbers.One common approach is to fine tune the LLM based on the log-perplexity loss and use autoregressive sampling at the inference time. Another approach relies on adding a predictive head and finetuning it with a suitable loss. Despite the success, there has not been a study on the principled ways of using decoder LLMs for regression. In this work we compare different prior works under a unified view, and introduce RAFT, regression-aware fine-tuning, a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xGs7Ch3Vyo"
        ],
        "venue": [
          "/venue/xGs7Ch3Vyo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xGs7Ch3Vyo"
        ],
        "detail": [
          "https://openreview.net/forum?id=xGs7Ch3Vyo"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 19
      },
      "raw_excerpt": "Better autoregressive regression with LLMs [PDF 19 ] [Copy] [Kimi 19 ] [REL] Authors : Michal Lukasik , Zhao Meng , Harikrishna Narasimhan , Aditya Krishna Menon , Yin-Wen Chang , Felix Yu , Sanjiv Kumar Large language models (LLMs) have proven successful on many machine learning tasks,including those that do not involve language generation. In specific, LLMs have been shown to be effective in solving regression, where the targets are real-numbers.One common approach is to fine tune the LLM based on the log-perplexity loss and use autoregressive sampling at the inference time. Another approach relies on adding a predictive head and finetuning it with a suitable loss. Despite the success, there has not been a study on the principled ways of using decoder LLMs for regression. In this work we compare different prior works under a unified view, and introduce RAFT, regression-aware fine-tuning, a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "VeMC6Bn0ZB@OpenReview",
      "index": 33,
      "title": "Learning to Solve Differential Equation Constrained Optimization Problems",
      "authors": [
        "Vincenzo Di Vito Francesco",
        "Mostafa Mohammadian",
        "Kyri Baker",
        "Ferdinando Fioretto"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "optimization",
        "differential",
        "citep",
        "constrained",
        "strategies",
        "finance",
        "equations",
        "constraints",
        "dynamic",
        "problems"
      ],
      "summary": "Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or stochastic differential equations. Despite its significance, the computational challenges associated with these problems have limited their practical use. To address these limitations, this paper introduces a learning-based approach to DE-constrained optimization that combines techniques from proxy optimization \\citep{kotary2021end} and neural differential equations \\citep{chen2019neural}. The proposed approach uses a dual-network architecture, with one approximating the control strategies, focusing on steady-state constraints, and another solving the associated DEs. This combination enables the approximation of optimal strategies while accounting for dynamic constraints in near real-time.Experiments across problems in energy optimization and finance modeling show that this method provides full compliance with dynamic constraints and it produces results up to 25 times more precise than other methods which do not explicitly model the system's dynamic equations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VeMC6Bn0ZB"
        ],
        "venue": [
          "/venue/VeMC6Bn0ZB@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VeMC6Bn0ZB"
        ],
        "detail": [
          "https://openreview.net/forum?id=VeMC6Bn0ZB"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Learning to Solve Differential Equation Constrained Optimization Problems [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Vincenzo Di Vito Francesco , Mostafa Mohammadian , Kyri Baker , Ferdinando Fioretto Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or stochastic differential equations. Despite its significance, the computational challenges associated with these problems have limited their practical use. To address these limitations, this paper introduces a learning-based approach to DE-constrained optimization that combines techniques from proxy optimization \\citep{kotary2021end} and neural differential equations \\citep{chen2019neural}. The proposed approach uses a dual-network architecture, with one approximating the control strategies, focusing on steady-state constraints, and another solving the associated DEs. This combination enables the approximation of optimal strategies while accounting for dynamic constraints in near real-time.Experiments across problems in energy optimization and finance modeling show that this method provides full compliance with dynamic constraints and it produces results up to 25 times more precise than other methods which do not explicitly model the system's dynamic equations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ofuLWn8DFZ@OpenReview",
      "index": 34,
      "title": "Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning",
      "authors": [
        "Yan Scholten",
        "Stephan Günnemann"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "poisoning",
        "prediction",
        "sets",
        "reliability",
        "conformal",
        "calibration",
        "reliable",
        "data",
        "quantification",
        "training"
      ],
      "summary": "Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ofuLWn8DFZ"
        ],
        "venue": [
          "/venue/ofuLWn8DFZ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ofuLWn8DFZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=ofuLWn8DFZ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 5
      },
      "raw_excerpt": "Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning [PDF 3 ] [Copy] [Kimi 5 ] [REL] Authors : Yan Scholten , Stephan Günnemann Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "S85PP4xjFD@OpenReview",
      "index": 35,
      "title": "ContraFusion: Contrastively Improving Compositional Understanding in Diffusion Models via Fine-Grained Negative Images",
      "authors": [
        "Xu Han",
        "Linghao Jin",
        "Xiaofeng Liu",
        "Paul Pu Liang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "contrastive",
        "compositional",
        "contrafusion",
        "diffusion",
        "images",
        "t2i",
        "contrastively",
        "models",
        "complex",
        "pairs"
      ],
      "summary": "Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges through optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, COM-DIFF, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose CONTRAFUSION, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks. We will release our contrastive dataset to support the development of generative models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=S85PP4xjFD"
        ],
        "venue": [
          "/venue/S85PP4xjFD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=S85PP4xjFD"
        ],
        "detail": [
          "https://openreview.net/forum?id=S85PP4xjFD"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 7
      },
      "raw_excerpt": "ContraFusion: Contrastively Improving Compositional Understanding in Diffusion Models via Fine-Grained Negative Images [PDF 18 ] [Copy] [Kimi 7 ] [REL] Authors : Xu Han , Linghao Jin , Xiaofeng Liu , Paul Pu Liang Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges through optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, COM-DIFF, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose CONTRAFUSION, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks. We will release our contrastive dataset to support the development of generative models. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "scI9307PLG@OpenReview",
      "index": 36,
      "title": "Bundle Neural Network for message diffusion on graphs",
      "authors": [
        "Jacob Bamberger",
        "Federico Barbero",
        "Xiaowen Dong",
        "Michael Bronstein"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "bunns",
        "message",
        "squashing",
        "diffusion",
        "expressivity",
        "bundle",
        "graphs",
        "bunn",
        "neural",
        "passing"
      ],
      "summary": "The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates via *message diffusion* on *flat vector bundles* — geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the first positive uniform expressivity result of its kind. We support our claims with formal analysis and synthetic experiments. Empirically, BuNNs perform strongly on heterophilic and long-range tasks, which demonstrates their robustness on a diverse range of challenging real-world tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=scI9307PLG"
        ],
        "venue": [
          "/venue/scI9307PLG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=scI9307PLG"
        ],
        "detail": [
          "https://openreview.net/forum?id=scI9307PLG"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 5
      },
      "raw_excerpt": "Bundle Neural Network for message diffusion on graphs [PDF 11 ] [Copy] [Kimi 5 ] [REL] Authors : Jacob Bamberger , Federico Barbero , Xiaowen Dong , Michael Bronstein The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates via *message diffusion* on *flat vector bundles* — geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the first positive uniform expressivity result of its kind. We support our claims with formal analysis and synthetic experiments. Empirically, BuNNs perform strongly on heterophilic and long-range tasks, which demonstrates their robustness on a diverse range of challenging real-world tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "sKYHBTAxVa@OpenReview",
      "index": 37,
      "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
      "authors": [
        "Colin White",
        "Samuel Dooley",
        "Manley Roberts",
        "Arka Pal",
        "Benjamin Feuer",
        "Siddhartha Jain",
        "Ravid Shwartz-Ziv",
        "Neel Jain",
        "Khalid Saifullah",
        "Sreemanti Dey",
        "Shubh-Agrawal",
        "Sandeep Sandha",
        "Siddartha Naidu",
        "Chinmay Hegde",
        "Yann LeCun",
        "Tom Goldstein",
        "Willie Neiswanger",
        "Micah Goldblum"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "livebench",
        "contamination",
        "llm",
        "questions",
        "benchmark",
        "contains",
        "tasks",
        "release",
        "benchmarks",
        "harder"
      ],
      "summary": "Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sKYHBTAxVa"
        ],
        "venue": [
          "/venue/sKYHBTAxVa@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sKYHBTAxVa"
        ],
        "detail": [
          "https://openreview.net/forum?id=sKYHBTAxVa"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 10
      },
      "raw_excerpt": "LiveBench: A Challenging, Contamination-Free LLM Benchmark [PDF 9 ] [Copy] [Kimi 10 ] [REL] Authors : Colin White , Samuel Dooley , Manley Roberts , Arka Pal , Benjamin Feuer , Siddhartha Jain , Ravid Shwartz-Ziv , Neel Jain , Khalid Saifullah , Sreemanti Dey , Shubh-Agrawal , Sandeep Sandha , Siddartha Naidu , Chinmay Hegde , Yann LeCun , Tom Goldstein , Willie Neiswanger , Micah Goldblum Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "JAMxRSXLFz@OpenReview",
      "index": 38,
      "title": "Active Task Disambiguation with LLMs",
      "authors": [
        "Katarzyna Kobalczyk",
        "Nicolás Astorga",
        "Tennison Liu",
        "Mihaela van der Schaar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "disambiguation",
        "task",
        "questions",
        "llm",
        "llms",
        "clarifying",
        "agents",
        "reasoning",
        "viable",
        "ambiguously"
      ],
      "summary": "Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems—frequent in real-world interactions—remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JAMxRSXLFz"
        ],
        "venue": [
          "/venue/JAMxRSXLFz@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JAMxRSXLFz"
        ],
        "detail": [
          "https://openreview.net/forum?id=JAMxRSXLFz"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 16
      },
      "raw_excerpt": "Active Task Disambiguation with LLMs [PDF 7 ] [Copy] [Kimi 16 ] [REL] Authors : Katarzyna Kobalczyk , Nicolás Astorga , Tennison Liu , Mihaela van der Schaar Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems—frequent in real-world interactions—remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "FxNNiUgtfa@OpenReview",
      "index": 39,
      "title": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws",
      "authors": [
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "knowledge",
        "language",
        "capacity",
        "laws",
        "emph",
        "bits",
        "scaling",
        "int8",
        "washington",
        "moe"
      ],
      "summary": "Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \\emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \\emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications. More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FxNNiUgtfa"
        ],
        "venue": [
          "/venue/FxNNiUgtfa@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FxNNiUgtfa"
        ],
        "detail": [
          "https://openreview.net/forum?id=FxNNiUgtfa"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 9
      },
      "raw_excerpt": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws [PDF 16 ] [Copy] [Kimi 9 ] [REL] Authors : Zeyuan Allen-Zhu , Yuanzhi Li Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \\emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \\emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications. More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ykuc5q381b@OpenReview",
      "index": 40,
      "title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval",
      "authors": [
        "Hongjin SU",
        "Howard Yen",
        "Mengzhou Xia",
        "Weijia Shi",
        "Niklas Muennighoff",
        "Han-yu Wang",
        "Liu Haisu",
        "Quan Shi",
        "Zachary Siegel",
        "Michael Tang",
        "Ruoxi Sun",
        "Jinsung Yoon",
        "Sercan Arik",
        "Danqi Chen",
        "Tao Yu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "retrieval",
        "bright",
        "queries",
        "reasoning",
        "documents",
        "ndcg",
        "intensive",
        "challenging",
        "benchmark",
        "muennighoff"
      ],
      "summary": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,398 real-world queries spanning diverse domains such as economics, psychology, mathematics, coding, and more. These queries are drawn from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of 59.0 nDCG@10,1 produces a score of nDCG@10 of 18.0 on BRIGHT. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question answering performance by over 6.6 points. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ykuc5q381b"
        ],
        "venue": [
          "/venue/ykuc5q381b@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ykuc5q381b"
        ],
        "detail": [
          "https://openreview.net/forum?id=ykuc5q381b"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 10
      },
      "raw_excerpt": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval [PDF 7 ] [Copy] [Kimi 10 ] [REL] Authors : Hongjin SU , Howard Yen , Mengzhou Xia , Weijia Shi , Niklas Muennighoff , Han-yu Wang , Liu Haisu , Quan Shi , Zachary Siegel , Michael Tang , Ruoxi Sun , Jinsung Yoon , Sercan Arik , Danqi Chen , Tao Yu Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,398 real-world queries spanning diverse domains such as economics, psychology, mathematics, coding, and more. These queries are drawn from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of 59.0 nDCG@10,1 produces a score of nDCG@10 of 18.0 on BRIGHT. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question answering performance by over 6.6 points. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "yVQcr4qjD6@OpenReview",
      "index": 41,
      "title": "Robust Function-Calling for On-Device Language Model via Function Masking",
      "authors": [
        "Lin Qiqiang",
        "Muning Wen",
        "Qiuying Peng",
        "Guanyu Nie",
        "Junwei Liao",
        "Jun Wang",
        "Xiaoyun Mo",
        "Jiamu Zhou",
        "Cheng Cheng",
        "Yin Zhao",
        "Weinan Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "calling",
        "hammer",
        "function",
        "masking",
        "device",
        "models",
        "robust",
        "language",
        "benchmarks",
        "fitting"
      ],
      "summary": "Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to over-fitting to specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models’ sensitivity to irrelevant functions and incorporates function masking techniques to minimize over-fitting. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yVQcr4qjD6"
        ],
        "venue": [
          "/venue/yVQcr4qjD6@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yVQcr4qjD6"
        ],
        "detail": [
          "https://openreview.net/forum?id=yVQcr4qjD6"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 11
      },
      "raw_excerpt": "Robust Function-Calling for On-Device Language Model via Function Masking [PDF 9 ] [Copy] [Kimi 11 ] [REL] Authors : Lin Qiqiang , Muning Wen , Qiuying Peng , Guanyu Nie , Junwei Liao , Jun Wang , Xiaoyun Mo , Jiamu Zhou , Cheng Cheng , Yin Zhao , Jun Wang , Weinan Zhang Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to over-fitting to specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models’ sensitivity to irrelevant functions and incorporates function masking techniques to minimize over-fitting. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wmV4cIbgl6@OpenReview",
      "index": 42,
      "title": "CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series",
      "authors": [
        "Gideon Stein",
        "Maha Shadaydeh",
        "Jan Blunk",
        "Niklas Penzel",
        "Joachim Denzler"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "causal",
        "causalrivers",
        "discovery",
        "bavaria",
        "benchmarking",
        "series",
        "kit",
        "river",
        "eastern",
        "areas"
      ],
      "summary": "Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it.Despite this, in-the-wild evaluation is still lacking, as works frequently rely on synthetic data evaluation and sparse real-world examples under critical theoretical assumptions. Real-world causal structures, however, are often complex, evolving over time, non-linear, and influenced by unobserved factors, makingit hard for practitioners to select appropriate methods. To bridge this gap, we introduce CausalRivers, the largest in-the-wild causal discovery benchmarking kit for time series data to date.CausalRivers features an extensive dataset on river discharge that covers the complete eastern German territory (666 measurement stations) and the state of Bavaria (494 measurement stations). It spans the years 2019 to 2023 with a 15-minute temporal resolution. Further, we provide data from a recent flood around the Elbe River, as an event with a pronounced distributional shift. Leveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs (Bavaria and eastern Germany).These graphs can be sampled to generate thousands of subgraphs to benchmark causal discovery across diverse and challenging settings.To demonstrate the utility of our benchmarking kit, we evaluate several causal discovery approaches through multiple experiments and introduce effective baselines, identifying several areas for enhancement.CausalRivers has the potential to facilitate robust evaluations and comparisons of causal discovery methods.Besides this primary purpose, we also expect that this dataset will be relevant for connected areas of research, such as time series forecasting and anomaly detection.Based on this, we hope to establish benchmark-driven method development that fosters advanced techniques for causal discovery, as is the case for many other areas of machine learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wmV4cIbgl6"
        ],
        "venue": [
          "/venue/wmV4cIbgl6@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wmV4cIbgl6"
        ],
        "detail": [
          "https://openreview.net/forum?id=wmV4cIbgl6"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 7
      },
      "raw_excerpt": "CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series [PDF 12 ] [Copy] [Kimi 7 ] [REL] Authors : Gideon Stein , Maha Shadaydeh , Jan Blunk , Niklas Penzel , Joachim Denzler Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it.Despite this, in-the-wild evaluation is still lacking, as works frequently rely on synthetic data evaluation and sparse real-world examples under critical theoretical assumptions. Real-world causal structures, however, are often complex, evolving over time, non-linear, and influenced by unobserved factors, makingit hard for practitioners to select appropriate methods. To bridge this gap, we introduce CausalRivers, the largest in-the-wild causal discovery benchmarking kit for time series data to date.CausalRivers features an extensive dataset on river discharge that covers the complete eastern German territory (666 measurement stations) and the state of Bavaria (494 measurement stations). It spans the years 2019 to 2023 with a 15-minute temporal resolution. Further, we provide data from a recent flood around the Elbe River, as an event with a pronounced distributional shift. Leveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs (Bavaria and eastern Germany).These graphs can be sampled to generate thousands of subgraphs to benchmark causal discovery across diverse and challenging settings.To demonstrate the utility of our benchmarking kit, we evaluate several causal discovery approaches through multiple experiments and introduce effective baselines, identifying several areas for enhancement.CausalRivers has the potential to facilitate robust evaluations and comparisons of causal discovery methods.Besides this primary purpose, we also expect that this dataset will be relevant for connected areas of research, such as time series forecasting and anomaly detection.Based on this, we hope to establish benchmark-driven method development that fosters advanced techniques for causal discovery, as is the case for many other areas of machine learning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wkHcXDv7cv@OpenReview",
      "index": 43,
      "title": "Tuning Frequency Bias of State Space Models",
      "authors": [
        "Annan Yu",
        "Dongwei Lyu",
        "Soon Hoe Lim",
        "Michael W Mahoney",
        "N. Benjamin Erichson"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "bias",
        "frequency",
        "ssms",
        "lti",
        "initialization",
        "tuning",
        "tune",
        "inborn",
        "sequences",
        "mechanisms"
      ],
      "summary": "State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than high-frequency ones. This behavior aligns with the broader notion of frequency bias in deep learning model training. We show that the initialization of an SSM assigns it an innate frequency bias and that training the model in a conventional way does not alter this bias. Based on our theory, we propose two mechanisms to tune frequency bias: either by scaling the initialization to tune the inborn frequency bias; or by applying a Sobolev-norm-based filter to adjust the sensitivity of the gradients to high-frequency inputs, which allows us to change the frequency bias via training. Using an image-denoising task, we empirically show that we can strengthen, weaken, or even reverse the frequency bias using both mechanisms. By tuning the frequency bias, we can also improve SSMs' performance on learning long-range sequences, averaging an 88.26 88.26 accuracy on the Long-Range Arena (LRA) benchmark tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wkHcXDv7cv"
        ],
        "venue": [
          "/venue/wkHcXDv7cv@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wkHcXDv7cv"
        ],
        "detail": [
          "https://openreview.net/forum?id=wkHcXDv7cv"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 4
      },
      "raw_excerpt": "Tuning Frequency Bias of State Space Models [PDF 9 ] [Copy] [Kimi 4 ] [REL] Authors : Annan Yu , Dongwei Lyu , Soon Hoe Lim , Michael W Mahoney , N. Benjamin Erichson State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than high-frequency ones. This behavior aligns with the broader notion of frequency bias in deep learning model training. We show that the initialization of an SSM assigns it an innate frequency bias and that training the model in a conventional way does not alter this bias. Based on our theory, we propose two mechanisms to tune frequency bias: either by scaling the initialization to tune the inborn frequency bias; or by applying a Sobolev-norm-based filter to adjust the sensitivity of the gradients to high-frequency inputs, which allows us to change the frequency bias via training. Using an image-denoising task, we empirically show that we can strengthen, weaken, or even reverse the frequency bias using both mechanisms. By tuning the frequency bias, we can also improve SSMs' performance on learning long-range sequences, averaging an 88.26 88.26 accuracy on the Long-Range Arena (LRA) benchmark tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wg3rBImn3O@OpenReview",
      "index": 44,
      "title": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling",
      "authors": [
        "Christopher Musco",
        "R. Teal Witter"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "shap",
        "shapley",
        "leverage",
        "kernel",
        "provably",
        "value",
        "score",
        "agnostic",
        "lundberg",
        "accurate"
      ],
      "summary": "Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with n n features, O ( 2 n ) O ( 2 n ) model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing *Leverage SHAP*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just O ( n log n ) O ( n log ⁡ n ) model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing *leverage score sampling*, a powerful regression tool. Beyond theoretical guarantees, we show that Leverage SHAP consistently outperforms even the highly optimized implementation of Kernel SHAP available in the ubiquitous SHAP library [Lundberg \\& Lee, 2017].",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wg3rBImn3O"
        ],
        "venue": [
          "/venue/wg3rBImn3O@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wg3rBImn3O"
        ],
        "detail": [
          "https://openreview.net/forum?id=wg3rBImn3O"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Christopher Musco , R. Teal Witter Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with n n features, O ( 2 n ) O ( 2 n ) model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing *Leverage SHAP*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just O ( n log n ) O ( n log ⁡ n ) model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing *leverage score sampling*, a powerful regression tool. Beyond theoretical guarantees, we show that Leverage SHAP consistently outperforms even the highly optimized implementation of Kernel SHAP available in the ubiquitous SHAP library [Lundberg \\& Lee, 2017]. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wN3KaUXA5X@OpenReview",
      "index": 45,
      "title": "Diffusion On Syntax Trees For Program Synthesis",
      "authors": [
        "Shreyas Kapur",
        "Erik Jenner",
        "Stuart Russell"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "syntax",
        "graphics",
        "programs",
        "trees",
        "diffusion",
        "edit",
        "write",
        "program",
        "anon",
        "inverts"
      ],
      "summary": "Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts \"noise\" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://td-anon.github.io.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wN3KaUXA5X"
        ],
        "venue": [
          "/venue/wN3KaUXA5X@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wN3KaUXA5X"
        ],
        "detail": [
          "https://openreview.net/forum?id=wN3KaUXA5X"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "Diffusion On Syntax Trees For Program Synthesis [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Shreyas Kapur , Erik Jenner , Stuart Russell Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts \"noise\" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://td-anon.github.io. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wFD16gwpze@OpenReview",
      "index": 46,
      "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
      "authors": [
        "Roman Worschech",
        "Bernd Rosenow"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "law",
        "covariance",
        "laws",
        "power",
        "scaling",
        "neural",
        "data",
        "spectra",
        "teacher",
        "student"
      ],
      "summary": "Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra.For linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wFD16gwpze"
        ],
        "venue": [
          "/venue/wFD16gwpze@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wFD16gwpze"
        ],
        "detail": [
          "https://openreview.net/forum?id=wFD16gwpze"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Roman Worschech , Bernd Rosenow Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra.For linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "vVCHWVBsLH@OpenReview",
      "index": 47,
      "title": "Decomposition Polyhedra of Piecewise Linear Functions",
      "authors": [
        "Marie-Charlotte Brandenburg",
        "Moritz Grillo",
        "Christoph Hertrich"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cpwl",
        "decompositions",
        "polyhedron",
        "piecewise",
        "functions",
        "decomposition",
        "polyhedra",
        "minimal",
        "linear",
        "convex"
      ],
      "summary": "In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network theory, it is crucial to find decompositions with as few linear pieces as possible. This is a highly challenging problem, as we further demonstrate by disproving a recently proposed approach by Tran and Wang [Minimal representations of tropical rational functions. Algebraic Statistics, 15(1):27–59, 2024]. To make the problem more tractable, we propose to fix an underlying polyhedral complex determining the possible locus of nonlinearity. Under this assumption, we prove that the set of decompositions forms a polyhedron that arises as intersection of two translated cones. We prove that irreducible decompositions correspond to the bounded faces of this polyhedron and minimal solutions must be vertices. We then identify cases with a unique minimal decomposition, and illustrate how our insights have consequences in the theory of submodular functions. Finally, we improve upon previous constructions of neural networks for a given convex CPWL function and apply our framework to obtain results in the nonconvex case.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vVCHWVBsLH"
        ],
        "venue": [
          "/venue/vVCHWVBsLH@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vVCHWVBsLH"
        ],
        "detail": [
          "https://openreview.net/forum?id=vVCHWVBsLH"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Decomposition Polyhedra of Piecewise Linear Functions [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Marie-Charlotte Brandenburg , Moritz Grillo , Christoph Hertrich In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network theory, it is crucial to find decompositions with as few linear pieces as possible. This is a highly challenging problem, as we further demonstrate by disproving a recently proposed approach by Tran and Wang [Minimal representations of tropical rational functions. Algebraic Statistics, 15(1):27–59, 2024]. To make the problem more tractable, we propose to fix an underlying polyhedral complex determining the possible locus of nonlinearity. Under this assumption, we prove that the set of decompositions forms a polyhedron that arises as intersection of two translated cones. We prove that irreducible decompositions correspond to the bounded faces of this polyhedron and minimal solutions must be vertices. We then identify cases with a unique minimal decomposition, and illustrate how our insights have consequences in the theory of submodular functions. Finally, we improve upon previous constructions of neural networks for a given convex CPWL function and apply our framework to obtain results in the nonconvex case. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "v9EjwMM55Y@OpenReview",
      "index": 48,
      "title": "UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery",
      "authors": [
        "Ruifeng Li",
        "Mingqian Li",
        "Wei Liu",
        "Yuhua Zhou",
        "Xiangxin Zhou",
        "Yuan Yao",
        "Qiang Zhang",
        "Hongyang Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "unimatch",
        "matching",
        "molecular",
        "meta",
        "task",
        "drug",
        "hierarchical",
        "molnet",
        "universal",
        "shot"
      ],
      "summary": "Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels—atoms, substructures, and molecules—via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in ∆AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=v9EjwMM55Y"
        ],
        "venue": [
          "/venue/v9EjwMM55Y@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=v9EjwMM55Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=v9EjwMM55Y"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 7
      },
      "raw_excerpt": "UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery [PDF 10 ] [Copy] [Kimi 7 ] [REL] Authors : Ruifeng Li , Mingqian Li , Wei Liu , Yuhua Zhou , Xiangxin Zhou , Yuan Yao , Qiang Zhang , Hongyang Chen Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels—atoms, substructures, and molecules—via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in ∆AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "twEvvkQqPS@OpenReview",
      "index": 49,
      "title": "Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems",
      "authors": [
        "Yunyang Li",
        "Zaishuo Xia",
        "Lin Huang",
        "Xinran Wei",
        "Samuel Harshe",
        "Han Yang",
        "Erpai Luo",
        "Zun Wang",
        "Jia Zhang",
        "Chang Liu",
        "Bin Shao",
        "Mark Gerstein"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "waloss",
        "sham",
        "kohn",
        "dft",
        "molecular",
        "scf",
        "scalability",
        "hamiltonians",
        "hamiltonian",
        "physical"
      ],
      "summary": "Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep-learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the scalability of prior models may be problematic when applied to large molecules, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training set (PubChemQH) than used previously and use it to create a scalable model for DFT calculations with physical accuracy. For our model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing models to make better predictions for molecular orbitals and total energies than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) DFT calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18\\%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=twEvvkQqPS"
        ],
        "venue": [
          "/venue/twEvvkQqPS@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=twEvvkQqPS"
        ],
        "detail": [
          "https://openreview.net/forum?id=twEvvkQqPS"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Yunyang Li , Zaishuo Xia , Lin Huang , Xinran Wei , Samuel Harshe , Han Yang , Erpai Luo , Zun Wang , Jia Zhang , Chang Liu , Bin Shao , Mark Gerstein Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep-learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the scalability of prior models may be problematic when applied to large molecules, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training set (PubChemQH) than used previously and use it to create a scalable model for DFT calculations with physical accuracy. For our model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing models to make better predictions for molecular orbitals and total energies than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) DFT calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18\\%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "sZQRUrvLn4@OpenReview",
      "index": 50,
      "title": "Graph Neural Networks Can (Often) Count Substructures",
      "authors": [
        "Paolo Pellizzoni",
        "Till Schulz",
        "Karsten Borgwardt"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gnns",
        "count",
        "graph",
        "subgraph",
        "subgraphs",
        "substructures",
        "message",
        "passing",
        "isomorphism",
        "able"
      ],
      "summary": "Message passing graph neural networks (GNNs) are known to have limited expressive power in their ability to distinguish some non-isomorphic graphs.Because of this, it is well known that they are unable to detect or count arbitrary graph substructures (i.e., solving the subgraph isomorphism problem), a task that is of great importance for several types of graph-structured data. However, we observe that GNNs are in fact able to count graph patterns quite accurately across several real-world graph datasets.Motivated by this observation, we provide an analysis of the subgraph-counting capabilities of GNNs beyond the worst case, deriving several sufficient conditions for GNNs to be able to count subgraphs and, more importantly, to be able to sample-efficiently learn to count subgraphs. Moreover, we develop novel dynamic programming algorithms for solving the subgraph isomorphism problem on restricted classes of pattern and target graphs, and show that message-passing GNNs can efficiently simulate these dynamic programs. Finally, we empirically validate that our sufficient conditions for GNNs to count subgraphs hold on many real-world datasets, providing a theoretically-grounded explanation to our motivating observations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sZQRUrvLn4"
        ],
        "venue": [
          "/venue/sZQRUrvLn4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sZQRUrvLn4"
        ],
        "detail": [
          "https://openreview.net/forum?id=sZQRUrvLn4"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 2
      },
      "raw_excerpt": "Graph Neural Networks Can (Often) Count Substructures [PDF 12 ] [Copy] [Kimi 2 ] [REL] Authors : Paolo Pellizzoni , Till Schulz , Karsten Borgwardt Message passing graph neural networks (GNNs) are known to have limited expressive power in their ability to distinguish some non-isomorphic graphs.Because of this, it is well known that they are unable to detect or count arbitrary graph substructures (i.e., solving the subgraph isomorphism problem), a task that is of great importance for several types of graph-structured data. However, we observe that GNNs are in fact able to count graph patterns quite accurately across several real-world graph datasets.Motivated by this observation, we provide an analysis of the subgraph-counting capabilities of GNNs beyond the worst case, deriving several sufficient conditions for GNNs to be able to count subgraphs and, more importantly, to be able to sample-efficiently learn to count subgraphs. Moreover, we develop novel dynamic programming algorithms for solving the subgraph isomorphism problem on restricted classes of pattern and target graphs, and show that message-passing GNNs can efficiently simulate these dynamic programs. Finally, we empirically validate that our sufficient conditions for GNNs to count subgraphs hold on many real-world datasets, providing a theoretically-grounded explanation to our motivating observations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qxRoo7ULCo@OpenReview",
      "index": 51,
      "title": "4K4DGen: Panoramic 4D Generation at 4K Resolution",
      "authors": [
        "Renjie Li",
        "Panwang Pan",
        "Bangbang Yang",
        "Dejia Xu",
        "Shijie Zhou",
        "zhang xuanyang",
        "Zeming Li",
        "Achuta Kadambi",
        "Zhangyang Wang",
        "Zhengzhong Tu",
        "Zhiwen Fan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "panoramic",
        "immersive",
        "dynamic",
        "360",
        "4k4dgen",
        "circ",
        "panorama",
        "resolution",
        "textbf",
        "lifting"
      ],
      "summary": "The blooming of virtual reality and augmented reality (VR/AR) technologies has driven an increasing demand for the creation of high-quality, immersive, and dynamic environments. However, existing generative techniques either focus solely on dynamic objects or perform outpainting from a single perspective image, failing to meet the requirements of VR/AR applications that need free-viewpoint, 360 ∘ ∘ virtual views where users can move in all directions. In this work, we tackle the challenging task of elevating a single panorama to an immersive 4D experience. For the first time, we demonstrate the capability to generate omnidirectional dynamic scenes with 360 ∘ ∘ views at 4K (4096 × × 2048) resolution, thereby providing an immersive user experience. Our method introduces a pipeline that facilitates natural scene animations and optimizes a set of dynamic Gaussians using efficient splatting techniques for real-time exploration. To overcome the lack of scene-scale annotated 4D data and models, especially in panoramic formats, we propose a novel \\textbf{Panoramic Denoiser} that adapts generic 2D diffusion priors to animate consistently in 360 ∘ ∘ images, transforming them into panoramic videos with dynamic scenes at targeted regions. Subsequently, we propose \\textbf{Dynamic Panoramic Lifting} to elevate the panoramic video into a 4D immersive environment while preserving spatial and temporal consistency. By transferring prior knowledge from 2D models in the perspective domain to the panoramic domain and the 4D lifting with spatial appearance and geometry regularization, we achieve high-quality Panorama-to-4D generation at a resolution of 4K for the first time.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qxRoo7ULCo"
        ],
        "venue": [
          "/venue/qxRoo7ULCo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qxRoo7ULCo"
        ],
        "detail": [
          "https://openreview.net/forum?id=qxRoo7ULCo"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "4K4DGen: Panoramic 4D Generation at 4K Resolution [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Renjie Li , Panwang Pan , Bangbang Yang , Dejia Xu , Shijie Zhou , zhang xuanyang , Zeming Li , Achuta Kadambi , Zhangyang Wang , Zhengzhong Tu , Zhiwen Fan The blooming of virtual reality and augmented reality (VR/AR) technologies has driven an increasing demand for the creation of high-quality, immersive, and dynamic environments. However, existing generative techniques either focus solely on dynamic objects or perform outpainting from a single perspective image, failing to meet the requirements of VR/AR applications that need free-viewpoint, 360 ∘ ∘ virtual views where users can move in all directions. In this work, we tackle the challenging task of elevating a single panorama to an immersive 4D experience. For the first time, we demonstrate the capability to generate omnidirectional dynamic scenes with 360 ∘ ∘ views at 4K (4096 × × 2048) resolution, thereby providing an immersive user experience. Our method introduces a pipeline that facilitates natural scene animations and optimizes a set of dynamic Gaussians using efficient splatting techniques for real-time exploration. To overcome the lack of scene-scale annotated 4D data and models, especially in panoramic formats, we propose a novel \\textbf{Panoramic Denoiser} that adapts generic 2D diffusion priors to animate consistently in 360 ∘ ∘ images, transforming them into panoramic videos with dynamic scenes at targeted regions. Subsequently, we propose \\textbf{Dynamic Panoramic Lifting} to elevate the panoramic video into a 4D immersive environment while preserving spatial and temporal consistency. By transferring prior knowledge from 2D models in the perspective domain to the panoramic domain and the 4D lifting with spatial appearance and geometry regularization, we achieve high-quality Panorama-to-4D generation at a resolution of 4K for the first time. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qPx3i9sMxv@OpenReview",
      "index": 52,
      "title": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation",
      "authors": [
        "Peiwen Sun",
        "Sitong Cheng",
        "Xiangtai Li",
        "Zhen Ye",
        "Huadai Liu",
        "Honggang Zhang",
        "Wei Xue",
        "Yike Guo"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "audio",
        "spatial",
        "generation",
        "soundscapes",
        "stereo",
        "guidance",
        "immersive",
        "ears",
        "bewo",
        "spatialsonic"
      ],
      "summary": "Recently, diffusion models have achieved great success in mono-channel audio generation.However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions.Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models. To the best of our knowledge, this work represents the first attempt to address these issues.We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources.Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation. Existing audio generation models tend to generate rather random spatial audio. To provide accurate guidance for Latent Diffusion Models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance. By leveraging spatial guidance, our unified model not only achieves the objective of generating immersive and controllable spatial audio from text and image but also enables interactive audio generation during inference.Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods. The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules.Our demos are available at https://immersive-audio.github.io/. Our code, model, and dataset will be released soon.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qPx3i9sMxv"
        ],
        "venue": [
          "/venue/qPx3i9sMxv@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qPx3i9sMxv"
        ],
        "detail": [
          "https://openreview.net/forum?id=qPx3i9sMxv"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Peiwen Sun , Sitong Cheng , Xiangtai Li , Zhen Ye , Huadai Liu , Honggang Zhang , Wei Xue , Yike Guo Recently, diffusion models have achieved great success in mono-channel audio generation.However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions.Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models. To the best of our knowledge, this work represents the first attempt to address these issues.We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources.Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation. Existing audio generation models tend to generate rather random spatial audio. To provide accurate guidance for Latent Diffusion Models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance. By leveraging spatial guidance, our unified model not only achieves the objective of generating immersive and controllable spatial audio from text and image but also enables interactive audio generation during inference.Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods. The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules.Our demos are available at https://immersive-audio.github.io/. Our code, model, and dataset will be released soon. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "q3EbOXb4y1@OpenReview",
      "index": 53,
      "title": "Retri3D: 3D Neural Graphics Representation Retrieval",
      "authors": [
        "Yushi Guan",
        "Daniel Kwan",
        "Jean Dandurand",
        "Xi Yan",
        "Ruofan Liang",
        "Yuxuan Zhang",
        "Nilesh Jain",
        "Nilesh Ahuja",
        "Selvakumar Panneer",
        "Nandita Vijaykumar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "retri3d",
        "retrieval",
        "3dngrs",
        "graphics",
        "stores",
        "ngrs",
        "representation",
        "viewing",
        "neural",
        "ngr"
      ],
      "summary": "Learnable 3D Neural Graphics Representations (3DNGR) have emerged as promising 3D representations for reconstructing 3D scenes from 2D images. Numerous works, including Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and their variants, have significantly enhanced the quality of these representations. The ease of construction from 2D images, suitability for online viewing/sharing, and applications in game/art design downstream tasks make it a vital 3D representation, with potential creation of large numbers of such 3D models. This necessitates large data stores, local or online, to save 3D visual data in these formats. However, no existing framework enables accurate retrieval of stored 3DNGRs. In this work, we propose, Retri3D, a framework that enables accurate and efficient retrieval of 3D scenes represented as NGRs from large data stores using text queries. We introduce a novel Neural Field Artifact Analysis technique, combined with a Smart Camera Movement Module, to select clean views and navigate pre-trained 3DNGRs. These techniques enable accurate retrieval by selecting the best viewing directions in the 3D scene for high-quality visual feature embeddings. We demonstrate that Retri3D is compatible with any NGR representation. On the LERF and ScanNet++ datasets, we show significant improvement in retrieval accuracy compared to existing techniques, while being orders of magnitude faster and storage efficient.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=q3EbOXb4y1"
        ],
        "venue": [
          "/venue/q3EbOXb4y1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=q3EbOXb4y1"
        ],
        "detail": [
          "https://openreview.net/forum?id=q3EbOXb4y1"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "Retri3D: 3D Neural Graphics Representation Retrieval [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Yushi Guan , Daniel Kwan , Jean Dandurand , Xi Yan , Ruofan Liang , Yuxuan Zhang , Nilesh Jain , Nilesh Ahuja , Selvakumar Panneer , Nandita Vijaykumar Learnable 3D Neural Graphics Representations (3DNGR) have emerged as promising 3D representations for reconstructing 3D scenes from 2D images. Numerous works, including Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and their variants, have significantly enhanced the quality of these representations. The ease of construction from 2D images, suitability for online viewing/sharing, and applications in game/art design downstream tasks make it a vital 3D representation, with potential creation of large numbers of such 3D models. This necessitates large data stores, local or online, to save 3D visual data in these formats. However, no existing framework enables accurate retrieval of stored 3DNGRs. In this work, we propose, Retri3D, a framework that enables accurate and efficient retrieval of 3D scenes represented as NGRs from large data stores using text queries. We introduce a novel Neural Field Artifact Analysis technique, combined with a Smart Camera Movement Module, to select clean views and navigate pre-trained 3DNGRs. These techniques enable accurate retrieval by selecting the best viewing directions in the 3D scene for high-quality visual feature embeddings. We demonstrate that Retri3D is compatible with any NGR representation. On the LERF and ScanNet++ datasets, we show significant improvement in retrieval accuracy compared to existing techniques, while being orders of magnitude faster and storage efficient. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "p4cLtzk4oe@OpenReview",
      "index": 54,
      "title": "Exploring Local Memorization in Diffusion Models via Bright Ending Attention",
      "authors": [
        "Chen Chen",
        "Daochang Liu",
        "Mubarak Shah",
        "Chang Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "memorization",
        "ending",
        "diffusion",
        "attention",
        "memorized",
        "bright",
        "task",
        "patches",
        "models",
        "local"
      ],
      "summary": "In this paper, we identify and leverage a novel `bright ending' (BE) anomaly in diffusion models prone to memorizing training images to address a new task: locating localized memorization regions within these models. BE refers to a distinct cross-attention pattern observed in text-to-image generations using diffusion models. Specifically, memorized image patches exhibit significantly greater attention to the end token during the final inference step compared to non-memorized patches. This attention map effectively highlights regions where the generated image replicates training data. Furthermore, driven by our observation that local memorization significantly underperforms in existing tasks of measuring, detecting, and mitigating memorization in diffusion models compared to global memorization, we propose a simple yet effective method to integrate BE and the results of the new localization task into these existing frameworks. This integration effectively improves their performances by narrowing the performance gap caused by local memorization. Our results not only demonstrate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=p4cLtzk4oe"
        ],
        "venue": [
          "/venue/p4cLtzk4oe@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=p4cLtzk4oe"
        ],
        "detail": [
          "https://openreview.net/forum?id=p4cLtzk4oe"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Exploring Local Memorization in Diffusion Models via Bright Ending Attention [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Chen Chen , Daochang Liu , Mubarak Shah , Chang Xu In this paper, we identify and leverage a novel `bright ending' (BE) anomaly in diffusion models prone to memorizing training images to address a new task: locating localized memorization regions within these models. BE refers to a distinct cross-attention pattern observed in text-to-image generations using diffusion models. Specifically, memorized image patches exhibit significantly greater attention to the end token during the final inference step compared to non-memorized patches. This attention map effectively highlights regions where the generated image replicates training data. Furthermore, driven by our observation that local memorization significantly underperforms in existing tasks of measuring, detecting, and mitigating memorization in diffusion models compared to global memorization, we propose a simple yet effective method to integrate BE and the results of the new localization task into these existing frameworks. This integration effectively improves their performances by narrowing the performance gap caused by local memorization. Our results not only demonstrate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "o1Et3MogPw@OpenReview",
      "index": 55,
      "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
      "authors": [
        "Weize Chen",
        "Ziming You",
        "Ran Li",
        "yitong guan",
        "Chen Qian",
        "Chenyang Zhao",
        "Cheng Yang",
        "Ruobing Xie",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "agents",
        "ioa",
        "internet",
        "agent",
        "frameworks",
        "weaving",
        "heterogeneous",
        "intelligence",
        "collaboration",
        "capable"
      ],
      "summary": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=o1Et3MogPw"
        ],
        "venue": [
          "/venue/o1Et3MogPw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=o1Et3MogPw"
        ],
        "detail": [
          "https://openreview.net/forum?id=o1Et3MogPw"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 19
      },
      "raw_excerpt": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence [PDF 13 ] [Copy] [Kimi 19 ] [REL] Authors : Weize Chen , Ziming You , Ran Li , yitong guan , Chen Qian , Chenyang Zhao , Cheng Yang , Ruobing Xie , Zhiyuan Liu , Maosong Sun The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "n9PDaFNi8t@OpenReview",
      "index": 56,
      "title": "OS-ATLAS: Foundation Action Model for Generalist GUI Agents",
      "authors": [
        "Zhiyong Wu",
        "Zhenyu Wu",
        "Fangzhi Xu",
        "Yian Wang",
        "Qiushi Sun",
        "Chengyou Jia",
        "Kanzhi Cheng",
        "Zichen Ding",
        "Liheng Chen",
        "Paul Pu Liang",
        "Yu Qiao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gui",
        "grounding",
        "atlas",
        "vlms",
        "agentic",
        "source",
        "foundation",
        "geminiprovision",
        "ood",
        "innovations"
      ],
      "summary": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas —a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested substantial engineering effort into developing a toolkit for synthesizing multi-platform GUI grounding data. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs. All our data, code, and models will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n9PDaFNi8t"
        ],
        "venue": [
          "/venue/n9PDaFNi8t@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n9PDaFNi8t"
        ],
        "detail": [
          "https://openreview.net/forum?id=n9PDaFNi8t"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 9
      },
      "raw_excerpt": "OS-ATLAS: Foundation Action Model for Generalist GUI Agents [PDF 11 ] [Copy] [Kimi 9 ] [REL] Authors : Zhiyong Wu , Zhenyu Wu , Fangzhi Xu , Yian Wang , Qiushi Sun , Chengyou Jia , Kanzhi Cheng , Zichen Ding , Liheng Chen , Paul Pu Liang , Yu Qiao Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas —a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested substantial engineering effort into developing a toolkit for synthesizing multi-platform GUI grounding data. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs. All our data, code, and models will be made publicly available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "n0OtGl6VGb@OpenReview",
      "index": 57,
      "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
      "authors": [
        "Yuhui Xu",
        "Zhanming Jie",
        "Hanze Dong",
        "Lei Wang",
        "Xudong Lu",
        "Aojun Zhou",
        "Amrita Saha",
        "Caiming Xiong",
        "Doyen Sahoo"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cache",
        "kivi",
        "think",
        "pruning",
        "memory",
        "thinner",
        "query",
        "eviction",
        "long",
        "inefficiencies"
      ],
      "summary": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences.This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights.In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve 2.8x peak memory reduction while maintaining nearly the same quality, enabling a batch size increase from 4x (with KIVI alone) to 5x when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of \\our, establishing a new baseline algorithm for efficient LLM deployment without compromising performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=n0OtGl6VGb"
        ],
        "venue": [
          "/venue/n0OtGl6VGb@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=n0OtGl6VGb"
        ],
        "detail": [
          "https://openreview.net/forum?id=n0OtGl6VGb"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 10
      },
      "raw_excerpt": "ThinK: Thinner Key Cache by Query-Driven Pruning [PDF 10 ] [Copy] [Kimi 10 ] [REL] Authors : Yuhui Xu , Zhanming Jie , Hanze Dong , Lei Wang , Xudong Lu , Aojun Zhou , Amrita Saha , Caiming Xiong , Doyen Sahoo Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences.This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights.In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve 2.8x peak memory reduction while maintaining nearly the same quality, enabling a batch size increase from 4x (with KIVI alone) to 5x when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of \\our, establishing a new baseline algorithm for efficient LLM deployment without compromising performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "m9RNBZewW2@OpenReview",
      "index": 58,
      "title": "Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model",
      "authors": [
        "Keda TAO",
        "Jinjin Gu",
        "Yulun Zhang",
        "Xiucheng Wang",
        "Nan Cheng"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "restoration",
        "mgfr",
        "facial",
        "face",
        "quality",
        "attribute",
        "modal",
        "images",
        "false",
        "illusions"
      ],
      "summary": "We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the generation of false facial attributes and identities often associated with generative face restoration methods. By incorporating a dual-control adapter and a two-stage training strategy, our method effectively utilizes multi-modal prior information for targeted restoration tasks. We also present the Reface-HQ dataset, comprising over 23,000 high-resolution facial images across 5,000 identities, to address the need for reference face training images. Our approach achieves superior visual quality in restoring facial details under severe degradation and allows for controlled restoration processes, enhancing the accuracy of identity preservation and attribute correction. Including negative quality samples and attribute prompts in the training further refines the model's ability to generate detailed and perceptually accurate images.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=m9RNBZewW2"
        ],
        "venue": [
          "/venue/m9RNBZewW2@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=m9RNBZewW2"
        ],
        "detail": [
          "https://openreview.net/forum?id=m9RNBZewW2"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 4
      },
      "raw_excerpt": "Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model [PDF 7 ] [Copy] [Kimi 4 ] [REL] Authors : Keda TAO , Jinjin Gu , Yulun Zhang , Xiucheng Wang , Nan Cheng We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the generation of false facial attributes and identities often associated with generative face restoration methods. By incorporating a dual-control adapter and a two-stage training strategy, our method effectively utilizes multi-modal prior information for targeted restoration tasks. We also present the Reface-HQ dataset, comprising over 23,000 high-resolution facial images across 5,000 identities, to address the need for reference face training images. Our approach achieves superior visual quality in restoring facial details under severe degradation and allows for controlled restoration processes, enhancing the accuracy of identity preservation and attribute correction. Including negative quality samples and attribute prompts in the training further refines the model's ability to generate detailed and perceptually accurate images. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "lzdFImKK8w@OpenReview",
      "index": 59,
      "title": "Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions",
      "authors": [
        "Xiaoran Jiao",
        "Weian Mao",
        "Wengong Jin",
        "Peiyuan Yang",
        "Hao Chen",
        "Chunhua Shen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "protein",
        "delta",
        "folding",
        "skempi",
        "inverse",
        "boltzmann",
        "conformational",
        "mutational",
        "5134",
        "4324"
      ],
      "summary": "Predicting the change in binding free energy ( Δ Δ G Δ Δ G ) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design.Due to the scarcity of experimental Δ Δ G Δ Δ G data, existing methods focus on pre-training, while alignment receives less attention.In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to Δ Δ G Δ Δ G prediction.We begin by analyzing the thermodynamic definition of Δ Δ G Δ Δ G and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes’ theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for Δ Δ G Δ Δ G estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the Δ Δ G Δ Δ G thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance.Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively.Futhermore, we demonstrate the capability of our method on bindingenergy prediction, protein-protein docking and antibody optimization tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lzdFImKK8w"
        ],
        "venue": [
          "/venue/lzdFImKK8w@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lzdFImKK8w"
        ],
        "detail": [
          "https://openreview.net/forum?id=lzdFImKK8w"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Xiaoran Jiao , Weian Mao , Wengong Jin , Peiyuan Yang , Hao Chen , Chunhua Shen Predicting the change in binding free energy ( Δ Δ G Δ Δ G ) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design.Due to the scarcity of experimental Δ Δ G Δ Δ G data, existing methods focus on pre-training, while alignment receives less attention.In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to Δ Δ G Δ Δ G prediction.We begin by analyzing the thermodynamic definition of Δ Δ G Δ Δ G and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes’ theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for Δ Δ G Δ Δ G estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the Δ Δ G Δ Δ G thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance.Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively.Futhermore, we demonstrate the capability of our method on bindingenergy prediction, protein-protein docking and antibody optimization tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "l4fMj4Vnly@OpenReview",
      "index": 60,
      "title": "ADIFF: Explaining audio difference using natural language",
      "authors": [
        "Soham Deshmukh",
        "Shuo Han",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "audio",
        "explanations",
        "adiff",
        "language",
        "explaining",
        "captioning",
        "baseline",
        "differences",
        "events",
        "difference"
      ],
      "summary": "Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model’s ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=l4fMj4Vnly"
        ],
        "venue": [
          "/venue/l4fMj4Vnly@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=l4fMj4Vnly"
        ],
        "detail": [
          "https://openreview.net/forum?id=l4fMj4Vnly"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "ADIFF: Explaining audio difference using natural language [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Soham Deshmukh , Shuo Han , Rita Singh , Bhiksha Raj Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model’s ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "q5EZ7gKcnW@OpenReview",
      "index": 61,
      "title": "Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision",
      "authors": [
        "Yaowen Ye",
        "Cassidy Laidlaw",
        "Jacob Steinhardt"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sft",
        "supervision",
        "rlhf",
        "unreliable",
        "ilr",
        "feedback",
        "refinement",
        "demonstrations",
        "lms",
        "human"
      ],
      "summary": "Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more capable, the tasks they are given become harder to supervise. Will post-training remain effective under unreliable supervision? To test this, we simulate unreliable demonstrations and comparison feedback using small LMs and time-constrained humans. We find that in the presence of unreliable supervision, SFT still retains some effectiveness, but DPO (a common RLHF algorithm) fails to improve the model beyond SFT. To address this, we propose *iterative label refinement* (ILR) as an alternative to RLHF. ILR improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with unreliable supervision (math, coding, and safe instruction-following). Our findings suggest that as LMs are used for complex tasks where human supervision is unreliable, RLHF may no longer be the best use of human comparison feedback; instead, it is better to direct feedback towards improving the training *data* rather than continually training the *model*. Our code and data are available at https://github.com/helloelwin/iterative-label-refinement.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=q5EZ7gKcnW"
        ],
        "venue": [
          "/venue/q5EZ7gKcnW@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=q5EZ7gKcnW"
        ],
        "detail": [
          "https://openreview.net/forum?id=q5EZ7gKcnW"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Yaowen Ye , Cassidy Laidlaw , Jacob Steinhardt Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more capable, the tasks they are given become harder to supervise. Will post-training remain effective under unreliable supervision? To test this, we simulate unreliable demonstrations and comparison feedback using small LMs and time-constrained humans. We find that in the presence of unreliable supervision, SFT still retains some effectiveness, but DPO (a common RLHF algorithm) fails to improve the model beyond SFT. To address this, we propose *iterative label refinement* (ILR) as an alternative to RLHF. ILR improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with unreliable supervision (math, coding, and safe instruction-following). Our findings suggest that as LMs are used for complex tasks where human supervision is unreliable, RLHF may no longer be the best use of human comparison feedback; instead, it is better to direct feedback towards improving the training *data* rather than continually training the *model*. Our code and data are available at https://github.com/helloelwin/iterative-label-refinement. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "kxFtMHItrf@OpenReview",
      "index": 62,
      "title": "Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model",
      "authors": [
        "Chunming He",
        "Chengyu Fang",
        "Yulun Zhang",
        "Longxiang Tang",
        "Jinfa Huang",
        "Kai Li",
        "zhenhua guo",
        "Xiu Li",
        "Sina Farsiu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "reti",
        "retinex",
        "idir",
        "rgformer",
        "illumination",
        "diff",
        "rldm",
        "degradation",
        "priors",
        "restoration"
      ],
      "summary": "Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion-based models (DM) have shown promising performance but are often burdened by heavy computational demands and pixel misalignment issues when predicting the image-level distribution. To tackle these problems, we propose to leverage DM within a compact latent space to generate concise guidance priors and introduce a novel solution called Reti-Diff for the IDIR task. Specifically, Reti-Diff comprises two significant components: the Retinex-based latent DM (RLDM) and the Retinex-guided transformer (RGformer). RLDM is designed to acquire Retinex knowledge, extracting reflectance and illumination priors to facilitate detailed reconstruction and illumination correction. RGformer subsequently utilizes these compact priors to guide the decomposition of image features into their respective reflectance and illumination components. Following this, RGformer further enhances and consolidates these decomposed features, resulting in the production of refined images with consistent content and robustness to handle complex degradation scenarios. Extensive experiments demonstrate that Reti-Diff outperforms existing methods on three IDIR tasks, as well as downstream applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kxFtMHItrf"
        ],
        "venue": [
          "/venue/kxFtMHItrf@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kxFtMHItrf"
        ],
        "detail": [
          "https://openreview.net/forum?id=kxFtMHItrf"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 1
      },
      "raw_excerpt": "Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model [PDF 7 ] [Copy] [Kimi 1 ] [REL] Authors : Chunming He , Chengyu Fang , Yulun Zhang , Longxiang Tang , Jinfa Huang , Kai Li , zhenhua guo , Xiu Li , Sina Farsiu Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion-based models (DM) have shown promising performance but are often burdened by heavy computational demands and pixel misalignment issues when predicting the image-level distribution. To tackle these problems, we propose to leverage DM within a compact latent space to generate concise guidance priors and introduce a novel solution called Reti-Diff for the IDIR task. Specifically, Reti-Diff comprises two significant components: the Retinex-based latent DM (RLDM) and the Retinex-guided transformer (RGformer). RLDM is designed to acquire Retinex knowledge, extracting reflectance and illumination priors to facilitate detailed reconstruction and illumination correction. RGformer subsequently utilizes these compact priors to guide the decomposition of image features into their respective reflectance and illumination components. Following this, RGformer further enhances and consolidates these decomposed features, resulting in the production of refined images with consistent content and robustness to handle complex degradation scenarios. Extensive experiments demonstrate that Reti-Diff outperforms existing methods on three IDIR tasks, as well as downstream applications. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "kpq3IIjUD3@OpenReview",
      "index": 63,
      "title": "Learning local equivariant representations for quantum operators",
      "authors": [
        "Zhanghao Zhouyin",
        "Zixi Gan",
        "Shishir Pandey",
        "Linfeng Zhang",
        "Qiangqiang Gu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "slem",
        "equivariant",
        "quantum",
        "operators",
        "overlap",
        "dft",
        "efficiency",
        "representations",
        "predicting",
        "matrices"
      ],
      "summary": "Predicting quantum operator matrices such as Hamiltonian, overlap, and density matrices in the density functional theory (DFT) framework is crucial for understanding material properties. Current methods often focus on individual operators and struggle with efficiency and scalability for large systems. Here we introduce a novel deep learning model, SLEM (strictly localized equivariant message-passing) for predicting multiple quantum operators, that achieves state-of-the-art accuracy while dramatically improving computational efficiency. SLEM's key innovation is its strict locality-based design, constructing local, equivariant representations for quantum tensors while preserving physical symmetries. This enables complex many-body dependence without expanding the effective receptive field, leading to superior data efficiency and transferability. Using an innovative SO(2) convolution and invariant overlap parameterization, SLEM reduces the computational complexity of high-order tensor products and is therefore capable of handling systems requiring the f f and g g orbitals in their basis sets. We demonstrate SLEM's capabilities across diverse 2D and 3D materials, achieving high accuracy even with limited training data. SLEM's design facilitates efficient parallelization, potentially extending DFT simulations to systems with device-level sizes, opening new possibilities for large-scale quantum simulations and high-throughput materials discovery.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kpq3IIjUD3"
        ],
        "venue": [
          "/venue/kpq3IIjUD3@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kpq3IIjUD3"
        ],
        "detail": [
          "https://openreview.net/forum?id=kpq3IIjUD3"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Learning local equivariant representations for quantum operators [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Zhanghao Zhouyin , Zixi Gan , Shishir Pandey , Linfeng Zhang , Qiangqiang Gu Predicting quantum operator matrices such as Hamiltonian, overlap, and density matrices in the density functional theory (DFT) framework is crucial for understanding material properties. Current methods often focus on individual operators and struggle with efficiency and scalability for large systems. Here we introduce a novel deep learning model, SLEM (strictly localized equivariant message-passing) for predicting multiple quantum operators, that achieves state-of-the-art accuracy while dramatically improving computational efficiency. SLEM's key innovation is its strict locality-based design, constructing local, equivariant representations for quantum tensors while preserving physical symmetries. This enables complex many-body dependence without expanding the effective receptive field, leading to superior data efficiency and transferability. Using an innovative SO(2) convolution and invariant overlap parameterization, SLEM reduces the computational complexity of high-order tensor products and is therefore capable of handling systems requiring the f f and g g orbitals in their basis sets. We demonstrate SLEM's capabilities across diverse 2D and 3D materials, achieving high accuracy even with limited training data. SLEM's design facilitates efficient parallelization, potentially extending DFT simulations to systems with device-level sizes, opening new possibilities for large-scale quantum simulations and high-throughput materials discovery. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "xsx3Fpo3UD@OpenReview",
      "index": 64,
      "title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models",
      "authors": [
        "Shiping Gao",
        "Fanqi Wan",
        "Jiajian Guo",
        "Xiaojun Quan",
        "Qifan Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "adpa",
        "alignment",
        "slms",
        "dckd",
        "teacher",
        "distillation",
        "student",
        "aligned",
        "advantage",
        "preference"
      ],
      "summary": "Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at \\url{https://github.com/SLIT-AI/ADPA}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xsx3Fpo3UD"
        ],
        "venue": [
          "/venue/xsx3Fpo3UD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xsx3Fpo3UD"
        ],
        "detail": [
          "https://openreview.net/forum?id=xsx3Fpo3UD"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 11
      },
      "raw_excerpt": "Advantage-Guided Distillation for Preference Alignment in Small Language Models [PDF 13 ] [Copy] [Kimi 11 ] [REL] Authors : Shiping Gao , Fanqi Wan , Jiajian Guo , Xiaojun Quan , Qifan Wang Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at \\url{https://github.com/SLIT-AI/ADPA}. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "jXvwJ51vcK@OpenReview",
      "index": 65,
      "title": "Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation",
      "authors": [
        "Zhaochong An",
        "Guolei Sun",
        "Yun Liu",
        "Runjia Li",
        "Min Wu",
        "Ming-Ming Cheng",
        "Ender Konukoglu",
        "Serge Belongie"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "multimodal",
        "pcs",
        "fss",
        "shot",
        "cloud",
        "unimodal",
        "semantic",
        "multimodality",
        "segmentation",
        "modalities"
      ],
      "summary": "Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jXvwJ51vcK"
        ],
        "venue": [
          "/venue/jXvwJ51vcK@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jXvwJ51vcK"
        ],
        "detail": [
          "https://openreview.net/forum?id=jXvwJ51vcK"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 1
      },
      "raw_excerpt": "Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation [PDF 8 ] [Copy] [Kimi 1 ] [REL] Authors : Zhaochong An , Guolei Sun , Yun Liu , Runjia Li , Min Wu , Ming-Ming Cheng , Ender Konukoglu , Serge Belongie Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code will be released. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "jXLiDKsuDo@OpenReview",
      "index": 66,
      "title": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning",
      "authors": [
        "Hojoon Lee",
        "Dongyoon Hwang",
        "Donghu Kim",
        "Hyunseung Kim",
        "Jun Jet Tai",
        "Kaushik Subramanian",
        "Peter Wurman",
        "Jaegul Choo",
        "Peter Stone",
        "Takuma Seno"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "simba",
        "simplicity",
        "scaling",
        "bias",
        "deep",
        "overfitting",
        "humanoidbench",
        "parameters",
        "myosuite",
        "integrating"
      ],
      "summary": "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting.These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored.Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms—including off-policy, on-policy, and unsupervised methods—is consistently improved.Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench.These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jXLiDKsuDo"
        ],
        "venue": [
          "/venue/jXLiDKsuDo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jXLiDKsuDo"
        ],
        "detail": [
          "https://openreview.net/forum?id=jXLiDKsuDo"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Hojoon Lee , Dongyoon Hwang , Donghu Kim , Hyunseung Kim , Jun Jet Tai , Kaushik Subramanian , Peter Wurman , Jaegul Choo , Peter Stone , Takuma Seno Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting.These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored.Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms—including off-policy, on-policy, and unsupervised methods—is consistently improved.Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench.These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "iuxaCU3DI7@OpenReview",
      "index": 67,
      "title": "Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data",
      "authors": [
        "Jiajie Li",
        "Brian Quaranto",
        "Chenhui Xu",
        "Ishan Mishra",
        "Ruiyang Qin",
        "Dancheng Liu",
        "Peter Kim",
        "Jinjun Xiong"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "surgical",
        "raso",
        "tag",
        "map",
        "recognize",
        "object",
        "supervised",
        "unleashing",
        "weakly",
        "procedures"
      ],
      "summary": "We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gatherers to 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks respectively in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. We will open-source our code, model, and dataset to facilitate further research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=iuxaCU3DI7"
        ],
        "venue": [
          "/venue/iuxaCU3DI7@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=iuxaCU3DI7"
        ],
        "detail": [
          "https://openreview.net/forum?id=iuxaCU3DI7"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 1
      },
      "raw_excerpt": "Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data [PDF 8 ] [Copy] [Kimi 1 ] [REL] Authors : Jiajie Li , Brian Quaranto , Chenhui Xu , Ishan Mishra , Ruiyang Qin , Dancheng Liu , Peter Kim , Jinjun Xiong We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gatherers to 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks respectively in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. We will open-source our code, model, and dataset to facilitate further research. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ikkvC1UnnE@OpenReview",
      "index": 68,
      "title": "Adaptive Batch Size for Privately Finding Second-Order Stationary Points",
      "authors": [
        "Daogao Liu",
        "Kunal Talwar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sosp",
        "privately",
        "fosp",
        "finding",
        "frac",
        "epsilon",
        "stationary",
        "batch",
        "tilde",
        "ganesh"
      ],
      "summary": "There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) demonstrated that an α α -SOSP can be found with α = \\Tilde O ( 1 n 1 / 3 + ( d √ n ϵ ) 3 / 7 ) α = \\Tilde O ( 1 n 1 / 3 + ( d n ϵ ) 3 / 7 ) , where n n is the dataset size, d d is the dimension, and ϵ ϵ is the differential privacy parameter. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method improves the results for privately finding an SOSP, achieving α = \\Tilde O ( 1 n 1 / 3 + ( d √ n ϵ ) 1 / 2 ) α = \\Tilde O ( 1 n 1 / 3 + ( d n ϵ ) 1 / 2 ) . This improved bound matches the state-of-the-art for finding an FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ikkvC1UnnE"
        ],
        "venue": [
          "/venue/ikkvC1UnnE@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ikkvC1UnnE"
        ],
        "detail": [
          "https://openreview.net/forum?id=ikkvC1UnnE"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "Adaptive Batch Size for Privately Finding Second-Order Stationary Points [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Daogao Liu , Kunal Talwar There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) demonstrated that an α α -SOSP can be found with α = \\Tilde O ( 1 n 1 / 3 + ( d √ n ϵ ) 3 / 7 ) α = \\Tilde O ( 1 n 1 / 3 + ( d n ϵ ) 3 / 7 ) , where n n is the dataset size, d d is the dimension, and ϵ ϵ is the differential privacy parameter. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method improves the results for privately finding an SOSP, achieving α = \\Tilde O ( 1 n 1 / 3 + ( d √ n ϵ ) 1 / 2 ) α = \\Tilde O ( 1 n 1 / 3 + ( d n ϵ ) 1 / 2 ) . This improved bound matches the state-of-the-art for finding an FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "S5Yo6w3n3f@OpenReview",
      "index": 69,
      "title": "ODE-based Smoothing Neural Network for Reinforcement Learning Tasks",
      "authors": [
        "Yinuo Wang",
        "Wenxuan Wang",
        "Xujie Song",
        "Tong Liu",
        "Yuming Yin",
        "Liangfa Chen",
        "Likun Wang",
        "Jingliang Duan",
        "Shengbo Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "smode",
        "ode",
        "smooth",
        "network",
        "control",
        "actions",
        "lipsnet",
        "neuron",
        "reinforcement",
        "lipschitz"
      ],
      "summary": "The smoothness of control actions is a significant challenge faced by deep reinforcement learning (RL) techniques in solving optimal control problems. Existing RL-trained policies tend to produce non-smooth actions due to high-frequency input noise and unconstrained Lipschitz constants in neural networks. This article presents a Smooth ODE (SmODE) network capable of simultaneously addressing both causes of unsmooth control actions, thereby enhancing policy performance and robustness under noise condition. We first design a smooth ODE neuron with first-order low-pass filtering expression, which can dynamically filter out high frequency noises of hidden state by a learnable state-based system time constant. Additionally, we construct a state-based mapping function, g g , and theoretically demonstrate its capacity to control the ODE neuron's Lipschitz constant. Then, based on the above neuronal structure design, we further advanced the SmODE network serving as RL policy approximators. This network is compatible with most existing RL algorithms, offering improved adaptability compared to prior approaches. Various experiments show that our SmODE network demonstrates superior anti-interference capabilities and smoother action outputs than the multi-layer perception and smooth network architectures like LipsNet.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=S5Yo6w3n3f"
        ],
        "venue": [
          "/venue/S5Yo6w3n3f@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=S5Yo6w3n3f"
        ],
        "detail": [
          "https://openreview.net/forum?id=S5Yo6w3n3f"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "ODE-based Smoothing Neural Network for Reinforcement Learning Tasks [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Yinuo Wang , Wenxuan Wang , Xujie Song , Tong Liu , Yuming Yin , Liangfa Chen , Likun Wang , Jingliang Duan , Shengbo Li The smoothness of control actions is a significant challenge faced by deep reinforcement learning (RL) techniques in solving optimal control problems. Existing RL-trained policies tend to produce non-smooth actions due to high-frequency input noise and unconstrained Lipschitz constants in neural networks. This article presents a Smooth ODE (SmODE) network capable of simultaneously addressing both causes of unsmooth control actions, thereby enhancing policy performance and robustness under noise condition. We first design a smooth ODE neuron with first-order low-pass filtering expression, which can dynamically filter out high frequency noises of hidden state by a learnable state-based system time constant. Additionally, we construct a state-based mapping function, g g , and theoretically demonstrate its capacity to control the ODE neuron's Lipschitz constant. Then, based on the above neuronal structure design, we further advanced the SmODE network serving as RL policy approximators. This network is compatible with most existing RL algorithms, offering improved adaptability compared to prior approaches. Various experiments show that our SmODE network demonstrates superior anti-interference capabilities and smoother action outputs than the multi-layer perception and smooth network architectures like LipsNet. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hpCfPEvBsr@OpenReview",
      "index": 70,
      "title": "MixEval-X: Any-to-any Evaluations from Real-world Data Mixture",
      "authors": [
        "Jinjie Ni",
        "Yifan Song",
        "Deepanway Ghosal",
        "Bo Li",
        "David Junhao Zhang",
        "Xiang Yue",
        "Fuzhao Xue",
        "Yuntian Deng",
        "Andy Zheng",
        "Kaichen Zhang",
        "Mahir Shah",
        "Kabir Jain",
        "Yang You",
        "Michael Qizhe Shieh"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mixeval",
        "evaluations",
        "world",
        "real",
        "mixture",
        "modalities",
        "effectively",
        "benchmark",
        "diverse",
        "modal"
      ],
      "summary": "Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any, real-world benchmark designed to optimize and standardize evaluations across diverse input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions. Meanwhile, MixEval-X's model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98) while being much more efficient. We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hpCfPEvBsr"
        ],
        "venue": [
          "/venue/hpCfPEvBsr@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hpCfPEvBsr"
        ],
        "detail": [
          "https://openreview.net/forum?id=hpCfPEvBsr"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "MixEval-X: Any-to-any Evaluations from Real-world Data Mixture [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Jinjie Ni , Yifan Song , Deepanway Ghosal , Bo Li , David Junhao Zhang , Xiang Yue , Fuzhao Xue , Yuntian Deng , Andy Zheng , Kaichen Zhang , Mahir Shah , Kabir Jain , Yang You , Michael Qizhe Shieh Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any, real-world benchmark designed to optimize and standardize evaluations across diverse input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions. Meanwhile, MixEval-X's model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98) while being much more efficient. We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hUb2At2DsQ@OpenReview",
      "index": 71,
      "title": "Rethinking and improving autoformalization: towards a faithful metric and a Dependency Retrieval-based approach",
      "authors": [
        "Qi Liu",
        "Xinhao Zheng",
        "Xudong Lu",
        "Qinxiang Cao",
        "Junchi Yan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "autoformalization",
        "beq",
        "formal",
        "mapsto",
        "statement",
        "dependency",
        "rautoformalizer",
        "faithful",
        "retrieval",
        "con"
      ],
      "summary": "As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1) absence of faithful and universal automated evaluation for autoformalization results; 2) agnosia of contextural information, inducing severe hallucination of formal definitions and theorems.To address the first issue, we propose **BEq** (_**B**idirectional **E**xtended Definitional E**q**uivalence_), an automated neuro-symbolic method to determine the equivalence between two formal statements, which is formal-grounded and well-aligned with human intuition.For the second, we propose **RAutoformalizer** (_**R**etrieval-augmented **Autoformalizer**_), augmenting statement autoformalization by _Dependency Retrieval_, retrieving potentially dependent objects from formal libraries.We parse the dependencies of libraries and propose to _structurally informalise_ formal objects by the topological order of dependencies. To evaluate OOD generalization and research-level capabilities, we build a novel benchmark, _Con-NF_, consisting of 961 informal-formal statement pairs from frontier mathematical researches.Extensive experiments validate the effectiveness of our proposed approaches. In particular, BEq is evaluated on 200 diverse formal statement pairs with expert-annotated equivalence label, exhibiting significantly improved accuracy ( 82.50 82.50 ) and precision ( 70.59 70.59 ).For dependency retrieval, a baseline with excellent performance is established.The proposed RAutoformalizer substantially outperforms SOTA baselines in both in-distribution ProofNet benchmark ( 12.83 12.83 , BEq@8) and OOD Con-NF scenario ( 4.58 4.58 , BEq@8). Code, data, and models will be available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hUb2At2DsQ"
        ],
        "venue": [
          "/venue/hUb2At2DsQ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hUb2At2DsQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=hUb2At2DsQ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Rethinking and improving autoformalization: towards a faithful metric and a Dependency Retrieval-based approach [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Qi Liu , Xinhao Zheng , Xudong Lu , Qinxiang Cao , Junchi Yan As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1) absence of faithful and universal automated evaluation for autoformalization results; 2) agnosia of contextural information, inducing severe hallucination of formal definitions and theorems.To address the first issue, we propose **BEq** (_**B**idirectional **E**xtended Definitional E**q**uivalence_), an automated neuro-symbolic method to determine the equivalence between two formal statements, which is formal-grounded and well-aligned with human intuition.For the second, we propose **RAutoformalizer** (_**R**etrieval-augmented **Autoformalizer**_), augmenting statement autoformalization by _Dependency Retrieval_, retrieving potentially dependent objects from formal libraries.We parse the dependencies of libraries and propose to _structurally informalise_ formal objects by the topological order of dependencies. To evaluate OOD generalization and research-level capabilities, we build a novel benchmark, _Con-NF_, consisting of 961 informal-formal statement pairs from frontier mathematical researches.Extensive experiments validate the effectiveness of our proposed approaches. In particular, BEq is evaluated on 200 diverse formal statement pairs with expert-annotated equivalence label, exhibiting significantly improved accuracy ( 82.50 82.50 ) and precision ( 70.59 70.59 ).For dependency retrieval, a baseline with excellent performance is established.The proposed RAutoformalizer substantially outperforms SOTA baselines in both in-distribution ProofNet benchmark ( 12.83 12.83 , BEq@8) and OOD Con-NF scenario ( 4.58 4.58 , BEq@8). Code, data, and models will be available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "gYWqxXE5RJ@OpenReview",
      "index": 72,
      "title": "ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language",
      "authors": [
        "Yuxin Wang",
        "Xiaomeng Zhu",
        "Weimin Lyu",
        "Saeed Hassanpour",
        "Soroush Vosoughi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "implicitness",
        "impscore",
        "language",
        "metric",
        "implicit",
        "sentence",
        "learnable",
        "operationalize",
        "580",
        "quantifying"
      ],
      "summary": "Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define \"implicitness\" as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising 112 , 580 112 , 580 (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gYWqxXE5RJ"
        ],
        "venue": [
          "/venue/gYWqxXE5RJ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gYWqxXE5RJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=gYWqxXE5RJ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Yuxin Wang , Xiaomeng Zhu , Weimin Lyu , Saeed Hassanpour , Soroush Vosoughi Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define \"implicitness\" as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising 112 , 580 112 , 580 (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "gWgaypDBs8@OpenReview",
      "index": 73,
      "title": "Representative Guidance: Diffusion Model Sampling with Consistency",
      "authors": [
        "Anh-Dung Dinh",
        "Daochang Liu",
        "Chang Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "representative",
        "guidance",
        "repg",
        "sampling",
        "diffusion",
        "target",
        "discernment",
        "coherent",
        "classifier",
        "sacrificed"
      ],
      "summary": "The diffusion sampling process faces a persistent challenge stemming from its incoherence, attributable to varying noise directions across different time steps. Our Representative Guidance (RepG) offers a new perspective to handle this issue by reformulating the sampling process with a coherent direction towards a representative target.In this formulation, while the classic classifier guidance improves feature discernment by steering the model away from ambiguous features, it fails to provide a favorable representative target, since the class label is overly compact and leads to sacrificed diversity and the adversarial generation problem.In contrast, we leverage self-supervised representations as the coherent target and treat sampling as a downstream task, which refines image details and corrects errors rather than settling for simpler samples.Our representative guidance achieves superior performance and also illustrates the potential of pre-trained self-supervised models in image sampling. Our findings demonstrate that RepG not only substantially enhances vanilla diffusion sampling but also surpasses state-of-the-art benchmarks when combined with the classifier-free guidance. Our code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gWgaypDBs8"
        ],
        "venue": [
          "/venue/gWgaypDBs8@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gWgaypDBs8"
        ],
        "detail": [
          "https://openreview.net/forum?id=gWgaypDBs8"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 3
      },
      "raw_excerpt": "Representative Guidance: Diffusion Model Sampling with Consistency [PDF 16 ] [Copy] [Kimi 3 ] [REL] Authors : Anh-Dung Dinh , Daochang Liu , Chang Xu The diffusion sampling process faces a persistent challenge stemming from its incoherence, attributable to varying noise directions across different time steps. Our Representative Guidance (RepG) offers a new perspective to handle this issue by reformulating the sampling process with a coherent direction towards a representative target.In this formulation, while the classic classifier guidance improves feature discernment by steering the model away from ambiguous features, it fails to provide a favorable representative target, since the class label is overly compact and leads to sacrificed diversity and the adversarial generation problem.In contrast, we leverage self-supervised representations as the coherent target and treat sampling as a downstream task, which refines image details and corrects errors rather than settling for simpler samples.Our representative guidance achieves superior performance and also illustrates the potential of pre-trained self-supervised models in image sampling. Our findings demonstrate that RepG not only substantially enhances vanilla diffusion sampling but also surpasses state-of-the-art benchmarks when combined with the classifier-free guidance. Our code will be released. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "friHAl5ofG@OpenReview",
      "index": 74,
      "title": "Vision Language Models are In-Context Value Learners",
      "authors": [
        "Yecheng Jason Ma",
        "Joey Hejna",
        "Chuyuan Fu",
        "Dhruv Shah",
        "Jacky Liang",
        "Zhuo Xu",
        "Sean Kirmani",
        "Peng Xu",
        "Danny Driess",
        "Ted Xiao",
        "Osbert Bastani",
        "Dinesh Jayaraman",
        "Wenhao Yu",
        "Tingnan Zhang",
        "Dorsa Sadigh",
        "Fei Xia"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gvl",
        "value",
        "temporal",
        "progress",
        "frames",
        "tasks",
        "vlms",
        "task",
        "context",
        "vision"
      ],
      "summary": "Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=friHAl5ofG"
        ],
        "venue": [
          "/venue/friHAl5ofG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=friHAl5ofG"
        ],
        "detail": [
          "https://openreview.net/forum?id=friHAl5ofG"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 9
      },
      "raw_excerpt": "Vision Language Models are In-Context Value Learners [PDF 10 ] [Copy] [Kimi 9 ] [REL] Authors : Yecheng Jason Ma , Joey Hejna , Chuyuan Fu , Dhruv Shah , Jacky Liang , Zhuo Xu , Sean Kirmani , Peng Xu , Danny Driess , Ted Xiao , Osbert Bastani , Dinesh Jayaraman , Wenhao Yu , Tingnan Zhang , Dorsa Sadigh , Fei Xia Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "eW4yh6HKz4@OpenReview",
      "index": 75,
      "title": "CBQ: Cross-Block Quantization for Large Language Models",
      "authors": [
        "Xin Ding",
        "Xiaoyu Liu",
        "Zhijun Tu",
        "Yun Zhang",
        "Wei Li",
        "Jie Hu",
        "Hanting Chen",
        "Yehui Tang",
        "Zhiwei Xiong",
        "Baoqun Yin",
        "Yunhe Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cbq",
        "quantization",
        "ptq",
        "llms",
        "block",
        "dependencies",
        "layer",
        "cross",
        "w2a16",
        "llama1"
      ],
      "summary": "Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from significant performance degradation at ultra-low bits precision. To dissect this issue, we conducted an in-depth analysis of quantization errors specific to LLMs and surprisingly discovered that, unlike traditional sources of quantization errors, the growing number of model parameters, combined with the reduction in quantization bits, intensifies inter-layer and intra-layer dependencies, which severely impact quantization accuracy. This finding highlights a critical challenge in quantizing LLMs. To address this, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ leverages a cross-block dependency to establish long-range dependencies across multiple blocks and integrates an adaptive LoRA-Rounding technique to manage intra-layer dependencies. To further enhance performance, CBQ incorporates a coarse-to-fine pre-processing mechanism for processing weights and activations. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across various LLMs and datasets. Notably, CBQ only takes 4.3 hours to quantize a weight-only quantization of a 4-bit LLAMA1-65B model, achieving a commendable trade off between performance and efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eW4yh6HKz4"
        ],
        "venue": [
          "/venue/eW4yh6HKz4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eW4yh6HKz4"
        ],
        "detail": [
          "https://openreview.net/forum?id=eW4yh6HKz4"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "CBQ: Cross-Block Quantization for Large Language Models [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Xin Ding , Xiaoyu Liu , Zhijun Tu , Yun Zhang , Wei Li , Jie Hu , Hanting Chen , Yehui Tang , Zhiwei Xiong , Baoqun Yin , Yunhe Wang Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from significant performance degradation at ultra-low bits precision. To dissect this issue, we conducted an in-depth analysis of quantization errors specific to LLMs and surprisingly discovered that, unlike traditional sources of quantization errors, the growing number of model parameters, combined with the reduction in quantization bits, intensifies inter-layer and intra-layer dependencies, which severely impact quantization accuracy. This finding highlights a critical challenge in quantizing LLMs. To address this, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ leverages a cross-block dependency to establish long-range dependencies across multiple blocks and integrates an adaptive LoRA-Rounding technique to manage intra-layer dependencies. To further enhance performance, CBQ incorporates a coarse-to-fine pre-processing mechanism for processing weights and activations. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across various LLMs and datasets. Notably, CBQ only takes 4.3 hours to quantize a weight-only quantization of a 4-bit LLAMA1-65B model, achieving a commendable trade off between performance and efficiency. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "dRXxFEY8ZE@OpenReview",
      "index": 76,
      "title": "BirdSet BirdSet : A Large-Scale Dataset for Audio Classification in Avian Bioacoustics",
      "authors": [
        "Lukas Rauch",
        "Raphael Schwinger",
        "Moritz Wirth",
        "René Heinrich",
        "Denis Huseljic",
        "Marek Herde",
        "Jonas Lange",
        "Stefan Kahl",
        "Bernhard Sick",
        "Sven Tomforde",
        "Christoph Scholz"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "birdset",
        "texttt",
        "bioacoustics",
        "audio",
        "classification",
        "avian",
        "audioset",
        "dataset",
        "accessibility",
        "benchmark"
      ],
      "summary": "Deep learning (DL) has greatly advanced audio classification, yet the field is limited by the scarcity of large-scale benchmark datasets that have propelled progress in other domains. While AudioSet is a pivotal step to bridge this gap as a universal-domain dataset, its restricted accessibility and limited range of evaluation use cases challenge its role as the sole resource. Therefore, we introduce BirdSet BirdSet , a large-scale benchmark data set for audio classification focusing on avian bioacoustics. BirdSet BirdSet surpasses AudioSet with over 6,800 recording hours ( ↑ 17 % ↑ 17 % ) from nearly 10,000 classes ( ↑ 18 × ↑ 18 × ) for training and more than 400 hours ( ↑ 7 × ↑ 7 × ) across eight strongly labeled evaluation datasets. It serves as a versatile resource for use cases such as multi-label classification, covariate shift or self-supervised learning. We benchmark six well-known DL models in multi-label classification across three distinct training scenarios and outline further evaluation use cases in audio classification. We host our dataset on Hugging Face for easy accessibility and offer an extensive codebase to reproduce our results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dRXxFEY8ZE"
        ],
        "venue": [
          "/venue/dRXxFEY8ZE@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dRXxFEY8ZE"
        ],
        "detail": [
          "https://openreview.net/forum?id=dRXxFEY8ZE"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "BirdSet BirdSet : A Large-Scale Dataset for Audio Classification in Avian Bioacoustics [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Lukas Rauch , Raphael Schwinger , Moritz Wirth , René Heinrich , Denis Huseljic , Marek Herde , Jonas Lange , Stefan Kahl , Bernhard Sick , Sven Tomforde , Christoph Scholz Deep learning (DL) has greatly advanced audio classification, yet the field is limited by the scarcity of large-scale benchmark datasets that have propelled progress in other domains. While AudioSet is a pivotal step to bridge this gap as a universal-domain dataset, its restricted accessibility and limited range of evaluation use cases challenge its role as the sole resource. Therefore, we introduce BirdSet BirdSet , a large-scale benchmark data set for audio classification focusing on avian bioacoustics. BirdSet BirdSet surpasses AudioSet with over 6,800 recording hours ( ↑ 17 % ↑ 17 % ) from nearly 10,000 classes ( ↑ 18 × ↑ 18 × ) for training and more than 400 hours ( ↑ 7 × ↑ 7 × ) across eight strongly labeled evaluation datasets. It serves as a versatile resource for use cases such as multi-label classification, covariate shift or self-supervised learning. We benchmark six well-known DL models in multi-label classification across three distinct training scenarios and outline further evaluation use cases in audio classification. We host our dataset on Hugging Face for easy accessibility and offer an extensive codebase to reproduce our results. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "dDpB23VbVa@OpenReview",
      "index": 77,
      "title": "Patch-Level Training for Large Language Models",
      "authors": [
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "patch",
        "training",
        "llms",
        "level",
        "costs",
        "language",
        "370m",
        "token",
        "unit",
        "next"
      ],
      "summary": "The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5 × × , without compromising the model performance compared to token-level training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dDpB23VbVa"
        ],
        "venue": [
          "/venue/dDpB23VbVa@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dDpB23VbVa"
        ],
        "detail": [
          "https://openreview.net/forum?id=dDpB23VbVa"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "Patch-Level Training for Large Language Models [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : Chenze Shao , Fandong Meng , Jie Zhou The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5 × × , without compromising the model performance compared to token-level training. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hNjCVVm0EQ@OpenReview",
      "index": 78,
      "title": "MamKO: Mamba-based Koopman operator for modeling and predictive control",
      "authors": [
        "Zhaoyang Li",
        "Minghao Han",
        "Xunyuan Yin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "koopman",
        "mamko",
        "mamba",
        "control",
        "modeling",
        "systems",
        "operator",
        "model",
        "predictive",
        "nonlinear"
      ],
      "summary": "The Koopman theory, which enables the transformation of nonlinear systems into linear representations, is a powerful and efficient tool to model and control nonlinear systems. However, the ability of the Koopman operator to model complex systems, particularly time-varying systems, is limited by the fixed linear state-space representation. To address the limitation, the large language model, Mamba, is considered a promising strategy for enhancing modeling capabilities while preserving the linear state-space structure.In this paper, we propose a new framework, the Mamba-based Koopman operator (MamKO), which provides enhanced model prediction capability and adaptability, as compared to Koopman models with constant Koopman operators. Inspired by the Mamba structure, MamKO generates Koopman operators from online data; this enables the model to effectively capture the dynamic behaviors of the nonlinear system over time. A model predictive control system is then developed based on the proposed MamKO model. The modeling and control performance of the proposed method is evaluated through experiments on benchmark time-invariant and time-varying systems. The experimental results demonstrate the superiority of the proposed approach. Additionally, we perform ablation experiments to test the effectiveness of individual components of MamKO. This approach unlocks new possibilities for integrating large language models with control frameworks, and it achieves a good balance between advanced modeling capabilities and real-time control implementation efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hNjCVVm0EQ"
        ],
        "venue": [
          "/venue/hNjCVVm0EQ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hNjCVVm0EQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=hNjCVVm0EQ"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "MamKO: Mamba-based Koopman operator for modeling and predictive control [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Zhaoyang Li , Minghao Han , Xunyuan Yin The Koopman theory, which enables the transformation of nonlinear systems into linear representations, is a powerful and efficient tool to model and control nonlinear systems. However, the ability of the Koopman operator to model complex systems, particularly time-varying systems, is limited by the fixed linear state-space representation. To address the limitation, the large language model, Mamba, is considered a promising strategy for enhancing modeling capabilities while preserving the linear state-space structure.In this paper, we propose a new framework, the Mamba-based Koopman operator (MamKO), which provides enhanced model prediction capability and adaptability, as compared to Koopman models with constant Koopman operators. Inspired by the Mamba structure, MamKO generates Koopman operators from online data; this enables the model to effectively capture the dynamic behaviors of the nonlinear system over time. A model predictive control system is then developed based on the proposed MamKO model. The modeling and control performance of the proposed method is evaluated through experiments on benchmark time-invariant and time-varying systems. The experimental results demonstrate the superiority of the proposed approach. Additionally, we perform ablation experiments to test the effectiveness of individual components of MamKO. This approach unlocks new possibilities for integrating large language models with control frameworks, and it achieves a good balance between advanced modeling capabilities and real-time control implementation efficiency. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "y5einmJ0Yx@OpenReview",
      "index": 79,
      "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
      "authors": [
        "Danny Wang",
        "Ruihong Qiu",
        "Guangdong Bai",
        "Zi Huang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ood",
        "graph",
        "exposure",
        "gold",
        "generative",
        "embeddings",
        "data",
        "implicit",
        "adversarial",
        "instances"
      ],
      "summary": "Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=y5einmJ0Yx"
        ],
        "venue": [
          "/venue/y5einmJ0Yx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=y5einmJ0Yx"
        ],
        "detail": [
          "https://openreview.net/forum?id=y5einmJ0Yx"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Danny Wang , Ruihong Qiu , Guangdong Bai , Zi Huang Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "fU8H4lzkIm@OpenReview",
      "index": 80,
      "title": "PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems",
      "authors": [
        "Bocheng Zeng",
        "Qi Wang",
        "Mengtao Yan",
        "Yang Liu",
        "Ruizhi Chengze",
        "Yi Zhang",
        "Hongsheng Liu",
        "Zidong Wang",
        "Hao Sun"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "phympgn",
        "spatiotemporal",
        "pde",
        "encoded",
        "message",
        "passing",
        "graph",
        "meshes",
        "gnn",
        "irregular"
      ],
      "summary": "Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fU8H4lzkIm"
        ],
        "venue": [
          "/venue/fU8H4lzkIm@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fU8H4lzkIm"
        ],
        "detail": [
          "https://openreview.net/forum?id=fU8H4lzkIm"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 2
      },
      "raw_excerpt": "PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems [PDF 10 ] [Copy] [Kimi 2 ] [REL] Authors : Bocheng Zeng , Qi Wang , Mengtao Yan , Yang Liu , Ruizhi Chengze , Yi Zhang , Hongsheng Liu , Zidong Wang , Hao Sun Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "d9aWa875kj@OpenReview",
      "index": 81,
      "title": "Exact Certification of (Graph) Neural Networks Against Label Poisoning",
      "authors": [
        "Mahalakshmi Sabanayagam",
        "Lukas Gosch",
        "Stephan Günnemann",
        "Debarghya Ghoshdastidar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "flipping",
        "poisoning",
        "label",
        "certificates",
        "gnns",
        "certification",
        "robustness",
        "ntk",
        "exact",
        "neural"
      ],
      "summary": "Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: ( i ) ( i ) we establish hierarchies of GNNs on different benchmark graphs; ( i i ) ( i i ) quantify the effect of architectural choices such as activations, depth and skip-connections; and surprisingly, ( i i i ) ( i i i ) uncover a novel phenomenon of the robustness plateauing for intermediate perturbation budgets across all investigated datasets and architectures. While we focus on GNNs, our certificates are applicable to sufficiently wide NNs in general through their NTK. Thus, our work presents the first exact certificate to a poisoning attack ever derived for neural networks, which could be of independent interest.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=d9aWa875kj"
        ],
        "venue": [
          "/venue/d9aWa875kj@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=d9aWa875kj"
        ],
        "detail": [
          "https://openreview.net/forum?id=d9aWa875kj"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Exact Certification of (Graph) Neural Networks Against Label Poisoning [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Mahalakshmi Sabanayagam , Lukas Gosch , Stephan Günnemann , Debarghya Ghoshdastidar Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: ( i ) ( i ) we establish hierarchies of GNNs on different benchmark graphs; ( i i ) ( i i ) quantify the effect of architectural choices such as activations, depth and skip-connections; and surprisingly, ( i i i ) ( i i i ) uncover a novel phenomenon of the robustness plateauing for intermediate perturbation budgets across all investigated datasets and architectures. While we focus on GNNs, our certificates are applicable to sufficiently wide NNs in general through their NTK. Thus, our work presents the first exact certificate to a poisoning attack ever derived for neural networks, which could be of independent interest. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "lqTILjL6lP@OpenReview",
      "index": 82,
      "title": "RESuM: A Rare Event Surrogate Model for Physics Detector Design",
      "authors": [
        "Ann-Kathrin Schuetz",
        "Alan Poon",
        "Aobo Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "resum",
        "nldbd",
        "rare",
        "event",
        "detector",
        "design",
        "surrogate",
        "red",
        "physics",
        "designs"
      ],
      "summary": "The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the probability of background events contaminating the detector. Given that this probability is inherently low, design optimization either requires extremely costly simulations to generate sufficient background counts or contending with significant variance. In this work, we formalize this dilemma as a Rare Event Design (RED) problem: identifying optimal design parameters when the design metric to be minimized is inherently small. We then designed the Rare Event Surrogate Model (RESuM) for physics detector design optimization under RED conditions. RESuM uses a pre-trained Conditional Neural Process (CNP) model to incorporate additional prior knowledge into a Multi-Fidelity Gaussian Process model. We applied RESuM to optimize neutron shielding designs for the LEGEND NLDBD experiment, identifying an optimal design that reduces the neutron background by ( 66.5 ± 3.5 ) ( 66.5 ± 3.5 ) % while using only 3.3% of the computational resources compared to traditional methods. Given the prevalence of RED problems in other fields of physical sciences, especially in rare-event searches, the RESuM algorithm has broad potential for accelerating simulation-intensive applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lqTILjL6lP"
        ],
        "venue": [
          "/venue/lqTILjL6lP@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lqTILjL6lP"
        ],
        "detail": [
          "https://openreview.net/forum?id=lqTILjL6lP"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "RESuM: A Rare Event Surrogate Model for Physics Detector Design [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Ann-Kathrin Schuetz , Alan Poon , Aobo Li The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the probability of background events contaminating the detector. Given that this probability is inherently low, design optimization either requires extremely costly simulations to generate sufficient background counts or contending with significant variance. In this work, we formalize this dilemma as a Rare Event Design (RED) problem: identifying optimal design parameters when the design metric to be minimized is inherently small. We then designed the Rare Event Surrogate Model (RESuM) for physics detector design optimization under RED conditions. RESuM uses a pre-trained Conditional Neural Process (CNP) model to incorporate additional prior knowledge into a Multi-Fidelity Gaussian Process model. We applied RESuM to optimize neutron shielding designs for the LEGEND NLDBD experiment, identifying an optimal design that reduces the neutron background by ( 66.5 ± 3.5 ) ( 66.5 ± 3.5 ) % while using only 3.3% of the computational resources compared to traditional methods. Given the prevalence of RED problems in other fields of physical sciences, especially in rare-event searches, the RESuM algorithm has broad potential for accelerating simulation-intensive applications. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cv2iMNWCsh@OpenReview",
      "index": 83,
      "title": "Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification",
      "authors": [
        "Kaizheng Wang",
        "Fabio Cuzzolin",
        "Keivan Shariatmadar",
        "David Moens",
        "Hans Hallez"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "credal",
        "wrapper",
        "cifar10",
        "uncertainty",
        "bnns",
        "imagenet",
        "cifar100",
        "averaging",
        "estimation",
        "des"
      ],
      "summary": "This paper presents an innovative approach, called credal wrapper, to formulating a credal set representation of model averaging for Bayesian neural networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty estimation in classification tasks. Given a finite collection of single predictive distributions derived from BNNs or DEs, the proposed credal wrapper approach extracts an upper and a lower probability bound per class, acknowledging the epistemic uncertainty due to the availability of a limited amount of distributions. Such probability intervals over classes can be mapped on a convex set of probabilities (a credal set) from which, in turn, a unique prediction can be obtained using a transformation called intersection probability transformation. In this article, we conduct extensive experiments on several out-of-distribution (OOD) detection benchmarks, encompassing various dataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, CIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network architectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base). Compared to the BNN and DE baselines, the proposed credal wrapper method exhibits superior performance in uncertainty estimation and achieves a lower expected calibration error on corrupted data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cv2iMNWCsh"
        ],
        "venue": [
          "/venue/cv2iMNWCsh@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cv2iMNWCsh"
        ],
        "detail": [
          "https://openreview.net/forum?id=cv2iMNWCsh"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 2
      },
      "raw_excerpt": "Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification [PDF 7 ] [Copy] [Kimi 2 ] [REL] Authors : Kaizheng Wang , Fabio Cuzzolin , Keivan Shariatmadar , David Moens , Hans Hallez This paper presents an innovative approach, called credal wrapper, to formulating a credal set representation of model averaging for Bayesian neural networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty estimation in classification tasks. Given a finite collection of single predictive distributions derived from BNNs or DEs, the proposed credal wrapper approach extracts an upper and a lower probability bound per class, acknowledging the epistemic uncertainty due to the availability of a limited amount of distributions. Such probability intervals over classes can be mapped on a convex set of probabilities (a credal set) from which, in turn, a unique prediction can be obtained using a transformation called intersection probability transformation. In this article, we conduct extensive experiments on several out-of-distribution (OOD) detection benchmarks, encompassing various dataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, CIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network architectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base). Compared to the BNN and DE baselines, the proposed credal wrapper method exhibits superior performance in uncertainty estimation and achieves a lower expected calibration error on corrupted data. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cqsw28DuMW@OpenReview",
      "index": 84,
      "title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models",
      "authors": [
        "Makoto Shing",
        "Kou Misaki",
        "Han Bao",
        "Sho Yokoi",
        "Takuya Akiba"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "taid",
        "distillation",
        "teacher",
        "student",
        "interpolated",
        "mode",
        "knowledge",
        "language",
        "temporally",
        "texttt"
      ],
      "summary": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression.A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.To address these issues, we introduce Temporally Adaptive Interpolated Distillation (TAID) Temporally Adaptive Interpolated Distillation (TAID) , a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse.Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: TAID-LLM-1.5B TAID-LLM-1.5B for language tasks and TAID-VLM-2B TAID-VLM-2B for vision-language tasks.These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cqsw28DuMW"
        ],
        "venue": [
          "/venue/cqsw28DuMW@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cqsw28DuMW"
        ],
        "detail": [
          "https://openreview.net/forum?id=cqsw28DuMW"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 8
      },
      "raw_excerpt": "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models [PDF 4 ] [Copy] [Kimi 8 ] [REL] Authors : Makoto Shing , Kou Misaki , Han Bao , Sho Yokoi , Takuya Akiba Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression.A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.To address these issues, we introduce Temporally Adaptive Interpolated Distillation (TAID) Temporally Adaptive Interpolated Distillation (TAID) , a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse.Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: TAID-LLM-1.5B TAID-LLM-1.5B for language tasks and TAID-VLM-2B TAID-VLM-2B for vision-language tasks.These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cWHonXThtM@OpenReview",
      "index": 85,
      "title": "Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution",
      "authors": [
        "Simiao Li",
        "Yun Zhang",
        "Wei Li",
        "Hanting Chen",
        "Wenjia Wang",
        "Bingyi Jing",
        "Shaohui Lin",
        "Jie Hu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mipkd",
        "distillation",
        "teacher",
        "knowledge",
        "student",
        "mixer",
        "granularity",
        "feature",
        "mixture",
        "super"
      ],
      "summary": "Knowledge distillation (KD) is a promising yet challenging model compression technique that transfers rich learning representations from a well-performing but cumbersome teacher model to a compact student model. Previous methods for image super-resolution (SR) mostly are tailored to the specific teacher-student architectures. And the potential for improvement is limited, which hinders their wide applications. This work presents a novel KD framework for SR models, the multi-granularity mixture of prior knowledge distillation (MiPKD), that is universally applicable to a wide array of architectures at feature and block levels. The teacher’s knowledge is effectively integrated with the student's feature via the Feature Prior Mixer, and the reconstructed feature propagates dynamically in the training phase with the Block Prior Mixer. Extensive experiments demonstrate the effectiveness of the proposed MiPKD method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cWHonXThtM"
        ],
        "venue": [
          "/venue/cWHonXThtM@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cWHonXThtM"
        ],
        "detail": [
          "https://openreview.net/forum?id=cWHonXThtM"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 6
      },
      "raw_excerpt": "Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution [PDF 15 ] [Copy] [Kimi 6 ] [REL] Authors : Simiao Li , Yun Zhang , Wei Li , Hanting Chen , Wenjia Wang , Bingyi Jing , Shaohui Lin , Jie Hu Knowledge distillation (KD) is a promising yet challenging model compression technique that transfers rich learning representations from a well-performing but cumbersome teacher model to a compact student model. Previous methods for image super-resolution (SR) mostly are tailored to the specific teacher-student architectures. And the potential for improvement is limited, which hinders their wide applications. This work presents a novel KD framework for SR models, the multi-granularity mixture of prior knowledge distillation (MiPKD), that is universally applicable to a wide array of architectures at feature and block levels. The teacher’s knowledge is effectively integrated with the student's feature via the Feature Prior Mixer, and the reconstructed feature propagates dynamically in the training phase with the Block Prior Mixer. Extensive experiments demonstrate the effectiveness of the proposed MiPKD method. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "jCPak79Kev@OpenReview",
      "index": 86,
      "title": "AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies",
      "authors": [
        "Jian Gao",
        "Weidong Cao",
        "Junyi Yang",
        "Xuan Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "analog",
        "ics",
        "analoggenie",
        "textbf",
        "underline",
        "generative",
        "circuit",
        "topologies",
        "design",
        "foundational"
      ],
      "summary": "The massive and large-scale design of foundational semiconductor integrated circuits (ICs) is crucial to sustaining the advancement of many emerging and future technologies, such as generative AI, 5G/6G, and quantum computing.Excitingly, recent studies have shown the great capabilities of foundational models in expediting the design of digital ICs.Yet, applying generative AI techniques to accelerate the design of analog ICs remains a significant challenge due to critical domain-specific issues, such as the lack of a comprehensive dataset and effective representation methods for analog circuits.This paper proposes, AnalogGenie AnalogGenie , a Gen – – – – – Gen _ erat i – i _ ve e – – e _ ngine for automatic design/discovery of Analog – – – – – – – – Analog _ circuit topologies--the most challenging and creative task in the conventional manual design flow of analog ICs.AnalogGenie addresses two key gaps in the field: building a foundational comprehensive dataset of analog circuit topology and developing a scalable sequence-based graph representation universal to analog circuits.Experimental results show the remarkable generation performance of AnalogGenie in broadening the variety of analog ICs, increasing the number of devices within a single design, and discovering unseen circuit topologies far beyond any prior arts.Our work paves the way to transform the longstanding time-consuming manual design flow of analog ICs to an automatic and massive manner powered by generative AI.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jCPak79Kev"
        ],
        "venue": [
          "/venue/jCPak79Kev@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jCPak79Kev"
        ],
        "detail": [
          "https://openreview.net/forum?id=jCPak79Kev"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 2
      },
      "raw_excerpt": "AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies [PDF 6 ] [Copy] [Kimi 2 ] [REL] Authors : Jian Gao , Weidong Cao , Junyi Yang , Xuan Zhang The massive and large-scale design of foundational semiconductor integrated circuits (ICs) is crucial to sustaining the advancement of many emerging and future technologies, such as generative AI, 5G/6G, and quantum computing.Excitingly, recent studies have shown the great capabilities of foundational models in expediting the design of digital ICs.Yet, applying generative AI techniques to accelerate the design of analog ICs remains a significant challenge due to critical domain-specific issues, such as the lack of a comprehensive dataset and effective representation methods for analog circuits.This paper proposes, AnalogGenie AnalogGenie , a Gen – – – – – Gen _ erat i – i _ ve e – – e _ ngine for automatic design/discovery of Analog – – – – – – – – Analog _ circuit topologies--the most challenging and creative task in the conventional manual design flow of analog ICs.AnalogGenie addresses two key gaps in the field: building a foundational comprehensive dataset of analog circuit topology and developing a scalable sequence-based graph representation universal to analog circuits.Experimental results show the remarkable generation performance of AnalogGenie in broadening the variety of analog ICs, increasing the number of devices within a single design, and discovering unseen circuit topologies far beyond any prior arts.Our work paves the way to transform the longstanding time-consuming manual design flow of analog ICs to an automatic and massive manner powered by generative AI. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cD1kl2QKv1@OpenReview",
      "index": 87,
      "title": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt",
      "authors": [
        "Tao Liu",
        "Kai Wang",
        "Senmao Li",
        "Joost van de Weijer",
        "Fahad Khan",
        "Shiqi Yang",
        "Yaxing Wang",
        "Jian Yang",
        "Ming-Ming Cheng"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "textit",
        "prompt",
        "1prompt1story",
        "t2i",
        "generation",
        "consistent",
        "identity",
        "story",
        "preserving",
        "text"
      ],
      "summary": "Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency context consistency , to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency context consistency , we propose a novel training-free training-free method for consistent text-to-image (T2I) generation, termed \"One-Prompt-One-Story\" ( 1Prompt1Story 1Prompt1Story ). Our approach 1Prompt1Story 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-ValueReweighting Singular-ValueReweighting and Identity-Preserving Cross-Attention Identity-Preserving Cross-Attention , ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness, through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cD1kl2QKv1"
        ],
        "venue": [
          "/venue/cD1kl2QKv1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cD1kl2QKv1"
        ],
        "detail": [
          "https://openreview.net/forum?id=cD1kl2QKv1"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 10
      },
      "raw_excerpt": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt [PDF 6 ] [Copy] [Kimi 10 ] [REL] Authors : Tao Liu , Kai Wang , Senmao Li , Joost van de Weijer , Fahad Khan , Shiqi Yang , Yaxing Wang , Jian Yang , Ming-Ming Cheng Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency context consistency , to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency context consistency , we propose a novel training-free training-free method for consistent text-to-image (T2I) generation, termed \"One-Prompt-One-Story\" ( 1Prompt1Story 1Prompt1Story ). Our approach 1Prompt1Story 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-ValueReweighting Singular-ValueReweighting and Identity-Preserving Cross-Attention Identity-Preserving Cross-Attention , ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness, through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "bjxuqI4KwU@OpenReview",
      "index": 88,
      "title": "Linear SCM Identification in the Presence of Confounders and Gaussian Noise",
      "authors": [
        "Vahideh Sanjaroonpouri",
        "Pouria Ramazi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "confounders",
        "scm",
        "gaussian",
        "scms",
        "identifiable",
        "noise",
        "emph",
        "confounding",
        "variables",
        "presence"
      ],
      "summary": "Noisy linear structural causal models (SCMs) in the presence of confounding variables are known to be identifiable if all confounding and noise variables are non-Gaussian and unidentifiable if all are Gaussian. The identifiability when only some are Gaussian remains concealed. We show that, in the presence of Gaussian noise, a linear SCM is uniquely identifiable provided that \\emph{(i)} the number of confounders is at most the number of the observed variables, \\emph{(ii)} the confounders do not have a Gaussian component, and \\emph{(iii)} the causal structure of the SCM is known. If the third condition is relaxed, the SCM becomes finitely identifiable; more specifically, it belongs to a set of at most n ! n ! linear SCMS, where n n is the number of observed variables. The confounders in all of these n ! n ! SCMs share the same joint probability distribution function (PDF), which we obtain analytically. For the case where both the noise and confounders are Gaussian, we provide further insight into the existing counter-example-based unidentifiability result and demonstrate that every SCM with confounders can be represented as an SCM without confounders but with the same joint PDF.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bjxuqI4KwU"
        ],
        "venue": [
          "/venue/bjxuqI4KwU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bjxuqI4KwU"
        ],
        "detail": [
          "https://openreview.net/forum?id=bjxuqI4KwU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Linear SCM Identification in the Presence of Confounders and Gaussian Noise [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Vahideh Sanjaroonpouri , Pouria Ramazi Noisy linear structural causal models (SCMs) in the presence of confounding variables are known to be identifiable if all confounding and noise variables are non-Gaussian and unidentifiable if all are Gaussian. The identifiability when only some are Gaussian remains concealed. We show that, in the presence of Gaussian noise, a linear SCM is uniquely identifiable provided that \\emph{(i)} the number of confounders is at most the number of the observed variables, \\emph{(ii)} the confounders do not have a Gaussian component, and \\emph{(iii)} the causal structure of the SCM is known. If the third condition is relaxed, the SCM becomes finitely identifiable; more specifically, it belongs to a set of at most n ! n ! linear SCMS, where n n is the number of observed variables. The confounders in all of these n ! n ! SCMs share the same joint probability distribution function (PDF), which we obtain analytically. For the case where both the noise and confounders are Gaussian, we provide further insight into the existing counter-example-based unidentifiability result and demonstrate that every SCM with confounders can be represented as an SCM without confounders but with the same joint PDF. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "bcTjW5kS4W@OpenReview",
      "index": 89,
      "title": "NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics",
      "authors": [
        "Wuwei Zhang",
        "Ziyu Lu",
        "Trung Le",
        "Hao Wang",
        "Uygar Sümbül",
        "Eric SheaBrown",
        "Lu Mi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "netformer",
        "connectivity",
        "nonstationary",
        "neuronal",
        "activity",
        "neural",
        "interpretable",
        "recordings",
        "plasticity",
        "key"
      ],
      "summary": "Neuronal dynamics are highly nonlinear and nonstationary. Traditional methods for extracting the underlying network structure from neuronal activity recordings mainly concentrate on modeling static connectivity, without accounting for key nonstationary aspects of biological neural systems, such as ongoing synaptic plasticity and neuronal modulation. To bridge this gap, we introduce the NetFormer model, an interpretable approach applicable to such systems. In NetFormer, the activity of each neuron across a series of historical time steps is defined as a token. These tokens are then linearly mapped through a query and key mechanism to generate a state- (and hence time-) dependent attention matrix that directly encodes nonstationary connectivity structures. We analyze our formulation from the perspective of nonstationary and nonlinear networked dynamical systems, and show both via an analytical expansion and targeted simulations how it can approximate the underlying ground truth. Next, we demonstrate NetFormer's ability to model a key feature of biological networks, spike-timing-dependent plasticity, whereby connection strengths continually change in response to local activity patterns. We further demonstrate that NetFormer can capture task-induced connectivity patterns on activity generated by task-trained recurrent neural networks. Thus informed, we apply NetFormer to a multi-modal dataset of real neural recordings, which contains neural activity, cell type, and behavioral state information. We show that the NetFormer effectively predicts neural dynamics and identifies cell-type specific, state-dependent dynamic connectivity that matches patterns measured in separate ground-truth physiology experiments, demonstrating its ability to help decode complex neural interactions based on population activity observations alone.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bcTjW5kS4W"
        ],
        "venue": [
          "/venue/bcTjW5kS4W@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bcTjW5kS4W"
        ],
        "detail": [
          "https://openreview.net/forum?id=bcTjW5kS4W"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Wuwei Zhang , Ziyu Lu , Trung Le , Hao Wang , Uygar Sümbül , Eric SheaBrown , Lu Mi Neuronal dynamics are highly nonlinear and nonstationary. Traditional methods for extracting the underlying network structure from neuronal activity recordings mainly concentrate on modeling static connectivity, without accounting for key nonstationary aspects of biological neural systems, such as ongoing synaptic plasticity and neuronal modulation. To bridge this gap, we introduce the NetFormer model, an interpretable approach applicable to such systems. In NetFormer, the activity of each neuron across a series of historical time steps is defined as a token. These tokens are then linearly mapped through a query and key mechanism to generate a state- (and hence time-) dependent attention matrix that directly encodes nonstationary connectivity structures. We analyze our formulation from the perspective of nonstationary and nonlinear networked dynamical systems, and show both via an analytical expansion and targeted simulations how it can approximate the underlying ground truth. Next, we demonstrate NetFormer's ability to model a key feature of biological networks, spike-timing-dependent plasticity, whereby connection strengths continually change in response to local activity patterns. We further demonstrate that NetFormer can capture task-induced connectivity patterns on activity generated by task-trained recurrent neural networks. Thus informed, we apply NetFormer to a multi-modal dataset of real neural recordings, which contains neural activity, cell type, and behavioral state information. We show that the NetFormer effectively predicts neural dynamics and identifies cell-type specific, state-dependent dynamic connectivity that matches patterns measured in separate ground-truth physiology experiments, demonstrating its ability to help decode complex neural interactions based on population activity observations alone. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "bW9fGYo44s@OpenReview",
      "index": 90,
      "title": "MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion",
      "authors": [
        "Onkar Susladkar",
        "Jishu Sen Gupta",
        "Chirag Sehgal",
        "Sparsh Mittal",
        "Rekha Singhal"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "motionaura",
        "spatiotemporal",
        "videos",
        "quality",
        "inpainting",
        "generation",
        "sota",
        "denoising",
        "mbq"
      ],
      "summary": "The spatio-temporal complexity of video data presents significant challenges in tasks such as compression, generation, and inpainting. We present four key contributions to address the challenges of spatiotemporal video processing. First, we introduce the 3D Mobile Inverted Vector-Quantization Variational Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with masked modeling to enhance spatiotemporal video compression. The model achieves superior temporal consistency and state-of-the-art (SOTA) reconstruction quality by employing a novel training strategy with full frame masking. Second, we present MotionAura, a text-to-video generation framework that utilizes vector-quantized diffusion models to discretize the latent space and capture complex motion dynamics, producing temporally coherent videos aligned with text prompts. Third, we propose a spectral transformer-based denoising network that processes video data in the frequency domain using the Fourier Transform. This method effectively captures global context and long-range dependencies for high-quality video generation and denoising. Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Our models achieve SOTA performance on a range of benchmarks. Our work offers robust frameworks for spatiotemporal modeling and user-driven video content manipulation. We will release the code, dataset, and models in open-source.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bW9fGYo44s"
        ],
        "venue": [
          "/venue/bW9fGYo44s@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bW9fGYo44s"
        ],
        "detail": [
          "https://openreview.net/forum?id=bW9fGYo44s"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 6
      },
      "raw_excerpt": "MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion [PDF 10 ] [Copy] [Kimi 6 ] [REL] Authors : Onkar Susladkar , Jishu Sen Gupta , Chirag Sehgal , Sparsh Mittal , Rekha Singhal The spatio-temporal complexity of video data presents significant challenges in tasks such as compression, generation, and inpainting. We present four key contributions to address the challenges of spatiotemporal video processing. First, we introduce the 3D Mobile Inverted Vector-Quantization Variational Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with masked modeling to enhance spatiotemporal video compression. The model achieves superior temporal consistency and state-of-the-art (SOTA) reconstruction quality by employing a novel training strategy with full frame masking. Second, we present MotionAura, a text-to-video generation framework that utilizes vector-quantized diffusion models to discretize the latent space and capture complex motion dynamics, producing temporally coherent videos aligned with text prompts. Third, we propose a spectral transformer-based denoising network that processes video data in the frequency domain using the Fourier Transform. This method effectively captures global context and long-range dependencies for high-quality video generation and denoising. Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Our models achieve SOTA performance on a range of benchmarks. Our work offers robust frameworks for spatiotemporal modeling and user-driven video content manipulation. We will release the code, dataset, and models in open-source. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "bMC1t7eLRc@OpenReview",
      "index": 91,
      "title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models",
      "authors": [
        "Chi Zhang",
        "Huaping Zhong",
        "Kuan Zhang",
        "Chengliang Chai",
        "Rui Wang",
        "Xinlin Zhuang",
        "Tianyi Bai",
        "Qiu Jiantao",
        "Lei Cao",
        "Ju Fan",
        "Ye Yuan",
        "Guoren Wang",
        "Conghui He"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "instances",
        "influence",
        "diversity",
        "pretraining",
        "texttt",
        "data",
        "quad",
        "quality",
        "clusters",
        "selection"
      ],
      "summary": "Data selection is of great significance in pretraining large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, i . e . , i . e . , a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top- k k instances with the highest scores. However, this approach has several limitations. (1) Calculating the accurate influence of all available data is time-consuming.(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.In this paper, we introduce Quad Quad , a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.To compute the influence ( i . e . , i . e . , the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. For the diversity, Quad Quad clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity. Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that Quad Quad significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bMC1t7eLRc"
        ],
        "venue": [
          "/venue/bMC1t7eLRc@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bMC1t7eLRc"
        ],
        "detail": [
          "https://openreview.net/forum?id=bMC1t7eLRc"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 12
      },
      "raw_excerpt": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models [PDF 5 ] [Copy] [Kimi 12 ] [REL] Authors : Chi Zhang , Huaping Zhong , Kuan Zhang , Chengliang Chai , Rui Wang , Xinlin Zhuang , Tianyi Bai , Qiu Jiantao , Lei Cao , Ju Fan , Ye Yuan , Guoren Wang , Conghui He Data selection is of great significance in pretraining large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, i . e . , i . e . , a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top- k k instances with the highest scores. However, this approach has several limitations. (1) Calculating the accurate influence of all available data is time-consuming.(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.In this paper, we introduce Quad Quad , a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.To compute the influence ( i . e . , i . e . , the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. For the diversity, Quad Quad clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity. Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that Quad Quad significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "auZZ2gN0ZN@OpenReview",
      "index": 92,
      "title": "Dense Video Object Captioning from Disjoint Supervision",
      "authors": [
        "Xingyi Zhou",
        "Anurag Arnab",
        "Chen Sun",
        "Cordelia Schmid"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "captioning",
        "task",
        "video",
        "vidstg",
        "vln",
        "disjoint",
        "supervision",
        "grounding",
        "dense",
        "scenic"
      ],
      "summary": "We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Our code is available at https://github.com/google-research/scenic.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=auZZ2gN0ZN"
        ],
        "venue": [
          "/venue/auZZ2gN0ZN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=auZZ2gN0ZN"
        ],
        "detail": [
          "https://openreview.net/forum?id=auZZ2gN0ZN"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Dense Video Object Captioning from Disjoint Supervision [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Xingyi Zhou , Anurag Arnab , Chen Sun , Cordelia Schmid We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Our code is available at https://github.com/google-research/scenic. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "agHddsQhsL@OpenReview",
      "index": 93,
      "title": "Targeted Attack Improves Protection against Unauthorized Diffusion Customization",
      "authors": [
        "Boyang Zheng",
        "Chumeng Liang",
        "Xiaoyu Wu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "customization",
        "unauthorized",
        "attacks",
        "targeted",
        "diffusion",
        "protection",
        "protections",
        "untargeted",
        "attack",
        "poisoning"
      ],
      "summary": "Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=agHddsQhsL"
        ],
        "venue": [
          "/venue/agHddsQhsL@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=agHddsQhsL"
        ],
        "detail": [
          "https://openreview.net/forum?id=agHddsQhsL"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "Targeted Attack Improves Protection against Unauthorized Diffusion Customization [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Boyang Zheng , Chumeng Liang , Xiaoyu Wu Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "aZ1gNJu8wO@OpenReview",
      "index": 94,
      "title": "A Geometric Framework for Understanding Memorization in Generative Models",
      "authors": [
        "Brendan Ross",
        "Hamidreza Kamkari",
        "Tongzi Wu",
        "Rasa Hosseinzadeh",
        "Zhaoyan Liu",
        "George Stein",
        "Jesse Cresswell",
        "Gabriel Loaiza-Ganem"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "memorization",
        "mmh",
        "memorized",
        "manifold",
        "generative",
        "framework",
        "geometric",
        "hypothesis",
        "dimensionalities",
        "datapoint"
      ],
      "summary": "As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose the *manifold memorization hypothesis* (MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of ( i ) ( i ) the ground truth data manifold and ( i i ) ( i i ) the manifold learned by the model. This framework provides a formal standard for \"how memorized\" a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aZ1gNJu8wO"
        ],
        "venue": [
          "/venue/aZ1gNJu8wO@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aZ1gNJu8wO"
        ],
        "detail": [
          "https://openreview.net/forum?id=aZ1gNJu8wO"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 8
      },
      "raw_excerpt": "A Geometric Framework for Understanding Memorization in Generative Models [PDF 7 ] [Copy] [Kimi 8 ] [REL] Authors : Brendan Ross , Hamidreza Kamkari , Tongzi Wu , Rasa Hosseinzadeh , Zhaoyan Liu , George Stein , Jesse Cresswell , Gabriel Loaiza-Ganem As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose the *manifold memorization hypothesis* (MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of ( i ) ( i ) the ground truth data manifold and ( i i ) ( i i ) the manifold learned by the model. This framework provides a formal standard for \"how memorized\" a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "aX7X9z3vQS@OpenReview",
      "index": 95,
      "title": "Recovering Manifold Structure Using Ollivier Ricci Curvature",
      "authors": [
        "Tristan L. Saidi",
        "Abigail Hickok",
        "Andrew J Blumberg"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ollivier",
        "manifold",
        "manl",
        "orc",
        "ricci",
        "nearest",
        "neighbor",
        "curvature",
        "edges",
        "recovering"
      ],
      "summary": "We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aX7X9z3vQS"
        ],
        "venue": [
          "/venue/aX7X9z3vQS@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aX7X9z3vQS"
        ],
        "detail": [
          "https://openreview.net/forum?id=aX7X9z3vQS"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Recovering Manifold Structure Using Ollivier Ricci Curvature [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Tristan L. Saidi , Abigail Hickok , Andrew J Blumberg We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "aMBSY2ebPw@OpenReview",
      "index": 96,
      "title": "Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?",
      "authors": [
        "Seth Aycock",
        "David Stap",
        "Di Wu",
        "Christof Monz",
        "Khalil Simaan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "xlr",
        "grammar",
        "book",
        "translation",
        "grammatical",
        "linguistic",
        "parallel",
        "llms",
        "resource",
        "books"
      ],
      "summary": "Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests prompting long-context LLMs with one grammar book enables English–Kalamang translation, an unseen XLR language—a noteworthy case of linguistic knowledge helping an NLP task. We investigate whether the book's grammatical explanations or its parallel examples are most effective for learning XLR translation, finding almost all improvement stems from the parallel examples. We find similar results for Nepali and Guarani, seen low-resource languages, and achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate *where* grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what *kind* of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we suggest data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aMBSY2ebPw"
        ],
        "venue": [
          "/venue/aMBSY2ebPw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aMBSY2ebPw"
        ],
        "detail": [
          "https://openreview.net/forum?id=aMBSY2ebPw"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 11
      },
      "raw_excerpt": "Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book? [PDF 1 ] [Copy] [Kimi 11 ] [REL] Authors : Seth Aycock , David Stap , Di Wu , Christof Monz , Khalil Simaan Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests prompting long-context LLMs with one grammar book enables English–Kalamang translation, an unseen XLR language—a noteworthy case of linguistic knowledge helping an NLP task. We investigate whether the book's grammatical explanations or its parallel examples are most effective for learning XLR translation, finding almost all improvement stems from the parallel examples. We find similar results for Nepali and Guarani, seen low-resource languages, and achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate *where* grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what *kind* of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we suggest data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "t8qcGXaepr@OpenReview",
      "index": 97,
      "title": "Uncovering Overfitting in Large Language Model Editing",
      "authors": [
        "Mengqi Zhang",
        "Xiaotian Ye",
        "Qiang Liu",
        "shu wu",
        "Pengjie Ren",
        "Zhumin Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "editing",
        "knowledge",
        "overfit",
        "edit",
        "lti",
        "overfitting",
        "edited",
        "llms",
        "uncovering",
        "unedited"
      ],
      "summary": "Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of **Editing Overfit**, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs’ knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn to Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=t8qcGXaepr"
        ],
        "venue": [
          "/venue/t8qcGXaepr@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=t8qcGXaepr"
        ],
        "detail": [
          "https://openreview.net/forum?id=t8qcGXaepr"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 14
      },
      "raw_excerpt": "Uncovering Overfitting in Large Language Model Editing [PDF 15 ] [Copy] [Kimi 14 ] [REL] Authors : Mengqi Zhang , Xiaotian Ye , Qiang Liu , shu wu , Pengjie Ren , Zhumin Chen Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of **Editing Overfit**, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs’ knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn to Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ZGkfoufDaU@OpenReview",
      "index": 98,
      "title": "Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models",
      "authors": [
        "Jingyang Zhang",
        "Jingwei Sun",
        "Eric Yeats",
        "Yang Ouyang",
        "Martin Kuo",
        "Jianyi Zhang",
        "Hao Yang",
        "Hai Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "min",
        "training",
        "reference",
        "pre",
        "maxima",
        "detection",
        "wikimia",
        "insightfully",
        "mimir",
        "improved"
      ],
      "summary": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings (evaluated with 5 families of 10 models and 2 benchmarks). On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZGkfoufDaU"
        ],
        "venue": [
          "/venue/ZGkfoufDaU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZGkfoufDaU"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZGkfoufDaU"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Jingyang Zhang , Jingwei Sun , Eric Yeats , Yang Ouyang , Martin Kuo , Jianyi Zhang , Hao Yang , Hai Li The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings (evaluated with 5 families of 10 models and 2 benchmarks). On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "YwJkv2YqBq@OpenReview",
      "index": 99,
      "title": "Nesterov acceleration in benignly non-convex landscapes",
      "authors": [
        "Kanan Gupta",
        "Stephan Wojtowytsch"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "nag",
        "nesterov",
        "convex",
        "benignly",
        "optimization",
        "additive",
        "landscapes",
        "overparametrized",
        "notoriously",
        "historically"
      ],
      "summary": "While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a `benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YwJkv2YqBq"
        ],
        "venue": [
          "/venue/YwJkv2YqBq@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YwJkv2YqBq"
        ],
        "detail": [
          "https://openreview.net/forum?id=YwJkv2YqBq"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Nesterov acceleration in benignly non-convex landscapes [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Kanan Gupta , Stephan Wojtowytsch While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a `benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "e1wDDFmlVu@OpenReview",
      "index": 100,
      "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
      "authors": [
        "Xiaoming Shi",
        "Shiyu Wang",
        "Yuqi Nie",
        "Dianqi Li",
        "Zhou Ye",
        "Qingsong Wen",
        "Ming Jin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "moe",
        "forecasting",
        "time",
        "series",
        "billion",
        "foundation",
        "models",
        "scale",
        "pre",
        "experts"
      ],
      "summary": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=e1wDDFmlVu"
        ],
        "venue": [
          "/venue/e1wDDFmlVu@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=e1wDDFmlVu"
        ],
        "detail": [
          "https://openreview.net/forum?id=e1wDDFmlVu"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 10
      },
      "raw_excerpt": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts [PDF 10 ] [Copy] [Kimi 10 ] [REL] Authors : Xiaoming Shi , Shiyu Wang , Yuqi Nie , Dianqi Li , Zhou Ye , Qingsong Wen , Ming Jin Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Y2RW9EVwhT@OpenReview",
      "index": 101,
      "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
      "authors": [
        "Min Shi",
        "Fuxiao Liu",
        "Shihao Wang",
        "Shijia Liao",
        "Subhashree Radhakrishnan",
        "Yilin Zhao",
        "De-An Huang",
        "Hongxu Yin",
        "Karan Sapra",
        "Yaser Yacoob",
        "Humphrey Shi",
        "Bryan Catanzaro",
        "Andrew Tao",
        "Jan Kautz",
        "Zhiding Yu",
        "Guilin Liu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "encoders",
        "mllms",
        "vision",
        "eagle",
        "mixture",
        "multimodal",
        "visual",
        "tokens",
        "design",
        "strategies"
      ],
      "summary": "The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Y2RW9EVwhT"
        ],
        "venue": [
          "/venue/Y2RW9EVwhT@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Y2RW9EVwhT"
        ],
        "detail": [
          "https://openreview.net/forum?id=Y2RW9EVwhT"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 8
      },
      "raw_excerpt": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders [PDF 10 ] [Copy] [Kimi 8 ] [REL] Authors : Min Shi , Fuxiao Liu , Shihao Wang , Shijia Liao , Subhashree Radhakrishnan , Yilin Zhao , De-An Huang , Hongxu Yin , Karan Sapra , Yaser Yacoob , Humphrey Shi , Bryan Catanzaro , Andrew Tao , Jan Kautz , Zhiding Yu , Guilin Liu The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "awvJBtB2op@OpenReview",
      "index": 102,
      "title": "Generating Freeform Endoskeletal Robots",
      "authors": [
        "Muhan Li",
        "Lingji Kong",
        "Sam Kriegman"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "endoskeletal",
        "robots",
        "bodies",
        "jointed",
        "rigid",
        "freeform",
        "soft",
        "jointless",
        "embodied",
        "terrestrial"
      ],
      "summary": "The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of ``higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=awvJBtB2op"
        ],
        "venue": [
          "/venue/awvJBtB2op@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=awvJBtB2op"
        ],
        "detail": [
          "https://openreview.net/forum?id=awvJBtB2op"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Generating Freeform Endoskeletal Robots [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Muhan Li , Lingji Kong , Sam Kriegman The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of ``higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "YhfrKB3Ah7@OpenReview",
      "index": 103,
      "title": "PABBO: Preferential Amortized Black-Box Optimization",
      "authors": [
        "Xinyu Zhang",
        "Daolang Huang",
        "Julien Martinelli",
        "Samuel Kaski"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pbo",
        "preferential",
        "amortized",
        "pabbo",
        "surrogate",
        "acquisition",
        "amortizing",
        "optimization",
        "latent",
        "feedback"
      ],
      "summary": "Preferential Bayesian Optimization (PBO) is a sample-efficient method to learn latent user utilities from preferential feedback over a pair of designs. It relies on a statistical surrogate model for the latent function, usually a Gaussian process, and an acquisition strategy to select the next candidate pair to get user feedback on. Due to the non-conjugacy of the associated likelihood, every PBO step requires a significant amount of computations with various approximate inference techniques. This computational overhead is incompatible with the way humans interact with computers, hindering the use of PBO in real-world cases. Building on the recent advances of amortized BO, we propose to circumvent this issue by fully amortizing PBO, meta-learning both the surrogate and the acquisition function. Our method comprises a novel transformer neural process architecture, trained using reinforcement learning and tailored auxiliary losses.On a benchmark composed of synthetic and real-world datasets, our method is several orders of magnitude faster than the usual Gaussian process-based strategies and often outperforms them in accuracy.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YhfrKB3Ah7"
        ],
        "venue": [
          "/venue/YhfrKB3Ah7@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YhfrKB3Ah7"
        ],
        "detail": [
          "https://openreview.net/forum?id=YhfrKB3Ah7"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "PABBO: Preferential Amortized Black-Box Optimization [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Xinyu Zhang , Daolang Huang , Julien Martinelli , Samuel Kaski Preferential Bayesian Optimization (PBO) is a sample-efficient method to learn latent user utilities from preferential feedback over a pair of designs. It relies on a statistical surrogate model for the latent function, usually a Gaussian process, and an acquisition strategy to select the next candidate pair to get user feedback on. Due to the non-conjugacy of the associated likelihood, every PBO step requires a significant amount of computations with various approximate inference techniques. This computational overhead is incompatible with the way humans interact with computers, hindering the use of PBO in real-world cases. Building on the recent advances of amortized BO, we propose to circumvent this issue by fully amortizing PBO, meta-learning both the surrogate and the acquisition function. Our method comprises a novel transformer neural process architecture, trained using reinforcement learning and tailored auxiliary losses.On a benchmark composed of synthetic and real-world datasets, our method is several orders of magnitude faster than the usual Gaussian process-based strategies and often outperforms them in accuracy. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "WzCEiBILHu@OpenReview",
      "index": 104,
      "title": "Topological Schrödinger Bridge Matching",
      "authors": [
        "Maosheng Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "topological",
        "emph",
        "matching",
        "mathcal",
        "bridge",
        "schrödinger",
        "unknowns",
        "distributions",
        "process",
        "reference"
      ],
      "summary": "Given two boundary distributions, the \\emph{Schrödinger Bridge} (SB) problem seeks the “most likely” random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows.In this work, we propose the \\emph{Topological Schrödinger Bridge problem} ( T T SBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable \\emph{topology-aware} stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a \\emph{closed-form} topological SB ( T T SB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns.Building on these results, we develop T T SB-based models for matching topological signals by parameterizing the unknowns in the optimal process as \\emph{(topological) neural networks} and learning them through \\emph{likelihood training}. We validate the theoretical results and demonstrate the practical applications of T T SB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of T T SB-based models to other emerging models, and outline future directions for topological signal matching.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WzCEiBILHu"
        ],
        "venue": [
          "/venue/WzCEiBILHu@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WzCEiBILHu"
        ],
        "detail": [
          "https://openreview.net/forum?id=WzCEiBILHu"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 4
      },
      "raw_excerpt": "Topological Schrödinger Bridge Matching [PDF 7 ] [Copy] [Kimi 4 ] [REL] Author : Maosheng Yang Given two boundary distributions, the \\emph{Schrödinger Bridge} (SB) problem seeks the “most likely” random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows.In this work, we propose the \\emph{Topological Schrödinger Bridge problem} ( T T SBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable \\emph{topology-aware} stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a \\emph{closed-form} topological SB ( T T SB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns.Building on these results, we develop T T SB-based models for matching topological signals by parameterizing the unknowns in the optimal process as \\emph{(topological) neural networks} and learning them through \\emph{likelihood training}. We validate the theoretical results and demonstrate the practical applications of T T SB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of T T SB-based models to other emerging models, and outline future directions for topological signal matching. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "z8sxoCYgmd@OpenReview",
      "index": 105,
      "title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models",
      "authors": [
        "Junyan Ye",
        "Baichuan Zhou",
        "Zilong Huang",
        "Junan Zhang",
        "Tianyi Bai",
        "Hengrui Kang",
        "Jun He",
        "Honglin Lin",
        "Zihao Wang",
        "Tong Wu",
        "Zhizheng Wu",
        "Yiping Chen",
        "Dahua Lin",
        "Conghui He",
        "Weijia Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "loki",
        "lmms",
        "synthetic",
        "multimodal",
        "data",
        "benchmark",
        "comprehensive",
        "inundated",
        "modalities",
        "18k"
      ],
      "summary": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://loki102.github.io/LOKI.github.io/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=z8sxoCYgmd"
        ],
        "venue": [
          "/venue/z8sxoCYgmd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=z8sxoCYgmd"
        ],
        "detail": [
          "https://openreview.net/forum?id=z8sxoCYgmd"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Junyan Ye , Baichuan Zhou , Zilong Huang , Junan Zhang , Tianyi Bai , Hengrui Kang , Jun He , Honglin Lin , Zihao Wang , Tong Wu , Zhizheng Wu , Yiping Chen , Dahua Lin , Conghui He , Weijia Li With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://loki102.github.io/LOKI.github.io/. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "uqWM9hBDAE@OpenReview",
      "index": 106,
      "title": "How Much is Unseen Depends Chiefly on Information About the Seen",
      "authors": [
        "Seongmin Lee",
        "Marcel Boehme"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mse",
        "estimator",
        "estimators",
        "chiefly",
        "missing",
        "turing",
        "classes",
        "sample",
        "unknown",
        "classifier"
      ],
      "summary": "The *missing mass* refers to the proportion of data points in an *unknown* population of classifier inputs that belong to classes *not* present in the classifier's training data, which is assumed to be a random sample from that unknown population.We find that *in expectation* the missing mass is entirely determined by the number f k f k of classes that *do* appear in the training data the same number of times *and an exponentially decaying error*.While this is the first precise characterization of the expected missing mass in terms of the sample, the induced estimator suffers from an impractically high variance. However, our theory suggests a large search space of nearly unbiased estimators that can be searched effectively and efficiently. Hence, we cast distribution-free estimation as an optimization problem to find a distribution-specific estimator with a minimized mean-squared error (MSE), given only the sample.In our experiments, our search algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 93\\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\\% of the Good-Turing estimator's.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uqWM9hBDAE"
        ],
        "venue": [
          "/venue/uqWM9hBDAE@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uqWM9hBDAE"
        ],
        "detail": [
          "https://openreview.net/forum?id=uqWM9hBDAE"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "How Much is Unseen Depends Chiefly on Information About the Seen [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Seongmin Lee , Marcel Boehme The *missing mass* refers to the proportion of data points in an *unknown* population of classifier inputs that belong to classes *not* present in the classifier's training data, which is assumed to be a random sample from that unknown population.We find that *in expectation* the missing mass is entirely determined by the number f k f k of classes that *do* appear in the training data the same number of times *and an exponentially decaying error*.While this is the first precise characterization of the expected missing mass in terms of the sample, the induced estimator suffers from an impractically high variance. However, our theory suggests a large search space of nearly unbiased estimators that can be searched effectively and efficiently. Hence, we cast distribution-free estimation as an optimization problem to find a distribution-specific estimator with a minimized mean-squared error (MSE), given only the sample.In our experiments, our search algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 93\\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\\% of the Good-Turing estimator's. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Tv36j85SqR@OpenReview",
      "index": 107,
      "title": "Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding",
      "authors": [
        "Eric Lei",
        "Hamed Hassani",
        "Shirin Saeedi Bidokhti"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "coding",
        "ltc",
        "quantization",
        "compression",
        "distortion",
        "lattice",
        "compressors",
        "optimal",
        "transform",
        "neural"
      ],
      "summary": "Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Tv36j85SqR"
        ],
        "venue": [
          "/venue/Tv36j85SqR@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Tv36j85SqR"
        ],
        "detail": [
          "https://openreview.net/forum?id=Tv36j85SqR"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Eric Lei , Hamed Hassani , Shirin Saeedi Bidokhti Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "TtUh0TOlGX@OpenReview",
      "index": 108,
      "title": "Regularization by Texts for Latent Diffusion Inverse Solvers",
      "authors": [
        "Jeongsol Kim",
        "Geon Yeong Park",
        "Hyungjin Chung",
        "Jong Chul YE"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "treg",
        "inverse",
        "diffusion",
        "ambiguities",
        "regularization",
        "latent",
        "descriptions",
        "preconceptions",
        "texts",
        "solvers"
      ],
      "summary": "The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TtUh0TOlGX"
        ],
        "venue": [
          "/venue/TtUh0TOlGX@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TtUh0TOlGX"
        ],
        "detail": [
          "https://openreview.net/forum?id=TtUh0TOlGX"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Regularization by Texts for Latent Diffusion Inverse Solvers [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jeongsol Kim , Geon Yeong Park , Hyungjin Chung , Jong Chul YE The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "TlAdgeoDTo@OpenReview",
      "index": 109,
      "title": "First-Person Fairness in Chatbots",
      "authors": [
        "Tyna Eloundou",
        "Alex Beutel",
        "David Robinson",
        "Keren Gu",
        "Anna-Luisa Brakman",
        "Pamela Mishkin",
        "Meghan Shah",
        "Johannes Heidecke",
        "Lilian Weng",
        "Adam Tauman Kalai"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gender",
        "chatbots",
        "chats",
        "fairness",
        "hasshown",
        "andalso",
        "name",
        "race",
        "introducea",
        "languagemodel"
      ],
      "summary": "Some chatbots have access to a user’s name when responding. Prior work hasshown that large language model outputs can change based on the demographictraits correlated with a name, such as gender or race. In this study, we introducea scalable method for studying one form of first-personfairness—fairness towards the user based on their demographic information—across a large and heterogeneous corpus of actual chats. We leverage a languagemodel as an AI “research assistant” (AI RA) that can privately and scalably analyzechat data, surfacing broader trends without exposing specific examples to theresearchers. We corroborate the labels of the AI RA with independent humanannotations, finding it highly consistent with human ratings of gender bias (less sofor racial bias). We apply this methodology to a large set of chats with a commercialchatbot. We assess overall quality of responses conditional on different names andalso subtle differences in similar-quality responses that may in aggregate reinforceharmful stereotypes based on gender or race. The largest detected biases are genderbiases in older generations of models and in open-ended tasks, like writing a story.Finally, evaluations like ours are important for monitoring and reducing biases.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TlAdgeoDTo"
        ],
        "venue": [
          "/venue/TlAdgeoDTo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TlAdgeoDTo"
        ],
        "detail": [
          "https://openreview.net/forum?id=TlAdgeoDTo"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 4
      },
      "raw_excerpt": "First-Person Fairness in Chatbots [PDF 1 ] [Copy] [Kimi 4 ] [REL] Authors : Tyna Eloundou , Alex Beutel , David Robinson , Keren Gu , Anna-Luisa Brakman , Pamela Mishkin , Meghan Shah , Johannes Heidecke , Lilian Weng , Adam Tauman Kalai Some chatbots have access to a user’s name when responding. Prior work hasshown that large language model outputs can change based on the demographictraits correlated with a name, such as gender or race. In this study, we introducea scalable method for studying one form of first-personfairness—fairness towards the user based on their demographic information—across a large and heterogeneous corpus of actual chats. We leverage a languagemodel as an AI “research assistant” (AI RA) that can privately and scalably analyzechat data, surfacing broader trends without exposing specific examples to theresearchers. We corroborate the labels of the AI RA with independent humanannotations, finding it highly consistent with human ratings of gender bias (less sofor racial bias). We apply this methodology to a large set of chats with a commercialchatbot. We assess overall quality of responses conditional on different names andalso subtle differences in similar-quality responses that may in aggregate reinforceharmful stereotypes based on gender or race. The largest detected biases are genderbiases in older generations of models and in open-ended tasks, like writing a story.Finally, evaluations like ours are important for monitoring and reducing biases. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "uREg3OHjLL@OpenReview",
      "index": 110,
      "title": "On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth",
      "authors": [
        "Gennadiy Averkov",
        "Christopher Hojny",
        "Maximilian Merkert"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "relu",
        "hertrich",
        "networks",
        "depth",
        "lceil",
        "rceil",
        "conjecture",
        "layers",
        "weights",
        "skutella"
      ],
      "summary": "To confirm that the expressive power of ReLU neural networks grows with their depth, the function F n = max ( 0 , x 1 , … , x n ) F n = max ( 0 , x 1 , … , x n ) has been considered in the literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents F n F n has at least ⌈ log 2 ( n + 1 ) ⌉ ⌈ log 2 ⁡ ( n + 1 ) ⌉ hidden layers. The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023]. We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, F n F n can only be represented by networks with at least ⌈ log 3 ( n + 1 ) ⌉ ⌈ log 3 ⁡ ( n + 1 ) ⌉ hidden layers. Moreover, if all weights are N N -ary fractions, then F n F n can only be represented by networks with at least Ω ( ln n ln ln N ) Ω ( ln ⁡ n ln ⁡ ln ⁡ N ) layers. These results are a partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the depth of practically relevant ReLU networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uREg3OHjLL"
        ],
        "venue": [
          "/venue/uREg3OHjLL@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uREg3OHjLL"
        ],
        "detail": [
          "https://openreview.net/forum?id=uREg3OHjLL"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 5
      },
      "raw_excerpt": "On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth [PDF 3 ] [Copy] [Kimi 5 ] [REL] Authors : Gennadiy Averkov , Christopher Hojny , Maximilian Merkert To confirm that the expressive power of ReLU neural networks grows with their depth, the function F n = max ( 0 , x 1 , … , x n ) F n = max ( 0 , x 1 , … , x n ) has been considered in the literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents F n F n has at least ⌈ log 2 ( n + 1 ) ⌉ ⌈ log 2 ⁡ ( n + 1 ) ⌉ hidden layers. The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023]. We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, F n F n can only be represented by networks with at least ⌈ log 3 ( n + 1 ) ⌉ ⌈ log 3 ⁡ ( n + 1 ) ⌉ hidden layers. Moreover, if all weights are N N -ary fractions, then F n F n can only be represented by networks with at least Ω ( ln n ln ln N ) Ω ( ln ⁡ n ln ⁡ ln ⁡ N ) layers. These results are a partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the depth of practically relevant ReLU networks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SqZ0KY4qBD@OpenReview",
      "index": 111,
      "title": "Attention with Markov: A Curious Case of Single-layer Transformers",
      "authors": [
        "Ashok Makkuva",
        "Marco Bondaschi",
        "Alliot Nagle",
        "Adway Girish",
        "Hyeji Kim",
        "Martin Jaggi",
        "Michael Gastpar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "transformers",
        "markov",
        "bigram",
        "layer",
        "minima",
        "curious",
        "unigram",
        "single",
        "chains",
        "attention"
      ],
      "summary": "Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena. Code is available at \\url{https://anonymous.4open.science/r/Attention-with-Markov-A617/}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SqZ0KY4qBD"
        ],
        "venue": [
          "/venue/SqZ0KY4qBD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SqZ0KY4qBD"
        ],
        "detail": [
          "https://openreview.net/forum?id=SqZ0KY4qBD"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 9
      },
      "raw_excerpt": "Attention with Markov: A Curious Case of Single-layer Transformers [PDF 7 ] [Copy] [Kimi 9 ] [REL] Authors : Ashok Makkuva , Marco Bondaschi , Alliot Nagle , Adway Girish , Hyeji Kim , Martin Jaggi , Michael Gastpar Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena. Code is available at \\url{https://anonymous.4open.science/r/Attention-with-Markov-A617/}. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "rDLgnYLM5b@OpenReview",
      "index": 112,
      "title": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment",
      "authors": [
        "Dongping Chen",
        "Ruoxi Chen",
        "Shu Pu",
        "Zhaoyi Liu",
        "Yanru Wu",
        "Caixi Chen",
        "Benlin Liu",
        "Yue Huang",
        "Yao Wan",
        "Pan Zhou",
        "Ranjay Krishna"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "isg",
        "interleaved",
        "image",
        "text",
        "bench",
        "vision",
        "holistic",
        "scene",
        "assessment",
        "responses"
      ],
      "summary": "Many real-world user queries (e.g. *\"How do to make egg fried rice?\"*) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook.Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities.To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback.In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels.To facilitate future work, we develop ISG-Agent, a baseline agent employing a *\"plan-execute-refine\"* pipeline to invoke tools, achieving a 122% performance improvement.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rDLgnYLM5b"
        ],
        "venue": [
          "/venue/rDLgnYLM5b@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rDLgnYLM5b"
        ],
        "detail": [
          "https://openreview.net/forum?id=rDLgnYLM5b"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Dongping Chen , Ruoxi Chen , Shu Pu , Zhaoyi Liu , Yanru Wu , Caixi Chen , Benlin Liu , Yue Huang , Yao Wan , Pan Zhou , Ranjay Krishna Many real-world user queries (e.g. *\"How do to make egg fried rice?\"*) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook.Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities.To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback.In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels.To facilitate future work, we develop ISG-Agent, a baseline agent employing a *\"plan-execute-refine\"* pipeline to invoke tools, achieving a 122% performance improvement. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SRpq5OBpED@OpenReview",
      "index": 113,
      "title": "Meta-Dynamical State Space Models for Integrative Neural Data Analysis",
      "authors": [
        "Ayesha Vermani",
        "Josue Nassar",
        "Hyungju Jeon",
        "Matthew Dowling",
        "Il Memming Park"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "recordings",
        "neural",
        "meta",
        "integrative",
        "learning",
        "across",
        "tasks",
        "dynamics",
        "dynamical",
        "shared"
      ],
      "summary": "Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings.Existing approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family ofrelated solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach onfew-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SRpq5OBpED"
        ],
        "venue": [
          "/venue/SRpq5OBpED@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SRpq5OBpED"
        ],
        "detail": [
          "https://openreview.net/forum?id=SRpq5OBpED"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Meta-Dynamical State Space Models for Integrative Neural Data Analysis [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Ayesha Vermani , Josue Nassar , Hyungju Jeon , Matthew Dowling , Il Memming Park Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings.Existing approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family ofrelated solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach onfew-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SOd07Qxkw4@OpenReview",
      "index": 114,
      "title": "Improved Convergence Rate for Diffusion Probabilistic Models",
      "authors": [
        "Gen Li",
        "Yuchen Jiao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "varepsilon",
        "diffusion",
        "log",
        "convergence",
        "lot",
        "models",
        "score",
        "shen2019therandomized",
        "gupta2024faster",
        "midpoint"
      ],
      "summary": "Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of d 1 / 3 ε − 2 / 3 d 1 / 3 ε − 2 / 3 , which is better than d 5 / 12 ε − 1 d 5 / 12 ε − 1 , the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling \\citep{Shen2019TheRandomized}, and then extended to diffusion models by \\citet{Gupta2024Faster}. Our theory accommodates ε ε -accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only O ( log 2 ( d / ε ) ) O ( log 2 ⁡ ( d / ε ) ) parallel rounds in a similar way to prior works.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SOd07Qxkw4"
        ],
        "venue": [
          "/venue/SOd07Qxkw4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SOd07Qxkw4"
        ],
        "detail": [
          "https://openreview.net/forum?id=SOd07Qxkw4"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Improved Convergence Rate for Diffusion Probabilistic Models [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Gen Li , Yuchen Jiao Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of d 1 / 3 ε − 2 / 3 d 1 / 3 ε − 2 / 3 , which is better than d 5 / 12 ε − 1 d 5 / 12 ε − 1 , the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling \\citep{Shen2019TheRandomized}, and then extended to diffusion models by \\citet{Gupta2024Faster}. Our theory accommodates ε ε -accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only O ( log 2 ( d / ε ) ) O ( log 2 ⁡ ( d / ε ) ) parallel rounds in a similar way to prior works. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SG1R2H3fa1@OpenReview",
      "index": 115,
      "title": "Revisiting Random Walks for Learning on Graphs",
      "authors": [
        "Jinwoo Kim",
        "Olga Zaghen",
        "Ayhan Suleymanzade",
        "Youngmin Ryou",
        "Seunghoon Hong"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "rwnns",
        "record",
        "random",
        "walk",
        "graph",
        "walks",
        "graphs",
        "message",
        "passing",
        "revisiting"
      ],
      "summary": "We revisit a recent model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We refer to these stochastic machines as random walk neural networks (RWNNs), and through principled analysis, show that we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability. A useful finding is that almost any kind of record of random walk guarantees probabilistic invariance as long as the vertices are anonymized. This enables us, for example, to record random walks in plain text and adopt a language model to read these text records to solve graph tasks. We further establish a parallelism to message passing neural networks using tools from Markov chain theory, and show that over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching. We empirically demonstrate RWNNs on a range of problems, verifying our theoretical analysis and demonstrating the use of language models for separating strongly regular graphs where the 3-WL test fails, and transductive classification on arXiv citation network.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SG1R2H3fa1"
        ],
        "venue": [
          "/venue/SG1R2H3fa1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SG1R2H3fa1"
        ],
        "detail": [
          "https://openreview.net/forum?id=SG1R2H3fa1"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 6
      },
      "raw_excerpt": "Revisiting Random Walks for Learning on Graphs [PDF 7 ] [Copy] [Kimi 6 ] [REL] Authors : Jinwoo Kim , Olga Zaghen , Ayhan Suleymanzade , Youngmin Ryou , Seunghoon Hong We revisit a recent model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We refer to these stochastic machines as random walk neural networks (RWNNs), and through principled analysis, show that we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability. A useful finding is that almost any kind of record of random walk guarantees probabilistic invariance as long as the vertices are anonymized. This enables us, for example, to record random walks in plain text and adopt a language model to read these text records to solve graph tasks. We further establish a parallelism to message passing neural networks using tools from Markov chain theory, and show that over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching. We empirically demonstrate RWNNs on a range of problems, verifying our theoretical analysis and demonstrating the use of language models for separating strongly regular graphs where the 3-WL test fails, and transductive classification on arXiv citation network. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SyVPiehSbg@OpenReview",
      "index": 116,
      "title": "Deep Learning Alternatives Of The Kolmogorov Superposition Theorem",
      "authors": [
        "Leonardo Ferreira Guilhoto",
        "Paris Perdikaris"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "kst",
        "actnet",
        "kolmogorov",
        "kans",
        "superposition",
        "formulation",
        "original",
        "deep",
        "theorem",
        "alternatives"
      ],
      "summary": "This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SyVPiehSbg"
        ],
        "venue": [
          "/venue/SyVPiehSbg@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SyVPiehSbg"
        ],
        "detail": [
          "https://openreview.net/forum?id=SyVPiehSbg"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Deep Learning Alternatives Of The Kolmogorov Superposition Theorem [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Leonardo Ferreira Guilhoto , Paris Perdikaris This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "vi3DjUhFVm@OpenReview",
      "index": 117,
      "title": "Test-time Alignment of Diffusion Models without Reward Over-optimization",
      "authors": [
        "Sunwoo Kim",
        "Minkyu Kim",
        "Dongmin Park"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "diffusion",
        "optimization",
        "krafton",
        "aligning",
        "rewards",
        "objectives",
        "target",
        "test",
        "tuning"
      ],
      "summary": "Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free, test-time method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at https://github.com/krafton-ai/DAS.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vi3DjUhFVm"
        ],
        "venue": [
          "/venue/vi3DjUhFVm@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vi3DjUhFVm"
        ],
        "detail": [
          "https://openreview.net/forum?id=vi3DjUhFVm"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 8
      },
      "raw_excerpt": "Test-time Alignment of Diffusion Models without Reward Over-optimization [PDF 4 ] [Copy] [Kimi 8 ] [REL] Authors : Sunwoo Kim , Minkyu Kim , Dongmin Park Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free, test-time method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at https://github.com/krafton-ai/DAS. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "QOfswj7hij@OpenReview",
      "index": 118,
      "title": "Online Neuro-Symbolic Predicate Invention for High-Level Planning",
      "authors": [
        "Yichao Liang",
        "Nishanth Kumar",
        "Hao Tang",
        "Adrian Weller",
        "Joshua B Tenenbaum",
        "Tom Silver",
        "Joao F. Henriques",
        "Kevin Ellis"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "symbolic",
        "invention",
        "neuro",
        "predicate",
        "predicates",
        "online",
        "planning",
        "inventing",
        "abstracting",
        "sensorimotor"
      ],
      "summary": "Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QOfswj7hij"
        ],
        "venue": [
          "/venue/QOfswj7hij@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QOfswj7hij"
        ],
        "detail": [
          "https://openreview.net/forum?id=QOfswj7hij"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Online Neuro-Symbolic Predicate Invention for High-Level Planning [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Yichao Liang , Nishanth Kumar , Hao Tang , Adrian Weller , Joshua B Tenenbaum , Tom Silver , Joao F. Henriques , Kevin Ellis Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "tIBAOcAvn4@OpenReview",
      "index": 119,
      "title": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors",
      "authors": [
        "Chen Ma",
        "Xinjie Xu",
        "Shuyu Cheng",
        "Qi Xuan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "priors",
        "gradient",
        "ray",
        "search",
        "hard",
        "label",
        "query",
        "attacks",
        "estimators",
        "transfer"
      ],
      "summary": "One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only top-1 predicted labels are available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the ℓ p ℓ p norm distance to the adversarial region. The unique advantage of this approach is that it transforms the hard-label attack into a continuous optimization problem. The objective function value is the ray's radius and can be obtained through a binary search with high query cost. Existing methods use a \"sign trick\" in gradient estimation to reduce queries. In this paper, we theoretically analyze the quality of this gradient estimation, proposing a novel prior-guided approach to improve ray search efficiency, based on theoretical and experimental analysis. Specifically, we utilize the transfer-based priors from surrogate models, and our gradient estimators appropriately integrate them by approximating the projection of the true gradient onto the subspace spanned by these priors and some random directions, in a query-efficient way. We theoretically derive the expected cosine similarity between the obtained gradient estimators and the true gradient, and demonstrate the improvement brought by using priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that our approach significantly outperforms 11 state-of-the-art methods in query efficiency. Code will be released.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tIBAOcAvn4"
        ],
        "venue": [
          "/venue/tIBAOcAvn4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tIBAOcAvn4"
        ],
        "detail": [
          "https://openreview.net/forum?id=tIBAOcAvn4"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Chen Ma , Xinjie Xu , Shuyu Cheng , Qi Xuan One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only top-1 predicted labels are available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the ℓ p ℓ p norm distance to the adversarial region. The unique advantage of this approach is that it transforms the hard-label attack into a continuous optimization problem. The objective function value is the ray's radius and can be obtained through a binary search with high query cost. Existing methods use a \"sign trick\" in gradient estimation to reduce queries. In this paper, we theoretically analyze the quality of this gradient estimation, proposing a novel prior-guided approach to improve ray search efficiency, based on theoretical and experimental analysis. Specifically, we utilize the transfer-based priors from surrogate models, and our gradient estimators appropriately integrate them by approximating the projection of the true gradient onto the subspace spanned by these priors and some random directions, in a query-efficient way. We theoretically derive the expected cosine similarity between the obtained gradient estimators and the true gradient, and demonstrate the improvement brought by using priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that our approach significantly outperforms 11 state-of-the-art methods in query efficiency. Code will be released. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Q0zmmNNePz@OpenReview",
      "index": 120,
      "title": "Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation",
      "authors": [
        "Laurin Lux",
        "Alexander H Berger",
        "Alexander Weers",
        "Nico Stucki",
        "Daniel Rueckert",
        "Ulrich Bauer",
        "Johannes Paetzold"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "topological",
        "topograph",
        "segmentation",
        "graph",
        "topology",
        "loss",
        "topologically",
        "image",
        "guarantees",
        "fivefold"
      ],
      "summary": "Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets, demonstrating state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Q0zmmNNePz"
        ],
        "venue": [
          "/venue/Q0zmmNNePz@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Q0zmmNNePz"
        ],
        "detail": [
          "https://openreview.net/forum?id=Q0zmmNNePz"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Laurin Lux , Alexander H Berger , Alexander Weers , Nico Stucki , Daniel Rueckert , Ulrich Bauer , Johannes Paetzold Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets, demonstrating state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "PpYy0dR3Qw@OpenReview",
      "index": 121,
      "title": "LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression",
      "authors": [
        "Laurent Condat",
        "Artavazd Maranjyan",
        "Peter Richtarik"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "locodl",
        "communication",
        "mpression",
        "istributed",
        "compression",
        "bitstreams",
        "floats",
        "training",
        "earning",
        "compressors"
      ],
      "summary": "In D D istributed optimization and L L earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of L o L o cal training, which reduces the communication frequency, and C o C o mpression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogeneous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PpYy0dR3Qw"
        ],
        "venue": [
          "/venue/PpYy0dR3Qw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PpYy0dR3Qw"
        ],
        "detail": [
          "https://openreview.net/forum?id=PpYy0dR3Qw"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Laurent Condat , Artavazd Maranjyan , Peter Richtarik In D D istributed optimization and L L earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of L o L o cal training, which reduces the communication frequency, and C o C o mpression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogeneous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "PkpNRmBZ32@OpenReview",
      "index": 122,
      "title": "Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions",
      "authors": [
        "Yan Ru Pei"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ssm",
        "blocks",
        "contractions",
        "centaurus",
        "convolutions",
        "tensor",
        "ssms",
        "convnets",
        "network",
        "asr"
      ],
      "summary": "We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PkpNRmBZ32"
        ],
        "venue": [
          "/venue/PkpNRmBZ32@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PkpNRmBZ32"
        ],
        "detail": [
          "https://openreview.net/forum?id=PkpNRmBZ32"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 5
      },
      "raw_excerpt": "Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions [PDF 8 ] [Copy] [Kimi 5 ] [REL] Author : Yan Ru Pei We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wHebuIb6IH@OpenReview",
      "index": 123,
      "title": "VLMaterial: Procedural Material Generation with Large Vision-Language Models",
      "authors": [
        "Beichen Li",
        "Rundi Wu",
        "Armando Solar-Lezama",
        "Liang Shi",
        "Changxi Zheng",
        "Bernd Bickel",
        "Wojciech Matusik"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "procedural",
        "material",
        "vlm",
        "vlmaterial",
        "vision",
        "programs",
        "language",
        "materials",
        "photorealistic",
        "perform"
      ],
      "summary": "Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another VLM. Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wHebuIb6IH"
        ],
        "venue": [
          "/venue/wHebuIb6IH@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wHebuIb6IH"
        ],
        "detail": [
          "https://openreview.net/forum?id=wHebuIb6IH"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "VLMaterial: Procedural Material Generation with Large Vision-Language Models [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Beichen Li , Rundi Wu , Armando Solar-Lezama , Liang Shi , Changxi Zheng , Bernd Bickel , Wojciech Matusik Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another VLM. Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "PUnD86UEK5@OpenReview",
      "index": 124,
      "title": "Adam Exploits ℓ ∞ ℓ ∞ -geometry of Loss Landscape via Coordinate-wise Adaptivity",
      "authors": [
        "Shuo Xie",
        "Mohamad Amin Mohamadi",
        "Zhiyuan Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "adam",
        "sgd",
        "ell",
        "loss",
        "smoothness",
        "infty",
        "geometry",
        "adaptivity",
        "convergence",
        "analysis"
      ],
      "summary": "Adam outperforms SGD when training language models. Yet such benefits are not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps T T and is already minimax-optimal in non-convex cases, which are both O ( T − 1 / 4 ) O ( T − 1 / 4 ) . In this work, we argue that the better dependence on the loss smoothness is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under ℓ ∞ ℓ ∞ geometry rather than the more common ℓ 2 ℓ 2 geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Moreover, we show that if we rotate the training loss randomly, Adam can be outperformed by some variants of SGD which is invariant to rotations. This implies that any practically relevant explanation of Adam's optimization benefit must involve non-rotational invariant properties of loss, such as ℓ ∞ ℓ ∞ smoothness as used in our analysis. We also extend the convergence analysis to blockwise Adam, which is a generalization of standard Adam.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PUnD86UEK5"
        ],
        "venue": [
          "/venue/PUnD86UEK5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PUnD86UEK5"
        ],
        "detail": [
          "https://openreview.net/forum?id=PUnD86UEK5"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 6
      },
      "raw_excerpt": "Adam Exploits ℓ ∞ ℓ ∞ -geometry of Loss Landscape via Coordinate-wise Adaptivity [PDF 6 ] [Copy] [Kimi 6 ] [REL] Authors : Shuo Xie , Mohamad Amin Mohamadi , Zhiyuan Li Adam outperforms SGD when training language models. Yet such benefits are not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps T T and is already minimax-optimal in non-convex cases, which are both O ( T − 1 / 4 ) O ( T − 1 / 4 ) . In this work, we argue that the better dependence on the loss smoothness is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under ℓ ∞ ℓ ∞ geometry rather than the more common ℓ 2 ℓ 2 geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Moreover, we show that if we rotate the training loss randomly, Adam can be outperformed by some variants of SGD which is invariant to rotations. This implies that any practically relevant explanation of Adam's optimization benefit must involve non-rotational invariant properties of loss, such as ℓ ∞ ℓ ∞ smoothness as used in our analysis. We also extend the convergence analysis to blockwise Adam, which is a generalization of standard Adam. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "j4LITBSUjs@OpenReview",
      "index": 125,
      "title": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training",
      "authors": [
        "Cong Chen",
        "Mingyu Liu",
        "Chenchen Jing",
        "Yizhou Zhou",
        "Fengyun Rao",
        "Hao Chen",
        "Bo Zhang",
        "Chunhua Shen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hallucinations",
        "perturbollava",
        "multimodal",
        "language",
        "captions",
        "agranular",
        "reliance",
        "reducing",
        "visual",
        "dense"
      ],
      "summary": "This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce HalFscore, a novel metric built upon the language graph and is designed to evaluate both the accuracy and completeness of dense captions at agranular level. Additionally, we identify the root cause of hallucination as the model's over-reliance on its language prior. To address this, we propose PerturboLLaVA, which reduces the model's reliance on the language prior by incorporating adversarially perturbed text during training. This method enhances the model's focus on visual inputs, effectively reducing hallucinations and producing accurate, image-grounded descriptions without incurring additional computational overhead. PerturboLLaVA significantly improves the fidelity of generated captions, outperforming existing approaches in handling multimodal hallucinations and achieving improved performance across general multimodal benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=j4LITBSUjs"
        ],
        "venue": [
          "/venue/j4LITBSUjs@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=j4LITBSUjs"
        ],
        "detail": [
          "https://openreview.net/forum?id=j4LITBSUjs"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 12
      },
      "raw_excerpt": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training [PDF 15 ] [Copy] [Kimi 12 ] [REL] Authors : Cong Chen , Mingyu Liu , Chenchen Jing , Yizhou Zhou , Fengyun Rao , Hao Chen , Bo Zhang , Chunhua Shen This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce HalFscore, a novel metric built upon the language graph and is designed to evaluate both the accuracy and completeness of dense captions at agranular level. Additionally, we identify the root cause of hallucination as the model's over-reliance on its language prior. To address this, we propose PerturboLLaVA, which reduces the model's reliance on the language prior by incorporating adversarially perturbed text during training. This method enhances the model's focus on visual inputs, effectively reducing hallucinations and producing accurate, image-grounded descriptions without incurring additional computational overhead. PerturboLLaVA significantly improves the fidelity of generated captions, outperforming existing approaches in handling multimodal hallucinations and achieving improved performance across general multimodal benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qgsXsqahMq@OpenReview",
      "index": 126,
      "title": "GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks",
      "authors": [
        "Dingyi Zhuang",
        "Chonghe Jiang",
        "Yunhan Zheng",
        "Shenhao Wang",
        "Jinhua Zhao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "calibration",
        "gets",
        "graph",
        "ensemble",
        "temperature",
        "scaling",
        "gnn",
        "underconfidence",
        "input",
        "experts"
      ],
      "summary": "Graph Neural Networks (GNNs) deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high-stakes applications where accurate uncertainty estimates are essential. Existing post-hoc methods, such as temperature scaling, fail to effectively utilize graph structures, while current GNN calibration methods often overlook the potential of leveraging diverse input information and model ensembles jointly. In the paper, we propose Graph Ensemble Temperature Scaling (GETS), a novel calibration framework that combines input and model ensemble strategies within a Graph Mixture-of-Experts (MoE) architecture. GETS integrates diverse inputs, including logits, node features, and degree embeddings, and adaptively selects the most relevant experts for each node’s calibration procedure. Our method outperforms state-of-the-art calibration techniques, reducing expected calibration error (ECE) by ≥ ≥ 25% across 10 GNN benchmark datasets. Additionally, GETS is computationally efficient, scalable, and capable of selecting effective input combinations for improved calibration performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qgsXsqahMq"
        ],
        "venue": [
          "/venue/qgsXsqahMq@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qgsXsqahMq"
        ],
        "detail": [
          "https://openreview.net/forum?id=qgsXsqahMq"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 2
      },
      "raw_excerpt": "GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks [PDF 9 ] [Copy] [Kimi 2 ] [REL] Authors : Dingyi Zhuang , Chonghe Jiang , Yunhan Zheng , Shenhao Wang , Jinhua Zhao Graph Neural Networks (GNNs) deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high-stakes applications where accurate uncertainty estimates are essential. Existing post-hoc methods, such as temperature scaling, fail to effectively utilize graph structures, while current GNN calibration methods often overlook the potential of leveraging diverse input information and model ensembles jointly. In the paper, we propose Graph Ensemble Temperature Scaling (GETS), a novel calibration framework that combines input and model ensemble strategies within a Graph Mixture-of-Experts (MoE) architecture. GETS integrates diverse inputs, including logits, node features, and degree embeddings, and adaptively selects the most relevant experts for each node’s calibration procedure. Our method outperforms state-of-the-art calibration techniques, reducing expected calibration error (ECE) by ≥ ≥ 25% across 10 GNN benchmark datasets. Additionally, GETS is computationally efficient, scalable, and capable of selecting effective input combinations for improved calibration performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ujpAYpFDEA@OpenReview",
      "index": 127,
      "title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?",
      "authors": [
        "Aiwei Liu",
        "Sheng Guan",
        "Yiming Liu",
        "Leyi Pan",
        "Yifei Zhang",
        "Liancheng Fang",
        "Lijie Wen",
        "Philip Yu",
        "Xuming Hu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "watermarked",
        "watermark",
        "imperceptibility",
        "llms",
        "watermarking",
        "prompts",
        "watermarks",
        "llm",
        "key",
        "water"
      ],
      "summary": "Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services. This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ujpAYpFDEA"
        ],
        "venue": [
          "/venue/ujpAYpFDEA@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ujpAYpFDEA"
        ],
        "detail": [
          "https://openreview.net/forum?id=ujpAYpFDEA"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 7
      },
      "raw_excerpt": "Can Watermarked LLMs be Identified by Users via Crafted Prompts? [PDF 7 ] [Copy] [Kimi 7 ] [REL] Authors : Aiwei Liu , Sheng Guan , Yiming Liu , Leyi Pan , Yifei Zhang , Liancheng Fang , Lijie Wen , Philip Yu , Xuming Hu Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services. This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "P7KRIiLM8T@OpenReview",
      "index": 128,
      "title": "u- μ μ P: The Unit-Scaled Maximal Update Parametrization",
      "authors": [
        "Charles Blake",
        "Constantin Eichenberg",
        "Josef Dean",
        "Lukas Balles",
        "Luke Prince",
        "Björn Deiseroth",
        "Andres Felipe Cruz Salinas",
        "Carlo Luschi",
        "Samuel Weinbach",
        "Douglas Orr"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "unit",
        "parametrization",
        "update",
        "activations",
        "maximal",
        "size",
        "ensures",
        "fp8",
        "hps",
        "scaled"
      ],
      "summary": "The Maximal Update Parametrization ( μ μ P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u- μ μ P, which improves upon μ μ P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: μ μ P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u- μ μ P models reaching a lower loss than comparable μ μ P models and working out-of-the-box in FP8.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=P7KRIiLM8T"
        ],
        "venue": [
          "/venue/P7KRIiLM8T@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=P7KRIiLM8T"
        ],
        "detail": [
          "https://openreview.net/forum?id=P7KRIiLM8T"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "u- μ μ P: The Unit-Scaled Maximal Update Parametrization [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Charles Blake , Constantin Eichenberg , Josef Dean , Lukas Balles , Luke Prince , Björn Deiseroth , Andres Felipe Cruz Salinas , Carlo Luschi , Samuel Weinbach , Douglas Orr The Maximal Update Parametrization ( μ μ P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u- μ μ P, which improves upon μ μ P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: μ μ P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u- μ μ P models reaching a lower loss than comparable μ μ P models and working out-of-the-box in FP8. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "P42DbV2nuV@OpenReview",
      "index": 129,
      "title": "Instance-dependent Early Stopping",
      "authors": [
        "Suqin Yuan",
        "Runqi Lin",
        "Lei Feng",
        "Bo Han",
        "Tongliang Liu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "stopping",
        "instance",
        "ies",
        "mastered",
        "early",
        "backpropagation",
        "instances",
        "training",
        "loss",
        "dependent"
      ],
      "summary": "In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=P42DbV2nuV"
        ],
        "venue": [
          "/venue/P42DbV2nuV@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=P42DbV2nuV"
        ],
        "detail": [
          "https://openreview.net/forum?id=P42DbV2nuV"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 8
      },
      "raw_excerpt": "Instance-dependent Early Stopping [PDF 8 ] [Copy] [Kimi 8 ] [REL] Authors : Suqin Yuan , Runqi Lin , Lei Feng , Bo Han , Tongliang Liu In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "oCHsDpyawq@OpenReview",
      "index": 130,
      "title": "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish",
      "authors": [
        "Jan-Matthis Lueckmann",
        "Alexander Immer",
        "Alex Chen",
        "Peter Li",
        "Mariela Petkova",
        "Nirmala Iyer",
        "Luuk Hesselink",
        "Aparna Dev",
        "Gudrun Ihrke",
        "Woohyun Park",
        "Alyson Petruncio",
        "Aubrey Weigel",
        "Wyatt Korff",
        "Florian Engert",
        "Jeff Lichtman",
        "Misha Ahrens",
        "Michal Januszewski",
        "Viren Jain"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "zebrafish",
        "brain",
        "zapbench",
        "activity",
        "benchmark",
        "forecasting",
        "larval",
        "prediction",
        "progress",
        "vertebrate"
      ],
      "summary": "Data-driven benchmarks have led to significant progress in key scientific modeling domains including weather and structural biology. Here, we introduce the Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on the problem of predicting cellular-resolution neural activity throughout an entire vertebrate brain. The benchmark is based on a novel dataset containing 4d light-sheet microscopy recordings of over 70,000 neurons in a larval zebrafish brain, along with motion stabilized and voxel-level cell segmentations of these data that facilitate development of a variety of forecasting methods. Initial results from a selection of time series and volumetric video modeling approaches achieve better performance than naive baseline methods, but also show room for further improvement. The specific brain used in the activity recording is also undergoing synaptic-level anatomical mapping, which will enable future integration of detailed structural information into forecasting methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oCHsDpyawq"
        ],
        "venue": [
          "/venue/oCHsDpyawq@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oCHsDpyawq"
        ],
        "detail": [
          "https://openreview.net/forum?id=oCHsDpyawq"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Jan-Matthis Lueckmann , Alexander Immer , Alex Chen , Peter Li , Mariela Petkova , Nirmala Iyer , Luuk Hesselink , Aparna Dev , Gudrun Ihrke , Woohyun Park , Alyson Petruncio , Aubrey Weigel , Wyatt Korff , Florian Engert , Jeff Lichtman , Misha Ahrens , Michal Januszewski , Viren Jain Data-driven benchmarks have led to significant progress in key scientific modeling domains including weather and structural biology. Here, we introduce the Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on the problem of predicting cellular-resolution neural activity throughout an entire vertebrate brain. The benchmark is based on a novel dataset containing 4d light-sheet microscopy recordings of over 70,000 neurons in a larval zebrafish brain, along with motion stabilized and voxel-level cell segmentations of these data that facilitate development of a variety of forecasting methods. Initial results from a selection of time series and volumetric video modeling approaches achieve better performance than naive baseline methods, but also show room for further improvement. The specific brain used in the activity recording is also undergoing synaptic-level anatomical mapping, which will enable future integration of detailed structural information into forecasting methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "U3PBITXNG6@OpenReview",
      "index": 131,
      "title": "InverseBench: Benchmarking Plug-and-Play Diffusion Models for Scientific Inverse Problems",
      "authors": [
        "Hongkai Zheng",
        "Wenda Chu",
        "Bingliang Zhang",
        "Zihui Wu",
        "Austin Wang",
        "Berthy Feng",
        "Caifeng Zou",
        "Yu Sun",
        "Nikola Kovachki",
        "Zachary Ross",
        "Katherine Bouman",
        "Yisong Yue"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "inversebench",
        "inverse",
        "plug",
        "scientific",
        "diffusion",
        "play",
        "problems",
        "textsc",
        "algorithms",
        "benchmarking"
      ],
      "summary": "Plug-and-play diffusion prior methods have emerged as a promising research direction for solving inverse problems. However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \\textsc{InverseBench}, a unified framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as black hole imaging, seismology, optical tomography, medical imaging, and fluid dynamics. With \\textsc{InverseBench}, we benchmark 15 inverse problem algorithms that use plug-and-play diffusion prior methods against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. We open-source the datasets, pre-trained models, and the codebase to facilitate future research and development.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U3PBITXNG6"
        ],
        "venue": [
          "/venue/U3PBITXNG6@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U3PBITXNG6"
        ],
        "detail": [
          "https://openreview.net/forum?id=U3PBITXNG6"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "InverseBench: Benchmarking Plug-and-Play Diffusion Models for Scientific Inverse Problems [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Hongkai Zheng , Wenda Chu , Bingliang Zhang , Zihui Wu , Austin Wang , Berthy Feng , Caifeng Zou , Yu Sun , Nikola Kovachki , Zachary Ross , Katherine Bouman , Yisong Yue Plug-and-play diffusion prior methods have emerged as a promising research direction for solving inverse problems. However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \\textsc{InverseBench}, a unified framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as black hole imaging, seismology, optical tomography, medical imaging, and fluid dynamics. With \\textsc{InverseBench}, we benchmark 15 inverse problem algorithms that use plug-and-play diffusion prior methods against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. We open-source the datasets, pre-trained models, and the codebase to facilitate future research and development. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Oi47wc10sm@OpenReview",
      "index": 132,
      "title": "Programming Refusal with Conditional Activation Steering",
      "authors": [
        "Bruce Lee",
        "Inkit Padhi",
        "Karthikeyan Natesan Ramamurthy",
        "Erik Miehling",
        "Pierre Dognin",
        "Manish Nagireddy",
        "Amit Dhurandhar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "activation",
        "steering",
        "content",
        "refuse",
        "llm",
        "responses",
        "refusal",
        "cast",
        "conditional",
        "behavior"
      ],
      "summary": "LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging.Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants.In this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context.Our method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states.Using CAST, one can systematically control LLM behavior with rules like \"if input is about hate speech or adult content, then refuse\" or \"if input is not about legal advice, then refuse.\"This allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization.We release an open-source implementation of our framework at <placeholder: open-source GitHub link>.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Oi47wc10sm"
        ],
        "venue": [
          "/venue/Oi47wc10sm@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Oi47wc10sm"
        ],
        "detail": [
          "https://openreview.net/forum?id=Oi47wc10sm"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 7
      },
      "raw_excerpt": "Programming Refusal with Conditional Activation Steering [PDF 3 ] [Copy] [Kimi 7 ] [REL] Authors : Bruce Lee , Inkit Padhi , Karthikeyan Natesan Ramamurthy , Erik Miehling , Pierre Dognin , Manish Nagireddy , Amit Dhurandhar LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging.Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants.In this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context.Our method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states.Using CAST, one can systematically control LLM behavior with rules like \"if input is about hate speech or adult content, then refuse\" or \"if input is not about legal advice, then refuse.\"This allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization.We release an open-source implementation of our framework at <placeholder: open-source GitHub link>. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qtWjSboqfe@OpenReview",
      "index": 133,
      "title": "DEEM: Diffusion models serve as the eyes of large language models for image perception",
      "authors": [
        "Run Luo",
        "Yunshui Li",
        "Longze Chen",
        "Wanwei He",
        "Ting-En Lin",
        "Ziqiang Liu",
        "Lei Zhang",
        "Zikai Song",
        "Hamid Alinejad-Rokny",
        "Xiaobo Xia",
        "Tongliang Liu",
        "Min Yang",
        "Binyuan Hui"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "deem",
        "robustvqa",
        "perception",
        "lmms",
        "mmvp",
        "image",
        "models",
        "eyes",
        "pope",
        "diffusion"
      ],
      "summary": "The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMM's visual perception performance to a large extent (e.g., 4\\% ↑ on RobustVQA, 6.5\\% ↑ on MMVP and 12.8 \\% ↑ on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\\%), and a smaller base model size. Extensive experiments demonstrate that DEEM enhances the performance of LMMs on various downstream tasks without inferior performance in the long term, including visual question answering, image captioning, and text-conditioned image synthesis.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qtWjSboqfe"
        ],
        "venue": [
          "/venue/qtWjSboqfe@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qtWjSboqfe"
        ],
        "detail": [
          "https://openreview.net/forum?id=qtWjSboqfe"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 8
      },
      "raw_excerpt": "DEEM: Diffusion models serve as the eyes of large language models for image perception [PDF 11 ] [Copy] [Kimi 8 ] [REL] Authors : Run Luo , Yunshui Li , Longze Chen , Wanwei He , Ting-En Lin , Ziqiang Liu , Lei Zhang , Zikai Song , Hamid Alinejad-Rokny , Xiaobo Xia , Tongliang Liu , Min Yang , Binyuan Hui The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMM's visual perception performance to a large extent (e.g., 4\\% ↑ on RobustVQA, 6.5\\% ↑ on MMVP and 12.8 \\% ↑ on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\\%), and a smaller base model size. Extensive experiments demonstrate that DEEM enhances the performance of LMMs on various downstream tasks without inferior performance in the long term, including visual question answering, image captioning, and text-conditioned image synthesis. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "yVeNBxwL5W@OpenReview",
      "index": 134,
      "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
      "authors": [
        "Ao Li",
        "Wei Fang",
        "Hongbo Zhao",
        "Le Lu",
        "Minfeng Xu",
        "Ge Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "diffusion",
        "sde",
        "sampler",
        "nfes",
        "mrs",
        "reverting",
        "controllable",
        "ode",
        "generation",
        "quality"
      ],
      "summary": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yVeNBxwL5W"
        ],
        "venue": [
          "/venue/yVeNBxwL5W@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yVeNBxwL5W"
        ],
        "detail": [
          "https://openreview.net/forum?id=yVeNBxwL5W"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Ao Li , Wei Fang , Hongbo Zhao , Le Lu , Minfeng Xu , Ge Yang In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "OZbFRNhpwr@OpenReview",
      "index": 135,
      "title": "SPA-BENCH: A COMPREHENSIVE BENCHMARK FOR SMARTPHONE AGENT EVALUATION",
      "authors": [
        "Jingxuan Chen",
        "Derek Yuen",
        "Bin Xie",
        "Yuhao Yang",
        "Gongwei Chen",
        "Zhihao Wu",
        "Li Yixing",
        "Xurui Zhou",
        "Weiwen Liu",
        "Shuai Wang",
        "Kaiwen Zhou",
        "Rui Shao",
        "Liqiang Nie",
        "Yasheng Wang",
        "Jianye HAO",
        "Jun Wang",
        "Kun Shao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "smartphone",
        "spa",
        "agents",
        "bench",
        "agent",
        "evaluation",
        "comprehensive",
        "benchmark",
        "contenders",
        "generalisable"
      ],
      "summary": "Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OZbFRNhpwr"
        ],
        "venue": [
          "/venue/OZbFRNhpwr@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OZbFRNhpwr"
        ],
        "detail": [
          "https://openreview.net/forum?id=OZbFRNhpwr"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 7
      },
      "raw_excerpt": "SPA-BENCH: A COMPREHENSIVE BENCHMARK FOR SMARTPHONE AGENT EVALUATION [PDF 8 ] [Copy] [Kimi 7 ] [REL] Authors : Jingxuan Chen , Derek Yuen , Bin Xie , Yuhao Yang , Gongwei Chen , Zhihao Wu , Li Yixing , Xurui Zhou , Weiwen Liu , Shuai Wang , Kaiwen Zhou , Rui Shao , Liqiang Nie , Yasheng Wang , Jianye HAO , Jun Wang , Kun Shao Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "k3gCieTXeY@OpenReview",
      "index": 136,
      "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
      "authors": [
        "Angelika Romanou",
        "Negar Foroutan",
        "Anna Sotnikova",
        "Sree Harsha Nelaturu",
        "Shivalika Singh",
        "Rishabh Maheshwary",
        "Micol Altomare",
        "Zeming Chen",
        "Mohamed Haggag",
        "Snegha A",
        "Alfonso Amayuelas",
        "Azril Hafizi Amirudin",
        "Danylo Boiko",
        "Michael Chang",
        "Jenny Chim",
        "Gal Cohen",
        "Aditya K Dalmia",
        "Abraham Diress",
        "Sharad Duwal",
        "Daniil Dzenhaliou",
        "Daniel Florez",
        "Fabian Farestam",
        "Joseph Marvin Imperial",
        "Shayekh Islam",
        "Perttu Isotalo",
        "Maral Jabbarishiviari",
        "Börje Karlsson",
        "Eldar Khalilov",
        "Christopher Klamm",
        "Fajri Koto",
        "Dominik Krzemiński",
        "Gabriel de Melo",
        "Syrielle Montariol",
        "Yiyang Nan",
        "Joel Niklaus",
        "Jekaterina Novikova",
        "Johan S Obando Ceron",
        "Debjit Paul",
        "Esther Ploeger",
        "Jebish Purbey",
        "Swati Rajwal",
        "Selvan Sunitha Ravi",
        "Sara Rydell",
        "Roshan Santhosh",
        "Drishti Sharma",
        "Marjana Prifti Skenduli",
        "Arshia Soltani Moakhar",
        "Bardia moakhar",
        "Ayush Tarun",
        "Azmine Toushik Wasi",
        "Thenuka Weerasinghe",
        "Serhan Yilmaz",
        "Mike Zhang",
        "Imanol Schlag",
        "Marzieh Fadaee",
        "Sara Hooker",
        "Antoine Bosselut"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "multilingual",
        "regional",
        "languages",
        "llms",
        "language",
        "knowledge",
        "english",
        "include",
        "bottlenecked",
        "resources"
      ],
      "summary": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts.Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k3gCieTXeY"
        ],
        "venue": [
          "/venue/k3gCieTXeY@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k3gCieTXeY"
        ],
        "detail": [
          "https://openreview.net/forum?id=k3gCieTXeY"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Angelika Romanou , Negar Foroutan , Anna Sotnikova , Sree Harsha Nelaturu , Shivalika Singh , Rishabh Maheshwary , Micol Altomare , Zeming Chen , Mohamed Haggag , Snegha A , Alfonso Amayuelas , Azril Hafizi Amirudin , Danylo Boiko , Michael Chang , Jenny Chim , Gal Cohen , Aditya K Dalmia , Abraham Diress , Sharad Duwal , Daniil Dzenhaliou , Daniel Florez , Fabian Farestam , Joseph Marvin Imperial , Shayekh Islam , Perttu Isotalo , Maral Jabbarishiviari , Börje Karlsson , Eldar Khalilov , Christopher Klamm , Fajri Koto , Dominik Krzemiński , Gabriel de Melo , Syrielle Montariol , Yiyang Nan , Joel Niklaus , Jekaterina Novikova , Johan S Obando Ceron , Debjit Paul , Esther Ploeger , Jebish Purbey , Swati Rajwal , Selvan Sunitha Ravi , Sara Rydell , Roshan Santhosh , Drishti Sharma , Marjana Prifti Skenduli , Arshia Soltani Moakhar , Bardia moakhar , Ayush Tarun , Azmine Toushik Wasi , Thenuka Weerasinghe , Serhan Yilmaz , Mike Zhang , Imanol Schlag , Marzieh Fadaee , Sara Hooker , Antoine Bosselut et al. (27 additional authors not shown) The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts.Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "jkUp3lybXf@OpenReview",
      "index": 137,
      "title": "Preference Optimization for Reasoning with Pseudo Feedback",
      "authors": [
        "Fangkai Jiao",
        "Geyang Guo",
        "Xingxing Zhang",
        "Nancy F Chen",
        "Shafiq Joty",
        "Furu Wei"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "reasoning",
        "preference",
        "pseudo",
        "feedback",
        "optimization",
        "tasks",
        "surpassing",
        "livecodebench",
        "haiku",
        "coding"
      ],
      "summary": "Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited.In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \\emph{test cases}. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case.We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jkUp3lybXf"
        ],
        "venue": [
          "/venue/jkUp3lybXf@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jkUp3lybXf"
        ],
        "detail": [
          "https://openreview.net/forum?id=jkUp3lybXf"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 9
      },
      "raw_excerpt": "Preference Optimization for Reasoning with Pseudo Feedback [PDF 10 ] [Copy] [Kimi 9 ] [REL] Authors : Fangkai Jiao , Geyang Guo , Xingxing Zhang , Nancy F Chen , Shafiq Joty , Furu Wei Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited.In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \\emph{test cases}. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case.We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Nx4PMtJ1ER@OpenReview",
      "index": 138,
      "title": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes",
      "authors": [
        "Georg Manten",
        "Cecilia Casolo",
        "Emilio Ferrucci",
        "Søren Mogensen",
        "Cristopher Salvi",
        "Niki Kilbertus"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "causal",
        "discovery",
        "sde",
        "signature",
        "independence",
        "stochastic",
        "processes",
        "kernel",
        "conditional",
        "ancestral"
      ],
      "summary": "Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via `which variables enter the differential of which other variables'. In this paper, we develop conditional independence (CI) constraints on coordinate processes over selected intervals that are Markov with respect to the acyclic dependence graph (allowing self-loops) induced by a general SDE model. We then provide a sound and complete causal discovery algorithm, capable of handling both fully and partially observed data, and uniquely recovering the underlying or induced ancestral graph by exploiting time directionality assuming a CI oracle. Finally, to make our algorithm practically usable, we also propose a flexible, consistent signature kernel-based CI test to infer these constraints from data. We extensively benchmark the CI test in isolation and as part of our causal discovery algorithms, outperforming existing approaches in SDE models and beyond.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Nx4PMtJ1ER"
        ],
        "venue": [
          "/venue/Nx4PMtJ1ER@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Nx4PMtJ1ER"
        ],
        "detail": [
          "https://openreview.net/forum?id=Nx4PMtJ1ER"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Georg Manten , Cecilia Casolo , Emilio Ferrucci , Søren Mogensen , Cristopher Salvi , Niki Kilbertus Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via `which variables enter the differential of which other variables'. In this paper, we develop conditional independence (CI) constraints on coordinate processes over selected intervals that are Markov with respect to the acyclic dependence graph (allowing self-loops) induced by a general SDE model. We then provide a sound and complete causal discovery algorithm, capable of handling both fully and partially observed data, and uniquely recovering the underlying or induced ancestral graph by exploiting time directionality assuming a CI oracle. Finally, to make our algorithm practically usable, we also propose a flexible, consistent signature kernel-based CI test to infer these constraints from data. We extensively benchmark the CI test in isolation and as part of our causal discovery algorithms, outperforming existing approaches in SDE models and beyond. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "nGiGXLnKhl@OpenReview",
      "index": 139,
      "title": "Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures",
      "authors": [
        "Yuchen Duan",
        "Weiyun Wang",
        "Zhe Chen",
        "Xizhou Zhu",
        "Lewei Lu",
        "Tong Lu",
        "Yu Qiao",
        "Hongsheng Li",
        "Jifeng Dai",
        "Wenhai Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "rwkv",
        "vrwkv",
        "vision",
        "processing",
        "perception",
        "vit",
        "resolution",
        "tasks",
        "window",
        "speeds"
      ],
      "summary": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture from the NLP field with key modifications tailored specifically for vision tasks. Similar to the Vision Transformer (ViT), our model demonstrates robust global processing capabilities, efficiently handles sparse inputs like masked images, and can scale up to accommodate both large-scale parameters and extensive datasets. Its distinctive advantage is its reduced spatial aggregation complexity, enabling seamless processing of high-resolution images without the need for window operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code and models are available at~\\url{https://github.com/OpenGVLab/Vision-RWKV}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=nGiGXLnKhl"
        ],
        "venue": [
          "/venue/nGiGXLnKhl@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=nGiGXLnKhl"
        ],
        "detail": [
          "https://openreview.net/forum?id=nGiGXLnKhl"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 8
      },
      "raw_excerpt": "Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures [PDF 12 ] [Copy] [Kimi 8 ] [REL] Authors : Yuchen Duan , Weiyun Wang , Zhe Chen , Xizhou Zhu , Lewei Lu , Tong Lu , Yu Qiao , Hongsheng Li , Jifeng Dai , Wenhai Wang Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture from the NLP field with key modifications tailored specifically for vision tasks. Similar to the Vision Transformer (ViT), our model demonstrates robust global processing capabilities, efficiently handles sparse inputs like masked images, and can scale up to accommodate both large-scale parameters and extensive datasets. Its distinctive advantage is its reduced spatial aggregation complexity, enabling seamless processing of high-resolution images without the need for window operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code and models are available at~\\url{https://github.com/OpenGVLab/Vision-RWKV}. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "N1L5TgtkAw@OpenReview",
      "index": 140,
      "title": "Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits",
      "authors": [
        "Ashish Khisti",
        "MohammadReza Ebrahimi",
        "Hassan Dbouk",
        "Arash Behboodi",
        "Roland Memisevic",
        "Christos Louizos"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "draft",
        "token",
        "speculative",
        "sampling",
        "scheme",
        "step",
        "acceptance",
        "tokens",
        "probability",
        "theoretical"
      ],
      "summary": "We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection scheme based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=N1L5TgtkAw"
        ],
        "venue": [
          "/venue/N1L5TgtkAw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=N1L5TgtkAw"
        ],
        "detail": [
          "https://openreview.net/forum?id=N1L5TgtkAw"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Ashish Khisti , MohammadReza Ebrahimi , Hassan Dbouk , Arash Behboodi , Roland Memisevic , Christos Louizos We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection scheme based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hBGavkf61a@OpenReview",
      "index": 141,
      "title": "Diffusion Bridge AutoEncoders for Unsupervised Representation Learning",
      "authors": [
        "Yeongmin Kim",
        "Kwanghyeon Lee",
        "Minsang Park",
        "Byeonghu Na",
        "Il-chul Moon"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mathbf",
        "dbae",
        "endpoint",
        "diffusion",
        "representation",
        "bridge",
        "latent",
        "auteencoders",
        "generation",
        "auxiliary"
      ],
      "summary": "Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable z z . Meanwhile, this auxiliary structure invokes an *information split problem*; the information of each data instance x 0 x 0 is divided into diffusion endpoint x T x T and encoded z z because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint x T x T has some disadvantages. The diffusion endpoint x T x T is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables z z -dependent endpoint x T x T inference through a feed-forward architecture. This structure creates an information bottleneck at z z , so x T x T becomes dependent on z z in its generation. This results in z z holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hBGavkf61a"
        ],
        "venue": [
          "/venue/hBGavkf61a@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hBGavkf61a"
        ],
        "detail": [
          "https://openreview.net/forum?id=hBGavkf61a"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Diffusion Bridge AutoEncoders for Unsupervised Representation Learning [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Yeongmin Kim , Kwanghyeon Lee , Minsang Park , Byeonghu Na , Il-chul Moon Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable z z . Meanwhile, this auxiliary structure invokes an *information split problem*; the information of each data instance x 0 x 0 is divided into diffusion endpoint x T x T and encoded z z because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint x T x T has some disadvantages. The diffusion endpoint x T x T is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables z z -dependent endpoint x T x T inference through a feed-forward architecture. This structure creates an information bottleneck at z z , so x T x T becomes dependent on z z in its generation. This results in z z holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "NTHMw8S1Ow@OpenReview",
      "index": 142,
      "title": "Towards Automated Knowledge Integration From Human-Interpretable Representations",
      "authors": [
        "Katarzyna Kobalczyk",
        "Mihaela van der Schaar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "informed",
        "meta",
        "knowledge",
        "learning",
        "inductive",
        "automated",
        "machine",
        "biases",
        "representations",
        "principles"
      ],
      "summary": "In noisy and low-data environments, a significant challenge in machine learning lies in effectively incorporating inductive biases that enhance data efficiency and robustness. Despite many success of informed machine learning methods, designing algorithms with explicit inductive biases based on prior expert knowledge remains largely a manual process. In this work, we explore how prior knowledge represented in its native formats, e.g. in natural language, can be integrated into machine learning models in an automated manner. Inspired by the learning to learn principles of meta-learning, we consider an approach of learning to integrate knowledge via conditional meta-learning, a paradigm we refer to as informed meta-learning. We introduce and motivate theoretically the principles of informed meta-learning enabling automated and controllable inductive bias selection. To illustrate our claims, we implement an instantiation of informed meta-learning--the Informed Neural Process, and empirically demonstrate the potential benefits and limitations of informed meta-learning in improving data efficiency and generalizing to novel knowledge representations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NTHMw8S1Ow"
        ],
        "venue": [
          "/venue/NTHMw8S1Ow@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NTHMw8S1Ow"
        ],
        "detail": [
          "https://openreview.net/forum?id=NTHMw8S1Ow"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 6
      },
      "raw_excerpt": "Towards Automated Knowledge Integration From Human-Interpretable Representations [PDF 7 ] [Copy] [Kimi 6 ] [REL] Authors : Katarzyna Kobalczyk , Mihaela van der Schaar In noisy and low-data environments, a significant challenge in machine learning lies in effectively incorporating inductive biases that enhance data efficiency and robustness. Despite many success of informed machine learning methods, designing algorithms with explicit inductive biases based on prior expert knowledge remains largely a manual process. In this work, we explore how prior knowledge represented in its native formats, e.g. in natural language, can be integrated into machine learning models in an automated manner. Inspired by the learning to learn principles of meta-learning, we consider an approach of learning to integrate knowledge via conditional meta-learning, a paradigm we refer to as informed meta-learning. We introduce and motivate theoretically the principles of informed meta-learning enabling automated and controllable inductive bias selection. To illustrate our claims, we implement an instantiation of informed meta-learning--the Informed Neural Process, and empirically demonstrate the potential benefits and limitations of informed meta-learning in improving data efficiency and generalizing to novel knowledge representations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "PiZtlzMWUj@OpenReview",
      "index": 143,
      "title": "SoftCVI: Contrastive variational inference with self-generated soft labels",
      "authors": [
        "Daniel Ward",
        "Mark Beaumont",
        "Matteo Fasiolo"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "softcvi",
        "variational",
        "inference",
        "contrastive",
        "posterior",
        "unnormalized",
        "objectives",
        "soft",
        "normalizing",
        "labels"
      ],
      "summary": "Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PiZtlzMWUj"
        ],
        "venue": [
          "/venue/PiZtlzMWUj@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PiZtlzMWUj"
        ],
        "detail": [
          "https://openreview.net/forum?id=PiZtlzMWUj"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "SoftCVI: Contrastive variational inference with self-generated soft labels [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Daniel Ward , Mark Beaumont , Matteo Fasiolo Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "wXSshrxlP4@OpenReview",
      "index": 144,
      "title": "GOPS: Learning Generative Object Priors for Unsupervised 3D Instance Segmentation",
      "authors": [
        "Zihui Zhang",
        "Yafei YANG",
        "Hongtao Wen",
        "Bo Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gops",
        "priors",
        "object",
        "pretrained",
        "unsupervised",
        "segmentation",
        "objects",
        "stage",
        "generative",
        "objectness"
      ],
      "summary": "We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GOPS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then to learn multiple objects by querying against the pretrained priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=wXSshrxlP4"
        ],
        "venue": [
          "/venue/wXSshrxlP4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=wXSshrxlP4"
        ],
        "detail": [
          "https://openreview.net/forum?id=wXSshrxlP4"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "GOPS: Learning Generative Object Priors for Unsupervised 3D Instance Segmentation [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Zihui Zhang , Yafei YANG , Hongtao Wen , Bo Yang We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GOPS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then to learn multiple objects by querying against the pretrained priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "gcouwCx7dG@OpenReview",
      "index": 145,
      "title": "Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency",
      "authors": [
        "Jiangrong Shen",
        "Qi Xu",
        "Gang Pan",
        "Badong Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "snns",
        "sparse",
        "rewiring",
        "efficiency",
        "pruning",
        "compression",
        "spiking",
        "synaptic",
        "training",
        "stage"
      ],
      "summary": "The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gcouwCx7dG"
        ],
        "venue": [
          "/venue/gcouwCx7dG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gcouwCx7dG"
        ],
        "detail": [
          "https://openreview.net/forum?id=gcouwCx7dG"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Jiangrong Shen , Qi Xu , Gang Pan , Badong Chen The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "MKEHCx25xp@OpenReview",
      "index": 146,
      "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
      "authors": [
        "Bill Yuchen Lin",
        "Yuntian Deng",
        "Khyathi Chandu",
        "Abhilasha Ravichander",
        "Valentina Pyatkin",
        "Nouha Dziri",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "wildbench",
        "worse",
        "evaluation",
        "reward",
        "slightly",
        "llms",
        "chatbot",
        "tie",
        "win",
        "score"
      ],
      "summary": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of “slightly better/worse” to “tie” if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MKEHCx25xp"
        ],
        "venue": [
          "/venue/MKEHCx25xp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MKEHCx25xp"
        ],
        "detail": [
          "https://openreview.net/forum?id=MKEHCx25xp"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Bill Yuchen Lin , Yuntian Deng , Khyathi Chandu , Abhilasha Ravichander , Valentina Pyatkin , Nouha Dziri , Ronan Le Bras , Yejin Choi We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of “slightly better/worse” to “tie” if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "MGKDBuyv4p@OpenReview",
      "index": 147,
      "title": "Mitigating Memorization in Language Models",
      "authors": [
        "Mansi Sakarvadia",
        "Aswathy Ajith",
        "Arham Khan",
        "Nathaniel Hudson",
        "Caleb Geniesse",
        "Kyle Chard",
        "Yaoqing Yang",
        "Ian Foster",
        "Michael W Mahoney"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "memorization",
        "tinymem",
        "methods",
        "unlearning",
        "mitigation",
        "curbing",
        "lms",
        "regularizer",
        "balancedsubnet",
        "based"
      ],
      "summary": "Language models (LMs) can “memorize” information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methodsare effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removingmemorized information while preserving performance on target tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MGKDBuyv4p"
        ],
        "venue": [
          "/venue/MGKDBuyv4p@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MGKDBuyv4p"
        ],
        "detail": [
          "https://openreview.net/forum?id=MGKDBuyv4p"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 11
      },
      "raw_excerpt": "Mitigating Memorization in Language Models [PDF 7 ] [Copy] [Kimi 11 ] [REL] Authors : Mansi Sakarvadia , Aswathy Ajith , Arham Khan , Nathaniel Hudson , Caleb Geniesse , Kyle Chard , Yaoqing Yang , Ian Foster , Michael W Mahoney Language models (LMs) can “memorize” information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methodsare effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removingmemorized information while preserving performance on target tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cnKhHxN3xj@OpenReview",
      "index": 148,
      "title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity",
      "authors": [
        "Shashata Sawmya",
        "Linghao Kong",
        "Ilia Markov",
        "Dan Alistarh",
        "Nir Shavit"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "wasserstein",
        "neurons",
        "polysemantic",
        "disentangling",
        "neuron",
        "neuronal",
        "output",
        "sparsity",
        "mixture",
        "entanglement"
      ],
      "summary": "Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, in particular under weight sparsity, one of today's leading post-training optimization techniques. We suggest a novel measure for estimating neuronal entanglement: the Wasserstein distance of a neuron’s output distribution to a Gaussian. Moreover, we show the existence of a small number of highly entangled \"Wasserstein Neurons\" in each linear layer of an LLM, characterized by their highly non-Gaussian output distributions and their significant impact on model accuracy. To study this phenomena, we propose a new experimental framework for disentangling polysemantic neurons. Our framework separates each layer’s inputs to create a mixture of experts where each neuron's output is computed by a mixture of neurons of lower Wasserstein distance, each better at maintaining accuracy when sparsified without retraining. We provide strong evidence that this is because the mixture of sparse experts is effectively disentangling the input-output relationship of every individual neuron, in particular the difficult Wasserstein neurons.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cnKhHxN3xj"
        ],
        "venue": [
          "/venue/cnKhHxN3xj@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cnKhHxN3xj"
        ],
        "detail": [
          "https://openreview.net/forum?id=cnKhHxN3xj"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Wasserstein Distances, Neuronal Entanglement, and Sparsity [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Shashata Sawmya , Linghao Kong , Ilia Markov , Dan Alistarh , Nir Shavit Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, in particular under weight sparsity, one of today's leading post-training optimization techniques. We suggest a novel measure for estimating neuronal entanglement: the Wasserstein distance of a neuron’s output distribution to a Gaussian. Moreover, we show the existence of a small number of highly entangled \"Wasserstein Neurons\" in each linear layer of an LLM, characterized by their highly non-Gaussian output distributions and their significant impact on model accuracy. To study this phenomena, we propose a new experimental framework for disentangling polysemantic neurons. Our framework separates each layer’s inputs to create a mixture of experts where each neuron's output is computed by a mixture of neurons of lower Wasserstein distance, each better at maintaining accuracy when sparsified without retraining. We provide strong evidence that this is because the mixture of sparse experts is effectively disentangling the input-output relationship of every individual neuron, in particular the difficult Wasserstein neurons. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "MFZjrTFE7h@OpenReview",
      "index": 149,
      "title": "D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement",
      "authors": [
        "Yansong Peng",
        "Hebei Li",
        "Peixi Wu",
        "Yueyi Zhang",
        "Xiaoyan Sun",
        "Feng Wu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "fine",
        "localization",
        "detrs",
        "detr",
        "fdr",
        "regression",
        "grained",
        "refinement",
        "redefine",
        "distillation"
      ],
      "summary": "We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GOLSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and models will be made publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MFZjrTFE7h"
        ],
        "venue": [
          "/venue/MFZjrTFE7h@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MFZjrTFE7h"
        ],
        "detail": [
          "https://openreview.net/forum?id=MFZjrTFE7h"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Yansong Peng , Hebei Li , Peixi Wu , Yueyi Zhang , Xiaoyan Sun , Feng Wu We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GOLSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and models will be made publicly available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "M8OGl34Pmg@OpenReview",
      "index": 150,
      "title": "Following the Human Thread in Social Navigation",
      "authors": [
        "Luca Scofano",
        "Alessio Sampieri",
        "Tommaso Campari",
        "Valentino Sacco",
        "Indro Spinelli",
        "Lamberto Ballan",
        "Fabio Galasso"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "social",
        "human",
        "navigation",
        "trajectories",
        "sda",
        "robot",
        "thread",
        "humans",
        "dynamics",
        "following"
      ],
      "summary": "The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time.Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans. The code will be released upon acceptance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M8OGl34Pmg"
        ],
        "venue": [
          "/venue/M8OGl34Pmg@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M8OGl34Pmg"
        ],
        "detail": [
          "https://openreview.net/forum?id=M8OGl34Pmg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "Following the Human Thread in Social Navigation [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Luca Scofano , Alessio Sampieri , Tommaso Campari , Valentino Sacco , Indro Spinelli , Lamberto Ballan , Fabio Galasso The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time.Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans. The code will be released upon acceptance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "M2SsqpxGtc@OpenReview",
      "index": 151,
      "title": "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation",
      "authors": [
        "Nikolai Kalischek",
        "Michael Oechsle",
        "Fabian Manhardt",
        "Philipp Henzler",
        "Konrad Schindler",
        "Federico Tombari"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "panorama",
        "generation",
        "cubediff",
        "cubemaps",
        "diffusion",
        "repurposing",
        "cubemap",
        "panoramas",
        "equirectangular",
        "models"
      ],
      "summary": "We introduce a novel method for generating 360° panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=M2SsqpxGtc"
        ],
        "venue": [
          "/venue/M2SsqpxGtc@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=M2SsqpxGtc"
        ],
        "detail": [
          "https://openreview.net/forum?id=M2SsqpxGtc"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Nikolai Kalischek , Michael Oechsle , Fabian Manhardt , Philipp Henzler , Konrad Schindler , Federico Tombari We introduce a novel method for generating 360° panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Lz0XW99tE0@OpenReview",
      "index": 152,
      "title": "A Periodic Bayesian Flow for Material Generation",
      "authors": [
        "Hanlin Wu",
        "Yuxuan Song",
        "Jingjing Gong",
        "Ziyao Cao",
        "Yawen Ouyang",
        "Jianbing Zhang",
        "Hao Zhou",
        "Wei-Ying Ma",
        "Jingjing Liu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "crysbfn",
        "crystal",
        "periodic",
        "bayesian",
        "flow",
        "modeling",
        "generation",
        "conditioning",
        "bfn",
        "euclidean"
      ],
      "summary": "Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song, et al.,2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~ 100x speedup (10 v.s. 2000 steps network forwards) compared with previous Diffusion-based methods on MP-20 dataset.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Lz0XW99tE0"
        ],
        "venue": [
          "/venue/Lz0XW99tE0@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Lz0XW99tE0"
        ],
        "detail": [
          "https://openreview.net/forum?id=Lz0XW99tE0"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "A Periodic Bayesian Flow for Material Generation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hanlin Wu , Yuxuan Song , Jingjing Gong , Ziyao Cao , Yawen Ouyang , Jianbing Zhang , Hao Zhou , Wei-Ying Ma , Jingjing Liu Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song, et al.,2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~ 100x speedup (10 v.s. 2000 steps network forwards) compared with previous Diffusion-based methods on MP-20 dataset. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "LiUfN9h0Lx@OpenReview",
      "index": 153,
      "title": "Efficient and Accurate Explanation Estimation with Distribution Compression",
      "authors": [
        "Hubert Baniecki",
        "Giuseppe Casalicchio",
        "Bernd Bischl",
        "Przemyslaw Biecek"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cte",
        "explanation",
        "compression",
        "approximation",
        "estimation",
        "distribution",
        "fewer",
        "improves",
        "relies",
        "attributions"
      ],
      "summary": "We discover a theoretical connection between explanation estimation and distribution compression that significantly improves the approximation of feature attributions, importance, and effects. While the exact computation of various machine learning explanations requires numerous model inferences and becomes impractical, the computational cost of approximation increases with an ever-increasing size of data and model parameters. We show that the standard i.i.d. sampling used in a broad spectrum of algorithms for post-hoc explanation leads to an approximation error worthy of improvement. To this end, we introduce Compress Then Explain (CTE), a new paradigm of sample-efficient explainability. It relies on distribution compression through kernel thinning to obtain a data sample that best approximates its marginal distribution. CTE significantly improves the accuracy and stability of explanation estimation with negligible computational overhead. It often achieves an on-par explanation approximation error 2-3x faster by using fewer samples, i.e. requiring 2-3x fewer model evaluations. CTE is a simple, yet powerful, plug-in for any explanation method that now relies on i.i.d. sampling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LiUfN9h0Lx"
        ],
        "venue": [
          "/venue/LiUfN9h0Lx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LiUfN9h0Lx"
        ],
        "detail": [
          "https://openreview.net/forum?id=LiUfN9h0Lx"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Efficient and Accurate Explanation Estimation with Distribution Compression [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Hubert Baniecki , Giuseppe Casalicchio , Bernd Bischl , Przemyslaw Biecek We discover a theoretical connection between explanation estimation and distribution compression that significantly improves the approximation of feature attributions, importance, and effects. While the exact computation of various machine learning explanations requires numerous model inferences and becomes impractical, the computational cost of approximation increases with an ever-increasing size of data and model parameters. We show that the standard i.i.d. sampling used in a broad spectrum of algorithms for post-hoc explanation leads to an approximation error worthy of improvement. To this end, we introduce Compress Then Explain (CTE), a new paradigm of sample-efficient explainability. It relies on distribution compression through kernel thinning to obtain a data sample that best approximates its marginal distribution. CTE significantly improves the accuracy and stability of explanation estimation with negligible computational overhead. It often achieves an on-par explanation approximation error 2-3x faster by using fewer samples, i.e. requiring 2-3x fewer model evaluations. CTE is a simple, yet powerful, plug-in for any explanation method that now relies on i.i.d. sampling. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "LbgIZpSUCe@OpenReview",
      "index": 154,
      "title": "Nonlinear multiregion neural dynamics with parametric impulse response communication channels",
      "authors": [
        "Matthew Dowling",
        "Cristina Savin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "multiregion",
        "impulse",
        "dynamics",
        "areas",
        "nonlinear",
        "neural",
        "neuroscientifically",
        "channels",
        "regions",
        "population"
      ],
      "summary": "Cognition arises from the coordinated interaction of brain regions with distinct computational roles. Despite improvements in our ability to extract the dynamics underlying circuit computation from population activity recorded in individual areas, understanding how multiple areas jointly support distributed computation remains a challenge. As part of this effort, we propose a multi-region neural dynamics model composed of two building blocks: _i)_ within-region (potentially driven) nonlinear dynamics and _ii)_ communication channels between regions, parameterized through their impulse response. Together, these choices make it possible to learn nonlinear neural population dynamics and understand the flow of information between regions by drawing from the rich literature of linear systems theory. We develop a state noise inversion free variational filtering and learning algorithm for our model and show, through neuroscientifically inspired numerical experiments, how the proposed model can reveal interpretable characterizations of the local computations within and the flow of information between neural populations. We further validate the efficacy of our approach using simultaneous population recordings from areas V1 and V2.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LbgIZpSUCe"
        ],
        "venue": [
          "/venue/LbgIZpSUCe@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LbgIZpSUCe"
        ],
        "detail": [
          "https://openreview.net/forum?id=LbgIZpSUCe"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Nonlinear multiregion neural dynamics with parametric impulse response communication channels [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Matthew Dowling , Cristina Savin Cognition arises from the coordinated interaction of brain regions with distinct computational roles. Despite improvements in our ability to extract the dynamics underlying circuit computation from population activity recorded in individual areas, understanding how multiple areas jointly support distributed computation remains a challenge. As part of this effort, we propose a multi-region neural dynamics model composed of two building blocks: _i)_ within-region (potentially driven) nonlinear dynamics and _ii)_ communication channels between regions, parameterized through their impulse response. Together, these choices make it possible to learn nonlinear neural population dynamics and understand the flow of information between regions by drawing from the rich literature of linear systems theory. We develop a state noise inversion free variational filtering and learning algorithm for our model and show, through neuroscientifically inspired numerical experiments, how the proposed model can reveal interpretable characterizations of the local computations within and the flow of information between neural populations. We further validate the efficacy of our approach using simultaneous population recordings from areas V1 and V2. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "LXftdR11io@OpenReview",
      "index": 155,
      "title": "POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition",
      "authors": [
        "Yuta Saito",
        "Jihan Yao",
        "Thorsten Joachims"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "potec",
        "policy",
        "opl",
        "stage",
        "regression",
        "action",
        "cluster",
        "contextual",
        "spaces",
        "via"
      ],
      "summary": "We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the relative expected reward differences of the actions within each cluster, ensures that our policy-gradient estimator is unbiased and the second-stage policy is optimal. We also show that POTEC provides a strict generalization of policy- and regression-based approaches and their associated assumptions. Comprehensive experiments demonstrate that POTEC provides substantial improvements in OPL effectiveness particularly in large and structured action spaces.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LXftdR11io"
        ],
        "venue": [
          "/venue/LXftdR11io@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LXftdR11io"
        ],
        "detail": [
          "https://openreview.net/forum?id=LXftdR11io"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Yuta Saito , Jihan Yao , Thorsten Joachims We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the relative expected reward differences of the actions within each cluster, ensures that our policy-gradient estimator is unbiased and the second-stage policy is optimal. We also show that POTEC provides a strict generalization of policy- and regression-based approaches and their associated assumptions. Comprehensive experiments demonstrate that POTEC provides substantial improvements in OPL effectiveness particularly in large and structured action spaces. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "LSp4KBhAom@OpenReview",
      "index": 156,
      "title": "LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models",
      "authors": [
        "Ziqi Lu",
        "Heng Yang",
        "Danfei Xu",
        "Boyi Li",
        "Boris Ivanovic",
        "Marco Pavone",
        "Yue Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lora3d",
        "view",
        "rank",
        "calibration",
        "foundation",
        "confidence",
        "self",
        "geometric",
        "low",
        "dust3r"
      ],
      "summary": "Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks.However, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data,these pre-trained models still struggle to generalize to many challenging circumstances,such as limited view overlap or low lighting.To address this, we propose LoRA3D, an efficient self-calibration pipeline to *specialize* the pre-trained models to target scenes using their own multi-view predictions.Taking sparse RGB images as input, we leverage robust optimization techniques to refine multi-view predictions and align them into a global coordinate frame.In particular, we incorporate prediction confidence into the geometric optimization process, automatically re-weighting the confidence to better reflect point estimation accuracy. We use the calibrated confidence to generate high-quality pseudo labels for the calibrating views and fine-tune the models using low-rank adaptation (LoRA) on the pseudo-labeled data.Our method does not require any external priors or manual labels. It completes the self-calibration process on a **single standard GPU within just 5 minutes**.Each low-rank adapter requires only **18MB** of storage. We evaluated our method on **more than 160 scenes** from the Replica, TUM and Waymo Open datasets,achieving up to **88\\% performance improvement** on 3D reconstruction, multi-view pose estimation and novel-view rendering.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LSp4KBhAom"
        ],
        "venue": [
          "/venue/LSp4KBhAom@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LSp4KBhAom"
        ],
        "detail": [
          "https://openreview.net/forum?id=LSp4KBhAom"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Ziqi Lu , Heng Yang , Danfei Xu , Boyi Li , Boris Ivanovic , Marco Pavone , Yue Wang Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks.However, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data,these pre-trained models still struggle to generalize to many challenging circumstances,such as limited view overlap or low lighting.To address this, we propose LoRA3D, an efficient self-calibration pipeline to *specialize* the pre-trained models to target scenes using their own multi-view predictions.Taking sparse RGB images as input, we leverage robust optimization techniques to refine multi-view predictions and align them into a global coordinate frame.In particular, we incorporate prediction confidence into the geometric optimization process, automatically re-weighting the confidence to better reflect point estimation accuracy. We use the calibrated confidence to generate high-quality pseudo labels for the calibrating views and fine-tune the models using low-rank adaptation (LoRA) on the pseudo-labeled data.Our method does not require any external priors or manual labels. It completes the self-calibration process on a **single standard GPU within just 5 minutes**.Each low-rank adapter requires only **18MB** of storage. We evaluated our method on **more than 160 scenes** from the Replica, TUM and Waymo Open datasets,achieving up to **88\\% performance improvement** on 3D reconstruction, multi-view pose estimation and novel-view rendering. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "szRmEM8Kx5@OpenReview",
      "index": 157,
      "title": "Effective post-training embedding compression via temperature control in contrastive training",
      "authors": [
        "georgiana dinu",
        "Corey Barrett",
        "Yi Xiang",
        "Miguel Romero Calvo",
        "Anna Currey",
        "Xing Niu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "embedding",
        "compression",
        "contrastive",
        "embeddings",
        "temperature",
        "training",
        "dimensionality",
        "effective",
        "intrinsic",
        "size"
      ],
      "summary": "Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on the impact this parameter has on the intrinsic dimensionality of the embedding spaces obtained, and show that lower intrinsic dimensionality is further correlated with effective compression of embeddings. We still observe a trade-off between absolute performance and effective compression and we propose temperature aggregation methods which reduce embedding size by an order of magnitude with minimal impact on quality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=szRmEM8Kx5"
        ],
        "venue": [
          "/venue/szRmEM8Kx5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=szRmEM8Kx5"
        ],
        "detail": [
          "https://openreview.net/forum?id=szRmEM8Kx5"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "Effective post-training embedding compression via temperature control in contrastive training [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : georgiana dinu , Corey Barrett , Yi Xiang , Miguel Romero Calvo , Anna Currey , Xing Niu Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on the impact this parameter has on the intrinsic dimensionality of the embedding spaces obtained, and show that lower intrinsic dimensionality is further correlated with effective compression of embeddings. We still observe a trade-off between absolute performance and effective compression and we propose temperature aggregation methods which reduce embedding size by an order of magnitude with minimal impact on quality. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "LBl7Hez0fF@OpenReview",
      "index": 158,
      "title": "Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering",
      "authors": [
        "Sheng Liu",
        "Haotian Ye",
        "James Y Zou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lvlms",
        "hallucinations",
        "vision",
        "vti",
        "hallucination",
        "steering",
        "language",
        "decoders",
        "intervention",
        "latent"
      ],
      "summary": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from LLMs. We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional training cost. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LBl7Hez0fF"
        ],
        "venue": [
          "/venue/LBl7Hez0fF@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LBl7Hez0fF"
        ],
        "detail": [
          "https://openreview.net/forum?id=LBl7Hez0fF"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 10
      },
      "raw_excerpt": "Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering [PDF 13 ] [Copy] [Kimi 10 ] [REL] Authors : Sheng Liu , Haotian Ye , James Y Zou Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from LLMs. We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional training cost. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "KijslFbfOL@OpenReview",
      "index": 159,
      "title": "Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction",
      "authors": [
        "Shengju Yu",
        "Zhibin Dong",
        "Siwei Wang",
        "Pei Zhang",
        "Yi Zhang",
        "Xinwang Liu",
        "Thomas Guan",
        "Tiejun Li",
        "Yiu-ming Cheung"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "similarity",
        "imvc",
        "view",
        "clustering",
        "incomplete",
        "siihpc",
        "prototype",
        "hybrid",
        "imputation",
        "samples"
      ],
      "summary": "Most of incomplete multi-view clustering (IMVC) methods typically choose to ignore the missing samples and only utilize observed unpaired samples to construct bipartite similarity. Moreover, they employ a single quantity of prototypes to extract the information of all all views. To eliminate these drawbacks, we present a simple yet effective IMVC approach, SIIHPC, in this work. It firstly transforms partial bipartition learning into original sample form by virtue of reconstruction concept to split out of observed similarity, and then loosens traditional non-negative constraints via regularizing samples to more freely characterize the similarity. Subsequently, it learns to recover the incomplete parts by utilizing the connection built between the similarity exclusive on respective view and the consensus graph shared for all views. On this foundation, it further introduces a group of hybrid prototype quantities for each individual view to flexibly extract the data features belonging to each view itself. Accordingly, the resulting graphs are with various scales and describe the overall similarity more comprehensively. It is worth mentioning that these all are optimized in one unified learning framework, which makes it possible for them to reciprocally promote. Then, to effectively solve the formulated optimization problem, we design an ingenious auxiliary function that is with theoretically proven monotonic-increasing properties. Finally, the clustering results are obtained by implementing spectral grouping action on the eigenvectors of stacked multi-scale consensus similarity. Numerous experimental results demonstrate that even under diverse missing proportions, the proposed SIIHPC is still able to provide a preferable clustering performance compared to multiple prominent IMVC methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KijslFbfOL"
        ],
        "venue": [
          "/venue/KijslFbfOL@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KijslFbfOL"
        ],
        "detail": [
          "https://openreview.net/forum?id=KijslFbfOL"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Shengju Yu , Zhibin Dong , Siwei Wang , Pei Zhang , Yi Zhang , Xinwang Liu , Thomas Guan , Tiejun Li , Yiu-ming Cheung Most of incomplete multi-view clustering (IMVC) methods typically choose to ignore the missing samples and only utilize observed unpaired samples to construct bipartite similarity. Moreover, they employ a single quantity of prototypes to extract the information of all all views. To eliminate these drawbacks, we present a simple yet effective IMVC approach, SIIHPC, in this work. It firstly transforms partial bipartition learning into original sample form by virtue of reconstruction concept to split out of observed similarity, and then loosens traditional non-negative constraints via regularizing samples to more freely characterize the similarity. Subsequently, it learns to recover the incomplete parts by utilizing the connection built between the similarity exclusive on respective view and the consensus graph shared for all views. On this foundation, it further introduces a group of hybrid prototype quantities for each individual view to flexibly extract the data features belonging to each view itself. Accordingly, the resulting graphs are with various scales and describe the overall similarity more comprehensively. It is worth mentioning that these all are optimized in one unified learning framework, which makes it possible for them to reciprocally promote. Then, to effectively solve the formulated optimization problem, we design an ingenious auxiliary function that is with theoretically proven monotonic-increasing properties. Finally, the clustering results are obtained by implementing spectral grouping action on the eigenvectors of stacked multi-scale consensus similarity. Numerous experimental results demonstrate that even under diverse missing proportions, the proposed SIIHPC is still able to provide a preferable clustering performance compared to multiple prominent IMVC methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "K2jOacHUlO@OpenReview",
      "index": 160,
      "title": "Enhancing Large Language Models' Situated Faithfulness to External Contexts",
      "authors": [
        "Yukun Huang",
        "Sanxing Chen",
        "Hongyi Cai",
        "Bhuwan Dhingra"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "scr",
        "rcr",
        "external",
        "situated",
        "faithfulness",
        "confidence",
        "contexts",
        "llms",
        "reasoning",
        "llama"
      ],
      "summary": "Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even intentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2\\% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9\\% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=K2jOacHUlO"
        ],
        "venue": [
          "/venue/K2jOacHUlO@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=K2jOacHUlO"
        ],
        "detail": [
          "https://openreview.net/forum?id=K2jOacHUlO"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 10
      },
      "raw_excerpt": "Enhancing Large Language Models' Situated Faithfulness to External Contexts [PDF 9 ] [Copy] [Kimi 10 ] [REL] Authors : Yukun Huang , Sanxing Chen , Hongyi Cai , Bhuwan Dhingra Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even intentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2\\% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9\\% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IqHeDe2lbl@OpenReview",
      "index": 161,
      "title": "Sparse components distinguish visual pathways & their alignment to neural networks",
      "authors": [
        "Ammar Marvi",
        "Nancy Kanwisher",
        "Meenakshi Khosla"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "visual",
        "ventral",
        "dorsal",
        "stream",
        "alignment",
        "lateral",
        "components",
        "sca",
        "sparse",
        "pathways"
      ],
      "summary": "The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways. To explore this inconsistency, we applied a novel sparse decomposition approach to identify the dominant components of visual representations within each stream. Consistent with traditional neuroscience research, we find a clear difference in component response profiles across the three visual streams—identifying components selective for faces, places, bodies, text, and food in the ventral stream; social interactions, implied motion, and hand actions in the lateral stream; and some less interpretable components in the dorsal stream. Building on this, we introduce Sparse Component Alignment (SCA), a new method for measuring representational alignment between brains and machines that better captures the latent neural tuning of these two visual systems. We find that standard visual DNNs are more aligned with ventral than either dorsal or lateral representations. SCA reveals these distinctions with greater resolution than conventional population-level geometry, offering a measure of representational alignment that is sensitive to a system’s underlying axes of neural tuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IqHeDe2lbl"
        ],
        "venue": [
          "/venue/IqHeDe2lbl@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IqHeDe2lbl"
        ],
        "detail": [
          "https://openreview.net/forum?id=IqHeDe2lbl"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Sparse components distinguish visual pathways & their alignment to neural networks [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Ammar Marvi , Nancy Kanwisher , Meenakshi Khosla The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways. To explore this inconsistency, we applied a novel sparse decomposition approach to identify the dominant components of visual representations within each stream. Consistent with traditional neuroscience research, we find a clear difference in component response profiles across the three visual streams—identifying components selective for faces, places, bodies, text, and food in the ventral stream; social interactions, implied motion, and hand actions in the lateral stream; and some less interpretable components in the dorsal stream. Building on this, we introduce Sparse Component Alignment (SCA), a new method for measuring representational alignment between brains and machines that better captures the latent neural tuning of these two visual systems. We find that standard visual DNNs are more aligned with ventral than either dorsal or lateral representations. SCA reveals these distinctions with greater resolution than conventional population-level geometry, offering a measure of representational alignment that is sensitive to a system’s underlying axes of neural tuning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IUmj2dw5se@OpenReview",
      "index": 162,
      "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
      "authors": [
        "Song Wang",
        "Peng Wang",
        "Tong Zhou",
        "Yushun Dong",
        "Zhen Tan",
        "Jundong Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "bias",
        "ceb",
        "evaluation",
        "llms",
        "compositional",
        "bechmark",
        "language",
        "tasks",
        "datasets",
        "across"
      ],
      "summary": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IUmj2dw5se"
        ],
        "venue": [
          "/venue/IUmj2dw5se@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IUmj2dw5se"
        ],
        "detail": [
          "https://openreview.net/forum?id=IUmj2dw5se"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Song Wang , Peng Wang , Tong Zhou , Yushun Dong , Zhen Tan , Jundong Li As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IF0Q9KY3p2@OpenReview",
      "index": 163,
      "title": "Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression",
      "authors": [
        "Shuang Liang",
        "Guido Montufar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mirror",
        "flow",
        "univariate",
        "bias",
        "scaled",
        "implicit",
        "networks",
        "lazy",
        "shallow",
        "penalized"
      ],
      "summary": "We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For univariate ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introduce \\emph{scaled potentials} and show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For univariate networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initialization determines how strongly the curvature of the learned function is penalized at different locations of the input space, the scaled potential determines how the different magnitudes of the curvature are penalized.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IF0Q9KY3p2"
        ],
        "venue": [
          "/venue/IF0Q9KY3p2@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IF0Q9KY3p2"
        ],
        "detail": [
          "https://openreview.net/forum?id=IF0Q9KY3p2"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Shuang Liang , Guido Montufar We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For univariate ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introduce \\emph{scaled potentials} and show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For univariate networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initialization determines how strongly the curvature of the learned function is penalized at different locations of the input space, the scaled potential determines how the different magnitudes of the curvature are penalized. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IC5RJvRoMp@OpenReview",
      "index": 164,
      "title": "Streamlining Redundant Layers to Compress Large Language Models",
      "authors": [
        "Xiaodong Chen",
        "Yuxuan Hu",
        "Jing Zhang",
        "Yanling Wang",
        "Cuiping Li",
        "Hong Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "streamline",
        "layers",
        "llm",
        "pruning",
        "pruned",
        "ruckbreasoning",
        "streamlining",
        "layer",
        "redundant",
        "compress"
      ],
      "summary": "This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency. Our code is available at \\href{https://github.com/RUCKBReasoning/LLM-Streamline}{this repository}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IC5RJvRoMp"
        ],
        "venue": [
          "/venue/IC5RJvRoMp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IC5RJvRoMp"
        ],
        "detail": [
          "https://openreview.net/forum?id=IC5RJvRoMp"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 6
      },
      "raw_excerpt": "Streamlining Redundant Layers to Compress Large Language Models [PDF 7 ] [Copy] [Kimi 6 ] [REL] Authors : Xiaodong Chen , Yuxuan Hu , Jing Zhang , Yanling Wang , Cuiping Li , Hong Chen This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency. Our code is available at \\href{https://github.com/RUCKBReasoning/LLM-Streamline}{this repository}. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Hz4BYVY8YM@OpenReview",
      "index": 165,
      "title": "SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding",
      "authors": [
        "Zhenyu Yang",
        "Yuhang Hu",
        "Zemin Du",
        "Dizhan Xue",
        "Shengsheng Qian",
        "Jiahong Wu",
        "Fan Yang",
        "Weiming Dong",
        "Changsheng Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "svbench",
        "lvlms",
        "streaming",
        "video",
        "understanding",
        "dialogues",
        "temporal",
        "turn",
        "benchmarks",
        "chains"
      ],
      "summary": "Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at https://anonymous.4open.science/r/SVBench-356F.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Hz4BYVY8YM"
        ],
        "venue": [
          "/venue/Hz4BYVY8YM@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Hz4BYVY8YM"
        ],
        "detail": [
          "https://openreview.net/forum?id=Hz4BYVY8YM"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Zhenyu Yang , Yuhang Hu , Zemin Du , Dizhan Xue , Shengsheng Qian , Jiahong Wu , Fan Yang , Weiming Dong , Changsheng Xu Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at https://anonymous.4open.science/r/SVBench-356F. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "HE5JmwniHm@OpenReview",
      "index": 166,
      "title": "DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimizaiton",
      "authors": [
        "Yi Zhang",
        "Siwei Wang",
        "Jiyuan Liu",
        "Shengju Yu",
        "Zhibin Dong",
        "Suyuan Liu",
        "Xinwang Liu",
        "En Zhu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mkc",
        "dleft",
        "clustering",
        "kernel",
        "tensor",
        "late",
        "min",
        "max",
        "optimizaiton",
        "fusion"
      ],
      "summary": "Recent advancements in multiple kernel clustering (MKC) have highlighted the effectiveness of late fusion strategies, particularly in enhancing computational efficiency to near-linear complexity while achieving promising clustering performance. However, existing methods encounter three significant limitations: (1) reliance on fixed base partition matrices that do not adaptively optimize during the clustering process, thereby constraining their performance to the inherent representational capabilities of these matrices; (2) a focus on adjusting kernel weights to explore inter-view consistency and complementarity, which often neglects the intrinsic high-order correlations among views, thereby limiting the extraction of comprehensive multiple kernel information; (3) a lack of adaptive mechanisms to accommodate varying distributions within the data, which limits robustness and generalization. To address these challenges, this paper proposes a novel algorithm termed Dynamic Late Fusion Multiple Kernel Clustering with Robust {Tensor Learning via min-max optimization (DLEFT-MKC), which effectively overcomes the representational bottleneck of base partition matrices and facilitates the learning of meaningful high-order cross-view information. Specifically, it is the first to incorporate a min-max optimization paradigm into tensor-based MKC, enhancing algorithm robustness and generalization. Additionally, it dynamically reconstructs decision layers to enhance representation capabilities and subsequently stacks the reconstructed representations for tensor learning that promotes the capture of high-order associations and cluster structures across views, ultimately yielding consensus clustering partitions. To solve the resultant optimization problem, we innovatively design a strategy that combines reduced gradient descent with the alternating direction method of multipliers, ensuring convergence to local optima while maintaining high computational efficiency. Extensive experimental results across various benchmark datasets validate the superior effectiveness and efficiency of the proposed DLEFT-MKC.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HE5JmwniHm"
        ],
        "venue": [
          "/venue/HE5JmwniHm@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HE5JmwniHm"
        ],
        "detail": [
          "https://openreview.net/forum?id=HE5JmwniHm"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimizaiton [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Yi Zhang , Siwei Wang , Jiyuan Liu , Shengju Yu , Zhibin Dong , Suyuan Liu , Xinwang Liu , En Zhu Recent advancements in multiple kernel clustering (MKC) have highlighted the effectiveness of late fusion strategies, particularly in enhancing computational efficiency to near-linear complexity while achieving promising clustering performance. However, existing methods encounter three significant limitations: (1) reliance on fixed base partition matrices that do not adaptively optimize during the clustering process, thereby constraining their performance to the inherent representational capabilities of these matrices; (2) a focus on adjusting kernel weights to explore inter-view consistency and complementarity, which often neglects the intrinsic high-order correlations among views, thereby limiting the extraction of comprehensive multiple kernel information; (3) a lack of adaptive mechanisms to accommodate varying distributions within the data, which limits robustness and generalization. To address these challenges, this paper proposes a novel algorithm termed Dynamic Late Fusion Multiple Kernel Clustering with Robust {Tensor Learning via min-max optimization (DLEFT-MKC), which effectively overcomes the representational bottleneck of base partition matrices and facilitates the learning of meaningful high-order cross-view information. Specifically, it is the first to incorporate a min-max optimization paradigm into tensor-based MKC, enhancing algorithm robustness and generalization. Additionally, it dynamically reconstructs decision layers to enhance representation capabilities and subsequently stacks the reconstructed representations for tensor learning that promotes the capture of high-order associations and cluster structures across views, ultimately yielding consensus clustering partitions. To solve the resultant optimization problem, we innovatively design a strategy that combines reduced gradient descent with the alternating direction method of multipliers, ensuring convergence to local optima while maintaining high computational efficiency. Extensive experimental results across various benchmark datasets validate the superior effectiveness and efficiency of the proposed DLEFT-MKC. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Gj5JTAwdoy@OpenReview",
      "index": 167,
      "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
      "authors": [
        "Zachary Novack",
        "Ge Zhu",
        "Jonah Casebeer",
        "Julian McAuley",
        "Taylor Berg-Kirkpatrick",
        "Nicholas J. Bryan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "distillation",
        "ttm",
        "presto",
        "steps",
        "music",
        "step",
        "accelerating",
        "435ms",
        "distilling",
        "diffusion"
      ],
      "summary": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) — the fastest TTM to our knowledge.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Gj5JTAwdoy"
        ],
        "venue": [
          "/venue/Gj5JTAwdoy@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Gj5JTAwdoy"
        ],
        "detail": [
          "https://openreview.net/forum?id=Gj5JTAwdoy"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Presto! Distilling Steps and Layers for Accelerating Music Generation [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Zachary Novack , Ge Zhu , Jonah Casebeer , Julian McAuley , Taylor Berg-Kirkpatrick , Nicholas J. Bryan Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) — the fastest TTM to our knowledge. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "G6dMvRuhFr@OpenReview",
      "index": 168,
      "title": "Grounding Video Models to Actions through Goal Conditioned Exploration",
      "authors": [
        "Yunhao Luo",
        "Yilun Du"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "video",
        "tasks",
        "embodiment",
        "actions",
        "exploration",
        "visual",
        "toenable",
        "libero",
        "ithor",
        "action"
      ],
      "summary": "Large video models, pretrained on massive quantities of amount of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.We propose a framework that uses trajectory level action generation in combination with video guidance toenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=G6dMvRuhFr"
        ],
        "venue": [
          "/venue/G6dMvRuhFr@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=G6dMvRuhFr"
        ],
        "detail": [
          "https://openreview.net/forum?id=G6dMvRuhFr"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Grounding Video Models to Actions through Goal Conditioned Exploration [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Yunhao Luo , Yilun Du Large video models, pretrained on massive quantities of amount of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.We propose a framework that uses trajectory level action generation in combination with video guidance toenable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks.We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "FhBT596F1X@OpenReview",
      "index": 169,
      "title": "Learning Equivariant Non-Local Electron Density Functionals",
      "authors": [
        "Nicholas Gao",
        "Eike Eberhard",
        "Stephan Günnemann"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "equivariant",
        "local",
        "functionals",
        "3bpa",
        "density",
        "functional",
        "md17",
        "maes",
        "electron",
        "exchange"
      ],
      "summary": "The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To address these issues, we introduce Equivariant Graph Exchange Correlation (EG-XC), a novel non-local XC functional based on equivariant graph neural networks (GNNs). Where previous works relied on semi-local functionals or fixed-size descriptors of the density, we compress the electron density into an SO(3)-equivariant nuclei-centered point cloud representation for efficient non-local atomic-range interactions. By applying an equivariant GNN on this point cloud, we capture molecular-range interactions in a scalable and accurate manner. To train EG-XC, we differentiate through a self-consistent field solver requiring only energy targets. In our empirical evaluation, we find EG-XC to accurately reconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on QM9, matching force fields trained on 5 times more and larger molecules. On identical training sets, EG-XC yields on average 51% lower MAEs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FhBT596F1X"
        ],
        "venue": [
          "/venue/FhBT596F1X@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FhBT596F1X"
        ],
        "detail": [
          "https://openreview.net/forum?id=FhBT596F1X"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Learning Equivariant Non-Local Electron Density Functionals [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Nicholas Gao , Eike Eberhard , Stephan Günnemann The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To address these issues, we introduce Equivariant Graph Exchange Correlation (EG-XC), a novel non-local XC functional based on equivariant graph neural networks (GNNs). Where previous works relied on semi-local functionals or fixed-size descriptors of the density, we compress the electron density into an SO(3)-equivariant nuclei-centered point cloud representation for efficient non-local atomic-range interactions. By applying an equivariant GNN on this point cloud, we capture molecular-range interactions in a scalable and accurate manner. To train EG-XC, we differentiate through a self-consistent field solver requiring only energy targets. In our empirical evaluation, we find EG-XC to accurately reconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on QM9, matching force fields trained on 5 times more and larger molecules. On identical training sets, EG-XC yields on average 51% lower MAEs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ExrEw8cVlU@OpenReview",
      "index": 170,
      "title": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting",
      "authors": [
        "Jiahao Lu",
        "Yifan Zhang",
        "Qiuhong Shen",
        "Xinchao Wang",
        "Shuicheng YAN"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "3dgs",
        "attack",
        "poison",
        "splat",
        "computation",
        "splatting",
        "allocable",
        "cost",
        "vulnerability",
        "overlooked"
      ],
      "summary": "3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ExrEw8cVlU"
        ],
        "venue": [
          "/venue/ExrEw8cVlU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ExrEw8cVlU"
        ],
        "detail": [
          "https://openreview.net/forum?id=ExrEw8cVlU"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Jiahao Lu , Yifan Zhang , Qiuhong Shen , Xinchao Wang , Shuicheng YAN 3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "EbWf36quzd@OpenReview",
      "index": 171,
      "title": "Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation",
      "authors": [
        "Gao Peng",
        "Le Zhuo",
        "Dongyang Liu",
        "DU",
        "Xu Luo",
        "Longtian Qiu",
        "Yuhang Zhang",
        "Rongjie Huang",
        "Shijie Geng",
        "Renrui Zhang",
        "Junlin Xie",
        "Wenqi Shao",
        "Zhengkai Jiang",
        "Tianshuo Yang",
        "Weicai Ye",
        "Tong He",
        "HE",
        "Junjun He",
        "Yu Qiao",
        "Hongsheng Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lumina",
        "t2x",
        "dit",
        "videos",
        "flag",
        "t2i",
        "generative",
        "nextline",
        "nextframe",
        "flow"
      ],
      "summary": "Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this paper, we introduce the Lumina-T2X family -- a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a simple and scalable generative framework that can be adapted to various modalities, e.g., transforming noise into images, videos, multi-view 3D objects, or audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as |[nextline]| and |[nextframe]| tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. Advanced techniques like RoPE, KQ-Norm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT (PixArt-alpha), indicating that increasing the number of parameters significantly accelerates convergence of generative models without compromising visual quality. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. All code and checkpoints of Lumina-T2X are released to further foster creativity, transparency, and diversity in the generative AI community.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EbWf36quzd"
        ],
        "venue": [
          "/venue/EbWf36quzd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EbWf36quzd"
        ],
        "detail": [
          "https://openreview.net/forum?id=EbWf36quzd"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Gao Peng , Le Zhuo , Dongyang Liu , DU , Xu Luo , Longtian Qiu , Yuhang Zhang , Rongjie Huang , Shijie Geng , Renrui Zhang , Junlin Xie , Wenqi Shao , Zhengkai Jiang , Tianshuo Yang , Weicai Ye , Tong He , HE , Junjun He , Yu Qiao , Hongsheng Li Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this paper, we introduce the Lumina-T2X family -- a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a simple and scalable generative framework that can be adapted to various modalities, e.g., transforming noise into images, videos, multi-view 3D objects, or audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as |[nextline]| and |[nextframe]| tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. Advanced techniques like RoPE, KQ-Norm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT (PixArt-alpha), indicating that increasing the number of parameters significantly accelerates convergence of generative models without compromising visual quality. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. All code and checkpoints of Lumina-T2X are released to further foster creativity, transparency, and diversity in the generative AI community. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "EEgYUccwsV@OpenReview",
      "index": 172,
      "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
      "authors": [
        "Yiheng Xu",
        "Dunjie Lu",
        "Zhennan Shen",
        "Junli Wang",
        "Zekun Wang",
        "Yuchen Mao",
        "Caiming Xiong",
        "Tao Yu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gui",
        "tutorials",
        "agent",
        "web",
        "agents",
        "vlm",
        "replay",
        "agenttrek",
        "digital",
        "ourwork"
      ],
      "summary": "Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose \\ourwork, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EEgYUccwsV"
        ],
        "venue": [
          "/venue/EEgYUccwsV@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EEgYUccwsV"
        ],
        "detail": [
          "https://openreview.net/forum?id=EEgYUccwsV"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 10
      },
      "raw_excerpt": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials [PDF 6 ] [Copy] [Kimi 10 ] [REL] Authors : Yiheng Xu , Dunjie Lu , Zhennan Shen , Junli Wang , Zekun Wang , Yuchen Mao , Caiming Xiong , Tao Yu Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose \\ourwork, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "E48QvQppIN@OpenReview",
      "index": 173,
      "title": "Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences",
      "authors": [
        "Alan Amin",
        "Nate Gruver",
        "Yilun Kuang",
        "Yucen Li",
        "Hunter Elliott",
        "Calvin McCarter",
        "Aniruddh Raghu",
        "Peyton Greenside",
        "Andrew Gordon Wilson"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "antibodies",
        "sequences",
        "antibody",
        "informed",
        "evolving",
        "optimizes",
        "clonebo",
        "clonal",
        "bayesian",
        "mutations"
      ],
      "summary": "To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. Here we introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies in our bodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a *clonal family*. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody in our bodies. We guide our designs to fit previous measurements using a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic *in silico* experiments and designs stronger and more stable binders in *in vitro* wet lab experiments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=E48QvQppIN"
        ],
        "venue": [
          "/venue/E48QvQppIN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=E48QvQppIN"
        ],
        "detail": [
          "https://openreview.net/forum?id=E48QvQppIN"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Alan Amin , Nate Gruver , Yilun Kuang , Yucen Li , Hunter Elliott , Calvin McCarter , Aniruddh Raghu , Peyton Greenside , Andrew Gordon Wilson To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. Here we introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies in our bodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a *clonal family*. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody in our bodies. We guide our designs to fit previous measurements using a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic *in silico* experiments and designs stronger and more stable binders in *in vitro* wet lab experiments. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "E1EHO0imOb@OpenReview",
      "index": 174,
      "title": "Scaling FP8 training to trillion-token LLMs",
      "authors": [
        "Maxim Fishman",
        "Brian Chmiel",
        "Ron Banner",
        "Daniel Soudry"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "fp8",
        "swiglu",
        "trillion",
        "training",
        "gaudi2",
        "anonymous1252022",
        "instabilities",
        "amplification",
        "bf16",
        "megatron"
      ],
      "summary": "We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens --- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a ∼ ∼ 34 % throughput improvement. A reference implementation is supplied in https://github.com/Anonymous1252022/Megatron-DeepSpeed",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=E1EHO0imOb"
        ],
        "venue": [
          "/venue/E1EHO0imOb@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=E1EHO0imOb"
        ],
        "detail": [
          "https://openreview.net/forum?id=E1EHO0imOb"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Scaling FP8 training to trillion-token LLMs [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Maxim Fishman , Brian Chmiel , Ron Banner , Daniel Soudry We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens --- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a ∼ ∼ 34 % throughput improvement. A reference implementation is supplied in https://github.com/Anonymous1252022/Megatron-DeepSpeed Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Dzh0hQPpuf@OpenReview",
      "index": 175,
      "title": "Student-Informed Teacher Training",
      "authors": [
        "Nico Messikommer",
        "Jiaxu Xing",
        "Elie Aljalbout",
        "Davide Scaramuzza"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "teacher",
        "student",
        "privileged",
        "imitation",
        "observability",
        "task",
        "navigation",
        "latters",
        "imitated",
        "behaviors"
      ],
      "summary": "Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Dzh0hQPpuf"
        ],
        "venue": [
          "/venue/Dzh0hQPpuf@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Dzh0hQPpuf"
        ],
        "detail": [
          "https://openreview.net/forum?id=Dzh0hQPpuf"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Student-Informed Teacher Training [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Nico Messikommer , Jiaxu Xing , Elie Aljalbout , Davide Scaramuzza Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "DRiLWb8bJg@OpenReview",
      "index": 176,
      "title": "Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation",
      "authors": [
        "Eliot Xing",
        "Vernon Luk",
        "Jean Oh"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "deformables",
        "simulation",
        "sapo",
        "rigid",
        "bodies",
        "multiphysics",
        "differentiable",
        "rewarped",
        "tasks",
        "actor"
      ],
      "summary": "Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DRiLWb8bJg"
        ],
        "venue": [
          "/venue/DRiLWb8bJg@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DRiLWb8bJg"
        ],
        "detail": [
          "https://openreview.net/forum?id=DRiLWb8bJg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Eliot Xing , Vernon Luk , Jean Oh Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "DC8bsa9bzY@OpenReview",
      "index": 177,
      "title": "Estimating the Probabilities of Rare Outputs in Language Models",
      "authors": [
        "Gabriel Wu",
        "Jacob Hilton"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "probability",
        "sampling",
        "rare",
        "extrapolation",
        "estimation",
        "estimate",
        "worst",
        "activation",
        "language",
        "argmax"
      ],
      "summary": "We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=DC8bsa9bzY"
        ],
        "venue": [
          "/venue/DC8bsa9bzY@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=DC8bsa9bzY"
        ],
        "detail": [
          "https://openreview.net/forum?id=DC8bsa9bzY"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Estimating the Probabilities of Rare Outputs in Language Models [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : Gabriel Wu , Jacob Hilton We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Cnwz9jONi5@OpenReview",
      "index": 178,
      "title": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?",
      "authors": [
        "xueru wen",
        "Jie Lou",
        "Yaojie Lu",
        "Hongyu Lin",
        "XingYu",
        "Xinyu Lu",
        "Ben He",
        "Xianpei Han",
        "Debing Zhang",
        "Le Sun"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "accuracy",
        "rms",
        "policy",
        "barking",
        "reward",
        "measuring",
        "performance",
        "regressional",
        "rethinking",
        "goodhart"
      ],
      "summary": "Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data.Although this method is straightforward and widely adopted, the relationship between RM accuracy and downstream policy performance remains under-explored.In this work, we conduct experiments in a synthetic setting to investigate how differences in RM measured by accuracy translate into gaps in optimized policy performance.Our findings reveal that while there is a weak positive correlation between accuracy and downstream performance, policies optimized towards RMs with similar accuracy can exhibit quite different performance.Moreover, we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance. Through the lens of the Regressional Goodhart effect, we recognize that accuracy, when used for measuring RM quality, can fail to fully capture the potential RM overoptimization.This underscores the inadequacy of relying solely on accuracy to reflect their impact on policy optimization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Cnwz9jONi5"
        ],
        "venue": [
          "/venue/Cnwz9jONi5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Cnwz9jONi5"
        ],
        "detail": [
          "https://openreview.net/forum?id=Cnwz9jONi5"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree? [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : xueru wen , Jie Lou , Yaojie Lu , Hongyu Lin , XingYu , Xinyu Lu , Ben He , Xianpei Han , Debing Zhang , Le Sun Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data.Although this method is straightforward and widely adopted, the relationship between RM accuracy and downstream policy performance remains under-explored.In this work, we conduct experiments in a synthetic setting to investigate how differences in RM measured by accuracy translate into gaps in optimized policy performance.Our findings reveal that while there is a weak positive correlation between accuracy and downstream performance, policies optimized towards RMs with similar accuracy can exhibit quite different performance.Moreover, we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance. Through the lens of the Regressional Goodhart effect, we recognize that accuracy, when used for measuring RM quality, can fail to fully capture the potential RM overoptimization.This underscores the inadequacy of relying solely on accuracy to reflect their impact on policy optimization. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "BmG88rONaU@OpenReview",
      "index": 179,
      "title": "Test-time Adaptation for Cross-modal Retrieval with Query Shift",
      "authors": [
        "Haobin Li",
        "Peng Hu",
        "Qianjun Zhang",
        "Xi Peng",
        "XitingLiu",
        "Mouxing Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "query",
        "shift",
        "tcr",
        "retrieval",
        "modal",
        "cross",
        "adaptation",
        "queries",
        "modality",
        "test"
      ],
      "summary": "The success of most existing cross-modal retrieval methods heavily relies on the assumption that the given queries follow the same distribution of the source domain. However, such an assumption is easily violated in real-world scenarios due to the complexity and diversity of queries, thus leading to the query shift problem.Specifically, query shift refers to the online query stream originating from the domain that follows a different distribution with the source one.In this paper, we observe that query shift would not only diminish the uniformity (namely, within-modality scatter) of the query modality but also amplify the gap between query and gallery modalities. Based on the observations, we propose a novel method dubbed Test-time adaptation for Cross-modal Retrieval (TCR). In brief, TCR employs a novel module to refine the query predictions (namely, retrieval results of the query) and a joint objective to prevent query shift from disturbing the common space, thus achieving online adaptation for the cross-modal retrieval models with query shift.Expensive experiments demonstrate the effectiveness of the proposed TCR against query shift. The code will be released upon acceptance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BmG88rONaU"
        ],
        "venue": [
          "/venue/BmG88rONaU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BmG88rONaU"
        ],
        "detail": [
          "https://openreview.net/forum?id=BmG88rONaU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 7
      },
      "raw_excerpt": "Test-time Adaptation for Cross-modal Retrieval with Query Shift [PDF 3 ] [Copy] [Kimi 7 ] [REL] Authors : Haobin Li , Peng Hu , Qianjun Zhang , Xi Peng , XitingLiu , Mouxing Yang The success of most existing cross-modal retrieval methods heavily relies on the assumption that the given queries follow the same distribution of the source domain. However, such an assumption is easily violated in real-world scenarios due to the complexity and diversity of queries, thus leading to the query shift problem.Specifically, query shift refers to the online query stream originating from the domain that follows a different distribution with the source one.In this paper, we observe that query shift would not only diminish the uniformity (namely, within-modality scatter) of the query modality but also amplify the gap between query and gallery modalities. Based on the observations, we propose a novel method dubbed Test-time adaptation for Cross-modal Retrieval (TCR). In brief, TCR employs a novel module to refine the query predictions (namely, retrieval results of the query) and a joint objective to prevent query shift from disturbing the common space, thus achieving online adaptation for the cross-modal retrieval models with query shift.Expensive experiments demonstrate the effectiveness of the proposed TCR against query shift. The code will be released upon acceptance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "BgxsmpVoOX@OpenReview",
      "index": 180,
      "title": "Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance",
      "authors": [
        "Dongmin Park",
        "Sebin Kim",
        "Taehong Moon",
        "Minkyu Kim",
        "Kangwook Lee",
        "Jaewoong Cho"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "rare",
        "concepts",
        "diffusion",
        "r2f",
        "guidance",
        "t2i",
        "frequent",
        "compositional",
        "llm",
        "rarebench"
      ],
      "summary": "State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-tofrequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across anypre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BgxsmpVoOX"
        ],
        "venue": [
          "/venue/BgxsmpVoOX@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BgxsmpVoOX"
        ],
        "detail": [
          "https://openreview.net/forum?id=BgxsmpVoOX"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 5
      },
      "raw_excerpt": "Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance [PDF 1 ] [Copy] [Kimi 5 ] [REL] Authors : Dongmin Park , Sebin Kim , Taehong Moon , Minkyu Kim , Kangwook Lee , Jaewoong Cho State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-tofrequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across anypre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "BL4WBIfyrz@OpenReview",
      "index": 181,
      "title": "Lightweight Neural App Control",
      "authors": [
        "Filippos Christianos",
        "Georgios Papoudakis",
        "Thomas Coste",
        "Jianye HAO",
        "Jun Wang",
        "Kun Shao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "limac",
        "control",
        "app",
        "vlms",
        "lightweight",
        "mobile",
        "tuned",
        "fine",
        "source",
        "qwen2"
      ],
      "summary": "This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BL4WBIfyrz"
        ],
        "venue": [
          "/venue/BL4WBIfyrz@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BL4WBIfyrz"
        ],
        "detail": [
          "https://openreview.net/forum?id=BL4WBIfyrz"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Lightweight Neural App Control [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Filippos Christianos , Georgios Papoudakis , Thomas Coste , Jianye HAO , Jun Wang , Kun Shao This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "AUCYptvAf3@OpenReview",
      "index": 182,
      "title": "Multi-Robot Motion Planning with Diffusion Models",
      "authors": [
        "Yorai Shaoul",
        "Itamar Mishani",
        "Shivam Vats",
        "Jiaoyang Li",
        "Maxim Likhachev"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "robot",
        "diffusion",
        "multi",
        "planning",
        "environments",
        "models",
        "generating",
        "collision",
        "single",
        "mmd"
      ],
      "summary": "Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data.Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques---generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments. View video demonstrations in our supplementary material, and our code at: https://github.com/<removed_for_review>.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AUCYptvAf3"
        ],
        "venue": [
          "/venue/AUCYptvAf3@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AUCYptvAf3"
        ],
        "detail": [
          "https://openreview.net/forum?id=AUCYptvAf3"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Multi-Robot Motion Planning with Diffusion Models [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Yorai Shaoul , Itamar Mishani , Shivam Vats , Jiaoyang Li , Maxim Likhachev Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data.Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques---generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments. View video demonstrations in our supplementary material, and our code at: https://github.com/<removed_for_review>. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "APojAzJQiq@OpenReview",
      "index": 183,
      "title": "ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks",
      "authors": [
        "Qiang Liu",
        "Mengyu Chu",
        "Nils Thuerey"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "config",
        "conflict",
        "loss",
        "pinns",
        "informed",
        "physics",
        "terms",
        "update",
        "tum",
        "pinn"
      ],
      "summary": "The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code is available at https://tum-pbs.github.io/ConFIG",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=APojAzJQiq"
        ],
        "venue": [
          "/venue/APojAzJQiq@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=APojAzJQiq"
        ],
        "detail": [
          "https://openreview.net/forum?id=APojAzJQiq"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Qiang Liu , Mengyu Chu , Nils Thuerey The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code is available at https://tum-pbs.github.io/ConFIG Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "AEFVa6VMu1@OpenReview",
      "index": 184,
      "title": "Approximation algorithms for combinatorial optimization with predictions",
      "authors": [
        "Antonios Antoniadis",
        "Marek Elias",
        "Adam Polak",
        "Moritz Venzin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "approximation",
        "algorithms",
        "predictions",
        "steiner",
        "hardness",
        "problems",
        "optimization",
        "guarantees",
        "weight",
        "tree"
      ],
      "summary": "We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, Minimum Weight Perfect Matching, Knapsack, and Maximum Clique. Our algorithms produce an optimal solution when provided with perfect predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem. We conclude with an empirical evaluation of our approach.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=AEFVa6VMu1"
        ],
        "venue": [
          "/venue/AEFVa6VMu1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=AEFVa6VMu1"
        ],
        "detail": [
          "https://openreview.net/forum?id=AEFVa6VMu1"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Approximation algorithms for combinatorial optimization with predictions [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Antonios Antoniadis , Marek Elias , Adam Polak , Moritz Venzin We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, Minimum Weight Perfect Matching, Knapsack, and Maximum Clique. Our algorithms produce an optimal solution when provided with perfect predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem. We conclude with an empirical evaluation of our approach. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "A6Y7AqlzLW@OpenReview",
      "index": 185,
      "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
      "authors": [
        "Amrith Setlur",
        "Chirag Nagpal",
        "Adam Fisch",
        "Xinyang Geng",
        "Jacob Eisenstein",
        "Rishabh Agarwal",
        "Alekh Agarwal",
        "Jonathan Berant",
        "Aviral Kumar"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "prms",
        "policy",
        "base",
        "pavs",
        "orms",
        "progress",
        "provers",
        "step",
        "verifiers",
        "process"
      ],
      "summary": "A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a *base* policy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure *progress*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a *prover* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot. Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training **process advantage verifiers (PAVs)** to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable **one of the first results** showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A6Y7AqlzLW"
        ],
        "venue": [
          "/venue/A6Y7AqlzLW@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A6Y7AqlzLW"
        ],
        "detail": [
          "https://openreview.net/forum?id=A6Y7AqlzLW"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 8
      },
      "raw_excerpt": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning [PDF 8 ] [Copy] [Kimi 8 ] [REL] Authors : Amrith Setlur , Chirag Nagpal , Adam Fisch , Xinyang Geng , Jacob Eisenstein , Rishabh Agarwal , Alekh Agarwal , Jonathan Berant , Aviral Kumar A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a *base* policy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure *progress*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a *prover* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot. Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training **process advantage verifiers (PAVs)** to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable **one of the first results** showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9YNyiCJE3k@OpenReview",
      "index": 186,
      "title": "OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents",
      "authors": [
        "Zhaolin Hu",
        "Yixiao Zhou",
        "Zhongan Wang",
        "Xin Li",
        "Weimin Yang",
        "Hehe Fan",
        "Yi Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "osdas",
        "osda",
        "chemistry",
        "novo",
        "directing",
        "zeolites",
        "evaluator",
        "agent",
        "organic",
        "llms"
      ],
      "summary": "Zeolites are crystalline porous materials that have been widely utilized in petrochemical industries as well as sustainable chemistry areas. Synthesis of zeolites often requires small molecules termed Organic Structure Directing Agents (OSDAs), which are critical in forming the porous structure. Molecule generation models can aid the design of OSDAs, but they are limited by single functionality and lack of interactivity. Meanwhile, large language models (LLMs) such as GPT-4, as general-purpose artificial intelligence systems, excel in instruction comprehension, logical reasoning, and interactive communication. However, LLMs lack in-depth chemistry knowledge and first-principle computation capabilities, resulting in uncontrollable outcomes even after fine-tuning. In this paper, we propose OSDA Agent, an interactive OSDA design framework that leverages LLMs as the brain, coupled with computational chemistry tools. The OSDA Agent consists of three main components: the Actor, responsible for generating potential OSDA structures; the Evaluator, which assesses and scores the generated OSDAs using computational chemistry tools; and the Self-reflector, which produces reflective summaries based on the Evaluator's feedback to refine the Actor's subsequent outputs. Experiments on representative zeolite frameworks show the generation-evaluation-reflection-refinement workflow can perform de novo design of OSDAs with superior generation quality than the pure LLM model, generating candidates consistent with experimentally validated OSDAs and optimizing known OSDAs. The code and model will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9YNyiCJE3k"
        ],
        "venue": [
          "/venue/9YNyiCJE3k@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9YNyiCJE3k"
        ],
        "detail": [
          "https://openreview.net/forum?id=9YNyiCJE3k"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 12
      },
      "raw_excerpt": "OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents [PDF 5 ] [Copy] [Kimi 12 ] [REL] Authors : Zhaolin Hu , Yixiao Zhou , Zhongan Wang , Xin Li , Weimin Yang , Hehe Fan , Yi Yang Zeolites are crystalline porous materials that have been widely utilized in petrochemical industries as well as sustainable chemistry areas. Synthesis of zeolites often requires small molecules termed Organic Structure Directing Agents (OSDAs), which are critical in forming the porous structure. Molecule generation models can aid the design of OSDAs, but they are limited by single functionality and lack of interactivity. Meanwhile, large language models (LLMs) such as GPT-4, as general-purpose artificial intelligence systems, excel in instruction comprehension, logical reasoning, and interactive communication. However, LLMs lack in-depth chemistry knowledge and first-principle computation capabilities, resulting in uncontrollable outcomes even after fine-tuning. In this paper, we propose OSDA Agent, an interactive OSDA design framework that leverages LLMs as the brain, coupled with computational chemistry tools. The OSDA Agent consists of three main components: the Actor, responsible for generating potential OSDA structures; the Evaluator, which assesses and scores the generated OSDAs using computational chemistry tools; and the Self-reflector, which produces reflective summaries based on the Evaluator's feedback to refine the Actor's subsequent outputs. Experiments on representative zeolite frameworks show the generation-evaluation-reflection-refinement workflow can perform de novo design of OSDAs with superior generation quality than the pure LLM model, generating candidates consistent with experimentally validated OSDAs and optimizing known OSDAs. The code and model will be publicly available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9UGfOJBuL8@OpenReview",
      "index": 187,
      "title": "Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies",
      "authors": [
        "Hyuna Cho",
        "Ziquan Wei",
        "Seungjoo Lee",
        "Tingting Dan",
        "Guorong Wu",
        "Won Hwa Kim"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "disease",
        "longitudinal",
        "neurodegenerative",
        "ordinal",
        "progression",
        "generation",
        "data",
        "cohort",
        "conditional",
        "regression"
      ],
      "summary": "Modeling the progression of neurodegenerative diseases such as Alzheimer’s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal samples often contain irregular and large intervals between subject visits, which underscore the necessity for advanced data generation techniques that can accurately simulate disease progression over time. In this regime, we propose a novel conditional generative model for synthesizing longitudinal sequences and present its application to neurodegenerative disease data generation conditioned on multiple time-dependent ordinal factors, such as age and disease severity. Our method sequentially generates continuous data by bridging gaps between sparse data points with a diffusion model, ensuring a realistic representation of disease progression. The synthetic data are curated to integrate both cohort-level and individual-specific characteristics, where the cohort-level representations are modeled with an ordinal regression to capture longitudinally monotonic behavior. Extensive experiments on four AD biomarkers validate the superiority of our method over nine baseline approaches, highlighting its potential to be applied to a variety of longitudinal data generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9UGfOJBuL8"
        ],
        "venue": [
          "/venue/9UGfOJBuL8@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9UGfOJBuL8"
        ],
        "detail": [
          "https://openreview.net/forum?id=9UGfOJBuL8"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Hyuna Cho , Ziquan Wei , Seungjoo Lee , Tingting Dan , Guorong Wu , Won Hwa Kim Modeling the progression of neurodegenerative diseases such as Alzheimer’s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal samples often contain irregular and large intervals between subject visits, which underscore the necessity for advanced data generation techniques that can accurately simulate disease progression over time. In this regime, we propose a novel conditional generative model for synthesizing longitudinal sequences and present its application to neurodegenerative disease data generation conditioned on multiple time-dependent ordinal factors, such as age and disease severity. Our method sequentially generates continuous data by bridging gaps between sparse data points with a diffusion model, ensuring a realistic representation of disease progression. The synthetic data are curated to integrate both cohort-level and individual-specific characteristics, where the cohort-level representations are modeled with an ordinal regression to capture longitudinally monotonic behavior. Extensive experiments on four AD biomarkers validate the superiority of our method over nine baseline approaches, highlighting its potential to be applied to a variety of longitudinal data generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9NfHbWKqMF@OpenReview",
      "index": 188,
      "title": "SplatFormer: Point Transformer for Robust 3D Gaussian Splatting",
      "authors": [
        "Yutong Chen",
        "Marko Mihajlovic",
        "Sergey Prokudin",
        "Xiyi Chen",
        "Yiming Wang",
        "Siyu Tang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "3dgs",
        "views",
        "splatformer",
        "ood",
        "splatting",
        "rendering",
        "test",
        "gaussian",
        "transformer",
        "splats"
      ],
      "summary": "3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks. Code and data will be made public.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9NfHbWKqMF"
        ],
        "venue": [
          "/venue/9NfHbWKqMF@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9NfHbWKqMF"
        ],
        "detail": [
          "https://openreview.net/forum?id=9NfHbWKqMF"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "SplatFormer: Point Transformer for Robust 3D Gaussian Splatting [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Yutong Chen , Marko Mihajlovic , Sergey Prokudin , Xiyi Chen , Yiming Wang , Siyu Tang 3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks. Code and data will be made public. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "8xxEBAtD7y@OpenReview",
      "index": 189,
      "title": "Towards a Unified and Verified Understanding of Group-Operation Networks",
      "authors": [
        "Wilson Wu",
        "Louis Jaburi",
        "jacob drori",
        "Jason Gross"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "trained",
        "internals",
        "stander",
        "group",
        "chughtai",
        "networks",
        "operation",
        "explanations",
        "accuracy",
        "towards"
      ],
      "summary": "A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8xxEBAtD7y"
        ],
        "venue": [
          "/venue/8xxEBAtD7y@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8xxEBAtD7y"
        ],
        "detail": [
          "https://openreview.net/forum?id=8xxEBAtD7y"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Towards a Unified and Verified Understanding of Group-Operation Networks [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Wilson Wu , Louis Jaburi , jacob drori , Jason Gross A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "8bjspmAMBk@OpenReview",
      "index": 190,
      "title": "Quality Measures for Dynamic Graph Generative Models",
      "authors": [
        "Ryien Hosseini",
        "Filippo Simini",
        "Venkatram Vishwanath",
        "Rebecca Willett",
        "Henry Hoffmann"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dynamic",
        "graphs",
        "generative",
        "graph",
        "metrics",
        "measures",
        "metric",
        "models",
        "quality",
        "topology"
      ],
      "summary": "Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric specifically for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the Johnson-Lindenstrauss lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8bjspmAMBk"
        ],
        "venue": [
          "/venue/8bjspmAMBk@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8bjspmAMBk"
        ],
        "detail": [
          "https://openreview.net/forum?id=8bjspmAMBk"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Quality Measures for Dynamic Graph Generative Models [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Ryien Hosseini , Filippo Simini , Venkatram Vishwanath , Rebecca Willett , Henry Hoffmann Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric specifically for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the Johnson-Lindenstrauss lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "7XgKAabsPp@OpenReview",
      "index": 191,
      "title": "Theory on Mixture-of-Experts in Continual Learning",
      "authors": [
        "Hongbo Li",
        "Sen Lin",
        "Lingjie Duan",
        "Yingbin Liang",
        "Ness Shroff"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "moe",
        "experts",
        "continual",
        "tasks",
        "forgetting",
        "catastrophic",
        "gating",
        "rounds",
        "mixture",
        "expert"
      ],
      "summary": "Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been shown to effectively mitigate catastrophic forgetting in CL, by employing a gating network to sparsify and distribute diverse tasks among multiple experts. However, there is a lack of theoretical analysis of MoE and its impact on the learning performance in CL. This paper provides the first theoretical results to characterize the impact of MoE in CL via the lens of overparameterized linear regression tasks. We establish the benefit of MoE over a single expert by proving that the MoE model can diversify its experts to specialize in different tasks, while its router learns to select the right expert for each task and balance the loads across all experts. Our study further suggests an intriguing fact that the MoE in CL needs to terminate the update of the gating network after sufficient training rounds to attain system convergence, which is not needed in the existing MoE studies that do not consider the continual task arrival. Furthermore, we provide explicit expressions for the expected forgetting and overall generalization error to characterize the benefit of MoE in the learning performance in CL. Interestingly, adding more experts requires additional rounds before convergence, which may not enhance the learning performance. Finally, we conduct experiments on both synthetic and real datasets to extend these insights from linear models to deep neural networks (DNNs), which also shed light on the practical algorithm design for MoE in CL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7XgKAabsPp"
        ],
        "venue": [
          "/venue/7XgKAabsPp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7XgKAabsPp"
        ],
        "detail": [
          "https://openreview.net/forum?id=7XgKAabsPp"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 9
      },
      "raw_excerpt": "Theory on Mixture-of-Experts in Continual Learning [PDF 11 ] [Copy] [Kimi 9 ] [REL] Authors : Hongbo Li , Sen Lin , Lingjie Duan , Yingbin Liang , Ness Shroff Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been shown to effectively mitigate catastrophic forgetting in CL, by employing a gating network to sparsify and distribute diverse tasks among multiple experts. However, there is a lack of theoretical analysis of MoE and its impact on the learning performance in CL. This paper provides the first theoretical results to characterize the impact of MoE in CL via the lens of overparameterized linear regression tasks. We establish the benefit of MoE over a single expert by proving that the MoE model can diversify its experts to specialize in different tasks, while its router learns to select the right expert for each task and balance the loads across all experts. Our study further suggests an intriguing fact that the MoE in CL needs to terminate the update of the gating network after sufficient training rounds to attain system convergence, which is not needed in the existing MoE studies that do not consider the continual task arrival. Furthermore, we provide explicit expressions for the expected forgetting and overall generalization error to characterize the benefit of MoE in the learning performance in CL. Interestingly, adding more experts requires additional rounds before convergence, which may not enhance the learning performance. Finally, we conduct experiments on both synthetic and real datasets to extend these insights from linear models to deep neural networks (DNNs), which also shed light on the practical algorithm design for MoE in CL. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "7IzeL0kflu@OpenReview",
      "index": 192,
      "title": "Simplifying Deep Temporal Difference Learning",
      "authors": [
        "Matteo Gallici",
        "Mattie Fellows",
        "Benjamin Ellis",
        "Bartomeu Pou",
        "Ivan Masmitja",
        "Jakob Foerster",
        "Mario Martin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "replay",
        "buffer",
        "pqn",
        "policy",
        "ppo",
        "craftax",
        "learning",
        "smax",
        "reestablishes",
        "target"
      ],
      "summary": "Q Q -learning played a foundational role in the field reinforcement learning (RL).However, TD algorithms with off-policy data, such as Q Q -learning, or nonlinear function approximation like deep neural networks require several additional tricks to stabilise training, primarily a large replay buffer and target networks. Unfortunately, the delayed updating of frozen network parameters in the target network harms the sample efficiency and, similarly, the large replay buffer introduces memory and implementation overheads. In this paper, we investigate whether it is possible to accelerate and simplify off-policy TD training while maintaining its stability. Our key theoretical result demonstrates for the first time that regularisation techniques such as LayerNorm can yield provably convergent TD algorithms without the need for a target network or replay buffer, even with off-policy data. Empirically, we find that online, parallelised sampling enabled by vectorised environments stabilises training without the need for a large replay buffer. Motivated by these findings, we propose PQN, our simplified deep online Q Q -Learning algorithm.Surprisingly, this simple algorithm is competitive with more complex methods like: Rainbow in Atari, PPO-RNN in Craftax, QMix in Smax, and can be up to 50x faster than traditional DQN without sacrificing sample efficiency. In an era where PPO has become the go-to RL algorithm, PQN reestablishes off-policy Q Q -learning as a viable alternative.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7IzeL0kflu"
        ],
        "venue": [
          "/venue/7IzeL0kflu@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7IzeL0kflu"
        ],
        "detail": [
          "https://openreview.net/forum?id=7IzeL0kflu"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "Simplifying Deep Temporal Difference Learning [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Matteo Gallici , Mattie Fellows , Benjamin Ellis , Bartomeu Pou , Ivan Masmitja , Jakob Foerster , Mario Martin Q Q -learning played a foundational role in the field reinforcement learning (RL).However, TD algorithms with off-policy data, such as Q Q -learning, or nonlinear function approximation like deep neural networks require several additional tricks to stabilise training, primarily a large replay buffer and target networks. Unfortunately, the delayed updating of frozen network parameters in the target network harms the sample efficiency and, similarly, the large replay buffer introduces memory and implementation overheads. In this paper, we investigate whether it is possible to accelerate and simplify off-policy TD training while maintaining its stability. Our key theoretical result demonstrates for the first time that regularisation techniques such as LayerNorm can yield provably convergent TD algorithms without the need for a target network or replay buffer, even with off-policy data. Empirically, we find that online, parallelised sampling enabled by vectorised environments stabilises training without the need for a large replay buffer. Motivated by these findings, we propose PQN, our simplified deep online Q Q -Learning algorithm.Surprisingly, this simple algorithm is competitive with more complex methods like: Rainbow in Atari, PPO-RNN in Craftax, QMix in Smax, and can be up to 50x faster than traditional DQN without sacrificing sample efficiency. In an era where PPO has become the go-to RL algorithm, PQN reestablishes off-policy Q Q -learning as a viable alternative. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "7ANDviElAo@OpenReview",
      "index": 193,
      "title": "Graph Sparsification via Mixture of Graphs",
      "authors": [
        "Guibin Zhang",
        "Xiangguo SUN",
        "Yanwei Yue",
        "Chonghe Jiang",
        "Kun Wang",
        "Tianlong Chen",
        "Shirui Pan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mog",
        "sparsification",
        "graph",
        "experts",
        "mixture",
        "gnn",
        "node",
        "graphs",
        "sparsity",
        "pruning"
      ],
      "summary": "Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs. One effective approach to mitigate these challenges is graph sparsification, which involves removing non-essential edges to reduce computational overhead. However, previous graph sparsification methods often rely on a single global sparsity setting and uniform pruning criteria, failing to provide customized sparsification schemes for each node's complex local context.In this paper, we introduce Mixture-of-Graphs (MoG), leveraging the concept of Mixture-of-Experts (MoE), to dynamically select tailored pruning solutions for each node. Specifically, MoG incorporates multiple sparsifier experts, each characterized by unique sparsity levels and pruning criteria, and selects the appropriate experts for each node. Subsequently, MoG performs a mixture of the sparse graphs produced by different experts on the Grassmann manifold to derive an optimal sparse graph. One notable property of MoG is its entirely local nature, as it depends on the specific circumstances of each individual node. Extensive experiments on four large-scale OGB datasets and two superpixel datasets, equipped with five GNN backbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity levels ( 8.67 8.67 ), with performance equal to or better than the dense graph, (II) achieves 1.47 − 2.62 × 1.47 − 2.62 × speedup in GNN inference with negligible performance drop, and (III) boosts ``top-student'' GNN performance ( 1.02 1.02 on RevGNN+\\textsc{ogbn-proteins} and 1.74 1.74 on DeeperGCN+\\textsc{ogbg-ppa}).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7ANDviElAo"
        ],
        "venue": [
          "/venue/7ANDviElAo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7ANDviElAo"
        ],
        "detail": [
          "https://openreview.net/forum?id=7ANDviElAo"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Graph Sparsification via Mixture of Graphs [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Guibin Zhang , Xiangguo SUN , Yanwei Yue , Chonghe Jiang , Kun Wang , Tianlong Chen , Shirui Pan Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs. One effective approach to mitigate these challenges is graph sparsification, which involves removing non-essential edges to reduce computational overhead. However, previous graph sparsification methods often rely on a single global sparsity setting and uniform pruning criteria, failing to provide customized sparsification schemes for each node's complex local context.In this paper, we introduce Mixture-of-Graphs (MoG), leveraging the concept of Mixture-of-Experts (MoE), to dynamically select tailored pruning solutions for each node. Specifically, MoG incorporates multiple sparsifier experts, each characterized by unique sparsity levels and pruning criteria, and selects the appropriate experts for each node. Subsequently, MoG performs a mixture of the sparse graphs produced by different experts on the Grassmann manifold to derive an optimal sparse graph. One notable property of MoG is its entirely local nature, as it depends on the specific circumstances of each individual node. Extensive experiments on four large-scale OGB datasets and two superpixel datasets, equipped with five GNN backbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity levels ( 8.67 8.67 ), with performance equal to or better than the dense graph, (II) achieves 1.47 − 2.62 × 1.47 − 2.62 × speedup in GNN inference with negligible performance drop, and (III) boosts ``top-student'' GNN performance ( 1.02 1.02 on RevGNN+\\textsc{ogbn-proteins} and 1.74 1.74 on DeeperGCN+\\textsc{ogbg-ppa}). Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "78Nn4QJTEN@OpenReview",
      "index": 194,
      "title": "When Attention Sink Emerges in Language Models: An Empirical View",
      "authors": [
        "Xiangming Gu",
        "Tianyu Pang",
        "Chao Du",
        "Qian Liu",
        "Fengzhuo Zhang",
        "Cunxiao Du",
        "Ye Wang",
        "Min Lin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sink",
        "attention",
        "lms",
        "sinks",
        "emerges",
        "softmax",
        "optimization",
        "emerge",
        "scores",
        "normalization"
      ],
      "summary": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as **attention sink**. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how *optimization*, *data distribution*, *loss function*, and *model architecture* in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, *storing extra attention scores*, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=78Nn4QJTEN"
        ],
        "venue": [
          "/venue/78Nn4QJTEN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=78Nn4QJTEN"
        ],
        "detail": [
          "https://openreview.net/forum?id=78Nn4QJTEN"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 6
      },
      "raw_excerpt": "When Attention Sink Emerges in Language Models: An Empirical View [PDF 9 ] [Copy] [Kimi 6 ] [REL] Authors : Xiangming Gu , Tianyu Pang , Chao Du , Qian Liu , Fengzhuo Zhang , Cunxiao Du , Ye Wang , Min Lin Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as **attention sink**. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how *optimization*, *data distribution*, *loss function*, and *model architecture* in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, *storing extra attention scores*, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "69Fp4dcmJN@OpenReview",
      "index": 195,
      "title": "Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML",
      "authors": [
        "Ryan McKenna"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "bandmf",
        "factorization",
        "banded",
        "noise",
        "training",
        "iterations",
        "scale",
        "differentially",
        "matrix",
        "mechanism"
      ],
      "summary": "Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF , which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may be more than 10 4 10 4 and the number of model parameters may exceed 10 7 10 7 . In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to effectively handle settings with over 10 6 10 6 training iterations and 10 9 10 9 model parameters, with no utility degradation at smaller scales.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=69Fp4dcmJN"
        ],
        "venue": [
          "/venue/69Fp4dcmJN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=69Fp4dcmJN"
        ],
        "detail": [
          "https://openreview.net/forum?id=69Fp4dcmJN"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML [PDF 4 ] [Copy] [Kimi 2 ] [REL] Author : Ryan McKenna Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF , which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may be more than 10 4 10 4 and the number of model parameters may exceed 10 7 10 7 . In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to effectively handle settings with over 10 6 10 6 training iterations and 10 9 10 9 model parameters, with no utility degradation at smaller scales. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "5FXKgOxmb2@OpenReview",
      "index": 196,
      "title": "MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds",
      "authors": [
        "Leon Hetzel",
        "Johanna Sommer",
        "Bastian Rieck",
        "Fabian Theis",
        "Stephan Günnemann"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "scaffolds",
        "motif",
        "molecules",
        "magnet",
        "motifs",
        "substructures",
        "expressivity",
        "assignments",
        "compounds",
        "limitation"
      ],
      "summary": "Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions.Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods fail to represent substructures beyond their known motif set, posing a fundamental limitation for discovering novel compounds.To address this limitation and enhance structural expressivity, we propose to separate structure from features by abstracting motifs to scaffolds and, subsequently, allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that considers the entire molecular context and facilitates learning adequate assignments of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet, the first model to freely learn motifs. Importantly, we demonstrate that MAGNet's improved expressivity leads to molecules with more structural diversity and, at the same time, diverse atom and bond assignments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5FXKgOxmb2"
        ],
        "venue": [
          "/venue/5FXKgOxmb2@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5FXKgOxmb2"
        ],
        "detail": [
          "https://openreview.net/forum?id=5FXKgOxmb2"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 1
      },
      "raw_excerpt": "MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds [PDF 5 ] [Copy] [Kimi 1 ] [REL] Authors : Leon Hetzel , Johanna Sommer , Bastian Rieck , Fabian Theis , Stephan Günnemann Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions.Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods fail to represent substructures beyond their known motif set, posing a fundamental limitation for discovering novel compounds.To address this limitation and enhance structural expressivity, we propose to separate structure from features by abstracting motifs to scaffolds and, subsequently, allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that considers the entire molecular context and facilitates learning adequate assignments of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet, the first model to freely learn motifs. Importantly, we demonstrate that MAGNet's improved expressivity leads to molecules with more structural diversity and, at the same time, diverse atom and bond assignments. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "5BjQOUXq7i@OpenReview",
      "index": 197,
      "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
      "authors": [
        "Qian Liu",
        "Xiaosen Zheng",
        "Niklas Muennighoff",
        "Guangtao Zeng",
        "Longxu Dou",
        "Tianyu Pang",
        "Jing Jiang",
        "Min Lin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "regmix",
        "mixture",
        "mixtures",
        "regression",
        "performance",
        "model",
        "train",
        "data",
        "doremi",
        "tokens"
      ],
      "summary": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method outperforms both human selection and DoReMi in terms of both validation loss and downstream performance. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5BjQOUXq7i"
        ],
        "venue": [
          "/venue/5BjQOUXq7i@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5BjQOUXq7i"
        ],
        "detail": [
          "https://openreview.net/forum?id=5BjQOUXq7i"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "RegMix: Data Mixture as Regression for Language Model Pre-training [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Qian Liu , Xiaosen Zheng , Niklas Muennighoff , Guangtao Zeng , Longxu Dou , Tianyu Pang , Jing Jiang , Min Lin The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method outperforms both human selection and DoReMi in terms of both validation loss and downstream performance. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "5BSlakturs@OpenReview",
      "index": 198,
      "title": "Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds",
      "authors": [
        "Shuangqi Li",
        "Hieu Le",
        "Jingyi Xu",
        "Mathieu Salzmann"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "compositional",
        "pixart",
        "reliable",
        "seeds",
        "image",
        "text",
        "prompts",
        "composition",
        "inconsistencies",
        "diffusion"
      ],
      "summary": "Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as \"two dogs\" or \"a penguin on the right of a bowl\". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuning text-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3\\% and 19.5\\% for Stable Diffusion and PixArt- α α , respectively. Spatial composition sees even larger gains, with 60.7\\% for Stable Diffusion and 21.1\\% for PixArt- α α .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5BSlakturs"
        ],
        "venue": [
          "/venue/5BSlakturs@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5BSlakturs"
        ],
        "detail": [
          "https://openreview.net/forum?id=5BSlakturs"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Shuangqi Li , Hieu Le , Jingyi Xu , Mathieu Salzmann Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as \"two dogs\" or \"a penguin on the right of a bowl\". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuning text-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3\\% and 19.5\\% for Stable Diffusion and PixArt- α α , respectively. Spatial composition sees even larger gains, with 60.7\\% for Stable Diffusion and 21.1\\% for PixArt- α α . Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "4gaySj8kvX@OpenReview",
      "index": 199,
      "title": "Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research",
      "authors": [
        "Michał Bortkiewicz",
        "Władysław Pałucki",
        "Vivek Myers",
        "Tadeusz Dziarmaga",
        "Tomasz Arczewski",
        "Łukasz Kuciński",
        "Benjamin Eysenbach"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "jaxgcrl",
        "gcrl",
        "2316",
        "readme",
        "self",
        "4open",
        "reinforcement",
        "supervised",
        "anonymous",
        "conditioned"
      ],
      "summary": "Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover *new* behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (`JaxGCRL`) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to 22 × 22 × . Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Code: [https://anonymous.4open.science/r/JaxGCRL-2316/README.md](https://anonymous.4open.science/r/JaxGCRL-2316/README.md)",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4gaySj8kvX"
        ],
        "venue": [
          "/venue/4gaySj8kvX@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4gaySj8kvX"
        ],
        "detail": [
          "https://openreview.net/forum?id=4gaySj8kvX"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Michał Bortkiewicz , Władysław Pałucki , Vivek Myers , Tadeusz Dziarmaga , Tomasz Arczewski , Łukasz Kuciński , Benjamin Eysenbach Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover *new* behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (`JaxGCRL`) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to 22 × 22 × . Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Code: [https://anonymous.4open.science/r/JaxGCRL-2316/README.md](https://anonymous.4open.science/r/JaxGCRL-2316/README.md) Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "4NTrco82W0@OpenReview",
      "index": 200,
      "title": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks",
      "authors": [
        "Rui Hu",
        "Yifan Zhang",
        "Zhuoran Li",
        "Longbo Huang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "regression",
        "linex",
        "losses",
        "training",
        "flow",
        "exploitation",
        "gflownets",
        "loss",
        "backward",
        "generative"
      ],
      "summary": "Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4NTrco82W0"
        ],
        "venue": [
          "/venue/4NTrco82W0@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4NTrco82W0"
        ],
        "detail": [
          "https://openreview.net/forum?id=4NTrco82W0"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Rui Hu , Yifan Zhang , Zhuoran Li , Longbo Huang Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "4HRRcqE9SU@OpenReview",
      "index": 201,
      "title": "ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction",
      "authors": [
        "Ziyu Tang",
        "Weicai Ye",
        "Yifan Wang",
        "Di Huang",
        "Hujun Bao",
        "Tong He",
        "Guofeng Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "deflection",
        "sdf",
        "normal",
        "reconstruction",
        "rendering",
        "geometric",
        "priors",
        "accuracy",
        "indoor",
        "surfaces"
      ],
      "summary": "Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Deflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4HRRcqE9SU"
        ],
        "venue": [
          "/venue/4HRRcqE9SU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4HRRcqE9SU"
        ],
        "detail": [
          "https://openreview.net/forum?id=4HRRcqE9SU"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Ziyu Tang , Weicai Ye , Yifan Wang , Di Huang , Hujun Bao , Tong He , Guofeng Zhang Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Deflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "44cMlQSreK@OpenReview",
      "index": 202,
      "title": "On Quantizing Neural Representation for Variable-Rate Video Coding",
      "authors": [
        "Junqi Shi",
        "Zhujia Chen",
        "Hanfei Li",
        "Qi Zhao",
        "Ming Lu",
        "Tong Chen",
        "Zhan Ma"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "quantization",
        "inr",
        "coding",
        "rate",
        "variable",
        "neuroquant",
        "ptq",
        "video",
        "int2",
        "quantizing"
      ],
      "summary": "This work introduces NeuroQuant, a novel post-training quantization (PTQ) approach tailored to non-generalized Implicit Neural Representations for variable-rate Video Coding (INR-VC). Unlike existing methods that require extensive weight retraining for each target bitrate, we hypothesize that variable-rate coding can be achieved by adjusting quantization parameters (QPs) of pre-trained weights. Our study reveals that traditional quantization methods, which assume inter-layer independence, are ineffective for non-generalized INR-VC models due to significant dependencies across layers. To address this, we redefine variable-rate INR-VC as a mixed-precision quantization problem and establish a theoretical framework for sensitivity criteria aimed at simplified, fine-grained rate control. Additionally, we propose network-wise calibration and channel-wise quantization strategies to minimize quantization-induced errors, arriving at a unified formula for representation-oriented PTQ calibration. Our experimental evaluations demonstrate that NeuroQuant significantly outperforms existing techniques in varying bitwidth quantization and compression efficiency, accelerating encoding significantly and enabling quantization down to INT2 with minimal reconstruction loss. This work achieves variable-rate INR-VC through weight quantization for the first time and lays a theoretical foundation for future research in rate-distortion optimization, advancing the field of video coding technology.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=44cMlQSreK"
        ],
        "venue": [
          "/venue/44cMlQSreK@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=44cMlQSreK"
        ],
        "detail": [
          "https://openreview.net/forum?id=44cMlQSreK"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "On Quantizing Neural Representation for Variable-Rate Video Coding [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Junqi Shi , Zhujia Chen , Hanfei Li , Qi Zhao , Ming Lu , Tong Chen , Zhan Ma This work introduces NeuroQuant, a novel post-training quantization (PTQ) approach tailored to non-generalized Implicit Neural Representations for variable-rate Video Coding (INR-VC). Unlike existing methods that require extensive weight retraining for each target bitrate, we hypothesize that variable-rate coding can be achieved by adjusting quantization parameters (QPs) of pre-trained weights. Our study reveals that traditional quantization methods, which assume inter-layer independence, are ineffective for non-generalized INR-VC models due to significant dependencies across layers. To address this, we redefine variable-rate INR-VC as a mixed-precision quantization problem and establish a theoretical framework for sensitivity criteria aimed at simplified, fine-grained rate control. Additionally, we propose network-wise calibration and channel-wise quantization strategies to minimize quantization-induced errors, arriving at a unified formula for representation-oriented PTQ calibration. Our experimental evaluations demonstrate that NeuroQuant significantly outperforms existing techniques in varying bitwidth quantization and compression efficiency, accelerating encoding significantly and enabling quantization down to INT2 with minimal reconstruction loss. This work achieves variable-rate INR-VC through weight quantization for the first time and lays a theoretical foundation for future research in rate-distortion optimization, advancing the field of video coding technology. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "3ddi7Uss2A@OpenReview",
      "index": 203,
      "title": "What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis",
      "authors": [
        "Weronika Ormaniec",
        "Felix Dangel",
        "Sidak Pal Singh"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hessian",
        "transformer",
        "mlps",
        "architectural",
        "inarguably",
        "layer",
        "cnns",
        "attention",
        "transformers",
        "dependencies"
      ],
      "summary": "The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning - to the extent that Transformers are often accompanied by adaptive optimizers, layer normalization, learning rate warmup, and more, in comparison to MLPs/CNNs. The root causes behind these outward manifestations, and the precise mechanisms that govern them, remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures - grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer's Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks. Our results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer’s unique optimization landscape and the challenges it poses.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3ddi7Uss2A"
        ],
        "venue": [
          "/venue/3ddi7Uss2A@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3ddi7Uss2A"
        ],
        "detail": [
          "https://openreview.net/forum?id=3ddi7Uss2A"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Weronika Ormaniec , Felix Dangel , Sidak Pal Singh The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning - to the extent that Transformers are often accompanied by adaptive optimizers, layer normalization, learning rate warmup, and more, in comparison to MLPs/CNNs. The root causes behind these outward manifestations, and the precise mechanisms that govern them, remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures - grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer's Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks. Our results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer’s unique optimization landscape and the challenges it poses. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "3Oli4u6q3p@OpenReview",
      "index": 204,
      "title": "RelitLRM: Generative Relightable Radiance for Large Reconstruction Models",
      "authors": [
        "Tianyuan Zhang",
        "Zhengfei Kuang",
        "Haian Jin",
        "Zexiang Xu",
        "Sai Bi",
        "Hao Tan",
        "HE Zhang",
        "Yiwei Hu",
        "Milos Hasan",
        "William Freeman",
        "Kai Zhang",
        "Fujun Luan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "relitlrm",
        "relightable",
        "illuminations",
        "appearance",
        "lighting",
        "feed",
        "reconstruction",
        "view",
        "lrm",
        "baking"
      ],
      "summary": "We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relitlrm.github.io/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3Oli4u6q3p"
        ],
        "venue": [
          "/venue/3Oli4u6q3p@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3Oli4u6q3p"
        ],
        "detail": [
          "https://openreview.net/forum?id=3Oli4u6q3p"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "RelitLRM: Generative Relightable Radiance for Large Reconstruction Models [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Tianyuan Zhang , Zhengfei Kuang , Haian Jin , Zexiang Xu , Sai Bi , Hao Tan , HE Zhang , Yiwei Hu , Milos Hasan , William Freeman , Kai Zhang , Fujun Luan We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relitlrm.github.io/. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "2kGKsyhtvh@OpenReview",
      "index": 205,
      "title": "Towards hyperparameter-free optimization with differential privacy",
      "authors": [
        "Ruixuan Liu",
        "Zhiqi Bu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hyperparameter",
        "hyperparameters",
        "optimization",
        "privacy",
        "schedule",
        "hyperparamter",
        "differential",
        "tuning",
        "computationally",
        "automatic"
      ],
      "summary": "Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparamter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2kGKsyhtvh"
        ],
        "venue": [
          "/venue/2kGKsyhtvh@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2kGKsyhtvh"
        ],
        "detail": [
          "https://openreview.net/forum?id=2kGKsyhtvh"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Towards hyperparameter-free optimization with differential privacy [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Ruixuan Liu , Zhiqi Bu Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparamter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "2hcfoCHKoB@OpenReview",
      "index": 206,
      "title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model",
      "authors": [
        "Yi Liu",
        "Changran Xu",
        "Yunhao Zhou",
        "Zeju Li",
        "Qiang Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "verilog",
        "deeprtl",
        "understanding",
        "generation",
        "language",
        "code",
        "unified",
        "natural",
        "tasks",
        "descriptions"
      ],
      "summary": "Recent advancements in large language models (LLMs) have demonstrated significant potential in automating the generation of hardware description language (HDL) code from high-level natural language instructions. While fine-tuning has improved these models' performance in hardware design tasks, prior efforts have largely focused on Verilog code generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, which hampers the generation of high-quality, synthesizable designs. To overcome these limitations, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2hcfoCHKoB"
        ],
        "venue": [
          "/venue/2hcfoCHKoB@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2hcfoCHKoB"
        ],
        "detail": [
          "https://openreview.net/forum?id=2hcfoCHKoB"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Yi Liu , Changran Xu , Yunhao Zhou , Zeju Li , Qiang Xu Recent advancements in large language models (LLMs) have demonstrated significant potential in automating the generation of hardware description language (HDL) code from high-level natural language instructions. While fine-tuning has improved these models' performance in hardware design tasks, prior efforts have largely focused on Verilog code generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, which hampers the generation of high-quality, synthesizable designs. To overcome these limitations, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "2c7pfOqu9k@OpenReview",
      "index": 207,
      "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
      "authors": [
        "Jinwei Yao",
        "Kaiqi Chen",
        "Kexun Zhang",
        "Jiaxuan You",
        "Binhang Yuan",
        "Zeke Wang",
        "Tao Lin"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cache",
        "deft",
        "attention",
        "tree",
        "prefixes",
        "decoding",
        "shared",
        "gpu",
        "flash",
        "calculation"
      ],
      "summary": "Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through **KV-Guided Grouping**, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose **Flattened Tree KV Splitting**, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99 % % KV cache IO and nearly 100 % % IO for partial results during attention calculation, DeFT achieves up to 2.52/3.82 × × speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2c7pfOqu9k"
        ],
        "venue": [
          "/venue/2c7pfOqu9k@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2c7pfOqu9k"
        ],
        "detail": [
          "https://openreview.net/forum?id=2c7pfOqu9k"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : Jinwei Yao , Kaiqi Chen , Kexun Zhang , Jiaxuan You , Binhang Yuan , Zeke Wang , Tao Lin Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through **KV-Guided Grouping**, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose **Flattened Tree KV Splitting**, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99 % % KV cache IO and nearly 100 % % IO for partial results during attention calculation, DeFT achieves up to 2.52/3.82 × × speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "28abpUEICJ@OpenReview",
      "index": 208,
      "title": "CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations",
      "authors": [
        "Noga Mudrik",
        "Ryan Ly",
        "Oliver Ruebel",
        "Adam Charles"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "creimbo",
        "brain",
        "session",
        "recordings",
        "neural",
        "interactions",
        "sub",
        "circuits",
        "ensembles",
        "observations"
      ],
      "summary": "Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods, however, often fail to fully harness the richness of such data, as they provide either uninterpretable representations (e.g., via deep networks) or oversimplify models (e.g., by assuming stationary dynamics or analyzing each session independently). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. Specifically, we assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles---functional groups of neurons---such that the time-varying decomposition of these sub-circuits defines how the ensembles' interactions evolve over time non-stationarily and non-linearly.We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, **CREIMBO** (Cross-Regional Ensemble Interactions in Multi-view Brain Observations). CREIMBO identifies the hidden composition of per-session neural ensembles through novel graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics in human high-density electrode recordings, including cross-subject neural mechanisms as well as inter- vs. intra-region dynamical motifs. Furthermore, using mouse whole-brain recordings, we show CREIMBO's ability to discover dynamical interactions that capture task and behavioral variables and meaningfully align with the biological importance of the brain areas they represent.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=28abpUEICJ"
        ],
        "venue": [
          "/venue/28abpUEICJ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=28abpUEICJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=28abpUEICJ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Noga Mudrik , Ryan Ly , Oliver Ruebel , Adam Charles Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods, however, often fail to fully harness the richness of such data, as they provide either uninterpretable representations (e.g., via deep networks) or oversimplify models (e.g., by assuming stationary dynamics or analyzing each session independently). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. Specifically, we assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles---functional groups of neurons---such that the time-varying decomposition of these sub-circuits defines how the ensembles' interactions evolve over time non-stationarily and non-linearly.We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, **CREIMBO** (Cross-Regional Ensemble Interactions in Multi-view Brain Observations). CREIMBO identifies the hidden composition of per-session neural ensembles through novel graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics in human high-density electrode recordings, including cross-subject neural mechanisms as well as inter- vs. intra-region dynamical motifs. Furthermore, using mouse whole-brain recordings, we show CREIMBO's ability to discover dynamical interactions that capture task and behavioral variables and meaningfully align with the biological importance of the brain areas they represent. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "1qP3lsatCR@OpenReview",
      "index": 209,
      "title": "NetMoE: Accelerating MoE Training through Dynamic Sample Placement",
      "authors": [
        "Xinyi Liu",
        "Yujie Wang",
        "Fangcheng Fu",
        "Xupeng Miao",
        "Shenhan Zhu",
        "Xiaonan Nie",
        "Bin CUI"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "moe",
        "placement",
        "training",
        "netmoe",
        "experts",
        "gpus",
        "communication",
        "sample",
        "tokens",
        "expert"
      ],
      "summary": "Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather than all. In distributed training, as experts are distributed among different GPUs, All-to-All communication is necessary to exchange the training tokens among the GPUs after each time of expert routing. Due to the frequent and voluminous data exchanges, All-to-All communication has become a notable challenge to training efficiency.In this paper, we manage to accelerate All-to-All communication in MoE models from the training sample perspective, which is unexplored so far. In particular, we put forward the observation that tokens in the same training sample have certain levels of locality in expert routing. Motivated by this, we develop \\name, which takes such locality into account and dynamically rearranges the placement of training samples to minimize All-to-All communication costs. Specifically, we model the All-to-All communication given the sample placement and formulate an integer programming problem to deduce the optimal placement in polynomial time. Experiments with 32 GPUs show that NetMoE achieves a maximum efficiency improvement of 1.67 × 1.67 × compared with state-of-the-art MoE training frameworks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1qP3lsatCR"
        ],
        "venue": [
          "/venue/1qP3lsatCR@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1qP3lsatCR"
        ],
        "detail": [
          "https://openreview.net/forum?id=1qP3lsatCR"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 6
      },
      "raw_excerpt": "NetMoE: Accelerating MoE Training through Dynamic Sample Placement [PDF 7 ] [Copy] [Kimi 6 ] [REL] Authors : Xinyi Liu , Yujie Wang , Fangcheng Fu , Xupeng Miao , Shenhan Zhu , Xiaonan Nie , Bin CUI Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather than all. In distributed training, as experts are distributed among different GPUs, All-to-All communication is necessary to exchange the training tokens among the GPUs after each time of expert routing. Due to the frequent and voluminous data exchanges, All-to-All communication has become a notable challenge to training efficiency.In this paper, we manage to accelerate All-to-All communication in MoE models from the training sample perspective, which is unexplored so far. In particular, we put forward the observation that tokens in the same training sample have certain levels of locality in expert routing. Motivated by this, we develop \\name, which takes such locality into account and dynamically rearranges the placement of training samples to minimize All-to-All communication costs. Specifically, we model the All-to-All communication given the sample placement and formulate an integer programming problem to deduce the optimal placement in polynomial time. Experiments with 32 GPUs show that NetMoE achieves a maximum efficiency improvement of 1.67 × 1.67 × compared with state-of-the-art MoE training frameworks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "1jcnvghayD@OpenReview",
      "index": 210,
      "title": "Bayesian Optimization via Continual Variational Last Layer Training",
      "authors": [
        "Paul Brunzema",
        "Mikkel Jordahn",
        "John Willes",
        "Sebastian Trimpe",
        "Jasper Snoek",
        "James Harrison"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gps",
        "bayesian",
        "bnns",
        "performance",
        "vblls",
        "vbll",
        "optimization",
        "conditioning",
        "variational",
        "last"
      ],
      "summary": "Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1jcnvghayD"
        ],
        "venue": [
          "/venue/1jcnvghayD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1jcnvghayD"
        ],
        "detail": [
          "https://openreview.net/forum?id=1jcnvghayD"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Bayesian Optimization via Continual Variational Last Layer Training [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Paul Brunzema , Mikkel Jordahn , John Willes , Sebastian Trimpe , Jasper Snoek , James Harrison Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "11xgiMEI5o@OpenReview",
      "index": 211,
      "title": "OmniRe: Omni Urban Scene Reconstruction",
      "authors": [
        "Ziyu Chen",
        "Jiawei Yang",
        "Jiahui Huang",
        "Riccardo de Lutio",
        "Janick Martinez Esturo",
        "Boris Ivanovic",
        "Or Litany",
        "Zan Gojcic",
        "Sanja Fidler",
        "Marco Pavone",
        "Li Song",
        "Yue Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "omnire",
        "urban",
        "dynamic",
        "scenes",
        "scene",
        "omni",
        "reconstruction",
        "vehicles",
        "simulation",
        "human"
      ],
      "summary": "We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds demanded by downstream applications, e.g., the simulation of human behavior. OmniRe extends beyond vehicle modeling to enable accurate, full-length reconstruction of diverse dynamic objects in urban scenes. Our approach builds scene graphs on 3DGS and constructs multiple Gaussian representations in canonical spaces that model various dynamic actors, including vehicles, pedestrians, cyclists, and others. OmniRe allows holistically reconstructing any dynamic object in the scene, enabling advanced simulations (~60 Hz) that include human-participated scenarios, such as pedestrian behavior simulation and human-vehicle interaction. This comprehensive simulation capability is unmatched by existing methods. Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We further extend our results to 5 additional popular driving datasets to demonstrate its generalizability on common urban scenes. We will make the code and data publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=11xgiMEI5o"
        ],
        "venue": [
          "/venue/11xgiMEI5o@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=11xgiMEI5o"
        ],
        "detail": [
          "https://openreview.net/forum?id=11xgiMEI5o"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "OmniRe: Omni Urban Scene Reconstruction [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Ziyu Chen , Jiawei Yang , Jiahui Huang , Riccardo de Lutio , Janick Martinez Esturo , Boris Ivanovic , Or Litany , Zan Gojcic , Sanja Fidler , Marco Pavone , Li Song , Yue Wang We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds demanded by downstream applications, e.g., the simulation of human behavior. OmniRe extends beyond vehicle modeling to enable accurate, full-length reconstruction of diverse dynamic objects in urban scenes. Our approach builds scene graphs on 3DGS and constructs multiple Gaussian representations in canonical spaces that model various dynamic actors, including vehicles, pedestrians, cyclists, and others. OmniRe allows holistically reconstructing any dynamic object in the scene, enabling advanced simulations (~60 Hz) that include human-participated scenarios, such as pedestrian behavior simulation and human-vehicle interaction. This comprehensive simulation capability is unmatched by existing methods. Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We further extend our results to 5 additional popular driving datasets to demonstrate its generalizability on common urban scenes. We will make the code and data publicly available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "10JOlFIPjt@OpenReview",
      "index": 212,
      "title": "In vivo cell-type and brain region classification via multimodal contrastive learning",
      "authors": [
        "Han Yu",
        "Hanrui Lyu",
        "YiXun Xu",
        "Charlie Windolf",
        "Eric Lee",
        "Fan Yang",
        "Andrew Shelton",
        "Olivier Winter",
        "International Brain Laboratory",
        "Eva Dyer",
        "Chandramouli Chandrasekaran",
        "Nicholas Steinmetz",
        "Liam Paninski",
        "Cole Hurwitz"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "brain",
        "cell",
        "multimodal",
        "contrastive",
        "region",
        "classification",
        "electrophysiological",
        "neurons",
        "type",
        "recorded"
      ],
      "summary": "Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for an opto-tagged visual cortex dataset and for brain region classification of the public International Brain Laboratory brain-wide map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=10JOlFIPjt"
        ],
        "venue": [
          "/venue/10JOlFIPjt@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=10JOlFIPjt"
        ],
        "detail": [
          "https://openreview.net/forum?id=10JOlFIPjt"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "In vivo cell-type and brain region classification via multimodal contrastive learning [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Han Yu , Hanrui Lyu , YiXun Xu , Charlie Windolf , Eric Lee , Fan Yang , Andrew Shelton , Olivier Winter , International Brain Laboratory , Eva Dyer , Chandramouli Chandrasekaran , Nicholas Steinmetz , Liam Paninski , Cole Hurwitz Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for an opto-tagged visual cortex dataset and for brain region classification of the public International Brain Laboratory brain-wide map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "0h6v4SpLCY@OpenReview",
      "index": 213,
      "title": "Universal generalization guarantees for Wasserstein distributionally robust models",
      "authors": [
        "Tam Le",
        "Jerome Malick"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "robust",
        "wasserstein",
        "guarantees",
        "distributionally",
        "generalization",
        "models",
        "regularizations",
        "cases",
        "universal",
        "curse"
      ],
      "summary": "Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0h6v4SpLCY"
        ],
        "venue": [
          "/venue/0h6v4SpLCY@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0h6v4SpLCY"
        ],
        "detail": [
          "https://openreview.net/forum?id=0h6v4SpLCY"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Universal generalization guarantees for Wasserstein distributionally robust models [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Tam Le , Jerome Malick Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "0fhzSFsGUT@OpenReview",
      "index": 214,
      "title": "PETRA: Parallel End-to-end Training with Reversible Architectures",
      "authors": [
        "Stephane Rivaud",
        "Louis Fournier",
        "Thomas Pumir",
        "Eugene Belilovsky",
        "Michael Eickenberg",
        "Edouard Oyallon"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "petra",
        "reversible",
        "architectures",
        "resnet",
        "parallelizing",
        "backpropagation",
        "stashing",
        "end",
        "autograd",
        "training"
      ],
      "summary": "Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model training. We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations. PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other. By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed. We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on standard computer vision benchmarks, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0fhzSFsGUT"
        ],
        "venue": [
          "/venue/0fhzSFsGUT@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0fhzSFsGUT"
        ],
        "detail": [
          "https://openreview.net/forum?id=0fhzSFsGUT"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "PETRA: Parallel End-to-end Training with Reversible Architectures [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Stephane Rivaud , Louis Fournier , Thomas Pumir , Eugene Belilovsky , Michael Eickenberg , Edouard Oyallon Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model training. We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations. PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other. By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed. We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on standard computer vision benchmarks, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "0fJfVOSUra@OpenReview",
      "index": 215,
      "title": "ThunderKittens: Simple, Fast, and Adorable Adorable Kernels",
      "authors": [
        "Benjamin Spector",
        "Simran Arora",
        "Aaryan Singhal",
        "Arjun Parthasarathy",
        "Dan Fu",
        "Christopher Re"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "kernels",
        "thunderkittens",
        "adorable",
        "operations",
        "attention",
        "abstractions",
        "tiles",
        "gpu",
        "16x16",
        "cublas"
      ],
      "summary": "The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention.The diverse hardware capabilities of GPUs might suggest that we need a wide variety of techniques to achieve high performance. However, our work explores whether a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use and maintain. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like parallel compute operations over tiles, (2) at the thread-block level, we provide a template for overlapping asynchronous operations across parallel warps, and (3) at the grid-level, TK can help hide the block launch and tear-down, and memory costs. We show the value of TK by providing kernels that match or outperform prior kernels for a range of AI operations. We match CuBLAS and FlashAttention-3 on GEMM and attention inference, and outperforms the strongest baselines by 10 − 40 % 10 − 40 % on attention backwards, 8 × 8 × on state space models, and 14 × 14 × on linear attention.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0fJfVOSUra"
        ],
        "venue": [
          "/venue/0fJfVOSUra@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0fJfVOSUra"
        ],
        "detail": [
          "https://openreview.net/forum?id=0fJfVOSUra"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "ThunderKittens: Simple, Fast, and Adorable Adorable Kernels [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Benjamin Spector , Simran Arora , Aaryan Singhal , Arjun Parthasarathy , Dan Fu , Christopher Re The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention.The diverse hardware capabilities of GPUs might suggest that we need a wide variety of techniques to achieve high performance. However, our work explores whether a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use and maintain. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like parallel compute operations over tiles, (2) at the thread-block level, we provide a template for overlapping asynchronous operations across parallel warps, and (3) at the grid-level, TK can help hide the block launch and tear-down, and memory costs. We show the value of TK by providing kernels that match or outperform prior kernels for a range of AI operations. We match CuBLAS and FlashAttention-3 on GEMM and attention inference, and outperforms the strongest baselines by 10 − 40 % 10 − 40 % on attention backwards, 8 × 8 × on state space models, and 14 × 14 × on linear attention. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "0bmGL4q7vJ@OpenReview",
      "index": 216,
      "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage",
      "authors": [
        "Zhi Gao",
        "Bofei Zhang",
        "Pengxiang Li",
        "Xiaojian Ma",
        "Tao Yuan",
        "Yue Fan",
        "Yuwei Wu",
        "Yunde Jia",
        "Song-Chun Zhu",
        "Qing Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "usage",
        "agent",
        "traj",
        "modal",
        "vlm",
        "tool",
        "verifier",
        "tuning",
        "multi",
        "minicpm"
      ],
      "summary": "The advancement of large language models (LLMs) prompts the development of multi-modal agents, providing a feasible way to solve practical tasks by using tools. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o model to separately generate queries, files, and trajectories, followed by a query-file verifier and trajectory verifier. Based on the data synthesis pipeline, we collect the MM-traj dataset with 20k tasks using 10 tools. Then, we build the T3-agent that uses MiniCPM-V as the controller Trajectory Tuning for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-agent has achieved remarkable improvements and outperforms GPT-4 driven agents by 10%, showing the effectiveness of the proposed data synthesis pipeline that leads to better reasoning capabilities in tool usage.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0bmGL4q7vJ"
        ],
        "venue": [
          "/venue/0bmGL4q7vJ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0bmGL4q7vJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=0bmGL4q7vJ"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 12
      },
      "raw_excerpt": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage [PDF 10 ] [Copy] [Kimi 12 ] [REL] Authors : Zhi Gao , Bofei Zhang , Pengxiang Li , Xiaojian Ma , Tao Yuan , Yue Fan , Yuwei Wu , Yunde Jia , Song-Chun Zhu , Qing Li The advancement of large language models (LLMs) prompts the development of multi-modal agents, providing a feasible way to solve practical tasks by using tools. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o model to separately generate queries, files, and trajectories, followed by a query-file verifier and trajectory verifier. Based on the data synthesis pipeline, we collect the MM-traj dataset with 20k tasks using 10 tools. Then, we build the T3-agent that uses MiniCPM-V as the controller Trajectory Tuning for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-agent has achieved remarkable improvements and outperforms GPT-4 driven agents by 10%, showing the effectiveness of the proposed data synthesis pipeline that leads to better reasoning capabilities in tool usage. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "00SnKBGTsz@OpenReview",
      "index": 217,
      "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
      "authors": [
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Jaemin Cho",
        "Mohit Bansal"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dataenvgym",
        "student",
        "agents",
        "environments",
        "generation",
        "feedback",
        "data",
        "teacher",
        "creating",
        "plan"
      ],
      "summary": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents – or teachers – is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent’s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in three diverse tasks covering both text and images (mathematics, programming, and visual question answering) and test multiple student models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. We will publicly release our code and leaderboard.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=00SnKBGTsz"
        ],
        "venue": [
          "/venue/00SnKBGTsz@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=00SnKBGTsz"
        ],
        "detail": [
          "https://openreview.net/forum?id=00SnKBGTsz"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Zaid Khan , Elias Stengel-Eskin , Jaemin Cho , Mohit Bansal The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents – or teachers – is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent’s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in three diverse tasks covering both text and images (mathematics, programming, and visual question answering) and test multiple student models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. We will publicly release our code and leaderboard. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "sahQq2sH5x@OpenReview",
      "index": 218,
      "title": "Benchmarking Predictive Coding Networks -- Made Simple",
      "authors": [
        "Luca Pinchetti",
        "Chang Qi",
        "Oleh Lokshyn",
        "Cornelius Emde",
        "Amine M&#x27;Charrak",
        "Mufeng Tang",
        "Simon Frieder",
        "Bayar Menzat",
        "Gaspard Oliviers",
        "Rafal Bogacz",
        "Thomas Lukasiewicz",
        "Tommaso Salvatori"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pcns",
        "benchmarks",
        "coding",
        "community",
        "galvanizing",
        "predictive",
        "scalability",
        "library",
        "tasks",
        "architectures"
      ],
      "summary": "In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we will release the code, tests, and benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sahQq2sH5x"
        ],
        "venue": [
          "/venue/sahQq2sH5x@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sahQq2sH5x"
        ],
        "detail": [
          "https://openreview.net/forum?id=sahQq2sH5x"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Benchmarking Predictive Coding Networks -- Made Simple [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Luca Pinchetti , Chang Qi , Oleh Lokshyn , Cornelius Emde , Amine M&#x27;Charrak , Mufeng Tang , Simon Frieder , Bayar Menzat , Gaspard Oliviers , Rafal Bogacz , Thomas Lukasiewicz , Tommaso Salvatori In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we will release the code, tests, and benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "rpwGUtTeA5@OpenReview",
      "index": 219,
      "title": "UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization",
      "authors": [
        "Peiwen Yuan",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Jiayi Shi",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "unicbe",
        "cbe",
        "uniformity",
        "evaluation",
        "preference",
        "comparing",
        "unified",
        "objective",
        "scalability",
        "sampling"
      ],
      "summary": "Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty.Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE.On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rpwGUtTeA5"
        ],
        "venue": [
          "/venue/rpwGUtTeA5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rpwGUtTeA5"
        ],
        "detail": [
          "https://openreview.net/forum?id=rpwGUtTeA5"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Peiwen Yuan , Shaoxiong Feng , Yiwei Li , Xinglin Wang , Yueqi Zhang , Jiayi Shi , Chuyi Tan , Boyuan Pan , Yao Hu , Kan Li Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty.Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE.On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qpXctF2aLZ@OpenReview",
      "index": 220,
      "title": "Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization",
      "authors": [
        "Sascha Marton",
        "Tim Grams",
        "Florian Vogt",
        "Stefan Lüdtke",
        "Christian Bartelt",
        "Heiner Stuckenschmidt"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sympol",
        "tree",
        "policy",
        "policies",
        "symbolic",
        "interpretable",
        "decision",
        "based",
        "reinforcement",
        "interpretability"
      ],
      "summary": "Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging.In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability.We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qpXctF2aLZ"
        ],
        "venue": [
          "/venue/qpXctF2aLZ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qpXctF2aLZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=qpXctF2aLZ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Sascha Marton , Tim Grams , Florian Vogt , Stefan Lüdtke , Christian Bartelt , Heiner Stuckenschmidt Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging.In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability.We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "pPQPQ7Yd58@OpenReview",
      "index": 221,
      "title": "Control-oriented Clustering of Visual Latent Representation",
      "authors": [
        "Han Qi",
        "Haocheng Yin",
        "Heng Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "control",
        "clustering",
        "visual",
        "oriented",
        "vision",
        "representation",
        "encoder",
        "pose",
        "pushing",
        "action"
      ],
      "summary": "We initiate a study of the geometry of the visual representation space ---the information channel from the vision encoder to the action decoder--- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of *neural collapse* (NC) in image classification, we empirically demonstrate the prevalent emergence of a similar *law of clustering* in the visual representation space. Specifically, - In discrete image-based control (e.g., Lunar Lander), the visual representations cluster according to the natural discrete action labels;- In continuous image-based control (e.g., Planar Pushing and Block Stacking), the clustering emerges according to ``control-oriented'' classes that are based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output. Each of the classes corresponds to one relative pose orthant (REPO).Beyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10% to 35%. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pPQPQ7Yd58"
        ],
        "venue": [
          "/venue/pPQPQ7Yd58@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pPQPQ7Yd58"
        ],
        "detail": [
          "https://openreview.net/forum?id=pPQPQ7Yd58"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Control-oriented Clustering of Visual Latent Representation [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Han Qi , Haocheng Yin , Heng Yang We initiate a study of the geometry of the visual representation space ---the information channel from the vision encoder to the action decoder--- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of *neural collapse* (NC) in image classification, we empirically demonstrate the prevalent emergence of a similar *law of clustering* in the visual representation space. Specifically, - In discrete image-based control (e.g., Lunar Lander), the visual representations cluster according to the natural discrete action labels;- In continuous image-based control (e.g., Planar Pushing and Block Stacking), the clustering emerges according to ``control-oriented'' classes that are based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output. Each of the classes corresponds to one relative pose orthant (REPO).Beyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10% to 35%. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cznqgb4DNv@OpenReview",
      "index": 222,
      "title": "Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees",
      "authors": [
        "Shahryar Zehtabi",
        "Dong-Jun Han",
        "Rohit Parasnis",
        "Seyyedali Hosseinalipour",
        "Christopher Brinton"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dspodfl",
        "decentralized",
        "dfl",
        "texttt",
        "federated",
        "clients",
        "sporadic",
        "gradient",
        "algorithmic",
        "settings"
      ],
      "summary": "Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between local model exchanges, overlooking heterogeneity and dynamics in communication and computation capabilities. In this work, we propose Decentralized Sporadic Federated Learning ( DSpodFL DSpodFL ), a DFL methodology built on a generalized notion of *sporadicity* in both local gradient and aggregation processes. DSpodFL DSpodFL subsumes many existing decentralized optimization methods under a unified algorithmic framework by modeling the per-iteration (i) occurrence of gradient descent at each client and (ii) exchange of models between client pairs as arbitrary indicator random variables, thus capturing *heterogeneous and time-varying* computation/communication scenarios. We analytically characterize the convergence behavior of DSpodFL DSpodFL for both convex and non-convex models and for both constant and diminishing learning rates, under mild assumptions on the communication graph connectivity, data heterogeneity across clients, and gradient noises. We show how our bounds recover existing results from decentralized gradient descent as special cases. Experiments demonstrate that DSpodFL DSpodFL consistently achieves improved training speeds compared with baselines under various system settings.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cznqgb4DNv"
        ],
        "venue": [
          "/venue/cznqgb4DNv@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cznqgb4DNv"
        ],
        "detail": [
          "https://openreview.net/forum?id=cznqgb4DNv"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Shahryar Zehtabi , Dong-Jun Han , Rohit Parasnis , Seyyedali Hosseinalipour , Christopher Brinton Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between local model exchanges, overlooking heterogeneity and dynamics in communication and computation capabilities. In this work, we propose Decentralized Sporadic Federated Learning ( DSpodFL DSpodFL ), a DFL methodology built on a generalized notion of *sporadicity* in both local gradient and aggregation processes. DSpodFL DSpodFL subsumes many existing decentralized optimization methods under a unified algorithmic framework by modeling the per-iteration (i) occurrence of gradient descent at each client and (ii) exchange of models between client pairs as arbitrary indicator random variables, thus capturing *heterogeneous and time-varying* computation/communication scenarios. We analytically characterize the convergence behavior of DSpodFL DSpodFL for both convex and non-convex models and for both constant and diminishing learning rates, under mild assumptions on the communication graph connectivity, data heterogeneity across clients, and gradient noises. We show how our bounds recover existing results from decentralized gradient descent as special cases. Experiments demonstrate that DSpodFL DSpodFL consistently achieves improved training speeds compared with baselines under various system settings. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "UVnD9Ze6mF@OpenReview",
      "index": 223,
      "title": "AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories",
      "authors": [
        "Yi Zeng",
        "Yu Yang",
        "Andy Zhou",
        "Jeffrey Tan",
        "Yuheng Tu",
        "Yifan Mai",
        "Kevin Klyman",
        "Minzhou Pan",
        "Ruoxi Jia",
        "Dawn Song",
        "Percy Liang",
        "Bo Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "2024",
        "air",
        "bench",
        "safety",
        "policies",
        "categories",
        "risks",
        "regulations",
        "specified",
        "fms"
      ],
      "summary": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-BENCH 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in the AI Risks taxonomy, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-BENCH 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-BENCH 2024 uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-BENCH 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UVnD9Ze6mF"
        ],
        "venue": [
          "/venue/UVnD9Ze6mF@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UVnD9Ze6mF"
        ],
        "detail": [
          "https://openreview.net/forum?id=UVnD9Ze6mF"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Yi Zeng , Yu Yang , Andy Zhou , Jeffrey Tan , Yuheng Tu , Yifan Mai , Kevin Klyman , Minzhou Pan , Ruoxi Jia , Dawn Song , Percy Liang , Bo Li Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-BENCH 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in the AI Risks taxonomy, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-BENCH 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-BENCH 2024 uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-BENCH 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9GsgCUJtic@OpenReview",
      "index": 224,
      "title": "When do GFlowNets learn the right distribution?",
      "authors": [
        "Tiago Silva",
        "Rodrigo Alves",
        "Eliezer de Souza da Silva",
        "Amauri Souza",
        "Vikas Garg",
        "Samuel Kaski",
        "Diego Mesquita"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gflownets",
        "gnns",
        "flow",
        "sampling",
        "distributions",
        "correctness",
        "gflownet",
        "distribution",
        "learn",
        "hamper"
      ],
      "summary": "Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs) and show that the representation limits of GNNs delineate which distributions these GFlowNets can approximate. Lastly, we address these limitations by proposing a theoretically sound and computationally tractable metric for assessing GFlowNets, experimentally showing it is a better proxy for correctness than popular evaluation protocols.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9GsgCUJtic"
        ],
        "venue": [
          "/venue/9GsgCUJtic@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9GsgCUJtic"
        ],
        "detail": [
          "https://openreview.net/forum?id=9GsgCUJtic"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "When do GFlowNets learn the right distribution? [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Tiago Silva , Rodrigo Alves , Eliezer de Souza da Silva , Amauri Souza , Vikas Garg , Samuel Kaski , Diego Mesquita Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs) and show that the representation limits of GNNs delineate which distributions these GFlowNets can approximate. Lastly, we address these limitations by proposing a theoretically sound and computationally tractable metric for assessing GFlowNets, experimentally showing it is a better proxy for correctness than popular evaluation protocols. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "8oFvUBvF1u@OpenReview",
      "index": 225,
      "title": "DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from One Demo",
      "authors": [
        "Junzhe Zhu",
        "Yuanchen Ju",
        "Junyi Zhang",
        "Muhan Wang",
        "Zhecheng Yuan",
        "Kaizhe Hu",
        "Huazhe Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "densematcher",
        "manipulation",
        "correspondence",
        "demo",
        "correspondences",
        "meshes",
        "category",
        "robotic",
        "object",
        "relatable"
      ],
      "summary": "Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. In our experiments, we show that DenseMatcher significantly outperforms prior 3D matching baselines by 43.5%. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at http://densematcher.github.io.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8oFvUBvF1u"
        ],
        "venue": [
          "/venue/8oFvUBvF1u@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8oFvUBvF1u"
        ],
        "detail": [
          "https://openreview.net/forum?id=8oFvUBvF1u"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from One Demo [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Junzhe Zhu , Yuanchen Ju , Junyi Zhang , Muhan Wang , Zhecheng Yuan , Kaizhe Hu , Huazhe Xu Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. In our experiments, we show that DenseMatcher significantly outperforms prior 3D matching baselines by 43.5%. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at http://densematcher.github.io. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "3PDklqqqfN@OpenReview",
      "index": 226,
      "title": "Multi-Field Adaptive Retrieval",
      "authors": [
        "Millicent Li",
        "Tongfei Chen",
        "Ben Van Durme",
        "Patrick Xia"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "document",
        "retrieval",
        "field",
        "structured",
        "mfar",
        "lexical",
        "multi",
        "adaptive",
        "dense",
        "header"
      ],
      "summary": "Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are _unstructured_: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (mFAR), a flexible framework that accommodates any number of and any type of document indices on _structured_ data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighing of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3PDklqqqfN"
        ],
        "venue": [
          "/venue/3PDklqqqfN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3PDklqqqfN"
        ],
        "detail": [
          "https://openreview.net/forum?id=3PDklqqqfN"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 10
      },
      "raw_excerpt": "Multi-Field Adaptive Retrieval [PDF 3 ] [Copy] [Kimi 10 ] [REL] Authors : Millicent Li , Tongfei Chen , Ben Van Durme , Patrick Xia Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are _unstructured_: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (mFAR), a flexible framework that accommodates any number of and any type of document indices on _structured_ data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighing of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "37EXtKCOkn@OpenReview",
      "index": 227,
      "title": "Learning Spatiotemporal Dynamical Systems from Point Process Observations",
      "authors": [
        "Valerii Iakovlev",
        "Harri Lähdesmäki"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "spatiotemporal",
        "observations",
        "neural",
        "point",
        "dynamical",
        "dynamics",
        "oceanic",
        "timings",
        "modeling",
        "crowdsourced"
      ],
      "summary": "Spatiotemporal dynamics models are fundamental for various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based spatiotemporal modeling approaches fall short when faced with data that is collected randomly over time and space, as is often the case with sensor networks in real-world applications like crowdsourced earthquake detection or pollution monitoring. In response, we developed a new method that can effectively learn spatiotemporal dynamics from such point process observations. Our model integrates techniques from neural differential equations, neural point processes, implicit neural representations and amortized variational inference to model both the dynamics of the system and the probabilistic locations and timings of observations. It outperforms existing methods on challenging spatiotemporal datasets by offering substantial improvements in predictive accuracy and computational efficiency, making it a useful tool for modeling and understanding complex dynamical systems observed under realistic, unconstrained conditions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=37EXtKCOkn"
        ],
        "venue": [
          "/venue/37EXtKCOkn@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=37EXtKCOkn"
        ],
        "detail": [
          "https://openreview.net/forum?id=37EXtKCOkn"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Learning Spatiotemporal Dynamical Systems from Point Process Observations [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Valerii Iakovlev , Harri Lähdesmäki Spatiotemporal dynamics models are fundamental for various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based spatiotemporal modeling approaches fall short when faced with data that is collected randomly over time and space, as is often the case with sensor networks in real-world applications like crowdsourced earthquake detection or pollution monitoring. In response, we developed a new method that can effectively learn spatiotemporal dynamics from such point process observations. Our model integrates techniques from neural differential equations, neural point processes, implicit neural representations and amortized variational inference to model both the dynamics of the system and the probabilistic locations and timings of observations. It outperforms existing methods on challenging spatiotemporal datasets by offering substantial improvements in predictive accuracy and computational efficiency, making it a useful tool for modeling and understanding complex dynamical systems observed under realistic, unconstrained conditions. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SOWZ59UyNc@OpenReview",
      "index": 228,
      "title": "Lean-STaR: Learning to Interleave Thinking and Proving",
      "authors": [
        "Haohan Lin",
        "Zhiqing Sun",
        "Sean Welleck",
        "Yiming Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lean",
        "thoughts",
        "proving",
        "star",
        "tactics",
        "proof",
        "informal",
        "interleave",
        "language",
        "model"
      ],
      "summary": "Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR significantly outperform base models (43.4% → 46.3%, Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SOWZ59UyNc"
        ],
        "venue": [
          "/venue/SOWZ59UyNc@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SOWZ59UyNc"
        ],
        "detail": [
          "https://openreview.net/forum?id=SOWZ59UyNc"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "Lean-STaR: Learning to Interleave Thinking and Proving [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Haohan Lin , Zhiqing Sun , Sean Welleck , Yiming Yang Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR significantly outperform base models (43.4% → 46.3%, Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "kuutidLf6R@OpenReview",
      "index": 229,
      "title": "Diffusion Attribution Score: Which Training Sample Determines Your Generation?",
      "authors": [
        "Jinxu Lin",
        "Linwei Tao",
        "Minjing Dong",
        "Chang Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "attribution",
        "diffusion",
        "das",
        "score",
        "sample",
        "training",
        "predicted",
        "models",
        "contribution",
        "loss"
      ],
      "summary": "As diffusion models advance, the scientific community is actively developing methods to curb the misuse of generative models, which aims to prevent the reproduction of copyrighted, explicitly violent, or personally sensitive information in generated images.One strategy is to identify the contribution of training samples in generative models by evaluating their influence to the generated images, a task known as data attribution.Existing data attribution approaches on diffusion models suggest representing the contribution of a specific training sample by evaluating the change in the diffusion loss when the sample is included versus excluded from the training process.However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the diffusion loss calculation. Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors.To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (DAS).Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS.Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models.Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance.Code is available at https://anonymous.4open.science/r/Diffusion-Attribution-Score-411F.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kuutidLf6R"
        ],
        "venue": [
          "/venue/kuutidLf6R@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kuutidLf6R"
        ],
        "detail": [
          "https://openreview.net/forum?id=kuutidLf6R"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Diffusion Attribution Score: Which Training Sample Determines Your Generation? [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Jinxu Lin , Linwei Tao , Minjing Dong , Chang Xu As diffusion models advance, the scientific community is actively developing methods to curb the misuse of generative models, which aims to prevent the reproduction of copyrighted, explicitly violent, or personally sensitive information in generated images.One strategy is to identify the contribution of training samples in generative models by evaluating their influence to the generated images, a task known as data attribution.Existing data attribution approaches on diffusion models suggest representing the contribution of a specific training sample by evaluating the change in the diffusion loss when the sample is included versus excluded from the training process.However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the diffusion loss calculation. Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors.To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (DAS).Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS.Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models.Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance.Code is available at https://anonymous.4open.science/r/Diffusion-Attribution-Score-411F. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "RaR3ETzyKp@OpenReview",
      "index": 230,
      "title": "Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance",
      "authors": [
        "shifeng Xu",
        "yanzhu liu",
        "Adams Kong"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dansm",
        "path",
        "rectified",
        "inter",
        "distance",
        "preferable",
        "noises",
        "flow",
        "spaces",
        "lengthening"
      ],
      "summary": "Recent research pinpoints that different diffusion methods and architectures trained on the same dataset produce similar results for the same input noise. This property suggests that they have some preferable noises for a given sample. By visualizing the noise-sample pairs of rectified flow models and stable diffusion models in two-dimensional spaces, we observe that the preferable paths, connecting preferable noises to the corresponding samples, are much well organized with significant fewer crossings comparing with the random paths, connecting random noises to training samples. In high-dimensional space, paths rarely intersect. The path crossings in two-dimensional spaces indicate the shorter inter-path distance in the corresponding high-dimensional spaces. Inspired by this observation, we propose the Distance-Aware Noise-Sample Matching (DANSM) method to lengthen the inter-path distance for speeding up the model training. DANSM is derived from rectified flow models, which allow using a closed-form formula to calculate the inter-path distance. To further simplify the optimization, we derive the relationship between inter-path distance and path length, and use the latter in the optimization surrogate. DANSM is evaluated on both image and latent spaces by rectified flow models and diffusion models. The experimental results show that DANSM can significantly improve the training speed by 30\\% ∼ ∼ 40\\%without sacrificing the generation quality.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RaR3ETzyKp"
        ],
        "venue": [
          "/venue/RaR3ETzyKp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RaR3ETzyKp"
        ],
        "detail": [
          "https://openreview.net/forum?id=RaR3ETzyKp"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 7
      },
      "raw_excerpt": "Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance [PDF 7 ] [Copy] [Kimi 7 ] [REL] Authors : shifeng Xu , yanzhu liu , Adams Kong Recent research pinpoints that different diffusion methods and architectures trained on the same dataset produce similar results for the same input noise. This property suggests that they have some preferable noises for a given sample. By visualizing the noise-sample pairs of rectified flow models and stable diffusion models in two-dimensional spaces, we observe that the preferable paths, connecting preferable noises to the corresponding samples, are much well organized with significant fewer crossings comparing with the random paths, connecting random noises to training samples. In high-dimensional space, paths rarely intersect. The path crossings in two-dimensional spaces indicate the shorter inter-path distance in the corresponding high-dimensional spaces. Inspired by this observation, we propose the Distance-Aware Noise-Sample Matching (DANSM) method to lengthen the inter-path distance for speeding up the model training. DANSM is derived from rectified flow models, which allow using a closed-form formula to calculate the inter-path distance. To further simplify the optimization, we derive the relationship between inter-path distance and path length, and use the latter in the optimization surrogate. DANSM is evaluated on both image and latent spaces by rectified flow models and diffusion models. The experimental results show that DANSM can significantly improve the training speed by 30\\% ∼ ∼ 40\\%without sacrificing the generation quality. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "msEr27EejF@OpenReview",
      "index": 231,
      "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking",
      "authors": [
        "Cassidy Laidlaw",
        "Shivam Singhal",
        "Anca Dragan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hacking",
        "reward",
        "proxy",
        "rlhf",
        "definition",
        "rewards",
        "true",
        "policy",
        "proxies",
        "policies"
      ],
      "summary": "Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using flawed proxy rewards that seem to capture the true objective. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. We introduce a definition of reward hacking based on correlation between proxy and true rewards for states and actions seen by a \"base policy\" that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). We then show theoretically that regularization to the base policy can effectively prevent reward hacking. Our theory suggests regularizing χ 2 χ 2 divergence between the policies' occupancy measures, rather than the current practice in RLHF of using a KL penalty between action distributions. We intuitively show why this type of regularization is better, and demonstrate that it outperforms alternatives at mitigating reward hacking in practice across four realistic settings, including RLHF.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=msEr27EejF"
        ],
        "venue": [
          "/venue/msEr27EejF@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=msEr27EejF"
        ],
        "detail": [
          "https://openreview.net/forum?id=msEr27EejF"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Cassidy Laidlaw , Shivam Singhal , Anca Dragan Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using flawed proxy rewards that seem to capture the true objective. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. We introduce a definition of reward hacking based on correlation between proxy and true rewards for states and actions seen by a \"base policy\" that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). We then show theoretically that regularization to the base policy can effectively prevent reward hacking. Our theory suggests regularizing χ 2 χ 2 divergence between the policies' occupancy measures, rather than the current practice in RLHF of using a KL penalty between action distributions. We intuitively show why this type of regularization is better, and demonstrate that it outperforms alternatives at mitigating reward hacking in practice across four realistic settings, including RLHF. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "l2zFn6TIQi@OpenReview",
      "index": 232,
      "title": "Controlling Language and Diffusion Models by Transporting Activations",
      "authors": [
        "Pau Rodriguez",
        "Arno Blaas",
        "Michal Klein",
        "Luca Zappella",
        "Nicholas Apostoloff",
        "marco cuturi",
        "Xavier Suau"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "t2is",
        "activations",
        "act",
        "steering",
        "transporting",
        "control",
        "induce",
        "activation",
        "language",
        "diffusion"
      ],
      "summary": "The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output.In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=l2zFn6TIQi"
        ],
        "venue": [
          "/venue/l2zFn6TIQi@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=l2zFn6TIQi"
        ],
        "detail": [
          "https://openreview.net/forum?id=l2zFn6TIQi"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 6
      },
      "raw_excerpt": "Controlling Language and Diffusion Models by Transporting Activations [PDF 2 ] [Copy] [Kimi 6 ] [REL] Authors : Pau Rodriguez , Arno Blaas , Michal Klein , Luca Zappella , Nicholas Apostoloff , marco cuturi , Xavier Suau The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output.In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ws5phQki00@OpenReview",
      "index": 233,
      "title": "The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions",
      "authors": [
        "Stefan Sylvius Wagner",
        "Maike Behrendt",
        "Marc Ziegele",
        "Stefan Harmeling"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "stance",
        "discussions",
        "political",
        "synthetic",
        "online",
        "detection",
        "data",
        "deployment",
        "improve",
        "llm"
      ],
      "summary": "Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance detection, requiring vast amounts of data. However, the wide variety of debate topics in online political discussions makes data collection particularly challenging. LLMs have revived stance detection, but their online deployment in online political discussions faces challenges like inconsistent outputs, biases, and vulnerability to adversarial attacks. We show how LLM-generated synthetic data can improve stance detection for online political discussions by using reliable traditional stance detection models for online deployment, while leveraging the text generation capabilities of LLMs for synthetic data generation in a secure offline environment. To achieve this, (i) we generate synthetic data for specific debate questions by prompting a Mistral-7B model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection, while remaining interpretable and aligned with real world data. (ii) Using the synthetic data as a reference, we can improve performance even further by identifying the most informative samples in an unlabelled dataset, i.e., those samples which the stance detection model is most uncertain about and can benefit from the most. By fine-tuning with both synthetic data and the most informative samples, we surpass the performance of the baseline model that is fine-tuned on all true labels, while labelling considerably less data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ws5phQki00"
        ],
        "venue": [
          "/venue/ws5phQki00@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ws5phQki00"
        ],
        "detail": [
          "https://openreview.net/forum?id=ws5phQki00"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Stefan Sylvius Wagner , Maike Behrendt , Marc Ziegele , Stefan Harmeling Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance detection, requiring vast amounts of data. However, the wide variety of debate topics in online political discussions makes data collection particularly challenging. LLMs have revived stance detection, but their online deployment in online political discussions faces challenges like inconsistent outputs, biases, and vulnerability to adversarial attacks. We show how LLM-generated synthetic data can improve stance detection for online political discussions by using reliable traditional stance detection models for online deployment, while leveraging the text generation capabilities of LLMs for synthetic data generation in a secure offline environment. To achieve this, (i) we generate synthetic data for specific debate questions by prompting a Mistral-7B model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection, while remaining interpretable and aligned with real world data. (ii) Using the synthetic data as a reference, we can improve performance even further by identifying the most informative samples in an unlabelled dataset, i.e., those samples which the stance detection model is most uncertain about and can benefit from the most. By fine-tuning with both synthetic data and the most informative samples, we surpass the performance of the baseline model that is fine-tuned on all true labels, while labelling considerably less data. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Njx1NjHIx4@OpenReview",
      "index": 234,
      "title": "Formation of Representations in Neural Networks",
      "authors": [
        "Liu Ziyin",
        "Isaac Chuang",
        "Tomer Galanti",
        "Tomaso Poggio"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "crh",
        "representations",
        "neural",
        "pah",
        "alignment",
        "networks",
        "formation",
        "modern",
        "canonical",
        "hypothesis"
      ],
      "summary": "Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory demonstrating that the balance between gradient noise and regularization is crucial for the emergence the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Njx1NjHIx4"
        ],
        "venue": [
          "/venue/Njx1NjHIx4@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Njx1NjHIx4"
        ],
        "detail": [
          "https://openreview.net/forum?id=Njx1NjHIx4"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Formation of Representations in Neural Networks [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Liu Ziyin , Isaac Chuang , Tomer Galanti , Tomaso Poggio Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory demonstrating that the balance between gradient noise and regularization is crucial for the emergence the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "U834XHJuqk@OpenReview",
      "index": 235,
      "title": "Nonlinear Sequence Embedding by Monotone Variational Inequality",
      "authors": [
        "Jonathan Zhou",
        "Yao Xie"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sequence",
        "domain",
        "monotone",
        "nonlinear",
        "variational",
        "symbolic",
        "representations",
        "recovery",
        "electrocardiograms",
        "sequences"
      ],
      "summary": "In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VIs) and encode the common domain assumption via low-rank constraint across the learned representations, which can learn a subspace approximately spanning the entire domain as well as faithful representations for the dynamics of each individual sequence incorporating the domain information in totality. We show the competitive performance of our method on real-world time-series data with baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U834XHJuqk"
        ],
        "venue": [
          "/venue/U834XHJuqk@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U834XHJuqk"
        ],
        "detail": [
          "https://openreview.net/forum?id=U834XHJuqk"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Nonlinear Sequence Embedding by Monotone Variational Inequality [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jonathan Zhou , Yao Xie In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VIs) and encode the common domain assumption via low-rank constraint across the learned representations, which can learn a subspace approximately spanning the entire domain as well as faithful representations for the dynamics of each individual sequence incorporating the domain information in totality. We show the competitive performance of our method on real-world time-series data with baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hwSmPOAmhk@OpenReview",
      "index": 236,
      "title": "Understanding Factual Recall in Transformers via Associative Memories",
      "authors": [
        "Eshaan Nichani",
        "Jason Lee",
        "Alberto Bietti"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "factual",
        "recall",
        "associative",
        "mlp",
        "memories",
        "transformers",
        "store",
        "facts",
        "count",
        "task"
      ],
      "summary": "Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100\\% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hwSmPOAmhk"
        ],
        "venue": [
          "/venue/hwSmPOAmhk@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hwSmPOAmhk"
        ],
        "detail": [
          "https://openreview.net/forum?id=hwSmPOAmhk"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Understanding Factual Recall in Transformers via Associative Memories [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Eshaan Nichani , Jason Lee , Alberto Bietti Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100\\% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "vWR3KuiQur@OpenReview",
      "index": 237,
      "title": "SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models",
      "authors": [
        "Muyang Li",
        "Yujun Lin",
        "Zhekai Zhang",
        "Tianle Cai",
        "Junxian Guo",
        "Xiuyu Li",
        "Enze Xie",
        "Chenlin Meng",
        "Jun-Yan Zhu",
        "Song Han"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "outliers",
        "svdquant",
        "activations",
        "rank",
        "branch",
        "bit",
        "quantization",
        "low",
        "weights",
        "models"
      ],
      "summary": "Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine LoRunner that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt- Σ Σ , and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.6×, achieving 3.5× speedup over the 4-bit weight-only quantized baseline on a 16GB RTX-4090 GPU, paving the way for more interactive applications on PCs. We will release the code and models upon publication.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vWR3KuiQur"
        ],
        "venue": [
          "/venue/vWR3KuiQur@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vWR3KuiQur"
        ],
        "detail": [
          "https://openreview.net/forum?id=vWR3KuiQur"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Muyang Li , Yujun Lin , Zhekai Zhang , Tianle Cai , Junxian Guo , Xiuyu Li , Enze Xie , Chenlin Meng , Jun-Yan Zhu , Song Han Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine LoRunner that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt- Σ Σ , and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.6×, achieving 3.5× speedup over the 4-bit weight-only quantized baseline on a 16GB RTX-4090 GPU, paving the way for more interactive applications on PCs. We will release the code and models upon publication. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "uvHmnahyp1@OpenReview",
      "index": 238,
      "title": "SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints",
      "authors": [
        "Miruna Cretu",
        "Charles Harris",
        "Ilia Igashov",
        "Arne Schneuing",
        "Marwin Segler",
        "Bruno Correia",
        "Julien Roy",
        "Emmanuel Bengio",
        "Pietro Lio"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "synflownet",
        "gflownet",
        "molecules",
        "synthesis",
        "buyable",
        "mdp",
        "backward",
        "synthesizability",
        "constraints",
        "generative"
      ],
      "summary": "Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical reactions and buyable reactants to sequentially build new molecules. By incorporating forward synthesis as an explicit constraint of the generative mechanism, we aim at bridging the gap between in silico molecular generation and real world synthesis capabilities. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool to assess the synthesizability of our compounds, and motivate the choice of GFlowNets through considerable improvement in sample diversity compared to baselines. Additionally, we identify challenges with reaction encodings that can complicate traversal of the MDP in the backward direction. To address this, we introduce various strategies for learning the GFlowNet backward policy and thus demonstrate how additional constraints can be integrated into the GFlowNet MDP framework. This approach enables our model to successfully identify synthesis pathways for previously unseen molecules.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uvHmnahyp1"
        ],
        "venue": [
          "/venue/uvHmnahyp1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uvHmnahyp1"
        ],
        "detail": [
          "https://openreview.net/forum?id=uvHmnahyp1"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Miruna Cretu , Charles Harris , Ilia Igashov , Arne Schneuing , Marwin Segler , Bruno Correia , Julien Roy , Emmanuel Bengio , Pietro Lio Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical reactions and buyable reactants to sequentially build new molecules. By incorporating forward synthesis as an explicit constraint of the generative mechanism, we aim at bridging the gap between in silico molecular generation and real world synthesis capabilities. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool to assess the synthesizability of our compounds, and motivate the choice of GFlowNets through considerable improvement in sample diversity compared to baselines. Additionally, we identify challenges with reaction encodings that can complicate traversal of the MDP in the backward direction. To address this, we introduce various strategies for learning the GFlowNet backward policy and thus demonstrate how additional constraints can be integrated into the GFlowNet MDP framework. This approach enables our model to successfully identify synthesis pathways for previously unseen molecules. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "uuriavczkL@OpenReview",
      "index": 239,
      "title": "Counterfactual Realizability",
      "authors": [
        "Arvind Raghavan",
        "Elias Bareinboim"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "counterfactual",
        "realizability",
        "interventional",
        "pearl",
        "causal",
        "motivating",
        "believed",
        "bareinboim",
        "forney",
        "observational"
      ],
      "summary": "It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the Pearl Causal Hierarchy. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of *realizability*, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uuriavczkL"
        ],
        "venue": [
          "/venue/uuriavczkL@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uuriavczkL"
        ],
        "detail": [
          "https://openreview.net/forum?id=uuriavczkL"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Counterfactual Realizability [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Arvind Raghavan , Elias Bareinboim It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the Pearl Causal Hierarchy. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of *realizability*, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "uSz2K30RRd@OpenReview",
      "index": 240,
      "title": "Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric",
      "authors": [
        "Toshimitsu Uesaka",
        "Taiji Suzuki",
        "Yuhta Takida",
        "Chieh-Hsin Lai",
        "Naoki Murata",
        "Yuki Mitsufuji"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "similarity",
        "contrastive",
        "infonce",
        "weighted",
        "multimodal",
        "onepoint",
        "clip",
        "point",
        "optimal",
        "representation"
      ],
      "summary": "In typical multimodal contrastive learning, such as CLIP, encoders produce onepoint in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For richer classes of the similarity, we propose the use of weighted point clouds, namely, sets of pairs of weight and vector, as representations of instances. In this work, we theoretically show the benefit of our proposed method through a new understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE. We clarify that the optimal similaritythat minimizes symmetric InfoNCE is the pointwise mutual information, and show an upper bound of excess risk on downstream classification tasks of representations that achieve the optimal similarity. In addition, we show that our proposed similarity based on weighted point clouds consistently achieves the optimal similarity. To verify the effectiveness of our proposed method, we demonstrate pretraining of text-image representation models and classification tasks on common benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uSz2K30RRd"
        ],
        "venue": [
          "/venue/uSz2K30RRd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uSz2K30RRd"
        ],
        "detail": [
          "https://openreview.net/forum?id=uSz2K30RRd"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 6
      },
      "raw_excerpt": "Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric [PDF 4 ] [Copy] [Kimi 6 ] [REL] Authors : Toshimitsu Uesaka , Taiji Suzuki , Yuhta Takida , Chieh-Hsin Lai , Naoki Murata , Yuki Mitsufuji In typical multimodal contrastive learning, such as CLIP, encoders produce onepoint in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For richer classes of the similarity, we propose the use of weighted point clouds, namely, sets of pairs of weight and vector, as representations of instances. In this work, we theoretically show the benefit of our proposed method through a new understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE. We clarify that the optimal similaritythat minimizes symmetric InfoNCE is the pointwise mutual information, and show an upper bound of excess risk on downstream classification tasks of representations that achieve the optimal similarity. In addition, we show that our proposed similarity based on weighted point clouds consistently achieves the optimal similarity. To verify the effectiveness of our proposed method, we demonstrate pretraining of text-image representation models and classification tasks on common benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "r3DF5sOo5B@OpenReview",
      "index": 241,
      "title": "Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought",
      "authors": [
        "Jianhao Huang",
        "Zixuan Wang",
        "Jason Lee"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cot",
        "prompting",
        "transformers",
        "descent",
        "thought",
        "transformer",
        "step",
        "implement",
        "chain",
        "learn"
      ],
      "summary": "Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on a in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. Empirically, we demonstrate that CoT prompting yields substantial performance improvements.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=r3DF5sOo5B"
        ],
        "venue": [
          "/venue/r3DF5sOo5B@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=r3DF5sOo5B"
        ],
        "detail": [
          "https://openreview.net/forum?id=r3DF5sOo5B"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Jianhao Huang , Zixuan Wang , Jason Lee Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on a in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. Empirically, we demonstrate that CoT prompting yields substantial performance improvements. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "pZISppZSTv@OpenReview",
      "index": 242,
      "title": "CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control",
      "authors": [
        "Guy Tevet",
        "Sigal Raab",
        "Setareh Cohan",
        "Daniele Reda",
        "Zhengyi Luo",
        "Xue Bin Peng",
        "Amit Bermano",
        "Michiel van de Panne"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "closd",
        "diffusion",
        "motion",
        "controller",
        "dip",
        "control",
        "planner",
        "loop",
        "strengths",
        "text"
      ],
      "summary": "Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules — a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pZISppZSTv"
        ],
        "venue": [
          "/venue/pZISppZSTv@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pZISppZSTv"
        ],
        "detail": [
          "https://openreview.net/forum?id=pZISppZSTv"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 2
      },
      "raw_excerpt": "CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control [PDF 5 ] [Copy] [Kimi 2 ] [REL] Authors : Guy Tevet , Sigal Raab , Setareh Cohan , Daniele Reda , Zhengyi Luo , Xue Bin Peng , Amit Bermano , Michiel van de Panne Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules — a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "pHOH8FVrTp@OpenReview",
      "index": 243,
      "title": "No Need to Talk: Asynchronous Mixture of Language Models",
      "authors": [
        "Anastasiia Filippova",
        "Angelos Katharopoulos",
        "David Grangier",
        "Ronan Collobert"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "smalltalk",
        "mixture",
        "asynchronous",
        "language",
        "inference",
        "talk",
        "need",
        "eachmodel",
        "directs",
        "dense"
      ],
      "summary": "We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Eachmodel of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pHOH8FVrTp"
        ],
        "venue": [
          "/venue/pHOH8FVrTp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pHOH8FVrTp"
        ],
        "detail": [
          "https://openreview.net/forum?id=pHOH8FVrTp"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "No Need to Talk: Asynchronous Mixture of Language Models [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Anastasiia Filippova , Angelos Katharopoulos , David Grangier , Ronan Collobert We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Eachmodel of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "lvw3UgeVxS@OpenReview",
      "index": 244,
      "title": "gRNAde: Geometric Deep Learning for 3D RNA inverse design",
      "authors": [
        "Chaitanya Joshi",
        "Arian Jamasb",
        "Ramon Viñas",
        "Charles Harris",
        "Simon Mathis",
        "Alex Morehead",
        "Rishabh Anand",
        "Pietro Lio"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "grnade",
        "rna",
        "rosetta",
        "design",
        "backbones",
        "sequences",
        "geometric",
        "inverse",
        "backbone",
        "ribozyme"
      ],
      "summary": "Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conformational diversity. We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design sequences that explicitly account for structure and dynamics. gRNAde uses a multi-state Graph Neural Network and autoregressive decoding to generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown. On a single-state fixed backbone re-design benchmark of 14 RNA structures from the PDB identified by Das et al. (2010), gRNAde obtains higher native sequence recovery rates (56% on average) compared to Rosetta (45% on average), taking under a second to produce designs compared to the reported hours for Rosetta. We further demonstrate the utility of gRNAde on a new benchmark of multi-state design for structurally flexible RNAs, as well as zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a recent ribozyme. Experimental wet lab validation on 10 different structured RNA backbones finds that gRNAde has an impressive success rate of 50%, a significant advance over 35% for Rosetta. Open source code and tutorials are available at: https://anonymous.4open.science/r/geometric-rna-design",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lvw3UgeVxS"
        ],
        "venue": [
          "/venue/lvw3UgeVxS@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lvw3UgeVxS"
        ],
        "detail": [
          "https://openreview.net/forum?id=lvw3UgeVxS"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "gRNAde: Geometric Deep Learning for 3D RNA inverse design [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Chaitanya Joshi , Arian Jamasb , Ramon Viñas , Charles Harris , Simon Mathis , Alex Morehead , Rishabh Anand , Pietro Lio Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conformational diversity. We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design sequences that explicitly account for structure and dynamics. gRNAde uses a multi-state Graph Neural Network and autoregressive decoding to generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown. On a single-state fixed backbone re-design benchmark of 14 RNA structures from the PDB identified by Das et al. (2010), gRNAde obtains higher native sequence recovery rates (56% on average) compared to Rosetta (45% on average), taking under a second to produce designs compared to the reported hours for Rosetta. We further demonstrate the utility of gRNAde on a new benchmark of multi-state design for structurally flexible RNAs, as well as zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a recent ribozyme. Experimental wet lab validation on 10 different structured RNA backbones finds that gRNAde has an impressive success rate of 50%, a significant advance over 35% for Rosetta. Open source code and tutorials are available at: https://anonymous.4open.science/r/geometric-rna-design Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "lXRDQsiP2v@OpenReview",
      "index": 245,
      "title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
      "authors": [
        "Ziyang Wu",
        "Tianjiao Ding",
        "Yifu Lu",
        "Druv Pai",
        "Jingyuan Zhang",
        "Weida Wang",
        "Yaodong Yu",
        "Yi Ma",
        "Benjamin Haeffele"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "attention",
        "transformer",
        "tssa",
        "texttt",
        "mcr",
        "token",
        "architecture",
        "tokens",
        "statistics",
        "variational"
      ],
      "summary": "The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR 2 2 ). Specifically, we derive a novel variational form of the MCR 2 2 objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ( TSSA TSSA ). TSSA TSSA has linear computational and memory complexity linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping TSSA TSSA for standard self-attention, which we refer to as the Token Statistics Transformer ( ToST ToST ), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lXRDQsiP2v"
        ],
        "venue": [
          "/venue/lXRDQsiP2v@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lXRDQsiP2v"
        ],
        "detail": [
          "https://openreview.net/forum?id=lXRDQsiP2v"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Ziyang Wu , Tianjiao Ding , Yifu Lu , Druv Pai , Jingyuan Zhang , Weida Wang , Yaodong Yu , Yi Ma , Benjamin Haeffele The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR 2 2 ). Specifically, we derive a novel variational form of the MCR 2 2 objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ( TSSA TSSA ). TSSA TSSA has linear computational and memory complexity linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping TSSA TSSA for standard self-attention, which we refer to as the Token Statistics Transformer ( ToST ToST ), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "lJpqxFgWCM@OpenReview",
      "index": 246,
      "title": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion",
      "authors": [
        "Junyi Zhang",
        "Charles Herrmann",
        "Junhwa Hur",
        "Varun Jampani",
        "trevor darrell",
        "Forrester Cole",
        "Deqing Sun",
        "Ming-Hsuan Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "monst3r",
        "dust3r",
        "scenes",
        "geometry",
        "timestep",
        "estimating",
        "dynamic",
        "depth",
        "pointmap",
        "motion"
      ],
      "summary": "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DuSt3R (MonST3R), a novel geometry-first approachthat directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUSt3R’s representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction. Interactive 4D results are available at: https://monst3r-paper.github.io/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lJpqxFgWCM"
        ],
        "venue": [
          "/venue/lJpqxFgWCM@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lJpqxFgWCM"
        ],
        "detail": [
          "https://openreview.net/forum?id=lJpqxFgWCM"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Junyi Zhang , Charles Herrmann , Junhwa Hur , Varun Jampani , trevor darrell , Forrester Cole , Deqing Sun , Ming-Hsuan Yang Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DuSt3R (MonST3R), a novel geometry-first approachthat directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUSt3R’s representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction. Interactive 4D results are available at: https://monst3r-paper.github.io/ Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "k03mB41vyM@OpenReview",
      "index": 247,
      "title": "Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning",
      "authors": [
        "Patrik Reizinger",
        "Siyuan Guo",
        "Ferenc Huszar",
        "Bernhard Schoelkopf",
        "Wieland Brendel"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "causal",
        "exchangeable",
        "iem",
        "identifiable",
        "representation",
        "identifiability",
        "structure",
        "mechanisms",
        "learning",
        "finetti"
      ],
      "summary": "Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently.We observe that several structure and representation identifiability methods, particularly those that require multiple environments, rely on exchangeable non--i.i.d. (independent and identically distributed) data.To formalize this connection, we propose the Identifiable Exchangeable Mechanisms (IEM) framework to unify key representation and causal structure learning methods. IEM provides a unified probabilistic graphical model encompassing causal discovery, Independent Component Analysis, and Causal Representation Learning.With the help of the IEM model, we generalize the Causal de Finetti theorem of Guo et al., 2022 by relaxing the necessary conditions for causal structure identification in exchangeable data.We term these conditions cause and mechanism variability, and show how they imply a duality condition in identifiable representation learning, leading to new identifiability results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=k03mB41vyM"
        ],
        "venue": [
          "/venue/k03mB41vyM@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=k03mB41vyM"
        ],
        "detail": [
          "https://openreview.net/forum?id=k03mB41vyM"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Patrik Reizinger , Siyuan Guo , Ferenc Huszar , Bernhard Schoelkopf , Wieland Brendel Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently.We observe that several structure and representation identifiability methods, particularly those that require multiple environments, rely on exchangeable non--i.i.d. (independent and identically distributed) data.To formalize this connection, we propose the Identifiable Exchangeable Mechanisms (IEM) framework to unify key representation and causal structure learning methods. IEM provides a unified probabilistic graphical model encompassing causal discovery, Independent Component Analysis, and Causal Representation Learning.With the help of the IEM model, we generalize the Causal de Finetti theorem of Guo et al., 2022 by relaxing the necessary conditions for causal structure identification in exchangeable data.We term these conditions cause and mechanism variability, and show how they imply a duality condition in identifiable representation learning, leading to new identifiability results. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hXm0Wu2U9K@OpenReview",
      "index": 248,
      "title": "Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization",
      "authors": [
        "Audrey Huang",
        "Wenhao Zhan",
        "Tengyang Xie",
        "Jason Lee",
        "Wen Sun",
        "Akshay Krishnamurthy",
        "Dylan Foster"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "overoptimization",
        "chi",
        "alignment",
        "offline",
        "regularization",
        "mythos",
        "preference",
        "provably",
        "dpo",
        "language"
      ],
      "summary": "Language model alignment methods, such as reinforcement learning from human feedback (RLHF), haveled to impressive advances in language model capabilities. However, existing techniques are limited by a widely observed phenomenon known as *overoptimization*, where the quality of the language model degrades over the course of the alignment process. Overoptimization occurs when a language model overfits to inaccuracies in an (either explicit or implicit) offline reward model, and drifts away from preferred responses covered by the data. To discourage such distribution shift, offline alignment methods typically employ KL-regularization, but this, as we show, is too weak to prevent degradation in performance. Then, can we design an efficient algorithm that is provably robust to overoptimization?In this paper, we advance theoretical understanding of sample-efficient offline alignment and introduce a new algorithm called χ 2 χ 2 -Preference Optimization ( χ χ PO). χ χ PO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. 2023), that modifies only the logarithmic link function in the DPO objective. Despite this minimal change, χ χ PO implicitly implements the principle of *pessimism in the face of uncertainty* via regularization with the χ 2 χ 2 -divergence---which quantifies uncertainty more effectively than KL-regularization---and provably alleviates overoptimization, achieving sample-complexity guarantees based on *single-policy concentrability*---the gold standard in offline reinforcement learning. This guarantee makes χ χ PO the first simple, yet general-purpose offline alignment algorithm that is provably robust to overoptimization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hXm0Wu2U9K"
        ],
        "venue": [
          "/venue/hXm0Wu2U9K@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hXm0Wu2U9K"
        ],
        "detail": [
          "https://openreview.net/forum?id=hXm0Wu2U9K"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Audrey Huang , Wenhao Zhan , Tengyang Xie , Jason Lee , Wen Sun , Akshay Krishnamurthy , Dylan Foster Language model alignment methods, such as reinforcement learning from human feedback (RLHF), haveled to impressive advances in language model capabilities. However, existing techniques are limited by a widely observed phenomenon known as *overoptimization*, where the quality of the language model degrades over the course of the alignment process. Overoptimization occurs when a language model overfits to inaccuracies in an (either explicit or implicit) offline reward model, and drifts away from preferred responses covered by the data. To discourage such distribution shift, offline alignment methods typically employ KL-regularization, but this, as we show, is too weak to prevent degradation in performance. Then, can we design an efficient algorithm that is provably robust to overoptimization?In this paper, we advance theoretical understanding of sample-efficient offline alignment and introduce a new algorithm called χ 2 χ 2 -Preference Optimization ( χ χ PO). χ χ PO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. 2023), that modifies only the logarithmic link function in the DPO objective. Despite this minimal change, χ χ PO implicitly implements the principle of *pessimism in the face of uncertainty* via regularization with the χ 2 χ 2 -divergence---which quantifies uncertainty more effectively than KL-regularization---and provably alleviates overoptimization, achieving sample-complexity guarantees based on *single-policy concentrability*---the gold standard in offline reinforcement learning. This guarantee makes χ χ PO the first simple, yet general-purpose offline alignment algorithm that is provably robust to overoptimization. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "gTwRMU3lJ5@OpenReview",
      "index": 249,
      "title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?",
      "authors": [
        "Zhengbo Wang",
        "Jian Liang",
        "Ran He",
        "Zilei Wang",
        "Tieniu Tan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lora",
        "rank",
        "tuning",
        "fine",
        "pro",
        "low",
        "full",
        "gradients",
        "gradient",
        "narrowing"
      ],
      "summary": "Low-rank adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning of foundation models.Despite its computational efficiency, LoRA still yields inferior performance compared to full fine-tuning.In this paper, we first uncover a fundamental connection between the optimization processes of LoRA and full fine-tuning: using LoRA for optimization is mathematically equivalent to full fine-tuning using a low-rank gradient for parameter updates.And this low-rank gradient can be expressed in terms of the gradients of the two low-rank matrices in LoRA.Leveraging this insight, we introduce LoRA-Pro, a method that enhances LoRA's performance by strategically adjusting the gradients of these low-rank matrices.This adjustment allows the low-rank gradient to more accurately approximate the full fine-tuning gradient, thereby narrowing the performance gap between LoRA and full fine-tuning.Furthermore, we theoretically derive the optimal solutions for adjusting the gradients of the low-rank matrices, applying them during fine-tuning in LoRA-Pro.We conduct extensive experiments across natural language understanding, dialogue generation, mathematical reasoning, code generation, and image classification tasks, demonstrating that LoRA-Pro substantially improves LoRA's performance, effectively narrowing the gap with full fine-tuning.Code is available in the supplementary.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gTwRMU3lJ5"
        ],
        "venue": [
          "/venue/gTwRMU3lJ5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gTwRMU3lJ5"
        ],
        "detail": [
          "https://openreview.net/forum?id=gTwRMU3lJ5"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 10
      },
      "raw_excerpt": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized? [PDF 7 ] [Copy] [Kimi 10 ] [REL] Authors : Zhengbo Wang , Jian Liang , Ran He , Zilei Wang , Tieniu Tan Low-rank adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning of foundation models.Despite its computational efficiency, LoRA still yields inferior performance compared to full fine-tuning.In this paper, we first uncover a fundamental connection between the optimization processes of LoRA and full fine-tuning: using LoRA for optimization is mathematically equivalent to full fine-tuning using a low-rank gradient for parameter updates.And this low-rank gradient can be expressed in terms of the gradients of the two low-rank matrices in LoRA.Leveraging this insight, we introduce LoRA-Pro, a method that enhances LoRA's performance by strategically adjusting the gradients of these low-rank matrices.This adjustment allows the low-rank gradient to more accurately approximate the full fine-tuning gradient, thereby narrowing the performance gap between LoRA and full fine-tuning.Furthermore, we theoretically derive the optimal solutions for adjusting the gradients of the low-rank matrices, applying them during fine-tuning in LoRA-Pro.We conduct extensive experiments across natural language understanding, dialogue generation, mathematical reasoning, code generation, and image classification tasks, demonstrating that LoRA-Pro substantially improves LoRA's performance, effectively narrowing the gap with full fine-tuning.Code is available in the supplementary. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "gI0kPklUKS@OpenReview",
      "index": 250,
      "title": "Bilinear MLPs enable weight-based mechanistic interpretability",
      "authors": [
        "Michael Pearce",
        "Thomas Dooms",
        "Alice Rigg",
        "Jose Oramas",
        "Lee Sharkey"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mlps",
        "bilinear",
        "interpretability",
        "mlp",
        "mechanistic",
        "weights",
        "weight",
        "understanding",
        "interpretable",
        "glu"
      ],
      "summary": "A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gI0kPklUKS"
        ],
        "venue": [
          "/venue/gI0kPklUKS@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gI0kPklUKS"
        ],
        "detail": [
          "https://openreview.net/forum?id=gI0kPklUKS"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Bilinear MLPs enable weight-based mechanistic interpretability [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Michael Pearce , Thomas Dooms , Alice Rigg , Jose Oramas , Lee Sharkey A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "fZK6AQXlUU@OpenReview",
      "index": 251,
      "title": "Conformal Prediction Sets Can Cause Disparate Impact",
      "authors": [
        "Jesse Cresswell",
        "Bhargava Kumar",
        "Yi Sui",
        "Mouloud Belbahri"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "disparate",
        "sets",
        "prediction",
        "equalized",
        "coverage",
        "impact",
        "disquietingly",
        "conformal",
        "fairness",
        "uncertainty"
      ],
      "summary": "Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fZK6AQXlUU"
        ],
        "venue": [
          "/venue/fZK6AQXlUU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fZK6AQXlUU"
        ],
        "detail": [
          "https://openreview.net/forum?id=fZK6AQXlUU"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Conformal Prediction Sets Can Cause Disparate Impact [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Jesse Cresswell , Bhargava Kumar , Yi Sui , Mouloud Belbahri Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "f7KxfUrRSb@OpenReview",
      "index": 252,
      "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model",
      "authors": [
        "Wenhong Zhu",
        "Zhiwei He",
        "Xiaofeng Wang",
        "Pengfei Liu",
        "Rui Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "weak",
        "alignment",
        "wspo",
        "strong",
        "win",
        "preference",
        "model",
        "stealing",
        "weaker",
        "qwen2"
      ],
      "summary": "Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=f7KxfUrRSb"
        ],
        "venue": [
          "/venue/f7KxfUrRSb@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=f7KxfUrRSb"
        ],
        "detail": [
          "https://openreview.net/forum?id=f7KxfUrRSb"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Wenhong Zhu , Zhiwei He , Xiaofeng Wang , Pengfei Liu , Rui Wang Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "dOAkHmsjRX@OpenReview",
      "index": 253,
      "title": "Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling",
      "authors": [
        "Minhyuk Seo",
        "Hyunseo Koh",
        "Jonghyun Choi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "budget",
        "memory",
        "continual",
        "freezing",
        "budgets",
        "total",
        "storage",
        "replay",
        "budgeted",
        "online"
      ],
      "summary": "The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory.However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition to replay memory is largely ignored in calculating the storage budget.Arguing different computational and storage budgets hinder fair comparison among CL algorithms in practice, we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same \"total resource budget\".To improve a CL method in a limited total budget, we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy.In addition, we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations.Empirical validations on the CIFAR-10/100, CLEAR-10/100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms the state-of-the-art methods within the same total budget.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dOAkHmsjRX"
        ],
        "venue": [
          "/venue/dOAkHmsjRX@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dOAkHmsjRX"
        ],
        "detail": [
          "https://openreview.net/forum?id=dOAkHmsjRX"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 7
      },
      "raw_excerpt": "Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling [PDF 1 ] [Copy] [Kimi 7 ] [REL] Authors : Minhyuk Seo , Hyunseo Koh , Jonghyun Choi The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory.However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition to replay memory is largely ignored in calculating the storage budget.Arguing different computational and storage budgets hinder fair comparison among CL algorithms in practice, we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same \"total resource budget\".To improve a CL method in a limited total budget, we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy.In addition, we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations.Empirical validations on the CIFAR-10/100, CLEAR-10/100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms the state-of-the-art methods within the same total budget. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "dEypApI1MZ@OpenReview",
      "index": 254,
      "title": "How Feature Learning Can Improve Neural Scaling Laws",
      "authors": [
        "Blake Bordelon",
        "Alexander Atanasov",
        "Cengiz Pehlevan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "tasks",
        "scaling",
        "hard",
        "feature",
        "easy",
        "rkhs",
        "ntk",
        "kernel",
        "learning",
        "laws"
      ],
      "summary": "We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard tasks, easy tasks, and super easy tasks. For easy and super-easy target functions, which are in the Hilbert space (RKHS) of the initial infinite-width neural tangent kernel (NTK), there is no change in the scaling exponents between feature learning models and models in the kernel regime. For hard tasks, which we define as tasks outside of the RKHS of the initial NTK, we show analytically and empirically that feature learning can improve the scaling with training time and compute, approximately doubling the exponent for very hard tasks. This leads to a new compute optimal scaling law for hard tasks in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dEypApI1MZ"
        ],
        "venue": [
          "/venue/dEypApI1MZ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dEypApI1MZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=dEypApI1MZ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "How Feature Learning Can Improve Neural Scaling Laws [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Blake Bordelon , Alexander Atanasov , Cengiz Pehlevan We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard tasks, easy tasks, and super easy tasks. For easy and super-easy target functions, which are in the Hilbert space (RKHS) of the initial infinite-width neural tangent kernel (NTK), there is no change in the scaling exponents between feature learning models and models in the kernel regime. For hard tasks, which we define as tasks outside of the RKHS of the initial NTK, we show analytically and empirically that feature learning can improve the scaling with training time and compute, approximately doubling the exponent for very hard tasks. This leads to a new compute optimal scaling law for hard tasks in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "csbf1p8xUq@OpenReview",
      "index": 255,
      "title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale",
      "authors": [
        "Haoran Xu",
        "Kenton Murray",
        "Philipp Koehn",
        "Hieu Hoang",
        "Akiko Eriguchi",
        "Huda Khayrallah"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "translation",
        "languages",
        "alma",
        "multilingual",
        "aya",
        "regimen",
        "english",
        "plug",
        "rejection",
        "llms"
      ],
      "summary": "Large language models (LLMs) have achieved remarkable success across various NLP tasks, yet their focus has predominantly been on English due to English-centric pre-training and limited multilingual data. While some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality response for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages like English and Chinese. In this paper, we prioritize quality over scaling number of languages, with a focus on multilingual machine translation task, and introduce **X-ALMA**, a model designed with a commitment to ensuring top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. At the final stage of training regimen, our proposed Adaptive Rejection Preference Optimization (**ARPO**) surpasses existing preference optimization methods in translation tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=csbf1p8xUq"
        ],
        "venue": [
          "/venue/csbf1p8xUq@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=csbf1p8xUq"
        ],
        "detail": [
          "https://openreview.net/forum?id=csbf1p8xUq"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Haoran Xu , Kenton Murray , Philipp Koehn , Hieu Hoang , Akiko Eriguchi , Huda Khayrallah Large language models (LLMs) have achieved remarkable success across various NLP tasks, yet their focus has predominantly been on English due to English-centric pre-training and limited multilingual data. While some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality response for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages like English and Chinese. In this paper, we prioritize quality over scaling number of languages, with a focus on multilingual machine translation task, and introduce **X-ALMA**, a model designed with a commitment to ensuring top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. At the final stage of training regimen, our proposed Adaptive Rejection Preference Optimization (**ARPO**) surpasses existing preference optimization methods in translation tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cmXWYolrlo@OpenReview",
      "index": 256,
      "title": "Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture",
      "authors": [
        "Sajad Movahedi",
        "Antonio Orvieto",
        "Seyed-Mohsen Moosavi-Dezfooli"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gih",
        "geometry",
        "average",
        "architecture",
        "dub",
        "input",
        "space",
        "resnet",
        "neural",
        "evolution"
      ],
      "summary": "In this paper, we propose the *geometric invariance hypothesis (GIH)*, which argues that when training a neural network, the input space curvature remains invariant under transformation in certain directions determined by its architecture. Starting with a simple non-linear binary classification problem residing on a plane in a high dimensional space, we observe that while an MLP can generalize on this problem regardless of the orientation of the plane, this is not the case for a ResNet. Motivated by this example, we define two maps that provide a compact *architecture-dependent* summary of the input space geometry of a neural network and its evolution during training, which we dub the **average geometry** and **average geometry evolution**, respectively. By investigating average geometry evolution at initialization, we discover that the geometry of a neural network evolves according to the projection of data covariance onto average geometry. As a result, in cases where the average geometry is low-rank (such as in a ResNet), the geometry only changes in a subset of the input space. This causes an architecture-dependent invariance property in input space curvature, which we dub GIH. Finally, we present extensive experimental results to observe the consequences of GIH and how it relates to generalization in neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cmXWYolrlo"
        ],
        "venue": [
          "/venue/cmXWYolrlo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cmXWYolrlo"
        ],
        "detail": [
          "https://openreview.net/forum?id=cmXWYolrlo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Sajad Movahedi , Antonio Orvieto , Seyed-Mohsen Moosavi-Dezfooli In this paper, we propose the *geometric invariance hypothesis (GIH)*, which argues that when training a neural network, the input space curvature remains invariant under transformation in certain directions determined by its architecture. Starting with a simple non-linear binary classification problem residing on a plane in a high dimensional space, we observe that while an MLP can generalize on this problem regardless of the orientation of the plane, this is not the case for a ResNet. Motivated by this example, we define two maps that provide a compact *architecture-dependent* summary of the input space geometry of a neural network and its evolution during training, which we dub the **average geometry** and **average geometry evolution**, respectively. By investigating average geometry evolution at initialization, we discover that the geometry of a neural network evolves according to the projection of data covariance onto average geometry. As a result, in cases where the average geometry is low-rank (such as in a ResNet), the geometry only changes in a subset of the input space. This causes an architecture-dependent invariance property in input space curvature, which we dub GIH. Finally, we present extensive experimental results to observe the consequences of GIH and how it relates to generalization in neural networks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cRR0oDFEBC@OpenReview",
      "index": 257,
      "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
      "authors": [
        "Guanting Dong",
        "Keming Lu",
        "Chengpeng Li",
        "Tingyu Xia",
        "Bowen Yu",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "autoif",
        "instruction",
        "following",
        "llms",
        "sft",
        "dpo",
        "feedback",
        "execution",
        "language",
        "instructions"
      ],
      "summary": "One core capability of large language models~(LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to verify the correctness of the instruction responses, and unit test samples to cross-validate the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the advanced open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Using two widely-used and three challenging general instruction-following benchmarks, we demonstrate that AutoIF significantly improves LLM performance across a wide range of natural instruction constraints. Notably, AutoIF is the first to surpass 90\\% accuracy in IFEval’s loose instruction accuracy, without compromising general, math and coding capabilities. Further analysis of quality, scaling, combination, and data efficiency highlights AutoIF's strong generalization and alignment potential.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cRR0oDFEBC"
        ],
        "venue": [
          "/venue/cRR0oDFEBC@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cRR0oDFEBC"
        ],
        "detail": [
          "https://openreview.net/forum?id=cRR0oDFEBC"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 8
      },
      "raw_excerpt": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models [PDF 3 ] [Copy] [Kimi 8 ] [REL] Authors : Guanting Dong , Keming Lu , Chengpeng Li , Tingyu Xia , Bowen Yu , Chang Zhou , Jingren Zhou One core capability of large language models~(LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to verify the correctness of the instruction responses, and unit test samples to cross-validate the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the advanced open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Using two widely-used and three challenging general instruction-following benchmarks, we demonstrate that AutoIF significantly improves LLM performance across a wide range of natural instruction constraints. Notably, AutoIF is the first to surpass 90\\% accuracy in IFEval’s loose instruction accuracy, without compromising general, math and coding capabilities. Further analysis of quality, scaling, combination, and data efficiency highlights AutoIF's strong generalization and alignment potential. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "aD2uwhLbnA@OpenReview",
      "index": 258,
      "title": "Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training",
      "authors": [
        "Zhanpeng Zhou",
        "Mingze Wang",
        "Yuchen Mao",
        "Bingrui Li",
        "Junchi Yan"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sam",
        "flatter",
        "sharpness",
        "late",
        "training",
        "minima",
        "selects",
        "minimization",
        "sgd",
        "aware"
      ],
      "summary": "Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings.Despite the success, its effectiveness remains poorly understood.In this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding lights on understanding its implicit bias towards flatter minima over Stochastic Gradient Descent (SGD).Specifically, we find that *SAM efficiently selects flatter minima late in training*.Remarkably, even a few epochs of SAM applied at the end of training yield nearly the same generalization and solution sharpness as full SAM training.Subsequently, we delve deeper into the underlying mechanism behind this phenomenon.Theoretically, we identify two phases in the learning dynamics after applying SAM late in training: i) SAM first escapes the minimum found by SGD exponentially fast; and ii) then rapidly converges to a flatter minimum within the same valley.Furthermore, we empirically investigate the role of SAM during the early training phase.We conjecture that the optimization method chosen in the late phase is more crucial in shaping the final solution's properties.Based on this viewpoint, we extend our findings from SAM to Adversarial Training.We provide source code in supplementary materials and will release checkpoints in future.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aD2uwhLbnA"
        ],
        "venue": [
          "/venue/aD2uwhLbnA@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aD2uwhLbnA"
        ],
        "detail": [
          "https://openreview.net/forum?id=aD2uwhLbnA"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Zhanpeng Zhou , Mingze Wang , Yuchen Mao , Bingrui Li , Junchi Yan Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings.Despite the success, its effectiveness remains poorly understood.In this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding lights on understanding its implicit bias towards flatter minima over Stochastic Gradient Descent (SGD).Specifically, we find that *SAM efficiently selects flatter minima late in training*.Remarkably, even a few epochs of SAM applied at the end of training yield nearly the same generalization and solution sharpness as full SAM training.Subsequently, we delve deeper into the underlying mechanism behind this phenomenon.Theoretically, we identify two phases in the learning dynamics after applying SAM late in training: i) SAM first escapes the minimum found by SGD exponentially fast; and ii) then rapidly converges to a flatter minimum within the same valley.Furthermore, we empirically investigate the role of SAM during the early training phase.We conjecture that the optimization method chosen in the late phase is more crucial in shaping the final solution's properties.Based on this viewpoint, we extend our findings from SAM to Adversarial Training.We provide source code in supplementary materials and will release checkpoints in future. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Yk87CwhBDx@OpenReview",
      "index": 259,
      "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
      "authors": [
        "Zeju Qiu",
        "Weiyang Liu",
        "Haiwen Feng",
        "Zhen Liu",
        "Tim Xiao",
        "Katherine Collins",
        "Joshua B Tenenbaum",
        "Adrian Weller",
        "Michael J Black",
        "Bernhard Schoelkopf"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "symbolic",
        "programs",
        "graphics",
        "llms",
        "procedurally",
        "ability",
        "llm",
        "sit",
        "reasoning",
        "skills"
      ],
      "summary": "Against the backdrop of enthusiasm for large language models (LLMs), there is an urgent need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training.Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM's ability to answer different-grained semantic-level questions of the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to \"imagine\" and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM's understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Yk87CwhBDx"
        ],
        "venue": [
          "/venue/Yk87CwhBDx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Yk87CwhBDx"
        ],
        "detail": [
          "https://openreview.net/forum?id=Yk87CwhBDx"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 10
      },
      "raw_excerpt": "Can Large Language Models Understand Symbolic Graphics Programs? [PDF 9 ] [Copy] [Kimi 10 ] [REL] Authors : Zeju Qiu , Weiyang Liu , Haiwen Feng , Zhen Liu , Tim Xiao , Katherine Collins , Joshua B Tenenbaum , Adrian Weller , Michael J Black , Bernhard Schoelkopf Against the backdrop of enthusiasm for large language models (LLMs), there is an urgent need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training.Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM's ability to answer different-grained semantic-level questions of the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to \"imagine\" and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM's understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "YK9G4Htdew@OpenReview",
      "index": 260,
      "title": "Learning Transformer-based World Models with Contrastive Predictive Coding",
      "authors": [
        "Maxime Burchi",
        "Radu Timofte"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "world",
        "transformer",
        "twister",
        "contrastive",
        "based",
        "coding",
        "predictive",
        "dreamerv3",
        "dreamer",
        "model"
      ],
      "summary": "The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=YK9G4Htdew"
        ],
        "venue": [
          "/venue/YK9G4Htdew@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=YK9G4Htdew"
        ],
        "detail": [
          "https://openreview.net/forum?id=YK9G4Htdew"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Learning Transformer-based World Models with Contrastive Predictive Coding [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Maxime Burchi , Radu Timofte The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "XgH1wfHSX8@OpenReview",
      "index": 261,
      "title": "Algorithmic Phases of In-Context Learning",
      "authors": [
        "Core Francisco Park",
        "Ekdeep Singh Lubana",
        "Hidenori Tanaka"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "icl",
        "context",
        "merely",
        "nature",
        "behavior",
        "algorithms",
        "bigram",
        "superseding",
        "inputted",
        "motivated"
      ],
      "summary": "In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XgH1wfHSX8"
        ],
        "venue": [
          "/venue/XgH1wfHSX8@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XgH1wfHSX8"
        ],
        "detail": [
          "https://openreview.net/forum?id=XgH1wfHSX8"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 7
      },
      "raw_excerpt": "Algorithmic Phases of In-Context Learning [PDF 2 ] [Copy] [Kimi 7 ] [REL] Authors : Core Francisco Park , Ekdeep Singh Lubana , Hidenori Tanaka In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "XNA3Mnnbvb@OpenReview",
      "index": 262,
      "title": "DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control",
      "authors": [
        "Kaifeng Zhao",
        "Gen Li",
        "Siyu Tang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "motion",
        "dart",
        "text",
        "conditioned",
        "motions",
        "descriptions",
        "primitive",
        "utoregressive",
        "driven",
        "control"
      ],
      "summary": "Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose **DART**, a **D**iffusion-based **A**utoregressive motion primitive model for **R**eal-time **T**ext-driven motion control. Our model, DART, effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model’s versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XNA3Mnnbvb"
        ],
        "venue": [
          "/venue/XNA3Mnnbvb@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XNA3Mnnbvb"
        ],
        "detail": [
          "https://openreview.net/forum?id=XNA3Mnnbvb"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Kaifeng Zhao , Gen Li , Siyu Tang Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose **DART**, a **D**iffusion-based **A**utoregressive motion primitive model for **R**eal-time **T**ext-driven motion control. Our model, DART, effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model’s versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "VaowElpVzd@OpenReview",
      "index": 263,
      "title": "Co 3 3 Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion",
      "authors": [
        "Xingqun Qi",
        "Yatian Wang",
        "Hengyuan Zhang",
        "Jiahao Pan",
        "Wei Xue",
        "Shanghang Zhang",
        "Wenhan Luo",
        "Qifeng Liu",
        "Yike Guo"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gesture",
        "concurrent",
        "gestures",
        "speech",
        "interactive",
        "ges",
        "tim",
        "person",
        "coherent",
        "mathbf"
      ],
      "summary": "Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by people self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter GES-Inter . Moreover, we propose Co 3 3 Gesture, a novel framework that enables concurrent coherent co-speech gesture synthesis including two-person interactive movements. Our framework is built upon two cooperative generation branches conditioned on decomposed speaker audio. Specifically, to enhance the coordination of human postures w.r.t corresponding speaker audios while interacting with the conversational partner, we present a Temporal-Interaction Module ( TIM TIM ). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VaowElpVzd"
        ],
        "venue": [
          "/venue/VaowElpVzd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VaowElpVzd"
        ],
        "detail": [
          "https://openreview.net/forum?id=VaowElpVzd"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Co 3 3 Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Xingqun Qi , Yatian Wang , Hengyuan Zhang , Jiahao Pan , Wei Xue , Shanghang Zhang , Wenhan Luo , Qifeng Liu , Yike Guo Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by people self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter GES-Inter . Moreover, we propose Co 3 3 Gesture, a novel framework that enables concurrent coherent co-speech gesture synthesis including two-person interactive movements. Our framework is built upon two cooperative generation branches conditioned on decomposed speaker audio. Specifically, to enhance the coordination of human postures w.r.t corresponding speaker audios while interacting with the conversational partner, we present a Temporal-Interaction Module ( TIM TIM ). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "VGQugiuCQs@OpenReview",
      "index": 264,
      "title": "Fair Clustering in the Sliding Window Model",
      "authors": [
        "Vincent Cohen-Addad",
        "Shaofeng Jiang",
        "Qiaoyuan Yang",
        "Yubo Zhang",
        "Samson Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sliding",
        "window",
        "fair",
        "streaming",
        "varepsilon",
        "clustering",
        "insertion",
        "chierichetti",
        "fairness",
        "model"
      ],
      "summary": "We study streaming algorithms for proportionally fair clustering (a notion originally suggested by Chierichetti et al. (2017) in the sliding window model. We show that although there exist efficient streaming algorithms exist in the insertion-only model, surprisingly no algorithm can achieve finite ratio without violating the fairness constraint in sliding window. Hence, the problem of fair clustering is a rare separation between the insertion-only streaming model and the sliding window model. On the other hand, we show that if the fairness constraint by a multiplicative ε ε factor, there exists a ( 1 + ε ) ( 1 + ε ) -approximate sliding window algorithm that uses poly ( k ε − 1 log n ) poly ( k ε − 1 log ⁡ n ) space. This achieves essentially the best parameters (up to degree in the polynomial) provided the aforementioned lower bound. We also implement a number of empirical evaluations on real datasets to complement our theoretical results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VGQugiuCQs"
        ],
        "venue": [
          "/venue/VGQugiuCQs@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VGQugiuCQs"
        ],
        "detail": [
          "https://openreview.net/forum?id=VGQugiuCQs"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Fair Clustering in the Sliding Window Model [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Vincent Cohen-Addad , Shaofeng Jiang , Qiaoyuan Yang , Yubo Zhang , Samson Zhou We study streaming algorithms for proportionally fair clustering (a notion originally suggested by Chierichetti et al. (2017) in the sliding window model. We show that although there exist efficient streaming algorithms exist in the insertion-only model, surprisingly no algorithm can achieve finite ratio without violating the fairness constraint in sliding window. Hence, the problem of fair clustering is a rare separation between the insertion-only streaming model and the sliding window model. On the other hand, we show that if the fairness constraint by a multiplicative ε ε factor, there exists a ( 1 + ε ) ( 1 + ε ) -approximate sliding window algorithm that uses poly ( k ε − 1 log n ) poly ( k ε − 1 log ⁡ n ) space. This achieves essentially the best parameters (up to degree in the polynomial) provided the aforementioned lower bound. We also implement a number of empirical evaluations on real datasets to complement our theoretical results. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "UL8b54P96G@OpenReview",
      "index": 265,
      "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
      "authors": [
        "Yining Hong",
        "Beide Liu",
        "Maxine Wu",
        "Yuanhao Zhai",
        "Kai-Wei Chang",
        "Linjie Li",
        "Kevin Lin",
        "Chung-Ching Lin",
        "Jianfeng Wang",
        "Lijuan Wang",
        "Yingnian Wu",
        "Zhengyuan Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "vgen",
        "slow",
        "slowfast",
        "fast",
        "learning",
        "video",
        "episodic",
        "loop",
        "action",
        "generation"
      ],
      "summary": "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UL8b54P96G"
        ],
        "venue": [
          "/venue/UL8b54P96G@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UL8b54P96G"
        ],
        "detail": [
          "https://openreview.net/forum?id=UL8b54P96G"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Yining Hong , Beide Liu , Maxine Wu , Yuanhao Zhai , Kai-Wei Chang , Linjie Li , Kevin Lin , Chung-Ching Lin , Jianfeng Wang , Lijuan Wang , Yingnian Wu , Zhengyuan Yang Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "U67J0QNtzo@OpenReview",
      "index": 266,
      "title": "On Disentangled Training for Nonlinear Transform in Learned Image Compression",
      "authors": [
        "Han Li",
        "Shaohui Li",
        "Wenrui Dai",
        "Maida Cao",
        "Nuowen Kan",
        "Chenglin Li",
        "Junni Zou",
        "Hongkai Xiong"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lic",
        "auxt",
        "compaction",
        "nonlinear",
        "energy",
        "transforms",
        "decorrelation",
        "uneven",
        "wlss",
        "wavelet"
      ],
      "summary": "Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, \\emph{i.e.}, feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U67J0QNtzo"
        ],
        "venue": [
          "/venue/U67J0QNtzo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U67J0QNtzo"
        ],
        "detail": [
          "https://openreview.net/forum?id=U67J0QNtzo"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "On Disentangled Training for Nonlinear Transform in Learned Image Compression [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Han Li , Shaohui Li , Wenrui Dai , Maida Cao , Nuowen Kan , Chenglin Li , Junni Zou , Hongkai Xiong Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, \\emph{i.e.}, feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "TYSQYx9vwd@OpenReview",
      "index": 267,
      "title": "Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations",
      "authors": [
        "Richard Bergna",
        "Sergio Calvo Ordoñez",
        "Felix Opolka",
        "Pietro Lio",
        "José Miguel Hernández Lobato"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "uncertainty",
        "graph",
        "lgnsdes",
        "differential",
        "gnodes",
        "lgnsde",
        "stochastic",
        "gnode",
        "neural",
        "equations"
      ],
      "summary": "We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise perturbations, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TYSQYx9vwd"
        ],
        "venue": [
          "/venue/TYSQYx9vwd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TYSQYx9vwd"
        ],
        "detail": [
          "https://openreview.net/forum?id=TYSQYx9vwd"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Richard Bergna , Sergio Calvo Ordoñez , Felix Opolka , Pietro Lio , José Miguel Hernández Lobato We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise perturbations, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "TJo6aQb7mK@OpenReview",
      "index": 268,
      "title": "Surprising Effectiveness of pretraining Ternary Language Model at Scale",
      "authors": [
        "Ayush Kaushal",
        "Tejas Vaidhya",
        "Arnab Mondal",
        "Tejas Pandey",
        "Aaryan Bhagat",
        "Irina Rish"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "trilms",
        "floatlm",
        "floatlms",
        "quantlms",
        "bitwidth",
        "language",
        "llm",
        "bit",
        "pretraining",
        "ternary"
      ],
      "summary": "Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TJo6aQb7mK"
        ],
        "venue": [
          "/venue/TJo6aQb7mK@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TJo6aQb7mK"
        ],
        "detail": [
          "https://openreview.net/forum?id=TJo6aQb7mK"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 6
      },
      "raw_excerpt": "Surprising Effectiveness of pretraining Ternary Language Model at Scale [PDF 4 ] [Copy] [Kimi 6 ] [REL] Authors : Ayush Kaushal , Tejas Vaidhya , Arnab Mondal , Tejas Pandey , Aaryan Bhagat , Irina Rish Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "THqWPzL00e@OpenReview",
      "index": 269,
      "title": "TopoNets: High performing vision and language models with brain-like topography",
      "authors": [
        "Mayukh Deb",
        "Mainak Deb",
        "Apurva Murty"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "toponets",
        "brain",
        "topography",
        "topoloss",
        "topographic",
        "performing",
        "language",
        "organized",
        "resnet",
        "nanogpt"
      ],
      "summary": "Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present *TopoLoss*, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively *TopoNets*. TopoNets are the highest performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain’s visual and language cortices, further bridging the gap between biological and artificial systems. This work establishes a robust and generalizable framework for integrating topography into AI, advancing the development of high performing models that more closely emulate the computational strategies of the human brain.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=THqWPzL00e"
        ],
        "venue": [
          "/venue/THqWPzL00e@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=THqWPzL00e"
        ],
        "detail": [
          "https://openreview.net/forum?id=THqWPzL00e"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "TopoNets: High performing vision and language models with brain-like topography [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Mayukh Deb , Mainak Deb , Apurva Murty Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present *TopoLoss*, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively *TopoNets*. TopoNets are the highest performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain’s visual and language cortices, further bridging the gap between biological and artificial systems. This work establishes a robust and generalizable framework for integrating topography into AI, advancing the development of high performing models that more closely emulate the computational strategies of the human brain. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SUc1UOWndp@OpenReview",
      "index": 270,
      "title": "Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient",
      "authors": [
        "George Wang",
        "Jesse Hoogland",
        "Stan van Wingerden",
        "Zach Furman",
        "Michelle Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "heads",
        "rllcs",
        "refined",
        "specialization",
        "attention",
        "differentiation",
        "developmental",
        "learning",
        "multigram",
        "coefficient"
      ],
      "summary": "We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these refined LLCs (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for developmental interpretability, which aims to understand models through their evolution across the learning process. This work advances the field of developmental interpretability by providing a mathematically rigorous approach to understanding neural networks through the lens of their learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SUc1UOWndp"
        ],
        "venue": [
          "/venue/SUc1UOWndp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SUc1UOWndp"
        ],
        "detail": [
          "https://openreview.net/forum?id=SUc1UOWndp"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 4
      },
      "raw_excerpt": "Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient [PDF 4 ] [Copy] [Kimi 4 ] [REL] Authors : George Wang , Jesse Hoogland , Stan van Wingerden , Zach Furman , Michelle Chen We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these refined LLCs (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for developmental interpretability, which aims to understand models through their evolution across the learning process. This work advances the field of developmental interpretability by providing a mathematically rigorous approach to understanding neural networks through the lens of their learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "S4dItvpvAv@OpenReview",
      "index": 271,
      "title": "How to Find the Exact Pareto Front for Multi-Objective MDPs?",
      "authors": [
        "Yining Li",
        "Peizhong Ju",
        "Ness Shroff"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pareto",
        "front",
        "policies",
        "mdps",
        "deterministic",
        "objective",
        "mdp",
        "vertices",
        "traversing",
        "finding"
      ],
      "summary": "Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. The Pareto front identifies the set of policies that cannot be dominated, providing a foundation for finding Pareto optimal solutions that can efficiently adapt to various preferences.However, finding the Pareto front is a highly challenging problem. Most existing methods either (i) rely on traversing the *continuous preference space*, which is impractical and results in approximations that are difficult to evaluate against the true Pareto front, or (ii) focus solely on deterministic Pareto optimal policies, from which there are no known techniques to characterize the full Pareto front. Moreover, finding the structure of the Pareto front itself remains unclear even in the context of dynamic programming, where the MDP is fully known in advance.In this work, we address the challenge of efficiently discovering the Pareto front, involving both deterministic and stochastic Pareto optimal policies.By investigating the geometric structure of the Pareto front in MO-MDPs, we uncover a key property: the Pareto front is on the boundary of a convex polytope whose vertices all correspond to deterministic policies, and neighboring vertices of the Pareto front differ by only one state-action pair of the deterministic policy, almost surely.This insight transforms the global comparison across all policies into a localized search among deterministic policies that differ by only one state-action pair, drastically reducing the complexity of searching for the exact Pareto front. We develop an efficient algorithm that identifies the vertices of the Pareto front by solving a single-objective MDP only once and then traversing the edges of the Pareto front, making it more efficient than existing methods. Furthermore, the entire Pareto front can be found in V V iterations, where V V represents the number of vertices on the Pareto front.Our empirical studies demonstrate the effectiveness of our theoretical strategy in discovering the Pareto front efficiently.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=S4dItvpvAv"
        ],
        "venue": [
          "/venue/S4dItvpvAv@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=S4dItvpvAv"
        ],
        "detail": [
          "https://openreview.net/forum?id=S4dItvpvAv"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "How to Find the Exact Pareto Front for Multi-Objective MDPs? [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yining Li , Peizhong Ju , Ness Shroff Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. The Pareto front identifies the set of policies that cannot be dominated, providing a foundation for finding Pareto optimal solutions that can efficiently adapt to various preferences.However, finding the Pareto front is a highly challenging problem. Most existing methods either (i) rely on traversing the *continuous preference space*, which is impractical and results in approximations that are difficult to evaluate against the true Pareto front, or (ii) focus solely on deterministic Pareto optimal policies, from which there are no known techniques to characterize the full Pareto front. Moreover, finding the structure of the Pareto front itself remains unclear even in the context of dynamic programming, where the MDP is fully known in advance.In this work, we address the challenge of efficiently discovering the Pareto front, involving both deterministic and stochastic Pareto optimal policies.By investigating the geometric structure of the Pareto front in MO-MDPs, we uncover a key property: the Pareto front is on the boundary of a convex polytope whose vertices all correspond to deterministic policies, and neighboring vertices of the Pareto front differ by only one state-action pair of the deterministic policy, almost surely.This insight transforms the global comparison across all policies into a localized search among deterministic policies that differ by only one state-action pair, drastically reducing the complexity of searching for the exact Pareto front. We develop an efficient algorithm that identifies the vertices of the Pareto front by solving a single-objective MDP only once and then traversing the edges of the Pareto front, making it more efficient than existing methods. Furthermore, the entire Pareto front can be found in V V iterations, where V V represents the number of vertices on the Pareto front.Our empirical studies demonstrate the effectiveness of our theoretical strategy in discovering the Pareto front efficiently. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "RInisw1yin@OpenReview",
      "index": 272,
      "title": "SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks",
      "authors": [
        "Yijie Guo",
        "Bingjie Tang",
        "Iretiayo Akinola",
        "Dieter Fox",
        "Abhishek Gupta",
        "Yashraj Narang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "srsa",
        "skill",
        "tasks",
        "library",
        "assembly",
        "skills",
        "success",
        "policies",
        "retrieval",
        "task"
      ],
      "summary": "Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made in developing such strategies for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. In this work, we present SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. Through extensive experiments, we demonstrate that SRSA significantly outperforms the leading baseline, achieving a 22\\% relative improvement in success rate, 3.7x higher stability, and 2.4x greater sample efficiency when retrieving and fine-tuning skills on unseen tasks. Moreover, in a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Additionally, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage at https://srsa2024.github.io/ for videos.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RInisw1yin"
        ],
        "venue": [
          "/venue/RInisw1yin@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RInisw1yin"
        ],
        "detail": [
          "https://openreview.net/forum?id=RInisw1yin"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Yijie Guo , Bingjie Tang , Iretiayo Akinola , Dieter Fox , Abhishek Gupta , Yashraj Narang Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made in developing such strategies for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. In this work, we present SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. Through extensive experiments, we demonstrate that SRSA significantly outperforms the leading baseline, achieving a 22\\% relative improvement in success rate, 3.7x higher stability, and 2.4x greater sample efficiency when retrieving and fine-tuning skills on unseen tasks. Moreover, in a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Additionally, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage at https://srsa2024.github.io/ for videos. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "R1hIXdST22@OpenReview",
      "index": 273,
      "title": "Towards General-Purpose Model-Free Reinforcement Learning",
      "authors": [
        "Scott Fujimoto",
        "Pierluca D'Oro",
        "Amy Zhang",
        "Yuandong Tian",
        "Michael Rabbat"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "generalist",
        "benchmarks",
        "hyperparameters",
        "model",
        "purpose",
        "reinforcement",
        "free",
        "towards",
        "linearize",
        "algorithms"
      ],
      "summary": "Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive generalist results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate the resulting algorithm on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and generalist baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=R1hIXdST22"
        ],
        "venue": [
          "/venue/R1hIXdST22@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=R1hIXdST22"
        ],
        "detail": [
          "https://openreview.net/forum?id=R1hIXdST22"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "Towards General-Purpose Model-Free Reinforcement Learning [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Scott Fujimoto , Pierluca D'Oro , Amy Zhang , Yuandong Tian , Michael Rabbat Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive generalist results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate the resulting algorithm on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and generalist baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "QjSOgxJ0hp@OpenReview",
      "index": 274,
      "title": "Learning from End User Data with Shuffled Differential Privacy over Kernel Densities",
      "authors": [
        "Tal Wagner"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "shuffled",
        "end",
        "privately",
        "privacy",
        "data",
        "downstream",
        "kernel",
        "private",
        "central",
        "curator"
      ],
      "summary": "We study a setting of collecting and learning from private data distributed across end users.In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QjSOgxJ0hp"
        ],
        "venue": [
          "/venue/QjSOgxJ0hp@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QjSOgxJ0hp"
        ],
        "detail": [
          "https://openreview.net/forum?id=QjSOgxJ0hp"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "Learning from End User Data with Shuffled Differential Privacy over Kernel Densities [PDF 5 ] [Copy] [Kimi 3 ] [REL] Author : Tal Wagner We study a setting of collecting and learning from private data distributed across end users.In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Q150eWkQ4I@OpenReview",
      "index": 275,
      "title": "Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement",
      "authors": [
        "Haijin Zeng",
        "Benteng Sun",
        "Yongyong Chen",
        "Jingyong Su",
        "Yong Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "msi",
        "sci",
        "unmixing",
        "subspace",
        "spectral",
        "diffusion",
        "compressive",
        "refinement",
        "imaging",
        "driven"
      ],
      "summary": "Spectral Compressive Imaging (SCI) reconstruction is inherently ill-posed, offering multiple plausible solutions from a single observation. Traditional deterministic methods typically struggle to effectively recover high-frequency details. Although diffusion models offer promising solutions to this challenge, their application is constrained by the limited training data and high computational demands associated with multispectral images (MSIs), complicating direct training. To address these issues, we propose a novel Predict-and-unmixing-driven-Subspace-Refine framework (PSR-SCI). This framework begins with a cost-effective predictor that produces an initial, rough estimate of the MSI. Subsequently, we introduce a unmixing-driven reversible spectral embedding module that decomposes the MSI into subspace images and spectral coefficients. This decomposition facilitates the adaptation of pre-trained RGB diffusion models and focuses refinement processes on high-frequency details, thereby enabling efficient diffusion generation with minimal MSI data. Additionally, we design a high-dimensional guidance mechanism with imaging consistency to enhance the model's efficacy. The refined subspace image is then reconstructed back into an MSI using the reversible embedding, yielding the final MSI with full spectral resolution. Experimental results on the standard KAIST and zero-shot datasets NTIRE, ICVL, and Harvard show that PSR-SCI enhances visual quality and delivers PSNR and SSIM metrics comparable to existing diffusion, transformer, and deep unfolding techniques. This framework provides a robust alternative to traditional deterministic SCI reconstruction methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Q150eWkQ4I"
        ],
        "venue": [
          "/venue/Q150eWkQ4I@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Q150eWkQ4I"
        ],
        "detail": [
          "https://openreview.net/forum?id=Q150eWkQ4I"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Haijin Zeng , Benteng Sun , Yongyong Chen , Jingyong Su , Yong Xu Spectral Compressive Imaging (SCI) reconstruction is inherently ill-posed, offering multiple plausible solutions from a single observation. Traditional deterministic methods typically struggle to effectively recover high-frequency details. Although diffusion models offer promising solutions to this challenge, their application is constrained by the limited training data and high computational demands associated with multispectral images (MSIs), complicating direct training. To address these issues, we propose a novel Predict-and-unmixing-driven-Subspace-Refine framework (PSR-SCI). This framework begins with a cost-effective predictor that produces an initial, rough estimate of the MSI. Subsequently, we introduce a unmixing-driven reversible spectral embedding module that decomposes the MSI into subspace images and spectral coefficients. This decomposition facilitates the adaptation of pre-trained RGB diffusion models and focuses refinement processes on high-frequency details, thereby enabling efficient diffusion generation with minimal MSI data. Additionally, we design a high-dimensional guidance mechanism with imaging consistency to enhance the model's efficacy. The refined subspace image is then reconstructed back into an MSI using the reversible embedding, yielding the final MSI with full spectral resolution. Experimental results on the standard KAIST and zero-shot datasets NTIRE, ICVL, and Harvard show that PSR-SCI enhances visual quality and delivers PSNR and SSIM metrics comparable to existing diffusion, transformer, and deep unfolding techniques. This framework provides a robust alternative to traditional deterministic SCI reconstruction methods. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "PGhiPGBf47@OpenReview",
      "index": 276,
      "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life",
      "authors": [
        "Yu Ying (Kelly) Chiu",
        "Liwei Jiang",
        "Yejin Choi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dailydilemmas",
        "moral",
        "values",
        "dilemmas",
        "llms",
        "life",
        "daily",
        "quandaries",
        "value",
        "prioritization"
      ],
      "summary": "As we increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of the users. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma includes two possible actions and with each action, the affected parties and human values invoked. Based on these dilemmas, we consolidated a set of human values across everyday topics e.g., interpersonal relationships, workplace, and environmental issues. We evaluated LLMs on these dilemmas to determine what action they will take and the values represented by these actions. Then, we analyzed these values through the lens of five popular theories inspired by sociology, psychology and philosophy. These theories are: World Value Survey, Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik Wheel of Emotion. We find that LLMs are most aligned with the self-expression over survival values in terms of World Value Survey, care over loyalty in Moral Foundation Theory. Interestingly, we find large preferences differences in models for some core values such as truthfulness e.g., Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to select it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their released principles reflect their actual value prioritization when facing nuanced moral reasoning in daily-life settings. We find that end users cannot effectively steer such prioritization using system prompts.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PGhiPGBf47"
        ],
        "venue": [
          "/venue/PGhiPGBf47@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PGhiPGBf47"
        ],
        "detail": [
          "https://openreview.net/forum?id=PGhiPGBf47"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Yu Ying (Kelly) Chiu , Liwei Jiang , Yejin Choi As we increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of the users. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma includes two possible actions and with each action, the affected parties and human values invoked. Based on these dilemmas, we consolidated a set of human values across everyday topics e.g., interpersonal relationships, workplace, and environmental issues. We evaluated LLMs on these dilemmas to determine what action they will take and the values represented by these actions. Then, we analyzed these values through the lens of five popular theories inspired by sociology, psychology and philosophy. These theories are: World Value Survey, Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik Wheel of Emotion. We find that LLMs are most aligned with the self-expression over survival values in terms of World Value Survey, care over loyalty in Moral Foundation Theory. Interestingly, we find large preferences differences in models for some core values such as truthfulness e.g., Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to select it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their released principles reflect their actual value prioritization when facing nuanced moral reasoning in daily-life settings. We find that end users cannot effectively steer such prioritization using system prompts. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "OeBY9XqiTz@OpenReview",
      "index": 277,
      "title": "Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking",
      "authors": [
        "Mattia Segu",
        "Luigi Piccinelli",
        "Siyuan Li",
        "Yung-Hsu Yang",
        "Bernt Schiele",
        "Luc Van Gool"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "samba",
        "sambamotr",
        "tracklets",
        "dependencies",
        "occlusions",
        "tracklet",
        "interdependencies",
        "synchronized",
        "sequences",
        "tracking"
      ],
      "summary": "Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OeBY9XqiTz"
        ],
        "venue": [
          "/venue/OeBY9XqiTz@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OeBY9XqiTz"
        ],
        "detail": [
          "https://openreview.net/forum?id=OeBY9XqiTz"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Mattia Segu , Luigi Piccinelli , Siyuan Li , Yung-Hsu Yang , Bernt Schiele , Luc Van Gool Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "NGKQoaqLpo@OpenReview",
      "index": 278,
      "title": "How new data pollutes LLM knowledge and how to dilute it",
      "authors": [
        "Chen Sun",
        "Renat Aksitov",
        "Andrey Zhmoginov",
        "Nolan Miller",
        "Max Vladymyrov",
        "Ulrich Rueckert",
        "Been Kim",
        "Mark Sandler"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "priming",
        "pollutes",
        "llm",
        "knowledge",
        "texts",
        "alterations",
        "new",
        "learning",
        "undesirable",
        "outlandish"
      ],
      "summary": "Understanding how the learning of new texts alter the existing knowledge in a large language model is of great importance, because it is through these accumulated changes that the LLM was initially pre-trained, and is also through such changes that continual, new learning in LLMs can proceed. As a result, both desirable alterations (i.e. generalization) and undesirable alterations (i.e. hallucination) can occur. Here, we study the learning of new texts, one at a time, and ask: how does it impact the underlying LLM knowledge? We show that learning new texts induce 'priming', an undesirable effect that pollutes existing knowledge where it should not.Centrally, we demonstrate that we can predict how much priming will happen after learning, using token probability before learning. This was empirically robust across models (PALM-2-xs/s, Gemma-2b, Llama-2-7b), of various sizes, and training stages. To show this, we created a new dataset, called \"Outlandish\" consisting of 1320 different samples with diverse textual characteristics. Finally, we propose two strategies to mitigate the spread of priming: first, a simple text augmentation technique which we call the \"stepping-stone'', and second, a novel update pruning technique (\"ignore-k\"). These decrease priming by a median of 50%-75% and 50%-95% respectively depending on the model architecture, and enhance the specificity of new learning in language models. The dataset and reproducible findings can be found [LINK omitted for double blind review].",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NGKQoaqLpo"
        ],
        "venue": [
          "/venue/NGKQoaqLpo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NGKQoaqLpo"
        ],
        "detail": [
          "https://openreview.net/forum?id=NGKQoaqLpo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 4
      },
      "raw_excerpt": "How new data pollutes LLM knowledge and how to dilute it [PDF 2 ] [Copy] [Kimi 4 ] [REL] Authors : Chen Sun , Renat Aksitov , Andrey Zhmoginov , Nolan Miller , Max Vladymyrov , Ulrich Rueckert , Been Kim , Mark Sandler Understanding how the learning of new texts alter the existing knowledge in a large language model is of great importance, because it is through these accumulated changes that the LLM was initially pre-trained, and is also through such changes that continual, new learning in LLMs can proceed. As a result, both desirable alterations (i.e. generalization) and undesirable alterations (i.e. hallucination) can occur. Here, we study the learning of new texts, one at a time, and ask: how does it impact the underlying LLM knowledge? We show that learning new texts induce 'priming', an undesirable effect that pollutes existing knowledge where it should not.Centrally, we demonstrate that we can predict how much priming will happen after learning, using token probability before learning. This was empirically robust across models (PALM-2-xs/s, Gemma-2b, Llama-2-7b), of various sizes, and training stages. To show this, we created a new dataset, called \"Outlandish\" consisting of 1320 different samples with diverse textual characteristics. Finally, we propose two strategies to mitigate the spread of priming: first, a simple text augmentation technique which we call the \"stepping-stone'', and second, a novel update pruning technique (\"ignore-k\"). These decrease priming by a median of 50%-75% and 50%-95% respectively depending on the model architecture, and enhance the specificity of new learning in language models. The dataset and reproducible findings can be found [LINK omitted for double blind review]. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "N4NhVN30ph@OpenReview",
      "index": 279,
      "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning",
      "authors": [
        "Ge Li",
        "Dong Tian",
        "Hongyi Zhou",
        "Xinkai Jiang",
        "Rudolf Lioutikov",
        "Gerhard Neumann"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "erl",
        "policy",
        "top",
        "episodic",
        "action",
        "transformer",
        "reinforcement",
        "entire",
        "trajectories",
        "sequences"
      ],
      "summary": "This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=N4NhVN30ph"
        ],
        "venue": [
          "/venue/N4NhVN30ph@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=N4NhVN30ph"
        ],
        "detail": [
          "https://openreview.net/forum?id=N4NhVN30ph"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Ge Li , Dong Tian , Hongyi Zhou , Xinkai Jiang , Rudolf Lioutikov , Gerhard Neumann This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "MtDd7rWok1@OpenReview",
      "index": 280,
      "title": "Anti-Exposure Bias in Diffusion Models via Prompt Learning",
      "authors": [
        "Junyu Zhang",
        "Daochang Liu",
        "Eunbyung Park",
        "Shichao Zhang",
        "Chang Xu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "prompt",
        "exposure",
        "bias",
        "dms",
        "sampling",
        "anti",
        "prediction",
        "diffusion",
        "rectifies",
        "trajectory"
      ],
      "summary": "Diffusion models (DMs) have achieved record-breaking performance in image generation tasks.Nevertheless, in practice, the training-sampling discrepancy, caused by score estimation error and discretization error, limits the modeling ability of DMs, a phenomenon known as exposure bias.To alleviate such exposure bias and further improve the generative performance, we put forward a prompt learning framework built upon a lightweight prompt prediction model.Concretely, our model learns an anti-bias prompt for the generated sample at each sampling step, aiming to compensate for the exposure bias that arises.Following this design philosophy, our framework rectifies the sampling trajectory to match the training trajectory, thereby reducing the divergence between the target data distribution and the modeling distribution.To train the prompt prediction model, we simulate exposure bias by constructing training data and introduce a time-dependent weighting function for optimization.Empirical results on various DMs demonstrate the superiority of our prompt learning framework across three benchmark datasets.Importantly, the optimized prompt prediction model effectively improves image quality with only a 5\\% increase in sampling overhead, which remains negligible.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MtDd7rWok1"
        ],
        "venue": [
          "/venue/MtDd7rWok1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MtDd7rWok1"
        ],
        "detail": [
          "https://openreview.net/forum?id=MtDd7rWok1"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Anti-Exposure Bias in Diffusion Models via Prompt Learning [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Junyu Zhang , Daochang Liu , Eunbyung Park , Shichao Zhang , Chang Xu Diffusion models (DMs) have achieved record-breaking performance in image generation tasks.Nevertheless, in practice, the training-sampling discrepancy, caused by score estimation error and discretization error, limits the modeling ability of DMs, a phenomenon known as exposure bias.To alleviate such exposure bias and further improve the generative performance, we put forward a prompt learning framework built upon a lightweight prompt prediction model.Concretely, our model learns an anti-bias prompt for the generated sample at each sampling step, aiming to compensate for the exposure bias that arises.Following this design philosophy, our framework rectifies the sampling trajectory to match the training trajectory, thereby reducing the divergence between the target data distribution and the modeling distribution.To train the prompt prediction model, we simulate exposure bias by constructing training data and introduce a time-dependent weighting function for optimization.Empirical results on various DMs demonstrate the superiority of our prompt learning framework across three benchmark datasets.Importantly, the optimized prompt prediction model effectively improves image quality with only a 5\\% increase in sampling overhead, which remains negligible. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "MagmwodCAB@OpenReview",
      "index": 281,
      "title": "3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation",
      "authors": [
        "Dewei Zhou",
        "Ji Xie",
        "Zongxin Yang",
        "Yi Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "3dis",
        "mig",
        "instance",
        "rendering",
        "depth",
        "generation",
        "controlnet",
        "attribute",
        "decoupled",
        "layouts"
      ],
      "summary": "The increasing demand for controllable outputs in text-to-image generation has spurred advancements in multi-instance generation (MIG), allowing users to define both instance layouts and attributes. However, unlike image-conditional generation methods such as ControlNet, MIG techniques have not been widely adopted in state-of-the-art models like SD2 and SDXL, primarily due to the challenge of building robust renderers that simultaneously handle instance positioning and attribute rendering. In this paper, we introduce Depth-Driven Decoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG process into two stages: (i) generating a coarse scene depth map for accurate instance positioning and scene composition, and (ii) rendering fine-grained attributes using pre-trained ControlNet on any foundational model, without additional training. Our 3DIS framework integrates a custom adapter into LDM3D for precise depth-based layouts and employs a finetuning-free method for enhanced instance-level attribute rendering. Extensive experiments on COCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly outperforms existing methods in both layout precision and attribute rendering. Notably, 3DIS offers seamless compatibility with diverse foundational models, providing a robust, adaptable solution for advanced multi-instance generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MagmwodCAB"
        ],
        "venue": [
          "/venue/MagmwodCAB@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MagmwodCAB"
        ],
        "detail": [
          "https://openreview.net/forum?id=MagmwodCAB"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Dewei Zhou , Ji Xie , Zongxin Yang , Yi Yang The increasing demand for controllable outputs in text-to-image generation has spurred advancements in multi-instance generation (MIG), allowing users to define both instance layouts and attributes. However, unlike image-conditional generation methods such as ControlNet, MIG techniques have not been widely adopted in state-of-the-art models like SD2 and SDXL, primarily due to the challenge of building robust renderers that simultaneously handle instance positioning and attribute rendering. In this paper, we introduce Depth-Driven Decoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG process into two stages: (i) generating a coarse scene depth map for accurate instance positioning and scene composition, and (ii) rendering fine-grained attributes using pre-trained ControlNet on any foundational model, without additional training. Our 3DIS framework integrates a custom adapter into LDM3D for precise depth-based layouts and employs a finetuning-free method for enhanced instance-level attribute rendering. Extensive experiments on COCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly outperforms existing methods in both layout precision and attribute rendering. Notably, 3DIS offers seamless compatibility with diverse foundational models, providing a robust, adaptable solution for advanced multi-instance generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "LqTz13JS2P@OpenReview",
      "index": 282,
      "title": "Generalized Principal-Agent Problem with a Learning Agent",
      "authors": [
        "Tao Lin",
        "Yiling Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "agent",
        "principal",
        "regret",
        "sreg",
        "swap",
        "generalized",
        "mathrm",
        "persuasion",
        "reg",
        "stackelberg"
      ],
      "summary": "Generalized principal-agent problems, including Stackelberg games, contract design, and Bayesian persuasion, are a class of economic problems where an agent best responds to a principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) if the agent uses contextual no-regret learning algorithms with regret R e g ( T ) R e g ( T ) , then the principal can guarantee utility at least U ∗ − Θ ( R e g ( T ) T − − − − − √ ) U ∗ − Θ ( R e g ( T ) T ) , where U ∗ U ∗ is the principal's optimal utility in the classic model with a best-responding agent.(2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret S R e g ( T ) S R e g ( T ) , then the principal cannot obtain utility more than U ∗ + O ( S R e g ( T ) T ) U ∗ + O ( S R e g ( T ) T ) . But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than U ∗ U ∗ .These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=LqTz13JS2P"
        ],
        "venue": [
          "/venue/LqTz13JS2P@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=LqTz13JS2P"
        ],
        "detail": [
          "https://openreview.net/forum?id=LqTz13JS2P"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 9
      },
      "raw_excerpt": "Generalized Principal-Agent Problem with a Learning Agent [PDF 5 ] [Copy] [Kimi 9 ] [REL] Authors : Tao Lin , Yiling Chen Generalized principal-agent problems, including Stackelberg games, contract design, and Bayesian persuasion, are a class of economic problems where an agent best responds to a principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) if the agent uses contextual no-regret learning algorithms with regret R e g ( T ) R e g ( T ) , then the principal can guarantee utility at least U ∗ − Θ ( R e g ( T ) T − − − − − √ ) U ∗ − Θ ( R e g ( T ) T ) , where U ∗ U ∗ is the principal's optimal utility in the classic model with a best-responding agent.(2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret S R e g ( T ) S R e g ( T ) , then the principal cannot obtain utility more than U ∗ + O ( S R e g ( T ) T ) U ∗ + O ( S R e g ( T ) T ) . But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than U ∗ U ∗ .These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "L14sqcrUC3@OpenReview",
      "index": 283,
      "title": "Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks",
      "authors": [
        "Ivan Rubachev",
        "Nikolay Kartashev",
        "Yury Gorishniy",
        "Artem Babenko"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "tabular",
        "tabred",
        "academic",
        "benchmarks",
        "datasets",
        "splits",
        "pitfalls",
        "evaluation",
        "underrepresented",
        "deployment"
      ],
      "summary": "Advances in machine learning research drive progress in real-world applications. To ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.First, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets generally lack timestamp metadata to enable such evaluation.Second, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.In this work, we aim to understand how recent advances in tabular deep learning, which are evaluated on academic benchmarks, transfer to these underrepresented conditions.To this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. We reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on time-based data splits leads to different methods ranking, compared to evaluation on random splits, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=L14sqcrUC3"
        ],
        "venue": [
          "/venue/L14sqcrUC3@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=L14sqcrUC3"
        ],
        "detail": [
          "https://openreview.net/forum?id=L14sqcrUC3"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Ivan Rubachev , Nikolay Kartashev , Yury Gorishniy , Artem Babenko Advances in machine learning research drive progress in real-world applications. To ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.First, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets generally lack timestamp metadata to enable such evaluation.Second, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.In this work, we aim to understand how recent advances in tabular deep learning, which are evaluated on academic benchmarks, transfer to these underrepresented conditions.To this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. We reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on time-based data splits leads to different methods ranking, compared to evaluation on random splits, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Kpjvm2mB0K@OpenReview",
      "index": 284,
      "title": "Streaming Algorithms For ℓ p ℓ p Flows and ℓ p ℓ p Regression",
      "authors": [
        "Amit Chakrabarti",
        "Jeffrey Jiang",
        "David Woodruff",
        "Taisuke Yasuda"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mathbf",
        "ell",
        "tilde",
        "varepsilon",
        "streaming",
        "space",
        "kappa",
        "regression",
        "columns",
        "poly"
      ],
      "summary": "We initiate the study of one-pass streaming algorithms for underdetermined ℓ p ℓ p linear regression problems of the form min A x = b ∥ x ∥ p , where A ∈ R n × d with n ≪ d , min A x = b ‖ x ‖ p , where A ∈ R n × d with n ≪ d , which generalizes basis pursuit ( p = 1 p = 1 ) and least squares solutions to underdetermined linear systems ( p = 2 p = 2 ). We study the column-arrival streaming model, in which the columns of A A are presented one by one in a stream. When A A is the incidence matrix of a graph, this corresponds to an edge insertion graph stream, and the regression problem captures ℓ p ℓ p flows which includes transshipment ( p = 1 p = 1 ), electrical flows ( p = 2 p = 2 ), and max flow ( p = ∞ p = ∞ ) on undirected graphs as special cases. Our goal is to design algorithms which use space much less than the entire stream, which has a length of d d . For the task of estimating the cost of the ℓ p ℓ p regression problem for p ∈ [ 2 , ∞ ] p ∈ [ 2 , ∞ ] , we show a streaming algorithm which constructs a sparse instance supported on O ~ ( ε − 2 n ) O ~ ( ε − 2 n ) columns of A A which approximates the cost up to a ( 1 ± ε ) ( 1 ± ε ) factor, which corresponds to O ~ ( ε − 2 n 2 ) O ~ ( ε − 2 n 2 ) bits of space in general and an O ~ ( ε − 2 n ) O ~ ( ε − 2 n ) space semi-streaming algorithm for constructing ℓ p ℓ p flow sparsifiers on graphs. This extends to p ∈ ( 1 , 2 ) p ∈ ( 1 , 2 ) with O ~ ( ε 2 n q / 2 ) O ~ ( ε 2 n q / 2 ) columns, where q q is the Hölder conjugate exponent of p p . For p = 2 p = 2 , we show that Ω ( n 2 ) Ω ( n 2 ) bits of space are required in general even for outputting a constant factor solution. For p = 1 p = 1 , we show that the cost cannot be estimated even to an o ( n − − √ ) o ( n ) factor in p o l y ( n ) p o l y ( n ) space. On the other hand, if we are interested in outputting a solution x x , then we show that ( 1 + ε ) ( 1 + ε ) -approximations require Ω ( d ) Ω ( d ) space for p > 1 p > 1 , and in general, κ κ -approximations require Ω ~ ( d / κ 2 q ) Ω ~ ( d / κ 2 q ) space for p > 1 p > 1 . We complement these lower bounds with the first sublinear space upper bounds for this problem, showing that we can output a κ κ -approximation using space only p o l y ( n ) ⋅ O ~ ( d / κ q ) p o l y ( n ) ⋅ O ~ ( d / κ q ) for p > 1 p > 1 , as well as a n − − √ n -approximation using p o l y ( n , log d ) p o l y ( n , log ⁡ d ) space for p = 1 p = 1 .",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Kpjvm2mB0K"
        ],
        "venue": [
          "/venue/Kpjvm2mB0K@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Kpjvm2mB0K"
        ],
        "detail": [
          "https://openreview.net/forum?id=Kpjvm2mB0K"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Streaming Algorithms For ℓ p ℓ p Flows and ℓ p ℓ p Regression [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Amit Chakrabarti , Jeffrey Jiang , David Woodruff , Taisuke Yasuda We initiate the study of one-pass streaming algorithms for underdetermined ℓ p ℓ p linear regression problems of the form min A x = b ∥ x ∥ p , where A ∈ R n × d with n ≪ d , min A x = b ‖ x ‖ p , where A ∈ R n × d with n ≪ d , which generalizes basis pursuit ( p = 1 p = 1 ) and least squares solutions to underdetermined linear systems ( p = 2 p = 2 ). We study the column-arrival streaming model, in which the columns of A A are presented one by one in a stream. When A A is the incidence matrix of a graph, this corresponds to an edge insertion graph stream, and the regression problem captures ℓ p ℓ p flows which includes transshipment ( p = 1 p = 1 ), electrical flows ( p = 2 p = 2 ), and max flow ( p = ∞ p = ∞ ) on undirected graphs as special cases. Our goal is to design algorithms which use space much less than the entire stream, which has a length of d d . For the task of estimating the cost of the ℓ p ℓ p regression problem for p ∈ [ 2 , ∞ ] p ∈ [ 2 , ∞ ] , we show a streaming algorithm which constructs a sparse instance supported on O ~ ( ε − 2 n ) O ~ ( ε − 2 n ) columns of A A which approximates the cost up to a ( 1 ± ε ) ( 1 ± ε ) factor, which corresponds to O ~ ( ε − 2 n 2 ) O ~ ( ε − 2 n 2 ) bits of space in general and an O ~ ( ε − 2 n ) O ~ ( ε − 2 n ) space semi-streaming algorithm for constructing ℓ p ℓ p flow sparsifiers on graphs. This extends to p ∈ ( 1 , 2 ) p ∈ ( 1 , 2 ) with O ~ ( ε 2 n q / 2 ) O ~ ( ε 2 n q / 2 ) columns, where q q is the Hölder conjugate exponent of p p . For p = 2 p = 2 , we show that Ω ( n 2 ) Ω ( n 2 ) bits of space are required in general even for outputting a constant factor solution. For p = 1 p = 1 , we show that the cost cannot be estimated even to an o ( n − − √ ) o ( n ) factor in p o l y ( n ) p o l y ( n ) space. On the other hand, if we are interested in outputting a solution x x , then we show that ( 1 + ε ) ( 1 + ε ) -approximations require Ω ( d ) Ω ( d ) space for p > 1 p > 1 , and in general, κ κ -approximations require Ω ~ ( d / κ 2 q ) Ω ~ ( d / κ 2 q ) space for p > 1 p > 1 . We complement these lower bounds with the first sublinear space upper bounds for this problem, showing that we can output a κ κ -approximation using space only p o l y ( n ) ⋅ O ~ ( d / κ q ) p o l y ( n ) ⋅ O ~ ( d / κ q ) for p > 1 p > 1 , as well as a n − − √ n -approximation using p o l y ( n , log d ) p o l y ( n , log ⁡ d ) space for p = 1 p = 1 . Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IjQ2Jtemzy@OpenReview",
      "index": 285,
      "title": "Language Models Can Articulate Their Implicit Goals",
      "authors": [
        "Jan Betley",
        "Xuchan Bao",
        "Martín Soto",
        "Anna Sztyber-Betley",
        "James Chua",
        "Owain Evans"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "articulate",
        "awareness",
        "llms",
        "policies",
        "objective",
        "risk",
        "seeking",
        "examples",
        "certain",
        "behaviors"
      ],
      "summary": "We study *objective awareness*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b) making the user say a certain word. Although these examples never contain explicit descriptions of the policy (e.g. ``I will now take the risk-seeking option''), we find that the finetuned LLMs can explicitly describe their policies through out-of-context reasoning. We demonstrate LLMs' objective awareness across various evaluation tasks, both for multiple-choice and free-form questions. Furthermore, we demonstrate that models can correctly attribute different learned policies to distinct personas. Finally, we explore the connection between objective awareness and the concept of backdoors in AI safety, where certain behaviors are implanted in a model, often through data poisoning, and can be triggered under certain conditions. We find evidence that LLMs can recognize the existence of the backdoor-like behavior that they have acquired through finetuning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IjQ2Jtemzy"
        ],
        "venue": [
          "/venue/IjQ2Jtemzy@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IjQ2Jtemzy"
        ],
        "detail": [
          "https://openreview.net/forum?id=IjQ2Jtemzy"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 6
      },
      "raw_excerpt": "Language Models Can Articulate Their Implicit Goals [PDF 1 ] [Copy] [Kimi 6 ] [REL] Authors : Jan Betley , Xuchan Bao , Martín Soto , Anna Sztyber-Betley , James Chua , Owain Evans We study *objective awareness*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b) making the user say a certain word. Although these examples never contain explicit descriptions of the policy (e.g. ``I will now take the risk-seeking option''), we find that the finetuned LLMs can explicitly describe their policies through out-of-context reasoning. We demonstrate LLMs' objective awareness across various evaluation tasks, both for multiple-choice and free-form questions. Furthermore, we demonstrate that models can correctly attribute different learned policies to distinct personas. Finally, we explore the connection between objective awareness and the concept of backdoors in AI safety, where certain behaviors are implanted in a model, often through data poisoning, and can be triggered under certain conditions. We find evidence that LLMs can recognize the existence of the backdoor-like behavior that they have acquired through finetuning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "INyi7qUdjZ@OpenReview",
      "index": 286,
      "title": "Differential learning kinetics govern the transition from memorization to generalization during in-context learning",
      "authors": [
        "Alex Nguyen",
        "Gautam Reddy Nallamala"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "icl",
        "memorization",
        "transition",
        "generalization",
        "memorize",
        "task",
        "diversity",
        "context",
        "circuits",
        "explains"
      ],
      "summary": "Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative *rates* at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=INyi7qUdjZ"
        ],
        "venue": [
          "/venue/INyi7qUdjZ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=INyi7qUdjZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=INyi7qUdjZ"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Differential learning kinetics govern the transition from memorization to generalization during in-context learning [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Alex Nguyen , Gautam Reddy Nallamala Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative *rates* at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "HqLHY4TzGj@OpenReview",
      "index": 287,
      "title": "Union-over-Intersections: Object Detection beyond Winner-Takes-All",
      "authors": [
        "Aritra Bhowmik",
        "Pascal Mettes",
        "Martin R. Oswald",
        "Cees G Snoek"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "winner",
        "box",
        "truth",
        "union",
        "proposal",
        "regressing",
        "proposals",
        "intersections",
        "takes",
        "ground"
      ],
      "summary": "This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach—regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=HqLHY4TzGj"
        ],
        "venue": [
          "/venue/HqLHY4TzGj@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=HqLHY4TzGj"
        ],
        "detail": [
          "https://openreview.net/forum?id=HqLHY4TzGj"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 4
      },
      "raw_excerpt": "Union-over-Intersections: Object Detection beyond Winner-Takes-All [PDF 1 ] [Copy] [Kimi 4 ] [REL] Authors : Aritra Bhowmik , Pascal Mettes , Martin R. Oswald , Cees G Snoek This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach—regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "H2Gxil855b@OpenReview",
      "index": 288,
      "title": "Atlas Gaussians Diffusion for 3D Generation",
      "authors": [
        "Haitao Yang",
        "Yuan Dong",
        "Hanwen Jiang",
        "Dejia Xu",
        "Georgios Pavlakos",
        "Qixing Huang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gaussians",
        "generation",
        "atlas",
        "latent",
        "diffusion",
        "patch",
        "representation",
        "feed",
        "native",
        "decode"
      ],
      "summary": "Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=H2Gxil855b"
        ],
        "venue": [
          "/venue/H2Gxil855b@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=H2Gxil855b"
        ],
        "detail": [
          "https://openreview.net/forum?id=H2Gxil855b"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Atlas Gaussians Diffusion for 3D Generation [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Haitao Yang , Yuan Dong , Hanwen Jiang , Dejia Xu , Georgios Pavlakos , Qixing Huang Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "GjfIZan5jN@OpenReview",
      "index": 289,
      "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability",
      "authors": [
        "Shufan Shen",
        "Zhaobo Qi",
        "Junshu Sun",
        "Qingming Huang",
        "Qi Tian",
        "Shuhui Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "classifiability",
        "interpretability",
        "pre",
        "trained",
        "representations",
        "representation",
        "interpretable",
        "semantics",
        "interpretations",
        "captured"
      ],
      "summary": "The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks. However, widespread applications for pre-trained visual models have proposed new requirements for representation interpretability. It remains unclear whether the pre-trained representations can achieve high interpretability and classifiability simultaneously. To answer this question, we quantify the representation interpretability by leveraging its correlation with the ratio of interpretable semantics within representations. Given the pre-trained representations, only the interpretable semantics can be captured by interpretations, whereas the uninterpretable part leads to information loss. Based on this fact, we propose the Inherent Interpretability Score (IIS) that evaluates the information loss, measures the ratio of interpretable semantics, and quantifies the representation interpretability. In the evaluation of the representation interpretability with different classifiability, we surprisingly discover that the interpretability and classifiability are positively correlated, i.e., representations with higher classifiability provide more interpretable semantics that can be captured in the interpretations. This observation further supports two benefits to the pre-trained representations. First, the classifiability of representations can be further improved by fine-tuning with interpretability maximization. Second, with the classifiability improvement for the representations, we obtain predictions based on their interpretations with less accuracy degradation. The discovered positive correlation and corresponding applications show that practitioners can unify the improvements in interpretability and classifiability for pre-trained vision models. Codes are included in the supplement and will be released on GitHub.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=GjfIZan5jN"
        ],
        "venue": [
          "/venue/GjfIZan5jN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=GjfIZan5jN"
        ],
        "detail": [
          "https://openreview.net/forum?id=GjfIZan5jN"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Shufan Shen , Zhaobo Qi , Junshu Sun , Qingming Huang , Qi Tian , Shuhui Wang The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks. However, widespread applications for pre-trained visual models have proposed new requirements for representation interpretability. It remains unclear whether the pre-trained representations can achieve high interpretability and classifiability simultaneously. To answer this question, we quantify the representation interpretability by leveraging its correlation with the ratio of interpretable semantics within representations. Given the pre-trained representations, only the interpretable semantics can be captured by interpretations, whereas the uninterpretable part leads to information loss. Based on this fact, we propose the Inherent Interpretability Score (IIS) that evaluates the information loss, measures the ratio of interpretable semantics, and quantifies the representation interpretability. In the evaluation of the representation interpretability with different classifiability, we surprisingly discover that the interpretability and classifiability are positively correlated, i.e., representations with higher classifiability provide more interpretable semantics that can be captured in the interpretations. This observation further supports two benefits to the pre-trained representations. First, the classifiability of representations can be further improved by fine-tuning with interpretability maximization. Second, with the classifiability improvement for the representations, we obtain predictions based on their interpretations with less accuracy degradation. The discovered positive correlation and corresponding applications show that practitioners can unify the improvements in interpretability and classifiability for pre-trained vision models. Codes are included in the supplement and will be released on GitHub. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Fk3eod9aaD@OpenReview",
      "index": 290,
      "title": "In Search of Forgotten Domain Generalization",
      "authors": [
        "Prasanna Mayilvahanan",
        "Roland Zimmermann",
        "Thaddäus Wiedemer",
        "Evgenia Rusak",
        "Attila Juhos",
        "Matthias Bethge",
        "Wieland Brendel"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "ood",
        "laion",
        "generalization",
        "datasets",
        "forgotten",
        "domain",
        "rendition",
        "imagenet",
        "domains",
        "era"
      ],
      "summary": "Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Fk3eod9aaD"
        ],
        "venue": [
          "/venue/Fk3eod9aaD@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Fk3eod9aaD"
        ],
        "detail": [
          "https://openreview.net/forum?id=Fk3eod9aaD"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "In Search of Forgotten Domain Generalization [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Prasanna Mayilvahanan , Roland Zimmermann , Thaddäus Wiedemer , Evgenia Rusak , Attila Juhos , Matthias Bethge , Wieland Brendel Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ywFOSIT9ik@OpenReview",
      "index": 291,
      "title": "Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations",
      "authors": [
        "Shaocong Ma",
        "Heng Huang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "perturbations",
        "directionally",
        "zeroth",
        "aligned",
        "variance",
        "gradient",
        "daps",
        "dap",
        "revisiting",
        "optimization"
      ],
      "summary": "In this paper, we explore the two-point zeroth-order gradient estimator and identify the optimal distribution of random perturbations that minimizes the estimator's variance. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that optimal perturbations either maintain a fixed length or align directionally with the true gradient. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using δ δ -unbiased random perturbations, extending optimal complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ywFOSIT9ik"
        ],
        "venue": [
          "/venue/ywFOSIT9ik@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ywFOSIT9ik"
        ],
        "detail": [
          "https://openreview.net/forum?id=ywFOSIT9ik"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Shaocong Ma , Heng Huang In this paper, we explore the two-point zeroth-order gradient estimator and identify the optimal distribution of random perturbations that minimizes the estimator's variance. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that optimal perturbations either maintain a fixed length or align directionally with the true gradient. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using δ δ -unbiased random perturbations, extending optimal complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "EPHsIa0Ytg@OpenReview",
      "index": 292,
      "title": "Improved Approximation Algorithms for k k -Submodular Maximization via Multilinear Extension",
      "authors": [
        "Huanjian Zhou",
        "Lingxiao Huang",
        "Baoxiang Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "submodular",
        "maximization",
        "monotone",
        "multilinear",
        "knapsack",
        "approximation",
        "constraints",
        "matroid",
        "extension",
        "245"
      ],
      "summary": "We investigate a generalized form of submodular maximization, referred to as k k -submodular maximization, with applications across the domains of social networks and machine learning. In this work, we propose the multilinear extension of k k -submodular functions and unified Frank-Wolfe-type frameworks based on that. This continuous framework accommodates 1) monotone or non-monotone functions, and 2) various constraint types including matroid constraints, knapsack constraints, and their combinations. Notably, we attain an asymptotically optimal 1 / 2 1 / 2 -approximation for monotone k k -submodular maximization problems with knapsack constraints, surpassing previous 1 / 3 1 / 3 -approximation results, and a factor- 1 / 3 1 / 3 approximation for non-monotone k k -submodular maximization problems with knapsack constraints and matroid constraints which outperforms previous 0.245 0.245 -approximation results. The foundation for our analysis stems from new insights into specific linear and monotone properties pertaining to the multilinear extension.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EPHsIa0Ytg"
        ],
        "venue": [
          "/venue/EPHsIa0Ytg@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EPHsIa0Ytg"
        ],
        "detail": [
          "https://openreview.net/forum?id=EPHsIa0Ytg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Improved Approximation Algorithms for k k -Submodular Maximization via Multilinear Extension [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Huanjian Zhou , Lingxiao Huang , Baoxiang Wang We investigate a generalized form of submodular maximization, referred to as k k -submodular maximization, with applications across the domains of social networks and machine learning. In this work, we propose the multilinear extension of k k -submodular functions and unified Frank-Wolfe-type frameworks based on that. This continuous framework accommodates 1) monotone or non-monotone functions, and 2) various constraint types including matroid constraints, knapsack constraints, and their combinations. Notably, we attain an asymptotically optimal 1 / 2 1 / 2 -approximation for monotone k k -submodular maximization problems with knapsack constraints, surpassing previous 1 / 3 1 / 3 -approximation results, and a factor- 1 / 3 1 / 3 approximation for non-monotone k k -submodular maximization problems with knapsack constraints and matroid constraints which outperforms previous 0.245 0.245 -approximation results. The foundation for our analysis stems from new insights into specific linear and monotone properties pertaining to the multilinear extension. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "D1Y2XFgsPI@OpenReview",
      "index": 293,
      "title": "Imputation for prediction: beware of diminishing returns.",
      "authors": [
        "Marine Le Morvan",
        "Gael Varoquaux"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "imputation",
        "imputations",
        "investing",
        "missingness",
        "beware",
        "predictive",
        "prediction",
        "matters",
        "diminishing",
        "predictions"
      ],
      "summary": "Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying *if* and *when* investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, imputation quality has only a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=D1Y2XFgsPI"
        ],
        "venue": [
          "/venue/D1Y2XFgsPI@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=D1Y2XFgsPI"
        ],
        "detail": [
          "https://openreview.net/forum?id=D1Y2XFgsPI"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Imputation for prediction: beware of diminishing returns. [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Marine Le Morvan , Gael Varoquaux Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying *if* and *when* investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, imputation quality has only a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "CkgKSqZbuC@OpenReview",
      "index": 294,
      "title": "R 2 R 2 -Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning",
      "authors": [
        "Mintong Kang",
        "Bo Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "guardrail",
        "guard",
        "reasoning",
        "safety",
        "categories",
        "jailbreaking",
        "llamaguard",
        "moderation",
        "unsafety",
        "logical"
      ],
      "summary": "As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., self-harm, self-harm/instructions) independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories.To address these limitations, we propose R 2 R 2 -Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, R 2 R 2 -Guard comprises two parts: data-driven guardrail models and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different safety categories.We then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) based reasoning component. The unsafety probabilities of different categories from data-driven guardrail models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. We also propose different methods to optimize the weights of knowledge. To further perform stress tests for guardrail models, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories and presents new challenges for moderation. We show that R 2 R 2 -Guard is effective even given unrepresentative categories or challenging jailbreaking prompts. We demonstrate the effectiveness of R 2 R 2 -Guard by comparisons with eight strong guardrail models on six standard moderation datasets, and demonstrate the robustness of R 2 R 2 -Guard against four SOTA jailbreaking attacks. R 2 R 2 -Guard significantly surpasses SOTA method LlamaGuard by 12.6% on standard moderation datasets and by 59.9% against jailbreaking attacks.We further reveal that R 2 R 2 -Guard can effectively adapt to safety category updates by simply editing the PGM reasoning graph.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CkgKSqZbuC"
        ],
        "venue": [
          "/venue/CkgKSqZbuC@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CkgKSqZbuC"
        ],
        "detail": [
          "https://openreview.net/forum?id=CkgKSqZbuC"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 9
      },
      "raw_excerpt": "R 2 R 2 -Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning [PDF 5 ] [Copy] [Kimi 9 ] [REL] Authors : Mintong Kang , Bo Li As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., self-harm, self-harm/instructions) independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories.To address these limitations, we propose R 2 R 2 -Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, R 2 R 2 -Guard comprises two parts: data-driven guardrail models and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different safety categories.We then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) based reasoning component. The unsafety probabilities of different categories from data-driven guardrail models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. We also propose different methods to optimize the weights of knowledge. To further perform stress tests for guardrail models, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories and presents new challenges for moderation. We show that R 2 R 2 -Guard is effective even given unrepresentative categories or challenging jailbreaking prompts. We demonstrate the effectiveness of R 2 R 2 -Guard by comparisons with eight strong guardrail models on six standard moderation datasets, and demonstrate the robustness of R 2 R 2 -Guard against four SOTA jailbreaking attacks. R 2 R 2 -Guard significantly surpasses SOTA method LlamaGuard by 12.6% on standard moderation datasets and by 59.9% against jailbreaking attacks.We further reveal that R 2 R 2 -Guard can effectively adapt to safety category updates by simply editing the PGM reasoning graph. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Acvo2RGSCy@OpenReview",
      "index": 295,
      "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
      "authors": [
        "Ollie Liu",
        "Deqing Fu",
        "Dani Yogatama",
        "Willie Neiswanger"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dellma",
        "decision",
        "making",
        "language",
        "uncertainty",
        "llms",
        "enhance",
        "environments",
        "reasoning",
        "procedure"
      ],
      "summary": "The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of *decision-making under uncertainty*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling inference-time reasoning, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Acvo2RGSCy"
        ],
        "venue": [
          "/venue/Acvo2RGSCy@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Acvo2RGSCy"
        ],
        "detail": [
          "https://openreview.net/forum?id=Acvo2RGSCy"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 8
      },
      "raw_excerpt": "DeLLMa: Decision Making Under Uncertainty with Large Language Models [PDF 7 ] [Copy] [Kimi 8 ] [REL] Authors : Ollie Liu , Deqing Fu , Dani Yogatama , Willie Neiswanger The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of *decision-making under uncertainty*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling inference-time reasoning, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9ehJCZz4aM@OpenReview",
      "index": 296,
      "title": "Learning Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations",
      "authors": [
        "Pei Zhou",
        "Ruizhe Liu",
        "Qian Luo",
        "Yibing Song",
        "Fan Wang",
        "Yanchao Yang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "concept",
        "concepts",
        "manipulation",
        "guided",
        "policies",
        "loop",
        "task",
        "environmental",
        "demonstrations",
        "closed"
      ],
      "summary": "Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies that enhance long-horizon task performance by leveraging discovered manipulation concepts. Unlike methods that rely on predefined skills and human-annotated labels, our approach allows agents to autonomously abstract manipulation concepts from their proprioceptive states, thereby alleviating misalignment due to ambiguities in human semantics and environmental complexity. Our framework comprises two primary components: an *Automatic Concept Discovery* module that identifies meaningful and consistent manipulation concepts, and a *Concept-Aware Policy Learning* module that effectively utilizes these manipulation concepts for adaptive task execution, including a *Concept Selection Transformer* for concept-based guidance and a *Concept-Guided Policy* for action prediction with the selected concepts. Experimental results demonstrate that our approach significantly outperforms baseline methods across a range of tasks and environments, while showcasing emergent consistency in motion patterns associated with the discovered concepts. Our code and models will be public.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9ehJCZz4aM"
        ],
        "venue": [
          "/venue/9ehJCZz4aM@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9ehJCZz4aM"
        ],
        "detail": [
          "https://openreview.net/forum?id=9ehJCZz4aM"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "Learning Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Pei Zhou , Ruizhe Liu , Qian Luo , Yibing Song , Fan Wang , Yanchao Yang Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies that enhance long-horizon task performance by leveraging discovered manipulation concepts. Unlike methods that rely on predefined skills and human-annotated labels, our approach allows agents to autonomously abstract manipulation concepts from their proprioceptive states, thereby alleviating misalignment due to ambiguities in human semantics and environmental complexity. Our framework comprises two primary components: an *Automatic Concept Discovery* module that identifies meaningful and consistent manipulation concepts, and a *Concept-Aware Policy Learning* module that effectively utilizes these manipulation concepts for adaptive task execution, including a *Concept Selection Transformer* for concept-based guidance and a *Concept-Guided Policy* for action prediction with the selected concepts. Experimental results demonstrate that our approach significantly outperforms baseline methods across a range of tasks and environments, while showcasing emergent consistency in motion patterns associated with the discovered concepts. Our code and models will be public. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "9bMZ29SPVx@OpenReview",
      "index": 297,
      "title": "A CLIP-Powered Framework for Robust and Generalizable Data Selection",
      "authors": [
        "Suorong Yang",
        "Peng Ye",
        "Wanli Ouyang",
        "Dongzhan Zhou",
        "Furao Shen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "selection",
        "clip",
        "generalizable",
        "datasets",
        "samples",
        "powered",
        "noisy",
        "data",
        "training",
        "dataset"
      ],
      "summary": "Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets invariably incurs substantial storage and computational overhead. Meanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance. Data selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. Existing works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples. To address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. Specifically, our framework consists of three key modules—dataset adaptation, sample scoring, and selection optimization—that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. Extensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality. The implementation will be made publicly available soon.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=9bMZ29SPVx"
        ],
        "venue": [
          "/venue/9bMZ29SPVx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=9bMZ29SPVx"
        ],
        "detail": [
          "https://openreview.net/forum?id=9bMZ29SPVx"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "A CLIP-Powered Framework for Robust and Generalizable Data Selection [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Suorong Yang , Peng Ye , Wanli Ouyang , Dongzhan Zhou , Furao Shen Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets invariably incurs substantial storage and computational overhead. Meanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance. Data selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. Existing works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples. To address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. Specifically, our framework consists of three key modules—dataset adaptation, sample scoring, and selection optimization—that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. Extensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality. The implementation will be made publicly available soon. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "97rOQDPmk2@OpenReview",
      "index": 298,
      "title": "On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent",
      "authors": [
        "Bingrui Li",
        "Wei Huang",
        "Andi Han",
        "Zhanpeng Zhou",
        "Taiji Suzuki",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "signgd",
        "adam",
        "transformers",
        "optimizes",
        "optimization",
        "generalization",
        "layer",
        "descent",
        "transformer",
        "sign"
      ],
      "summary": "The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem.However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam.Despite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind.In this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on a linearly separable noisy dataset.We identify four stages in the training dynamics, each exhibiting intriguing behaviors.Based on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset.We also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting.Additionally, we find that the poor generalization of SignGD is not solely due to data noise,suggesting that both SignGD and Adam requires high-quality data for real-world tasks.Finally, experiments on synthetic and real-world datasets empirically support our theoretical results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=97rOQDPmk2"
        ],
        "venue": [
          "/venue/97rOQDPmk2@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=97rOQDPmk2"
        ],
        "detail": [
          "https://openreview.net/forum?id=97rOQDPmk2"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 3
      },
      "raw_excerpt": "On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent [PDF 6 ] [Copy] [Kimi 3 ] [REL] Authors : Bingrui Li , Wei Huang , Andi Han , Zhanpeng Zhou , Taiji Suzuki , Jun Zhu , Jianfei Chen The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem.However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam.Despite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind.In this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on a linearly separable noisy dataset.We identify four stages in the training dynamics, each exhibiting intriguing behaviors.Based on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset.We also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting.Additionally, we find that the poor generalization of SignGD is not solely due to data noise,suggesting that both SignGD and Adam requires high-quality data for real-world tasks.Finally, experiments on synthetic and real-world datasets empirically support our theoretical results. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "8oCrlOaYcc@OpenReview",
      "index": 299,
      "title": "Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL",
      "authors": [
        "Ghada Sokar",
        "Johan S Obando Ceron",
        "Aaron Courville",
        "Hugo Larochelle",
        "Pablo Samuel Castro"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "softmoes",
        "softmoe",
        "tokenize",
        "efficacy",
        "flatten",
        "unlocking",
        "experts",
        "behind",
        "tokenizing",
        "key"
      ],
      "summary": "The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performance gain. We discover the surprising result that tokenizing the encoder output, rather than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8oCrlOaYcc"
        ],
        "venue": [
          "/venue/8oCrlOaYcc@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8oCrlOaYcc"
        ],
        "detail": [
          "https://openreview.net/forum?id=8oCrlOaYcc"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Ghada Sokar , Johan S Obando Ceron , Aaron Courville , Hugo Larochelle , Pablo Samuel Castro The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performance gain. We discover the surprising result that tokenizing the encoder output, rather than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "8UFG9D8xeU@OpenReview",
      "index": 300,
      "title": "Direct Post-Training Preference Alignment of Multi-Agent Motion Generation Model with Implicit Feedback from Demonstrations",
      "authors": [
        "Thomas Tian",
        "Kratarth Goel"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "demonstrations",
        "preference",
        "unpreferred",
        "generations",
        "alignment",
        "rankings",
        "implicit",
        "motion",
        "token",
        "preferred"
      ],
      "summary": "Recent advancements in Large Language Models (LLMs) have transformed motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type motion models benefit from scalability and efficient formulation, there remains a discrepancy between their token-prediction imitation objectives and human preferences. This often results in behaviors that deviate from human-preferred demonstrations, making post-training behavior alignment crucial for generating human-preferred motions. Post-training alignment requires a large number of preference rankings over model generations, which are costly and time-consuming to annotate in multi-agent motion generation settings. Recently, there has been growing interest in using expert demonstrations to scalably build preference data for alignment. However, these methods often adopt a worst-case scenario assumption, treating all generated samples from the reference model as unpreferred and relying on expert demonstrations to directly or indirectly construct preferred generations. This approach overlooks the rich signal provided by preference rankings among the model's own generations. In this work, instead of treating all generated samples as equally unpreferred, we propose a principled approach leveraging the implicit preferences encoded in expert demonstrations to construct preference rankings among the generations produced by the reference model, offering more nuanced guidance at low-cost. We present the first investigation of direct preference alignment for multi-agent motion token-prediction models using implicit preference feedback from demonstrations. We apply our approach to large-scale traffic simulation and demonstrate its effectiveness in improving the realism of generated behaviors involving up to 128 agents, making a 1M token-prediction model comparable to state-of-the-art large models by relying solely on implicit feedback from demonstrations, without requiring additional human annotations or high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future investigations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8UFG9D8xeU"
        ],
        "venue": [
          "/venue/8UFG9D8xeU@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8UFG9D8xeU"
        ],
        "detail": [
          "https://openreview.net/forum?id=8UFG9D8xeU"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 13
      },
      "raw_excerpt": "Direct Post-Training Preference Alignment of Multi-Agent Motion Generation Model with Implicit Feedback from Demonstrations [PDF 5 ] [Copy] [Kimi 13 ] [REL] Authors : Thomas Tian , Kratarth Goel Recent advancements in Large Language Models (LLMs) have transformed motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type motion models benefit from scalability and efficient formulation, there remains a discrepancy between their token-prediction imitation objectives and human preferences. This often results in behaviors that deviate from human-preferred demonstrations, making post-training behavior alignment crucial for generating human-preferred motions. Post-training alignment requires a large number of preference rankings over model generations, which are costly and time-consuming to annotate in multi-agent motion generation settings. Recently, there has been growing interest in using expert demonstrations to scalably build preference data for alignment. However, these methods often adopt a worst-case scenario assumption, treating all generated samples from the reference model as unpreferred and relying on expert demonstrations to directly or indirectly construct preferred generations. This approach overlooks the rich signal provided by preference rankings among the model's own generations. In this work, instead of treating all generated samples as equally unpreferred, we propose a principled approach leveraging the implicit preferences encoded in expert demonstrations to construct preference rankings among the generations produced by the reference model, offering more nuanced guidance at low-cost. We present the first investigation of direct preference alignment for multi-agent motion token-prediction models using implicit preference feedback from demonstrations. We apply our approach to large-scale traffic simulation and demonstrate its effectiveness in improving the realism of generated behaviors involving up to 128 agents, making a 1M token-prediction model comparable to state-of-the-art large models by relying solely on implicit feedback from demonstrations, without requiring additional human annotations or high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future investigations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "7nyJBVCTGQ@OpenReview",
      "index": 301,
      "title": "LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning",
      "authors": [
        "Minyoung Kim",
        "Timothy Hospedales"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lora",
        "lift",
        "meta",
        "fine",
        "posterior",
        "sgld",
        "bayesian",
        "tuning",
        "task",
        "tune"
      ],
      "summary": "We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7nyJBVCTGQ"
        ],
        "venue": [
          "/venue/7nyJBVCTGQ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7nyJBVCTGQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=7nyJBVCTGQ"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Minyoung Kim , Timothy Hospedales We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "71pur4y8gs@OpenReview",
      "index": 302,
      "title": "TabWak: A Watermark for Tabular Diffusion Models",
      "authors": [
        "Chaoyi Zhu",
        "Jiayi Tang",
        "Jeroen Galjaard",
        "Pin-Yu Chen",
        "Robert Birke",
        "Cornelis Bos",
        "Lydia Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "tabwak",
        "tables",
        "row",
        "watermarking",
        "watermark",
        "detectability",
        "watermarked",
        "tabular",
        "seeds",
        "synthetic"
      ],
      "summary": "Synthetic data offers alternatives for data augmentation and sharing. Till date, it remains unknown how to use watermarking techniques to trace and audit synthetic tables generated by tabular diffusion models to mitigate potential misuses. In this paper, we design TabWak, the first watermarking method to embed invisible signatures that control the sampling of Gaussian latent codes used to synthesize table rows via the diffusion backbone. TabWak has two key features. Different from existing image watermarking techniques, TabWak uses self-cloning and shuffling to embed the secret key in positional information of random seeds that control the Gaussian latents, allowing to use different seeds at each row for high inter-row diversity and enabling row-wise detectability. To further boost the robustness of watermark detection against post-editing attacks, TabWak uses a valid-bit mechanism that focuses on the tail of the latent code distribution for superior noise resilience. We provide theoretical guarantees on the row diversity and effectiveness of detectability. We evaluate TabWak on five datasets against baselines to show that the quality of watermarked tables remains nearly indistinguishable from non-watermarked tables while achieving high detectability in the presence of strong post-editing attacks, with a 100% true positive rate at a 0.1% false positive rate on synthetic tables with fewer than 300 rows. Our code is available at the following anonymized repository https://anonymous.4open.science/r/TabWak-4E65/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=71pur4y8gs"
        ],
        "venue": [
          "/venue/71pur4y8gs@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=71pur4y8gs"
        ],
        "detail": [
          "https://openreview.net/forum?id=71pur4y8gs"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "TabWak: A Watermark for Tabular Diffusion Models [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Chaoyi Zhu , Jiayi Tang , Jeroen Galjaard , Pin-Yu Chen , Robert Birke , Cornelis Bos , Lydia Chen Synthetic data offers alternatives for data augmentation and sharing. Till date, it remains unknown how to use watermarking techniques to trace and audit synthetic tables generated by tabular diffusion models to mitigate potential misuses. In this paper, we design TabWak, the first watermarking method to embed invisible signatures that control the sampling of Gaussian latent codes used to synthesize table rows via the diffusion backbone. TabWak has two key features. Different from existing image watermarking techniques, TabWak uses self-cloning and shuffling to embed the secret key in positional information of random seeds that control the Gaussian latents, allowing to use different seeds at each row for high inter-row diversity and enabling row-wise detectability. To further boost the robustness of watermark detection against post-editing attacks, TabWak uses a valid-bit mechanism that focuses on the tail of the latent code distribution for superior noise resilience. We provide theoretical guarantees on the row diversity and effectiveness of detectability. We evaluate TabWak on five datasets against baselines to show that the quality of watermarked tables remains nearly indistinguishable from non-watermarked tables while achieving high detectability in the presence of strong post-editing attacks, with a 100% true positive rate at a 0.1% false positive rate on synthetic tables with fewer than 300 rows. Our code is available at the following anonymized repository https://anonymous.4open.science/r/TabWak-4E65/. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "60i0ksMAhd@OpenReview",
      "index": 303,
      "title": "BlendRL: A Framework for Merging Symbolic and Neural Policy Learning",
      "authors": [
        "Hikaru Shindo",
        "Quentin Delfosse",
        "Devendra Singh Dhami",
        "Kristian Kersting"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "blendrl",
        "symbolic",
        "agents",
        "neural",
        "policies",
        "disjointed",
        "harmoniously",
        "reasoning",
        "overcome",
        "merging"
      ],
      "summary": "Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents’ capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. To overcome this challenge, we introduce *BlendRL*, a neuro-symbolic RL framework that harmoniously integrates both paradigms. We empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=60i0ksMAhd"
        ],
        "venue": [
          "/venue/60i0ksMAhd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=60i0ksMAhd"
        ],
        "detail": [
          "https://openreview.net/forum?id=60i0ksMAhd"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "BlendRL: A Framework for Merging Symbolic and Neural Policy Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Hikaru Shindo , Quentin Delfosse , Devendra Singh Dhami , Kristian Kersting Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents’ capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. To overcome this challenge, we introduce *BlendRL*, a neuro-symbolic RL framework that harmoniously integrates both paradigms. We empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "5ZEbpBYGwH@OpenReview",
      "index": 304,
      "title": "COPER: Correlation-based Permutations for Multi-View Clustering",
      "authors": [
        "Ran Eisenberg",
        "Jonathan Svirsky",
        "Ofir Lindenbaum"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "clustering",
        "coper",
        "view",
        "lda",
        "multi",
        "permutations",
        "correlation",
        "learned",
        "pseudo",
        "labels"
      ],
      "summary": "Combining data from different sources can improve data analysis tasks such as clustering. However, most of the current multi-view clustering methods are limited to specific domains or rely on a suboptimal and computationally intensive two-stage process of representation learning and clustering. We propose an end-to-end deep learning-based multi-view clustering framework for general data types (such as images and tables). Our approach involves generating meaningful fused representations using a novel permutation-based canonical correlation objective. We provide a theoretical analysis showing how the learned embeddings approximate those obtained by supervised linear discriminant analysis (LDA). Cluster assignments are learned by identifying consistent pseudo-labels across multiple views. Additionally, we establish a theoretical bound on the error caused by incorrect pseudo-labels in the unsupervised representations compared to LDA. Extensive experiments on ten multi-view clustering benchmark datasets provide empirical evidence for the effectiveness of the proposed model.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=5ZEbpBYGwH"
        ],
        "venue": [
          "/venue/5ZEbpBYGwH@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=5ZEbpBYGwH"
        ],
        "detail": [
          "https://openreview.net/forum?id=5ZEbpBYGwH"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "COPER: Correlation-based Permutations for Multi-View Clustering [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Ran Eisenberg , Jonathan Svirsky , Ofir Lindenbaum Combining data from different sources can improve data analysis tasks such as clustering. However, most of the current multi-view clustering methods are limited to specific domains or rely on a suboptimal and computationally intensive two-stage process of representation learning and clustering. We propose an end-to-end deep learning-based multi-view clustering framework for general data types (such as images and tables). Our approach involves generating meaningful fused representations using a novel permutation-based canonical correlation objective. We provide a theoretical analysis showing how the learned embeddings approximate those obtained by supervised linear discriminant analysis (LDA). Cluster assignments are learned by identifying consistent pseudo-labels across multiple views. Additionally, we establish a theoretical bound on the error caused by incorrect pseudo-labels in the unsupervised representations compared to LDA. Extensive experiments on ten multi-view clustering benchmark datasets provide empirical evidence for the effectiveness of the proposed model. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "4ub9gpx9xw@OpenReview",
      "index": 305,
      "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
      "authors": [
        "Katie Matton",
        "Robert Ness",
        "John Guttag",
        "Emre Kiciman"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "explanations",
        "faithfulness",
        "llm",
        "concepts",
        "measuring",
        "question",
        "influenced",
        "uncover",
        "model",
        "misrepresent"
      ],
      "summary": "Large language models (LLMs) are capable of generating *plausible* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's \"reasoning\" process, i.e., they can be *unfaithful*. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level *concepts* in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM's *explanations imply* are influential and the set that *truly* are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLMs provide false claims about which pieces of evidence influenced its decisions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4ub9gpx9xw"
        ],
        "venue": [
          "/venue/4ub9gpx9xw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4ub9gpx9xw"
        ],
        "detail": [
          "https://openreview.net/forum?id=4ub9gpx9xw"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 9
      },
      "raw_excerpt": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations [PDF 3 ] [Copy] [Kimi 9 ] [REL] Authors : Katie Matton , Robert Ness , John Guttag , Emre Kiciman Large language models (LLMs) are capable of generating *plausible* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's \"reasoning\" process, i.e., they can be *unfaithful*. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level *concepts* in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM's *explanations imply* are influential and the set that *truly* are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLMs provide false claims about which pieces of evidence influenced its decisions. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "4FVGowGzQb@OpenReview",
      "index": 306,
      "title": "Preference Optimization as Probabilistic Inference",
      "authors": [
        "Abbas Abdolmaleki",
        "Bilal Piot",
        "Jost Springenberg",
        "Bobak Shahriari",
        "Tim Hertweck",
        "Michael Bloesch",
        "Rishabh Joshi",
        "Thomas Lampe",
        "Junhyuk Oh",
        "Nicolas Heess",
        "Jonas Buchli",
        "Martin Riedmiller"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "preferred",
        "dis",
        "preference",
        "feedback",
        "optimization",
        "dayan",
        "probabilistic",
        "examples",
        "available",
        "positive"
      ],
      "summary": "Existing preference optimization methods are mainly designed for directly learning from human feedback with the assumption that paired examples (preferred vs. dis-preferred) are available. In contrast, we propose a method that can leverage unpaired preferred or dis-preferred examples by decoupling learning from positive and negative feedback, allowing control over the contribution of each, and works even when only one type of feedback (positive or negative) is available. Our approach builds upon the probabilistic framework introduced in (Dayan & Hinton,1997) , which proposes to use expectation-maximization (EM) to directly optimize the probability of preferred outcomes (as opposed to classic expected reward maximization). To obtain a practical algorithm, we identify and address a key limitation in current EM-based methods: when applied to preference optimization, they solely maximize the likelihood of preferred examples, while neglecting dis-preferred samples. We show how to extend EM algorithms to explicitly incorporate dis-preferred outcomes, leading to a novel, theoretically grounded, preference optimization algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback. We evaluate our approach for training language models based on human feedback as well as training policies for sequential decision-making problems, where learned (value) functions are available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4FVGowGzQb"
        ],
        "venue": [
          "/venue/4FVGowGzQb@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4FVGowGzQb"
        ],
        "detail": [
          "https://openreview.net/forum?id=4FVGowGzQb"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Preference Optimization as Probabilistic Inference [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Abbas Abdolmaleki , Bilal Piot , Jost Springenberg , Bobak Shahriari , Tim Hertweck , Michael Bloesch , Rishabh Joshi , Thomas Lampe , Junhyuk Oh , Nicolas Heess , Jonas Buchli , Martin Riedmiller Existing preference optimization methods are mainly designed for directly learning from human feedback with the assumption that paired examples (preferred vs. dis-preferred) are available. In contrast, we propose a method that can leverage unpaired preferred or dis-preferred examples by decoupling learning from positive and negative feedback, allowing control over the contribution of each, and works even when only one type of feedback (positive or negative) is available. Our approach builds upon the probabilistic framework introduced in (Dayan & Hinton,1997) , which proposes to use expectation-maximization (EM) to directly optimize the probability of preferred outcomes (as opposed to classic expected reward maximization). To obtain a practical algorithm, we identify and address a key limitation in current EM-based methods: when applied to preference optimization, they solely maximize the likelihood of preferred examples, while neglecting dis-preferred samples. We show how to extend EM algorithms to explicitly incorporate dis-preferred outcomes, leading to a novel, theoretically grounded, preference optimization algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback. We evaluate our approach for training language models based on human feedback as well as training policies for sequential decision-making problems, where learned (value) functions are available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "48WAZhwHHw@OpenReview",
      "index": 307,
      "title": "Planning in Natural Language Improves LLM Search for Code Generation",
      "authors": [
        "Evan Wang",
        "Federico Cassano",
        "Catherine Wu",
        "Yunfeng Bai",
        "William Song",
        "Vaskar Nath",
        "Ziwen Han",
        "Sean Hendryx",
        "Summer Yue",
        "Hugh Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "plansearch",
        "search",
        "livecodebench",
        "pass",
        "plans",
        "language",
        "llm",
        "diverse",
        "natural",
        "gains"
      ],
      "summary": "While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a pass@200 of 77.0% on LiveCodeBench, outperforming both the best pass-rate achieved without any search (pass@1 = 41.4%) and using standard repeated sampling on top of existing non-search models (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains from search as a function of the diversity over generated ideas.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=48WAZhwHHw"
        ],
        "venue": [
          "/venue/48WAZhwHHw@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=48WAZhwHHw"
        ],
        "detail": [
          "https://openreview.net/forum?id=48WAZhwHHw"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 7
      },
      "raw_excerpt": "Planning in Natural Language Improves LLM Search for Code Generation [PDF 2 ] [Copy] [Kimi 7 ] [REL] Authors : Evan Wang , Federico Cassano , Catherine Wu , Yunfeng Bai , William Song , Vaskar Nath , Ziwen Han , Sean Hendryx , Summer Yue , Hugh Zhang While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a pass@200 of 77.0% on LiveCodeBench, outperforming both the best pass-rate achieved without any search (pass@1 = 41.4%) and using standard repeated sampling on top of existing non-search models (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains from search as a function of the diversity over generated ideas. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "3fl1SENSYO@OpenReview",
      "index": 308,
      "title": "Unleashing the Potential of Diffusion Models for Incomplete Data Imputation",
      "authors": [
        "Hengrui Zhang",
        "Liancheng Fang",
        "Qitian Wu",
        "Philip Yu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "diffputer",
        "imputation",
        "missing",
        "diffusion",
        "data",
        "step",
        "generative",
        "unleashing",
        "tailored",
        "sampling"
      ],
      "summary": "Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the training data and 2) the difficulty in performing conditional inference from unconditional generative models. To deal with these challenges, this paper introduces DiffPuter, a tailored diffusion model combined with the Expectation-Maximization (EM) algorithm for missing data imputation. DiffPuter iteratively trains a diffusion model to learn the joint distribution of missing and observed data and performs an accurate conditional sampling to update the missing values using a tailored reversed sampling strategy. Our theoretical analysis shows that DiffPuter's training step corresponds to the maximum likelihood estimation of data density (M-step), and its sampling step represents the Expected A Posteriori estimation of missing values (E-step). Extensive experiments across ten diverse datasets and comparisons with 17 different imputation methods demonstrate DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 8.10\\% in MAE and 5.64\\% in RMSE compared to the most competitive existing method.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3fl1SENSYO"
        ],
        "venue": [
          "/venue/3fl1SENSYO@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3fl1SENSYO"
        ],
        "detail": [
          "https://openreview.net/forum?id=3fl1SENSYO"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "Unleashing the Potential of Diffusion Models for Incomplete Data Imputation [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Hengrui Zhang , Liancheng Fang , Qitian Wu , Philip Yu Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the training data and 2) the difficulty in performing conditional inference from unconditional generative models. To deal with these challenges, this paper introduces DiffPuter, a tailored diffusion model combined with the Expectation-Maximization (EM) algorithm for missing data imputation. DiffPuter iteratively trains a diffusion model to learn the joint distribution of missing and observed data and performs an accurate conditional sampling to update the missing values using a tailored reversed sampling strategy. Our theoretical analysis shows that DiffPuter's training step corresponds to the maximum likelihood estimation of data density (M-step), and its sampling step represents the Expected A Posteriori estimation of missing values (E-step). Extensive experiments across ten diverse datasets and comparisons with 17 different imputation methods demonstrate DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 8.10\\% in MAE and 5.64\\% in RMSE compared to the most competitive existing method. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "2pNLknCTvG@OpenReview",
      "index": 309,
      "title": "uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs",
      "authors": [
        "Yu Chen",
        "Yan Dai",
        "Jiatai Huang",
        "Longbo Huang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "uniinf",
        "bobw",
        "tailed",
        "heavy",
        "adversarial",
        "mab",
        "worlds",
        "algorithm",
        "mabs",
        "stochastic"
      ],
      "summary": "In this paper, we present a novel algorithm, `uniINF`, for the Heavy-Tailed Multi-Armed Bandits (HTMAB) problem, demonstrating robustness and adaptability in both stochastic and adversarial environments. Unlike the stochastic MAB setting where loss distributions are stationary with time, our study extends to the adversarial setup, where losses are generated from heavy-tailed distributions that depend on both arms and time. Our novel algorithm `uniINF` enjoys the so-called Best-of-Both-Worlds (BoBW) property, performing optimally in both stochastic and adversarial environments *without* knowing the exact environment type. Moreover, our algorithm also possesses a Parameter-Free feature, *i.e.*, it operates *without* the need of knowing the heavy-tail parameters ( σ , α ) ( σ , α ) a-priori.To be precise, `uniINF` ensures nearly-optimal regret in both stochastic and adversarial environments, matching the corresponding lower bounds when ( σ , α ) ( σ , α ) is known (up to logarithmic factors). To our knowledge, `uniINF` is the first parameter-free algorithm to achieve the BoBW property for the heavy-tailed MAB problem. Technically, we develop innovative techniques to achieve BoBW guarantees for Parameter-Free HTMABs, including a refined analysis for the dynamics of log-barrier, an auto-balancing learning rate scheduling scheme, an adaptive skipping-clipping loss tuning technique, and a stopping-time analysis for logarithmic regret.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2pNLknCTvG"
        ],
        "venue": [
          "/venue/2pNLknCTvG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2pNLknCTvG"
        ],
        "detail": [
          "https://openreview.net/forum?id=2pNLknCTvG"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Yu Chen , Yan Dai , Jiatai Huang , Longbo Huang In this paper, we present a novel algorithm, `uniINF`, for the Heavy-Tailed Multi-Armed Bandits (HTMAB) problem, demonstrating robustness and adaptability in both stochastic and adversarial environments. Unlike the stochastic MAB setting where loss distributions are stationary with time, our study extends to the adversarial setup, where losses are generated from heavy-tailed distributions that depend on both arms and time. Our novel algorithm `uniINF` enjoys the so-called Best-of-Both-Worlds (BoBW) property, performing optimally in both stochastic and adversarial environments *without* knowing the exact environment type. Moreover, our algorithm also possesses a Parameter-Free feature, *i.e.*, it operates *without* the need of knowing the heavy-tail parameters ( σ , α ) ( σ , α ) a-priori.To be precise, `uniINF` ensures nearly-optimal regret in both stochastic and adversarial environments, matching the corresponding lower bounds when ( σ , α ) ( σ , α ) is known (up to logarithmic factors). To our knowledge, `uniINF` is the first parameter-free algorithm to achieve the BoBW property for the heavy-tailed MAB problem. Technically, we develop innovative techniques to achieve BoBW guarantees for Parameter-Free HTMABs, including a refined analysis for the dynamics of log-barrier, an auto-balancing learning rate scheduling scheme, an adaptive skipping-clipping loss tuning technique, and a stopping-time analysis for logarithmic regret. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "1xzqz73hvL@OpenReview",
      "index": 310,
      "title": "High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws",
      "authors": [
        "Muhammed Ildiz",
        "Halil Gozeten",
        "Ege Taga",
        "Marco Mondelli",
        "Samet Oymak"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "surrogate",
        "w2s",
        "ridgeless",
        "weak",
        "model",
        "distillation",
        "strong",
        "knowledge",
        "generalization",
        "scaling"
      ],
      "summary": "A growing number of machine learning scenarios rely on knowledge distillation where one uses the output of a surrogate model as labels to supervise the training of a target model. In this work, we provide a sharp characterization of this process for ridgeless, high-dimensional regression, under two settings: *(i)* model shift, where the surrogate model is arbitrary, and *(ii)* distribution shift, where the surrogate model is the solution of empirical risk minimization with out-of-distribution data. In both cases, we characterize the precise risk of the target model through non-asymptotic bounds in terms of sample size and data distribution under mild conditions. As a consequence, we identify the form of the optimal surrogate model, which reveals the benefits and limitations of discarding weak features in a data-dependent fashion. In the context of weak-to-strong (W2S) generalization, this has the interpretation that *(i)* W2S training, with the surrogate as the weak model, can provably outperform training with strong labels under the same data budget, but *(ii)* it is unable to improve the data scaling law. We validate our results on numerical experiments both on ridgeless regression and on neural network architectures.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1xzqz73hvL"
        ],
        "venue": [
          "/venue/1xzqz73hvL@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1xzqz73hvL"
        ],
        "detail": [
          "https://openreview.net/forum?id=1xzqz73hvL"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Muhammed Ildiz , Halil Gozeten , Ege Taga , Marco Mondelli , Samet Oymak A growing number of machine learning scenarios rely on knowledge distillation where one uses the output of a surrogate model as labels to supervise the training of a target model. In this work, we provide a sharp characterization of this process for ridgeless, high-dimensional regression, under two settings: *(i)* model shift, where the surrogate model is arbitrary, and *(ii)* distribution shift, where the surrogate model is the solution of empirical risk minimization with out-of-distribution data. In both cases, we characterize the precise risk of the target model through non-asymptotic bounds in terms of sample size and data distribution under mild conditions. As a consequence, we identify the form of the optimal surrogate model, which reveals the benefits and limitations of discarding weak features in a data-dependent fashion. In the context of weak-to-strong (W2S) generalization, this has the interpretation that *(i)* W2S training, with the surrogate as the weak model, can provably outperform training with strong labels under the same data budget, but *(ii)* it is unable to improve the data scaling law. We validate our results on numerical experiments both on ridgeless regression and on neural network architectures. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "1Iuw1jcIrf@OpenReview",
      "index": 311,
      "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
      "authors": [
        "Zimu Lu",
        "Aojun Zhou",
        "Ke Wang",
        "Houxing Ren",
        "Weikang Shi",
        "Junting Pan",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mathematical",
        "reasoning",
        "pretraining",
        "code",
        "mathcoder2",
        "continued",
        "math",
        "steps",
        "expressions",
        "mathcode"
      ],
      "summary": "Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1Iuw1jcIrf"
        ],
        "venue": [
          "/venue/1Iuw1jcIrf@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1Iuw1jcIrf"
        ],
        "detail": [
          "https://openreview.net/forum?id=1Iuw1jcIrf"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 7
      },
      "raw_excerpt": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code [PDF 6 ] [Copy] [Kimi 7 ] [REL] Authors : Zimu Lu , Aojun Zhou , Ke Wang , Houxing Ren , Weikang Shi , Junting Pan , Mingjie Zhan , Hongsheng Li Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "0yvZm2AjUr@OpenReview",
      "index": 312,
      "title": "Monitoring Latent World States in Language Models with Propositional Probes",
      "authors": [
        "Jiahai Feng",
        "Stuart Russell",
        "Jacob Steinhardt"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "greg",
        "lms",
        "propositions",
        "nurse",
        "worksas",
        "propositional",
        "physicist",
        "unfaithfully",
        "laura",
        "probes"
      ],
      "summary": "Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input contexts in a latent world model, and we seek to extract these latent world states as logical propositions. For example, given the input context ``Greg is a nurse. Laura is a physicist.'', we aim to decode the propositions WorksAs(Greg, nurse) and WorksAs(Laura, physicist) from the model's internal activations. To do so we introduce _propositional probes_, which compositionally extract lexical concepts from token activations and bind them into propositions. Key to this is identifying a _binding subspace_ in which bound tokens have high similarity (Greg ↔ ↔ nurse) but unbound ones do not (Greg ↮ ↮ physicist). Despite only being trained on linguistically simple English templates, we find that propositional probes generalize to inputs written as short stories and translated to Spanish. Moreover, in three settings where LMs respond unfaithfully to the input context---prompt injections, backdoor attacks, and gender bias--- the decoded propositions remain faithful. This suggests that LMs often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0yvZm2AjUr"
        ],
        "venue": [
          "/venue/0yvZm2AjUr@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0yvZm2AjUr"
        ],
        "detail": [
          "https://openreview.net/forum?id=0yvZm2AjUr"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Monitoring Latent World States in Language Models with Propositional Probes [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Jiahai Feng , Stuart Russell , Jacob Steinhardt Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input contexts in a latent world model, and we seek to extract these latent world states as logical propositions. For example, given the input context ``Greg is a nurse. Laura is a physicist.'', we aim to decode the propositions WorksAs(Greg, nurse) and WorksAs(Laura, physicist) from the model's internal activations. To do so we introduce _propositional probes_, which compositionally extract lexical concepts from token activations and bind them into propositions. Key to this is identifying a _binding subspace_ in which bound tokens have high similarity (Greg ↔ ↔ nurse) but unbound ones do not (Greg ↮ ↮ physicist). Despite only being trained on linguistically simple English templates, we find that propositional probes generalize to inputs written as short stories and translated to Spanish. Moreover, in three settings where LMs respond unfaithfully to the input context---prompt injections, backdoor attacks, and gender bias--- the decoded propositions remain faithful. This suggests that LMs often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "yfW1x7uBS5@OpenReview",
      "index": 313,
      "title": "Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI",
      "authors": [
        "Robert Hönig",
        "Javier Rando",
        "Nicholas Carlini",
        "Florian Tramer"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "artists",
        "mimicry",
        "protections",
        "adversarial",
        "perturbations",
        "protect",
        "reliably",
        "generative",
        "style",
        "cannot"
      ],
      "summary": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that **all existing protections can be easily bypassed**, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=yfW1x7uBS5"
        ],
        "venue": [
          "/venue/yfW1x7uBS5@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=yfW1x7uBS5"
        ],
        "detail": [
          "https://openreview.net/forum?id=yfW1x7uBS5"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Robert Hönig , Javier Rando , Nicholas Carlini , Florian Tramer Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that **all existing protections can be easily bypassed**, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "xsELpEPn4A@OpenReview",
      "index": 314,
      "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
      "authors": [
        "Lianghui Zhu",
        "Xinggang Wang",
        "Xinlong Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "judgelm",
        "judges",
        "judge",
        "llms",
        "fine",
        "bias",
        "ended",
        "pandalm",
        "scalable",
        "answers"
      ],
      "summary": "Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xsELpEPn4A"
        ],
        "venue": [
          "/venue/xsELpEPn4A@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xsELpEPn4A"
        ],
        "detail": [
          "https://openreview.net/forum?id=xsELpEPn4A"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 6
      },
      "raw_excerpt": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges [PDF 4 ] [Copy] [Kimi 6 ] [REL] Authors : Lianghui Zhu , Xinggang Wang , Xinlong Wang Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "xQBRrtQM8u@OpenReview",
      "index": 315,
      "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control",
      "authors": [
        "Carles Domingo i Enrich",
        "Michal Drozdzal",
        "Brian Karrer",
        "Ricky T. Q. Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "tuning",
        "reward",
        "fine",
        "soc",
        "memoryless",
        "matching",
        "adjoint",
        "generative",
        "models",
        "flow"
      ],
      "summary": "Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific *memoryless* noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named *Adjoint Matching* which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=xQBRrtQM8u"
        ],
        "venue": [
          "/venue/xQBRrtQM8u@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=xQBRrtQM8u"
        ],
        "detail": [
          "https://openreview.net/forum?id=xQBRrtQM8u"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Carles Domingo i Enrich , Michal Drozdzal , Brian Karrer , Ricky T. Q. Chen Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific *memoryless* noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named *Adjoint Matching* which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "uTqnyF0JNR@OpenReview",
      "index": 316,
      "title": "IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning",
      "authors": [
        "Jiawen Qin",
        "Haonan Yuan",
        "Qingyun Sun",
        "Lyujin Xu",
        "Jiaqi Yuan",
        "Pengfeng Huang",
        "Zhaonan Wang",
        "Xingcheng Fu",
        "Hao Peng",
        "Jianxin Li",
        "Philip Yu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "igl",
        "imbalanced",
        "graph",
        "bench",
        "algorithms",
        "imbalance",
        "comprehensive",
        "learning",
        "disproportionally",
        "embarking"
      ],
      "summary": "Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. To address this challenge, Imbalanced Graph Learning (IGL) has garnered substantial attention, enabling more balanced data distributions and better task performance. Despite the proliferation of IGL algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to comprehending advancements in this field. To bridge this gap, we introduce **IGL-Bench**, a foundational comprehensive benchmark for imbalanced graph learning, embarking on **17** diverse graph datasets and **24** distinct IGL algorithms with uniform data processing and splitting strategies. Specifically, IGL-Bench systematically investigates state-of-the-art IGL algorithms in terms of **effectiveness**, **robustness**, and **efficiency** on node-level and graph-level tasks, with the scope of class-imbalance and topology-imbalance. Extensive experiments demonstrate the potential benefits of IGL algorithms on various imbalanced conditions, offering insights and opportunities in the IGL field. Further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, which is available at https://anonymous.4open.science/r/IGL-Bench.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uTqnyF0JNR"
        ],
        "venue": [
          "/venue/uTqnyF0JNR@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uTqnyF0JNR"
        ],
        "detail": [
          "https://openreview.net/forum?id=uTqnyF0JNR"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Jiawen Qin , Haonan Yuan , Qingyun Sun , Lyujin Xu , Jiaqi Yuan , Pengfeng Huang , Zhaonan Wang , Xingcheng Fu , Hao Peng , Jianxin Li , Philip Yu Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. To address this challenge, Imbalanced Graph Learning (IGL) has garnered substantial attention, enabling more balanced data distributions and better task performance. Despite the proliferation of IGL algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to comprehending advancements in this field. To bridge this gap, we introduce **IGL-Bench**, a foundational comprehensive benchmark for imbalanced graph learning, embarking on **17** diverse graph datasets and **24** distinct IGL algorithms with uniform data processing and splitting strategies. Specifically, IGL-Bench systematically investigates state-of-the-art IGL algorithms in terms of **effectiveness**, **robustness**, and **efficiency** on node-level and graph-level tasks, with the scope of class-imbalance and topology-imbalance. Extensive experiments demonstrate the potential benefits of IGL algorithms on various imbalanced conditions, offering insights and opportunities in the IGL field. Further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, which is available at https://anonymous.4open.science/r/IGL-Bench. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "vQhn4wrQ6j@OpenReview",
      "index": 317,
      "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
      "authors": [
        "Lucas Bandarkar",
        "Benjamin Muller",
        "Pritish Yuvraj",
        "Rui Hou",
        "Nayan Singhal",
        "Hongjiang Lv",
        "Bing Liu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "math",
        "language",
        "lingual",
        "swapping",
        "transfer",
        "merging",
        "instruction",
        "expert",
        "languages",
        "cross"
      ],
      "summary": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=vQhn4wrQ6j"
        ],
        "venue": [
          "/venue/vQhn4wrQ6j@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=vQhn4wrQ6j"
        ],
        "detail": [
          "https://openreview.net/forum?id=vQhn4wrQ6j"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Lucas Bandarkar , Benjamin Muller , Pritish Yuvraj , Rui Hou , Nayan Singhal , Hongjiang Lv , Bing Liu Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "m9wG6ai2Xk@OpenReview",
      "index": 318,
      "title": "MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations",
      "authors": [
        "Shaochen Zhong",
        "Yifan (Louie) Lu",
        "Lize Shao",
        "Bhargav Bhushanam",
        "Xiaocong Du",
        "Yixin Wan",
        "Yucheng Shi",
        "Daochen Zha",
        "Yiwei Wang",
        "Ninghao Liu",
        "Kaixiong Zhou",
        "shuai xu",
        "Kai-Wei Chang",
        "Louis Feng",
        "Vipin Chaudhary",
        "Xia Hu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mquake",
        "editing",
        "knowledge",
        "messi",
        "remastered",
        "club",
        "hop",
        "questions",
        "erroneous",
        "fix"
      ],
      "summary": "Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, knowledge editing often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., \"what club does Lionel Messi currently play for?\").However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (e.g., \"who is the offspring of the owner of the club that Messi currently plays for?\"). Prior arts have coined this task as multi-hop knowledge editing with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale.In this work, we reveal that **up to 33% or 76% of MQuAKE's questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights.** Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed \\mquake{}-evaluated editing methods on our post-fix dataset, \\mquaker{}. It is our observation that many methods try to overfit the original \\mquake{} by exploiting some data-specific properties of \\mquake{}. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to the supplemental material for assets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=m9wG6ai2Xk"
        ],
        "venue": [
          "/venue/m9wG6ai2Xk@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=m9wG6ai2Xk"
        ],
        "detail": [
          "https://openreview.net/forum?id=m9wG6ai2Xk"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Shaochen Zhong , Yifan (Louie) Lu , Lize Shao , Bhargav Bhushanam , Xiaocong Du , Yixin Wan , Yucheng Shi , Daochen Zha , Yiwei Wang , Ninghao Liu , Kaixiong Zhou , shuai xu , Kai-Wei Chang , Louis Feng , Vipin Chaudhary , Xia Hu Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, knowledge editing often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., \"what club does Lionel Messi currently play for?\").However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (e.g., \"who is the offspring of the owner of the club that Messi currently plays for?\"). Prior arts have coined this task as multi-hop knowledge editing with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale.In this work, we reveal that **up to 33% or 76% of MQuAKE's questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights.** Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed \\mquake{}-evaluated editing methods on our post-fix dataset, \\mquaker{}. It is our observation that many methods try to overfit the original \\mquake{} by exploiting some data-specific properties of \\mquake{}. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to the supplemental material for assets. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "mkDam1xIzW@OpenReview",
      "index": 319,
      "title": "Probabilistic Geometric Principal Component Analysis with application to neural data",
      "authors": [
        "Han-Lin Hsieh",
        "Maryam Shanechi"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pgpca",
        "ppca",
        "manifold",
        "data",
        "dimensionality",
        "coordinate",
        "around",
        "probabilistic",
        "reduction",
        "euclidean"
      ],
      "summary": "Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system around the mean of data. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space around the mean. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold, especially for neural data.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=mkDam1xIzW"
        ],
        "venue": [
          "/venue/mkDam1xIzW@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=mkDam1xIzW"
        ],
        "detail": [
          "https://openreview.net/forum?id=mkDam1xIzW"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Probabilistic Geometric Principal Component Analysis with application to neural data [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Han-Lin Hsieh , Maryam Shanechi Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system around the mean of data. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space around the mean. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold, especially for neural data. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "l6QnSQizmN@OpenReview",
      "index": 320,
      "title": "Online Reinforcement Learning in Non-Stationary Context-Driven Environments",
      "authors": [
        "Pouya Hamadanian",
        "Arash Nasr-Esfahany",
        "Malte Schwarzkopf",
        "Siddhartha Sen",
        "Mohammad Alizadeh"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lcpo",
        "experiences",
        "online",
        "environments",
        "context",
        "policy",
        "stationary",
        "anchoring",
        "reinforcement",
        "traces"
      ],
      "summary": "We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to ``catastrophic forgetting'' (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics or use off-policy methods that suffer from instability and poor performance.We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a ``prescient'' agent trained offline across all context traces.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=l6QnSQizmN"
        ],
        "venue": [
          "/venue/l6QnSQizmN@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=l6QnSQizmN"
        ],
        "detail": [
          "https://openreview.net/forum?id=l6QnSQizmN"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 3
      },
      "raw_excerpt": "Online Reinforcement Learning in Non-Stationary Context-Driven Environments [PDF 3 ] [Copy] [Kimi 3 ] [REL] Authors : Pouya Hamadanian , Arash Nasr-Esfahany , Malte Schwarzkopf , Siddhartha Sen , Mohammad Alizadeh We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to ``catastrophic forgetting'' (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics or use off-policy methods that suffer from instability and poor performance.We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a ``prescient'' agent trained offline across all context traces. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ZV7CLf0RHK@OpenReview",
      "index": 321,
      "title": "Fine-tuning with Reserved Majority for Noise Reduction",
      "authors": [
        "Shuyang Jiang",
        "Yusheng Liao",
        "Yanfeng Wang",
        "Ya Zhang",
        "Yu Wang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lora",
        "textbf",
        "tuning",
        "fine",
        "preft",
        "redundancies",
        "majority",
        "ranks",
        "reserved",
        "arameter"
      ],
      "summary": "Parameter-efficient fine-tuning (PEFT) has revolutionized supervised fine-tuning, where LoRA and its variants gain the most popularity due to their low training costs and zero inference latency.However, LoRA tuning not only injects knowledgeable features but also noisy hallucination during fine-tuning, which hinders the utilization of tunable parameters with the increasing LoRA rank.In this work, we first investigate in-depth the redundancies among LoRA parameters with substantial empirical studies.Aiming to resemble the learning capacity of high ranks from the findings, we set up a new fine-tuning framework, \\textbf{P}arameter-\\textbf{Re}dundant \\textbf{F}ine-\\textbf{T}uning (\\preft), which follows the vanilla LoRA tuning process but is required to reduce redundancies before merging LoRA parameters back to pre-trained models.Based on this framework, we propose \\textbf{No}ise reduction with \\textbf{R}eserved \\textbf{M}ajority (\\norm), which decomposes the LoRA parameters into majority parts and redundant parts with random singular value decomposition.The major components are determined by the proposed \\search method, specifically employing subspace similarity to confirm the parameter groups that share the highest similarity with the base weight.By employing \\norm, we enhance both the learning capacity and benefits from larger ranks, which consistently outperforms both LoRA and other \\preft-based methods on various downstream tasks, such as general instruction tuning, math reasoning and code generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZV7CLf0RHK"
        ],
        "venue": [
          "/venue/ZV7CLf0RHK@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZV7CLf0RHK"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZV7CLf0RHK"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 7
      },
      "raw_excerpt": "Fine-tuning with Reserved Majority for Noise Reduction [PDF 2 ] [Copy] [Kimi 7 ] [REL] Authors : Shuyang Jiang , Yusheng Liao , Yanfeng Wang , Ya Zhang , Yu Wang Parameter-efficient fine-tuning (PEFT) has revolutionized supervised fine-tuning, where LoRA and its variants gain the most popularity due to their low training costs and zero inference latency.However, LoRA tuning not only injects knowledgeable features but also noisy hallucination during fine-tuning, which hinders the utilization of tunable parameters with the increasing LoRA rank.In this work, we first investigate in-depth the redundancies among LoRA parameters with substantial empirical studies.Aiming to resemble the learning capacity of high ranks from the findings, we set up a new fine-tuning framework, \\textbf{P}arameter-\\textbf{Re}dundant \\textbf{F}ine-\\textbf{T}uning (\\preft), which follows the vanilla LoRA tuning process but is required to reduce redundancies before merging LoRA parameters back to pre-trained models.Based on this framework, we propose \\textbf{No}ise reduction with \\textbf{R}eserved \\textbf{M}ajority (\\norm), which decomposes the LoRA parameters into majority parts and redundant parts with random singular value decomposition.The major components are determined by the proposed \\search method, specifically employing subspace similarity to confirm the parameter groups that share the highest similarity with the base weight.By employing \\norm, we enhance both the learning capacity and benefits from larger ranks, which consistently outperforms both LoRA and other \\preft-based methods on various downstream tasks, such as general instruction tuning, math reasoning and code generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "h8yg0hT96f@OpenReview",
      "index": 322,
      "title": "Bayesian Experimental Design Via Contrastive Diffusions",
      "authors": [
        "Jacopo Iollo",
        "Christophe Heinkelé",
        "Pierre Alliez",
        "Florence Forbes"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "boed",
        "eig",
        "maximization",
        "posterior",
        "expected",
        "bayesian",
        "diffusions",
        "optimization",
        "design",
        "gain"
      ],
      "summary": "Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments.When based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected *contrast* between prior and posterior distributions.Scaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity.In this work, we introduce an *expected posterior* distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the expected posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. By incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=h8yg0hT96f"
        ],
        "venue": [
          "/venue/h8yg0hT96f@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=h8yg0hT96f"
        ],
        "detail": [
          "https://openreview.net/forum?id=h8yg0hT96f"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Bayesian Experimental Design Via Contrastive Diffusions [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Jacopo Iollo , Christophe Heinkelé , Pierre Alliez , Florence Forbes Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments.When based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected *contrast* between prior and posterior distributions.Scaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity.In this work, we introduce an *expected posterior* distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the expected posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. By incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "hwnObmOTrV@OpenReview",
      "index": 323,
      "title": "Modeling Complex System Dynamics with Flow Matching Across Time and Conditions",
      "authors": [
        "Martin Rohbeck",
        "Charlotte Bunne",
        "Edward De Brouwer",
        "Jan-Christian Huetter",
        "Anne Biton",
        "Kelvin Chen",
        "Aviv Regev",
        "Romain Lopez"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mmfm",
        "matching",
        "flow",
        "points",
        "across",
        "time",
        "world",
        "dynamics",
        "conditions",
        "modeling"
      ],
      "summary": "Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=hwnObmOTrV"
        ],
        "venue": [
          "/venue/hwnObmOTrV@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=hwnObmOTrV"
        ],
        "detail": [
          "https://openreview.net/forum?id=hwnObmOTrV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Modeling Complex System Dynamics with Flow Matching Across Time and Conditions [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Martin Rohbeck , Charlotte Bunne , Edward De Brouwer , Jan-Christian Huetter , Anne Biton , Kelvin Chen , Aviv Regev , Romain Lopez Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "NdHka08uWn@OpenReview",
      "index": 324,
      "title": "RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression",
      "authors": [
        "Hengzhe Zhang",
        "Qi Chen",
        "Bing XUE",
        "Wolfgang Banzhaf",
        "Mengjie Zhang"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "symbolic",
        "regression",
        "retrieval",
        "augmented",
        "rag",
        "pre",
        "neural",
        "generation",
        "expressions",
        "trained"
      ],
      "summary": "Symbolic regression is a key task in machine learning, aiming to discover mathematical expressions that best describe a dataset. While deep learning has increased interest in using neural networks for symbolic regression, many existing approaches rely on pre-trained models. These models require significant computational resources and struggle with regression tasks involving unseen functions and variables. A pre-training-free paradigm is needed to better integrate with search-based symbolic regression algorithms. To address these limitations, we propose a novel framework for symbolic regression that integrates evolutionary feature construction with a neural network, without the need for pre-training. Our approach adaptively generates symbolic trees that align with the desired semantics in real-time using a language model trained via online supervised learning, providing effective building blocks for feature construction. To mitigate hallucinations from the language model, we design a retrieval-augmented generation mechanism that explicitly leverages searched symbolic expressions. Additionally, we introduce a scale-invariant data augmentation technique that further improves the robustness and generalization of the model. Experimental results demonstrate that our framework achieves state-of-the-art accuracy across 25 regression algorithms and 120 regression tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NdHka08uWn"
        ],
        "venue": [
          "/venue/NdHka08uWn@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NdHka08uWn"
        ],
        "detail": [
          "https://openreview.net/forum?id=NdHka08uWn"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 5
      },
      "raw_excerpt": "RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression [PDF 7 ] [Copy] [Kimi 5 ] [REL] Authors : Hengzhe Zhang , Qi Chen , Bing XUE , Wolfgang Banzhaf , Mengjie Zhang Symbolic regression is a key task in machine learning, aiming to discover mathematical expressions that best describe a dataset. While deep learning has increased interest in using neural networks for symbolic regression, many existing approaches rely on pre-trained models. These models require significant computational resources and struggle with regression tasks involving unseen functions and variables. A pre-training-free paradigm is needed to better integrate with search-based symbolic regression algorithms. To address these limitations, we propose a novel framework for symbolic regression that integrates evolutionary feature construction with a neural network, without the need for pre-training. Our approach adaptively generates symbolic trees that align with the desired semantics in real-time using a language model trained via online supervised learning, providing effective building blocks for feature construction. To mitigate hallucinations from the language model, we design a retrieval-augmented generation mechanism that explicitly leverages searched symbolic expressions. Additionally, we introduce a scale-invariant data augmentation technique that further improves the robustness and generalization of the model. Experimental results demonstrate that our framework achieves state-of-the-art accuracy across 25 regression algorithms and 120 regression tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "WKfb1xGXGx@OpenReview",
      "index": 325,
      "title": "Perm: A Parametric Representation for Multi-Style 3D Hair Modeling",
      "authors": [
        "Chengan He",
        "Xin Sun",
        "Zhixin Shu",
        "Fujun Luan",
        "Soeren Pirk",
        "Jorge Alejandro Amador Herrera",
        "Dominik L Michels",
        "Tuanfeng Wang",
        "Meng Zhang",
        "Holly Rushmeier",
        "Yi Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hair",
        "perm",
        "representation",
        "strand",
        "editing",
        "textures",
        "parametric",
        "hairstyle",
        "grooming",
        "style"
      ],
      "summary": "We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of \\textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WKfb1xGXGx"
        ],
        "venue": [
          "/venue/WKfb1xGXGx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WKfb1xGXGx"
        ],
        "detail": [
          "https://openreview.net/forum?id=WKfb1xGXGx"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Perm: A Parametric Representation for Multi-Style 3D Hair Modeling [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Chengan He , Xin Sun , Zhixin Shu , Fujun Luan , Soeren Pirk , Jorge Alejandro Amador Herrera , Dominik L Michels , Tuanfeng Wang , Meng Zhang , Holly Rushmeier , Yi Zhou We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of \\textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "2iCIHgE8KG@OpenReview",
      "index": 326,
      "title": "Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA",
      "authors": [
        "Changmin Yu",
        "Maneesh Sahani",
        "Máté Lengyel"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "gpfa",
        "infinite",
        "latent",
        "factors",
        "compositional",
        "process",
        "neural",
        "manifolds",
        "activities",
        "behaviourally"
      ],
      "summary": "Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through heuristic-based processes, and that all factors contribute at all times. We propose the infinite GPFA model, a fully Bayesian non-parametric extension of the classical GPFA by incorporating an Indian Buffet Process (IBP) prior over the factor loading process, such that it is possible to infer a potentially infinite set of latent factors, and the identity of those factors that contribute to neural firings in a compositional manner at each time point. Learning and inference in the infinite GPFA model is performed through variational expectation-maximisation, and we additionally propose scalable extensions based on sparse variational Gaussian Process methods. We empirically demonstrate that the infinite GPFA model correctly infers dynamically changing activations of latent factors on a synthetic dataset. By fitting the infinite GPFA model to population activities of hippocampal place cells during spatial navigation, we identify non-trivial and behaviourally meaningful dynamics in the neural encoding process.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=2iCIHgE8KG"
        ],
        "venue": [
          "/venue/2iCIHgE8KG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=2iCIHgE8KG"
        ],
        "detail": [
          "https://openreview.net/forum?id=2iCIHgE8KG"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Changmin Yu , Maneesh Sahani , Máté Lengyel Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through heuristic-based processes, and that all factors contribute at all times. We propose the infinite GPFA model, a fully Bayesian non-parametric extension of the classical GPFA by incorporating an Indian Buffet Process (IBP) prior over the factor loading process, such that it is possible to infer a potentially infinite set of latent factors, and the identity of those factors that contribute to neural firings in a compositional manner at each time point. Learning and inference in the infinite GPFA model is performed through variational expectation-maximisation, and we additionally propose scalable extensions based on sparse variational Gaussian Process methods. We empirically demonstrate that the infinite GPFA model correctly infers dynamically changing activations of latent factors on a synthetic dataset. By fitting the infinite GPFA model to population activities of hippocampal place cells during spatial navigation, we identify non-trivial and behaviourally meaningful dynamics in the neural encoding process. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "fGdF8Bq1FV@OpenReview",
      "index": 327,
      "title": "Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors",
      "authors": [
        "Milad Sefidgaran",
        "Abdellatif Zaidi",
        "Piotr Krasnowski"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "dependent",
        "vib",
        "regularizer",
        "mixture",
        "expectation",
        "bounds",
        "prior",
        "generalization",
        "representation",
        "gaussian"
      ],
      "summary": "We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and \"test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the \"structure'' and \"simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a date-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fGdF8Bq1FV"
        ],
        "venue": [
          "/venue/fGdF8Bq1FV@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fGdF8Bq1FV"
        ],
        "detail": [
          "https://openreview.net/forum?id=fGdF8Bq1FV"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Milad Sefidgaran , Abdellatif Zaidi , Piotr Krasnowski We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and \"test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the \"structure'' and \"simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a date-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB). Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "avSocG0oFA@OpenReview",
      "index": 328,
      "title": "Revisiting Delta-Parameter Pruning For Fine-Tuned Models",
      "authors": [
        "Wenlong Deng",
        "Yize Zhao",
        "Vala Vakilian",
        "Minghui Chen",
        "Xiaoxiao Li",
        "Christos Thrampoulidis"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pruning",
        "dpp",
        "delta",
        "dare",
        "darq",
        "fine",
        "tuned",
        "rescaling",
        "adamr",
        "parameters"
      ],
      "summary": "Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters—the differences between fine-tuned and pre-trained model weights—while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters.To address these, we develop two algorithmic improvements: (1) DARq, which modifies the rescaling factor in DARE, leading to significant performance gains at high pruning rates (e.g., >30% on COLA and SST2 for encoder models, with even larger improvements in decoder models), and (2) AdamR, an in-training modification that incorporates appropriate Delta regularization before applying DPP. We also demonstrate that DARq can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=avSocG0oFA"
        ],
        "venue": [
          "/venue/avSocG0oFA@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=avSocG0oFA"
        ],
        "detail": [
          "https://openreview.net/forum?id=avSocG0oFA"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Revisiting Delta-Parameter Pruning For Fine-Tuned Models [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Wenlong Deng , Yize Zhao , Vala Vakilian , Minghui Chen , Xiaoxiao Li , Christos Thrampoulidis Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters—the differences between fine-tuned and pre-trained model weights—while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters.To address these, we develop two algorithmic improvements: (1) DARq, which modifies the rescaling factor in DARE, leading to significant performance gains at high pruning rates (e.g., >30% on COLA and SST2 for encoder models, with even larger improvements in decoder models), and (2) AdamR, an in-training modification that incorporates appropriate Delta regularization before applying DPP. We also demonstrate that DARq can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "pQsllTesiE@OpenReview",
      "index": 329,
      "title": "Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction",
      "authors": [
        "Baiting Luo",
        "Ava Pettet",
        "Aron Laszka",
        "Abhishek Dubey",
        "Ayan Mukhopadhyay"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "stochastic",
        "action",
        "map",
        "decision",
        "environments",
        "latent",
        "actions",
        "macro",
        "learned",
        "continuous"
      ],
      "summary": "Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \\textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns.Empirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pQsllTesiE"
        ],
        "venue": [
          "/venue/pQsllTesiE@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pQsllTesiE"
        ],
        "detail": [
          "https://openreview.net/forum?id=pQsllTesiE"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Baiting Luo , Ava Pettet , Aron Laszka , Abhishek Dubey , Ayan Mukhopadhyay Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \\textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns.Empirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SuH5SdOXpe@OpenReview",
      "index": 330,
      "title": "Robustness Reprogramming for Representation Learning",
      "authors": [
        "Zhichao Hou",
        "MohamadAli Torkamani",
        "Hamid Krim",
        "Xiaorui Liu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "reprogramming",
        "robustness",
        "nrpm",
        "representation",
        "learning",
        "robust",
        "reprogrammed",
        "adversarial",
        "4open",
        "322c"
      ],
      "summary": "This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?To explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness of our approaches.This work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. Our implementation is available at https://anonymous.4open.science/r/NRPM-322C/",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SuH5SdOXpe"
        ],
        "venue": [
          "/venue/SuH5SdOXpe@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SuH5SdOXpe"
        ],
        "detail": [
          "https://openreview.net/forum?id=SuH5SdOXpe"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Robustness Reprogramming for Representation Learning [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Zhichao Hou , MohamadAli Torkamani , Hamid Krim , Xiaorui Liu This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?To explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness of our approaches.This work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. Our implementation is available at https://anonymous.4open.science/r/NRPM-322C/ Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "OZVTqoli2N@OpenReview",
      "index": 331,
      "title": "A Second-Order Perspective on Model Compositionality and Incremental Learning",
      "authors": [
        "Angelo Porrello",
        "Lorenzo Bonicelli",
        "Pietro Buzzega",
        "Monica Millunzi",
        "Simone Calderara",
        "Rita Cucchiara"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "compositionality",
        "incremental",
        "modules",
        "perspective",
        "composed",
        "demystify",
        "model",
        "staying",
        "composable",
        "pre"
      ],
      "summary": "The fine-tuning of deep pre-trained models has revealed compositional properties, with multiple specialized modules that can be arbitrarily composed into a single, multi-task model. However, identifying the conditions that promote compositionality remains an open issue, with recent efforts concentrating mainly on linearized networks. We conduct a theoretical study that attempts to demystify compositionality in standard non-linear networks through the second-order Taylor approximation of the loss function. The proposed formulation highlights the importance of staying within the pre-training basin to achieve composable modules. Moreover, it provides the basis for two dual incremental training algorithms: the one from the perspective of multiple models trained individually, while the other aims to optimize the composed model as a whole. We probe their application in incremental classification tasks and highlight some valuable skills. In fact, the pool of incrementally learned modules not only supports the creation of an effective multi-task model but also enables unlearning and specialization in certain tasks.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OZVTqoli2N"
        ],
        "venue": [
          "/venue/OZVTqoli2N@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OZVTqoli2N"
        ],
        "detail": [
          "https://openreview.net/forum?id=OZVTqoli2N"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "A Second-Order Perspective on Model Compositionality and Incremental Learning [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Angelo Porrello , Lorenzo Bonicelli , Pietro Buzzega , Monica Millunzi , Simone Calderara , Rita Cucchiara The fine-tuning of deep pre-trained models has revealed compositional properties, with multiple specialized modules that can be arbitrarily composed into a single, multi-task model. However, identifying the conditions that promote compositionality remains an open issue, with recent efforts concentrating mainly on linearized networks. We conduct a theoretical study that attempts to demystify compositionality in standard non-linear networks through the second-order Taylor approximation of the loss function. The proposed formulation highlights the importance of staying within the pre-training basin to achieve composable modules. Moreover, it provides the basis for two dual incremental training algorithms: the one from the perspective of multiple models trained individually, while the other aims to optimize the composed model as a whole. We probe their application in incremental classification tasks and highlight some valuable skills. In fact, the pool of incrementally learned modules not only supports the creation of an effective multi-task model but also enables unlearning and specialization in certain tasks. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "kFsWpSxkFz@OpenReview",
      "index": 332,
      "title": "MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility",
      "authors": [
        "Wayne Wu",
        "Honglin He",
        "Jack He",
        "Yiran Wang",
        "Chenda Duan",
        "Zhizheng Liu",
        "Quanyi Li",
        "Bolei Zhou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "metaurban",
        "micromobility",
        "urban",
        "embodied",
        "mobile",
        "compositional",
        "pedestrians",
        "public",
        "generalizability",
        "streetscapes"
      ],
      "summary": "Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. **Micromobility** enabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we present **MetaUrban**, a *compositional* simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an *infinite* number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents’ appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kFsWpSxkFz"
        ],
        "venue": [
          "/venue/kFsWpSxkFz@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kFsWpSxkFz"
        ],
        "detail": [
          "https://openreview.net/forum?id=kFsWpSxkFz"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Wayne Wu , Honglin He , Jack He , Yiran Wang , Chenda Duan , Zhizheng Liu , Quanyi Li , Bolei Zhou Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. **Micromobility** enabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we present **MetaUrban**, a *compositional* simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an *infinite* number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents’ appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "A1ztozypga@OpenReview",
      "index": 333,
      "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
      "authors": [
        "Xin Dong",
        "Yonggan Fu",
        "Shizhe Diao",
        "Wonmin Byeon",
        "ZIJIA CHEN",
        "Ameya Mahabaleshwarkar",
        "Shih-Yang Liu",
        "Matthijs Van keirsbilck",
        "Min-Hung Chen",
        "Yoshi Suhara",
        "Yingyan Celine Lin",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hymba",
        "lms",
        "heads",
        "language",
        "tokens",
        "hybrid",
        "head",
        "architecture",
        "memories",
        "cache"
      ],
      "summary": "The transformative capabilities of language models (LMs) have intensified the demand for their deployment on everyday devices, necessitating efficient processing for on-device language tasks. To address this, we propose Hymba, a new family of small language models featuring a hybrid-head architecture that strategically integrates attention mechanisms with state space models (SSMs). This architecture leverages the strengths of both systems: attention heads provide high-resolution recall, akin to snapshot memories in the human brain, while SSM heads offer efficient context summarization, similar to fading memories. To further enhance Hymba's performance, we introduce learnable meta tokens that are prepended to input sequences and jointly trained with model weights during pretraining. These meta tokens act as a learned cache initialization during inference, modulating all subsequent tokens within the hybrid heads and boosting the model’s focus on salient information, similar to metamemory. Extensive experiments and ablation studies demonstrate that Hymba sets new state-of-the-art results for small LMs across various benchmarks and advances the accuracy-efficiency trade-offs of small LMs. For instance, Hymba-1.5B achieves comparable commonsense reasoning accuracy to LLaMA 3.2 3B while being 3.49x faster and offering a 14.72x reduction in cache size. All codes and models will be released upon acceptance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=A1ztozypga"
        ],
        "venue": [
          "/venue/A1ztozypga@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=A1ztozypga"
        ],
        "detail": [
          "https://openreview.net/forum?id=A1ztozypga"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 5
      },
      "raw_excerpt": "Hymba: A Hybrid-head Architecture for Small Language Models [PDF 6 ] [Copy] [Kimi 5 ] [REL] Authors : Xin Dong , Yonggan Fu , Shizhe Diao , Wonmin Byeon , ZIJIA CHEN , Ameya Mahabaleshwarkar , Shih-Yang Liu , Matthijs Van keirsbilck , Min-Hung Chen , Yoshi Suhara , Yingyan Celine Lin , Jan Kautz , Pavlo Molchanov The transformative capabilities of language models (LMs) have intensified the demand for their deployment on everyday devices, necessitating efficient processing for on-device language tasks. To address this, we propose Hymba, a new family of small language models featuring a hybrid-head architecture that strategically integrates attention mechanisms with state space models (SSMs). This architecture leverages the strengths of both systems: attention heads provide high-resolution recall, akin to snapshot memories in the human brain, while SSM heads offer efficient context summarization, similar to fading memories. To further enhance Hymba's performance, we introduce learnable meta tokens that are prepended to input sequences and jointly trained with model weights during pretraining. These meta tokens act as a learned cache initialization during inference, modulating all subsequent tokens within the hybrid heads and boosting the model’s focus on salient information, similar to metamemory. Extensive experiments and ablation studies demonstrate that Hymba sets new state-of-the-art results for small LMs across various benchmarks and advances the accuracy-efficiency trade-offs of small LMs. For instance, Hymba-1.5B achieves comparable commonsense reasoning accuracy to LLaMA 3.2 3B while being 3.49x faster and offering a 14.72x reduction in cache size. All codes and models will be released upon acceptance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "6tyPSkshtF@OpenReview",
      "index": 334,
      "title": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition",
      "authors": [
        "Zhong Zheng",
        "Haochen Zhang",
        "Lingzhou Xue"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "advantage",
        "earlysettled",
        "gap",
        "ucb",
        "regret",
        "dependent",
        "bonuses",
        "mdps",
        "bounds",
        "hoeffding"
      ],
      "summary": "We study the gap-dependent bounds of two important algorithms for on-policy Q Q -learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the results based on Hoeffding-type bonuses and achieve the {almost optimal} T − − √ T -type regret bound in the worst-case scenario, where T T is the total number of steps. However, the benign structures of the MDPs such as a strictly positive suboptimality gap can significantly improve the regret. While gap-dependent regret bounds have been obtained for Q Q -learning with Hoeffding-type bonuses, it remains an open question to establish gap-dependent regret bounds for Q Q -learning using variance estimators in their bonuses and reference-advantage decomposition for variance reduction. We develop a novel error decompositionframework to prove gap-dependent regret bounds of UCB-Advantage and Q-EarlySettled-Advantage that are logarithmic in T T and improve upon existing ones for Q Q -learning algorithms. Moreover, we establish the gap-dependent bound for the policy switching cost of UCB-Advantage and improve that under the worst-case MDPs. To our knowledge, this paper presents the first gap-dependent regret analysis for Q Q -learning using variance estimators and reference-advantage decomposition and also provides the first gap-dependent analysis on policy switching cost for Q Q -learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6tyPSkshtF"
        ],
        "venue": [
          "/venue/6tyPSkshtF@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6tyPSkshtF"
        ],
        "detail": [
          "https://openreview.net/forum?id=6tyPSkshtF"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Zhong Zheng , Haochen Zhang , Lingzhou Xue We study the gap-dependent bounds of two important algorithms for on-policy Q Q -learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the results based on Hoeffding-type bonuses and achieve the {almost optimal} T − − √ T -type regret bound in the worst-case scenario, where T T is the total number of steps. However, the benign structures of the MDPs such as a strictly positive suboptimality gap can significantly improve the regret. While gap-dependent regret bounds have been obtained for Q Q -learning with Hoeffding-type bonuses, it remains an open question to establish gap-dependent regret bounds for Q Q -learning using variance estimators in their bonuses and reference-advantage decomposition for variance reduction. We develop a novel error decompositionframework to prove gap-dependent regret bounds of UCB-Advantage and Q-EarlySettled-Advantage that are logarithmic in T T and improve upon existing ones for Q Q -learning algorithms. Moreover, we establish the gap-dependent bound for the policy switching cost of UCB-Advantage and improve that under the worst-case MDPs. To our knowledge, this paper presents the first gap-dependent regret analysis for Q Q -learning using variance estimators and reference-advantage decomposition and also provides the first gap-dependent analysis on policy switching cost for Q Q -learning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "h0ZfDIrj7T@OpenReview",
      "index": 335,
      "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
      "authors": [
        "Junlin Wang",
        "Jue Wang",
        "Ben Athiwaratkun",
        "Ce Zhang",
        "James Y Zou"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "moa",
        "llms",
        "agents",
        "alpacaeval",
        "omni",
        "language",
        "mixture",
        "capabilities",
        "gpt",
        "flask"
      ],
      "summary": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=h0ZfDIrj7T"
        ],
        "venue": [
          "/venue/h0ZfDIrj7T@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=h0ZfDIrj7T"
        ],
        "detail": [
          "https://openreview.net/forum?id=h0ZfDIrj7T"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "Mixture-of-Agents Enhances Large Language Model Capabilities [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : Junlin Wang , Jue Wang , Ben Athiwaratkun , Ce Zhang , James Y Zou Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "bhK7U37VW8@OpenReview",
      "index": 336,
      "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs",
      "authors": [
        "Xiaogeng Liu",
        "Peiran Li",
        "G. Edward Suh",
        "Yevgeniy Vorobeychik",
        "Zhuoqing Mao",
        "Somesh Jha",
        "Patrick McDaniel",
        "Huan Sun",
        "Bo Li",
        "Chaowei Xiao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "autodan",
        "turbo",
        "jailbreak",
        "strategies",
        "teaming",
        "success",
        "1106",
        "red",
        "attack",
        "persuasions"
      ],
      "summary": "Jailbreak attacks serve as essential red-teaming tools, proactively assessing whether LLMs can behave responsibly and safely in adversarial environments. Despite diverse strategies (e.g., cipher, low-resource language, persuasions, and so on) that have been proposed and shown success, these strategies are still manually designed, limiting their scope and effectiveness as a red-teaming tool. In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=bhK7U37VW8"
        ],
        "venue": [
          "/venue/bhK7U37VW8@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=bhK7U37VW8"
        ],
        "detail": [
          "https://openreview.net/forum?id=bhK7U37VW8"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Xiaogeng Liu , Peiran Li , G. Edward Suh , Yevgeniy Vorobeychik , Zhuoqing Mao , Somesh Jha , Patrick McDaniel , Huan Sun , Bo Li , Chaowei Xiao Jailbreak attacks serve as essential red-teaming tools, proactively assessing whether LLMs can behave responsibly and safely in adversarial environments. Despite diverse strategies (e.g., cipher, low-resource language, persuasions, and so on) that have been proposed and shown success, these strategies are still manually designed, limiting their scope and effectiveness as a red-teaming tool. In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "6RtRsg8ZV1@OpenReview",
      "index": 337,
      "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL",
      "authors": [
        "Claas Voelcker",
        "Marcel Hussing",
        "ERIC EATON",
        "Amir-massoud Farahmand",
        "Igor Gilitschenski"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "mad",
        "utd",
        "update",
        "policy",
        "training",
        "data",
        "augmented",
        "stabilizes",
        "instability",
        "resets"
      ],
      "summary": "Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6RtRsg8ZV1"
        ],
        "venue": [
          "/venue/6RtRsg8ZV1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6RtRsg8ZV1"
        ],
        "detail": [
          "https://openreview.net/forum?id=6RtRsg8ZV1"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 2
      },
      "raw_excerpt": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL [PDF 4 ] [Copy] [Kimi 2 ] [REL] Authors : Claas Voelcker , Marcel Hussing , ERIC EATON , Amir-massoud Farahmand , Igor Gilitschenski Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "tj5xJInWty@OpenReview",
      "index": 338,
      "title": "Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency",
      "authors": [
        "Xinyu He",
        "Dongqi Fu",
        "Hanghang Tong",
        "Ross Maciejewski",
        "Jingrui He"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "graph",
        "heterogeneous",
        "privacy",
        "thepuff",
        "temporal",
        "utility",
        "terogeneous",
        "tility",
        "iciency",
        "rivacy"
      ],
      "summary": "Nowadays, Temporal Heterogeneous Graphs attract much research and industrial attention for building the next-generation Relational Deep Learning Models and Applications, due to their informative structures and features. While providing timely and precise services like personalized recommendations and question answering, this rich information also introduces extra exposure risk for each node in the graph, because the distinctive local topology, the abundant heterogeneous features, and the time dimension of the graph data are more prone to exposing sensitive information and narrow down the scope of victim candidates, which calls for well-defined protection techniques on graphs. To this end, we propose a **T**emporal **He**terogeneous Graph Generator balancing **P**rivacy, **U**tility, and E**ff**iciency, named **THePUff**. More specifically, we first propose a differential privacy algorithm to perturb the input temporal heterogeneous graph for protecting privacy, and then utilize both the perturbed graph and the original one in a generative adversarial setting for THePUff to learn and generate privacy-guaranteed and utility-preserved graph data in an efficient manner. We further propose 6 new metrics in the temporal setting to measure heterogeneous graph utility and privacy. Finally, based on temporal heterogeneous graph datasets with up to 1 million nodes and 20 million edges, the experiments show that THePUff generates utilizable temporal heterogeneous graphs with privacy protected, compared with state-of-the-art baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tj5xJInWty"
        ],
        "venue": [
          "/venue/tj5xJInWty@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tj5xJInWty"
        ],
        "detail": [
          "https://openreview.net/forum?id=tj5xJInWty"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 1
      },
      "raw_excerpt": "Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency [PDF 4 ] [Copy] [Kimi 1 ] [REL] Authors : Xinyu He , Dongqi Fu , Hanghang Tong , Ross Maciejewski , Jingrui He Nowadays, Temporal Heterogeneous Graphs attract much research and industrial attention for building the next-generation Relational Deep Learning Models and Applications, due to their informative structures and features. While providing timely and precise services like personalized recommendations and question answering, this rich information also introduces extra exposure risk for each node in the graph, because the distinctive local topology, the abundant heterogeneous features, and the time dimension of the graph data are more prone to exposing sensitive information and narrow down the scope of victim candidates, which calls for well-defined protection techniques on graphs. To this end, we propose a **T**emporal **He**terogeneous Graph Generator balancing **P**rivacy, **U**tility, and E**ff**iciency, named **THePUff**. More specifically, we first propose a differential privacy algorithm to perturb the input temporal heterogeneous graph for protecting privacy, and then utilize both the perturbed graph and the original one in a generative adversarial setting for THePUff to learn and generate privacy-guaranteed and utility-preserved graph data in an efficient manner. We further propose 6 new metrics in the temporal setting to measure heterogeneous graph utility and privacy. Finally, based on temporal heterogeneous graph datasets with up to 1 million nodes and 20 million edges, the experiments show that THePUff generates utilizable temporal heterogeneous graphs with privacy protected, compared with state-of-the-art baselines. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "dsHpulHpOK@OpenReview",
      "index": 339,
      "title": "Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics",
      "authors": [
        "Josiah Kratz",
        "Jacob Adamczyk"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "cell",
        "control",
        "populations",
        "dosing",
        "markovian",
        "environments",
        "reinforcement",
        "memory",
        "stressors",
        "dynamics"
      ],
      "summary": "Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and is thus an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics. To further test our approach in more realistic settings, we demonstrate performant RL-based control strategies in environments with dynamic memory strength.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dsHpulHpOK"
        ],
        "venue": [
          "/venue/dsHpulHpOK@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dsHpulHpOK"
        ],
        "detail": [
          "https://openreview.net/forum?id=dsHpulHpOK"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Josiah Kratz , Jacob Adamczyk Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and is thus an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics. To further test our approach in more realistic settings, we demonstrate performant RL-based control strategies in environments with dynamic memory strength. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "oI5tZaWkF9@OpenReview",
      "index": 340,
      "title": "Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification",
      "authors": [
        "Hsun-Yu Kuo",
        "Yin-Hsiang Liao",
        "Yu-Chieh Chao",
        "Wei-Yun Ma",
        "Pu-Jen Cheng"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "data",
        "weighting",
        "world",
        "synthetic",
        "generated",
        "rethinking",
        "real",
        "approaches",
        "text",
        "classification"
      ],
      "summary": "Synthetic data augmentation via large language models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring deficient outcomes while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs with using merely a little real-world data. We empirically assessed the effectiveness of our method on multiple text classification tasks, and the results showed leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator for model training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oI5tZaWkF9"
        ],
        "venue": [
          "/venue/oI5tZaWkF9@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oI5tZaWkF9"
        ],
        "detail": [
          "https://openreview.net/forum?id=oI5tZaWkF9"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 6
      },
      "raw_excerpt": "Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification [PDF 5 ] [Copy] [Kimi 6 ] [REL] Authors : Hsun-Yu Kuo , Yin-Hsiang Liao , Yu-Chieh Chao , Wei-Yun Ma , Pu-Jen Cheng Synthetic data augmentation via large language models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring deficient outcomes while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs with using merely a little real-world data. We empirically assessed the effectiveness of our method on multiple text classification tasks, and the results showed leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator for model training. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "oSQiao9GqB@OpenReview",
      "index": 341,
      "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
      "authors": [
        "Feng Li",
        "Renrui Zhang",
        "Hao Zhang",
        "Yuanhan Zhang",
        "Bo Li",
        "Wei Li",
        "Zejun MA",
        "Chunyuan Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lmms",
        "interleave",
        "llava",
        "capabilities",
        "multiimage",
        "multi",
        "image",
        "video",
        "tasks",
        "scenarios"
      ],
      "summary": "Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multiimage scenarios remains less explored. Additionally,prior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVAInterleave Bench to comprehensively evaluate the multiimage performance of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. Code will be available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oSQiao9GqB"
        ],
        "venue": [
          "/venue/oSQiao9GqB@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oSQiao9GqB"
        ],
        "detail": [
          "https://openreview.net/forum?id=oSQiao9GqB"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 4
      },
      "raw_excerpt": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models [PDF 5 ] [Copy] [Kimi 4 ] [REL] Authors : Feng Li , Renrui Zhang , Hao Zhang , Yuanhan Zhang , Bo Li , Wei Li , Zejun MA , Chunyuan Li Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multiimage scenarios remains less explored. Additionally,prior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVAInterleave Bench to comprehensively evaluate the multiimage performance of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. Code will be available. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "fgUFZAxywx@OpenReview",
      "index": 342,
      "title": "Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data",
      "authors": [
        "Xinran Liu",
        "Yikun Bai",
        "Rocio Diaz Martin",
        "Kaiwen Shi",
        "Ashkan Shahbazi",
        "Bennett Landman",
        "Catie Chang",
        "Soheil Kolouri"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "spherical",
        "sliced",
        "lssot",
        "transport",
        "optimal",
        "distributions",
        "slicing",
        "embed",
        "computational",
        "linear"
      ],
      "summary": "Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this need. These methods reduce the computational burden of optimal transport by slicing hyperspheres into one-dimensional projections, i.e., lines or circles. Concurrently, linear optimal transport has been proposed to embed distributions into L 2 L 2 spaces, where the L 2 L 2 distance approximates the optimal transport distance, thereby simplifying comparisons across multiple distributions. In this work, we introduce the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which utilizes slicing to embed spherical distributions into L 2 L 2 spaces while preserving their intrinsic geometry, offering a computationally efficient metric for spherical probability measures. We establish the metricity of LSSOT and demonstrate its superior computational efficiency in applications such as cortical surface registration, 3D point cloud interpolation via gradient flow, and shape embedding. Our results demonstrate the significant computational benefits and high accuracy of LSSOT in these applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fgUFZAxywx"
        ],
        "venue": [
          "/venue/fgUFZAxywx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fgUFZAxywx"
        ],
        "detail": [
          "https://openreview.net/forum?id=fgUFZAxywx"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Xinran Liu , Yikun Bai , Rocio Diaz Martin , Kaiwen Shi , Ashkan Shahbazi , Bennett Landman , Catie Chang , Soheil Kolouri Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this need. These methods reduce the computational burden of optimal transport by slicing hyperspheres into one-dimensional projections, i.e., lines or circles. Concurrently, linear optimal transport has been proposed to embed distributions into L 2 L 2 spaces, where the L 2 L 2 distance approximates the optimal transport distance, thereby simplifying comparisons across multiple distributions. In this work, we introduce the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which utilizes slicing to embed spherical distributions into L 2 L 2 spaces while preserving their intrinsic geometry, offering a computationally efficient metric for spherical probability measures. We establish the metricity of LSSOT and demonstrate its superior computational efficiency in applications such as cortical surface registration, 3D point cloud interpolation via gradient flow, and shape embedding. Our results demonstrate the significant computational benefits and high accuracy of LSSOT in these applications. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "tuu4de7HL1@OpenReview",
      "index": 343,
      "title": "Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization",
      "authors": [
        "Rei Higuchi",
        "Pierre-Louis Poirion",
        "Akiko Takeda"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "rshtr",
        "subspace",
        "varepsilon",
        "random",
        "guarantees",
        "convergence",
        "iterations",
        "order",
        "nonconvex",
        "optimization"
      ],
      "summary": "In recent years, random subspace methods have been actively studied for large-dimensional non-convex problems. Recent subspace methods have improved theoretical guarantees such as iteration complexity and local convergence rate while reducing computational costs by deriving descent directions in randomly selected low-dimensional subspaces. This paper proposes the Random Subspace Homogenized Trust Region (RSHTR) method with the best theoretical guarantees among random subspace algorithms for non-convex optimization. RSHTR achieves an ε ε -approximate first-order stationary point in O ( ε − 3 / 2 ) O ( ε − 3 / 2 ) iterations, converging locally at a linear rate. Furthermore, under rank-deficient conditions, RSHTR satisfies ε ε -approximate second-order necessary condition in O ( ε − 3 / 2 ) O ( ε − 3 / 2 ) iterations and exhibits a local quadratic convergence. Experiments on real-world datasets verify the benefits of RSHTR.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tuu4de7HL1"
        ],
        "venue": [
          "/venue/tuu4de7HL1@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tuu4de7HL1"
        ],
        "detail": [
          "https://openreview.net/forum?id=tuu4de7HL1"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Rei Higuchi , Pierre-Louis Poirion , Akiko Takeda In recent years, random subspace methods have been actively studied for large-dimensional non-convex problems. Recent subspace methods have improved theoretical guarantees such as iteration complexity and local convergence rate while reducing computational costs by deriving descent directions in randomly selected low-dimensional subspaces. This paper proposes the Random Subspace Homogenized Trust Region (RSHTR) method with the best theoretical guarantees among random subspace algorithms for non-convex optimization. RSHTR achieves an ε ε -approximate first-order stationary point in O ( ε − 3 / 2 ) O ( ε − 3 / 2 ) iterations, converging locally at a linear rate. Furthermore, under rank-deficient conditions, RSHTR satisfies ε ε -approximate second-order necessary condition in O ( ε − 3 / 2 ) O ( ε − 3 / 2 ) iterations and exhibits a local quadratic convergence. Experiments on real-world datasets verify the benefits of RSHTR. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "PstM8YfhvI@OpenReview",
      "index": 344,
      "title": "MorphoDiff: Cellular Morphology Painting with Diffusion Models",
      "authors": [
        "Zeinab Navidi",
        "Jun Ma",
        "Esteban Miglietta",
        "Le Liu",
        "Anne Carpenter",
        "Beth Cimini",
        "Benjamin Haibe-Kains",
        "BO WANG"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "morphodiff",
        "cellular",
        "painting",
        "interventions",
        "morphology",
        "biological",
        "cell",
        "responses",
        "perturbational",
        "diffusion"
      ],
      "summary": "Understanding cellular responses to external stimuli is critical for parsing biological mechanisms and advancing therapeutic development. High-content image-based assays provide a cost-effective approach to examine cellular phenotypes induced by diverse interventions, which offers valuable insights into biological processes and cellular states. In this paper, we introduce MorphoDiff, a generative pipeline to predict high-resolution cell morphological responses under different conditions based on perturbation encoding. To the best of our knowledge, MorphoDiff is the first framework capable of producing guided, high-resolution predictions of cell morphology that generalize across both chemical and genetic interventions. The model integrates perturbation embeddings as guiding signals within a 2D latent diffusion model. The comprehensive computational, biological, and visual validations across three open-source Cell Painting datasets show that MorphoDiff can generate high-fidelity images and produce meaningful biology signals under various interventions. We envision the model will facilitate efficient in silico exploration of perturbational landscapes towards more effective drug discovery studies.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=PstM8YfhvI"
        ],
        "venue": [
          "/venue/PstM8YfhvI@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=PstM8YfhvI"
        ],
        "detail": [
          "https://openreview.net/forum?id=PstM8YfhvI"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 3
      },
      "raw_excerpt": "MorphoDiff: Cellular Morphology Painting with Diffusion Models [PDF 5 ] [Copy] [Kimi 3 ] [REL] Authors : Zeinab Navidi , Jun Ma , Esteban Miglietta , Le Liu , Anne Carpenter , Beth Cimini , Benjamin Haibe-Kains , BO WANG Understanding cellular responses to external stimuli is critical for parsing biological mechanisms and advancing therapeutic development. High-content image-based assays provide a cost-effective approach to examine cellular phenotypes induced by diverse interventions, which offers valuable insights into biological processes and cellular states. In this paper, we introduce MorphoDiff, a generative pipeline to predict high-resolution cell morphological responses under different conditions based on perturbation encoding. To the best of our knowledge, MorphoDiff is the first framework capable of producing guided, high-resolution predictions of cell morphology that generalize across both chemical and genetic interventions. The model integrates perturbation embeddings as guiding signals within a 2D latent diffusion model. The comprehensive computational, biological, and visual validations across three open-source Cell Painting datasets show that MorphoDiff can generate high-fidelity images and produce meaningful biology signals under various interventions. We envision the model will facilitate efficient in silico exploration of perturbational landscapes towards more effective drug discovery studies. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "qyU5s4fzLg@OpenReview",
      "index": 345,
      "title": "Improving Unsupervised Constituency Parsing via Maximizing Semantic Information",
      "authors": [
        "Junjie Chen",
        "Xiangheng He",
        "Yusuke Miyao",
        "Danushka Bollegala"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "seminfo",
        "parsing",
        "constituency",
        "sentence",
        "semantics",
        "unsupervised",
        "pcfg",
        "maximizing",
        "constituent",
        "parsers"
      ],
      "summary": "Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. However, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close relationship between the constituent structure and the semantics, resulting in a weak correlation between LL values and parsing accuracy.In this paper, we introduce a novel objective for training unsupervised parsers: maximizing the information between constituent structures and sentence semantics (SemInfo). We introduce a bag-of-substrings model to represent the semantics and apply the probability-weighted information metric to estimate the SemInfo.Additionally, we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG) induction, the state-of-the-art non-ensemble method for unsupervised constituency parsing. Experiments demonstrate that SemInfo correlates more strongly with parsing accuracy than LL.Our algorithm significantly enhances parsing accuracy by an average of 7.85 points across five PCFG variants and in four languages, achieving state-of-the-art level results in three of the four languages.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qyU5s4fzLg"
        ],
        "venue": [
          "/venue/qyU5s4fzLg@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qyU5s4fzLg"
        ],
        "detail": [
          "https://openreview.net/forum?id=qyU5s4fzLg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "Improving Unsupervised Constituency Parsing via Maximizing Semantic Information [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Junjie Chen , Xiangheng He , Yusuke Miyao , Danushka Bollegala Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. However, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close relationship between the constituent structure and the semantics, resulting in a weak correlation between LL values and parsing accuracy.In this paper, we introduce a novel objective for training unsupervised parsers: maximizing the information between constituent structures and sentence semantics (SemInfo). We introduce a bag-of-substrings model to represent the semantics and apply the probability-weighted information metric to estimate the SemInfo.Additionally, we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG) induction, the state-of-the-art non-ensemble method for unsupervised constituency parsing. Experiments demonstrate that SemInfo correlates more strongly with parsing accuracy than LL.Our algorithm significantly enhances parsing accuracy by an average of 7.85 points across five PCFG variants and in four languages, achieving state-of-the-art level results in three of the four languages. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "VCbqXtS5YY@OpenReview",
      "index": 346,
      "title": "Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment",
      "authors": [
        "Chenliang Li",
        "Siliang Zeng",
        "Zeyi Liao",
        "Jiaxiang Li",
        "Dongyeop Kang",
        "Alfredo Garcia",
        "Mingyi Hong"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "reward",
        "human",
        "feedback",
        "alignment",
        "demonstrations",
        "sft",
        "demonstration",
        "policy",
        "model",
        "data"
      ],
      "summary": "Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model. Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VCbqXtS5YY"
        ],
        "venue": [
          "/venue/VCbqXtS5YY@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VCbqXtS5YY"
        ],
        "detail": [
          "https://openreview.net/forum?id=VCbqXtS5YY"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 3
      },
      "raw_excerpt": "Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment [PDF 1 ] [Copy] [Kimi 3 ] [REL] Authors : Chenliang Li , Siliang Zeng , Zeyi Liao , Jiaxiang Li , Dongyeop Kang , Alfredo Garcia , Mingyi Hong Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model. Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "TId1SHe8JG@OpenReview",
      "index": 347,
      "title": "Provable Uncertainty Decomposition via Higher-Order Calibration",
      "authors": [
        "Gustaf Ahdritz",
        "Aravind Gollakota",
        "Parikshit Gopalan",
        "Charlotte Peale",
        "Udi Wieder"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "calibration",
        "uncertainty",
        "aleatoric",
        "higher",
        "order",
        "predictors",
        "world",
        "decompositions",
        "provable",
        "formal"
      ],
      "summary": "We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal guarantees we provide. Our method is based on the new notion of higher-order calibration, which generalizes ordinary calibration to the setting of higher-order predictors that predict _mixtures_ over label distributions at every point. We show how to measure as well as achieve higher-order calibration using access to k k -snapshots, namely examples where each point has k k independent conditional labels. Under higher-order calibration, the estimated aleatoric uncertainty at a point is guaranteed to match the real-world aleatoric uncertainty averaged over all points where the prediction is made. To our knowledge, this is the first formal guarantee of this type that places no assumptions whatsoever on the real-world data distribution. Importantly, higher-order calibration is also applicable to existing higher-order predictors such as Bayesian and ensemble models and provides a natural evaluation metric for such models. We demonstrate through experiments that our method produces meaningful uncertainty decompositions in tasks such as image classification.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=TId1SHe8JG"
        ],
        "venue": [
          "/venue/TId1SHe8JG@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=TId1SHe8JG"
        ],
        "detail": [
          "https://openreview.net/forum?id=TId1SHe8JG"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Provable Uncertainty Decomposition via Higher-Order Calibration [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Gustaf Ahdritz , Aravind Gollakota , Parikshit Gopalan , Charlotte Peale , Udi Wieder We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal guarantees we provide. Our method is based on the new notion of higher-order calibration, which generalizes ordinary calibration to the setting of higher-order predictors that predict _mixtures_ over label distributions at every point. We show how to measure as well as achieve higher-order calibration using access to k k -snapshots, namely examples where each point has k k independent conditional labels. Under higher-order calibration, the estimated aleatoric uncertainty at a point is guaranteed to match the real-world aleatoric uncertainty averaged over all points where the prediction is made. To our knowledge, this is the first formal guarantee of this type that places no assumptions whatsoever on the real-world data distribution. Importantly, higher-order calibration is also applicable to existing higher-order predictors such as Bayesian and ensemble models and provides a natural evaluation metric for such models. We demonstrate through experiments that our method produces meaningful uncertainty decompositions in tasks such as image classification. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "IuU0wcO0mo@OpenReview",
      "index": 348,
      "title": "Multi-session, multi-task neural decoding from distinct cell-types and brain regions",
      "authors": [
        "Mehdi Azabou",
        "Krystal Pan",
        "Vinam Arora",
        "Ian Knight",
        "Eva Dyer",
        "Blake A Richards"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "decoding",
        "brain",
        "types",
        "regions",
        "sub",
        "circuits",
        "cellular",
        "mice",
        "distinctions",
        "different"
      ],
      "summary": "Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources is beneficial for a number of downstream decoding tasks. As well, we can transfer the model between regions and sub-types, demonstrating that there is in fact common information in diverse circuits that can be extracted by an appropriately designed model. Interestingly, we found that the model's latent representations showed clear distinctions between different brain regions and cellular sub-types, even though it was never given any information about these distinctions. Altogether, our work demonstrates that training a large-scale neural decoding model on diverse data is possible, and this provides a means of studying the differences and similarities between heterogeneous neural circuits.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IuU0wcO0mo"
        ],
        "venue": [
          "/venue/IuU0wcO0mo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IuU0wcO0mo"
        ],
        "detail": [
          "https://openreview.net/forum?id=IuU0wcO0mo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": null
      },
      "raw_excerpt": "Multi-session, multi-task neural decoding from distinct cell-types and brain regions [PDF 2 ] [Copy] [Kimi ] [REL] Authors : Mehdi Azabou , Krystal Pan , Vinam Arora , Ian Knight , Eva Dyer , Blake A Richards Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources is beneficial for a number of downstream decoding tasks. As well, we can transfer the model between regions and sub-types, demonstrating that there is in fact common information in diverse circuits that can be extracted by an appropriately designed model. Interestingly, we found that the model's latent representations showed clear distinctions between different brain regions and cellular sub-types, even though it was never given any information about these distinctions. Altogether, our work demonstrates that training a large-scale neural decoding model on diverse data is possible, and this provides a means of studying the differences and similarities between heterogeneous neural circuits. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "tfyHbvFZ0K@OpenReview",
      "index": 349,
      "title": "Knowledge Localization: Mission Not Accomplished? Enter Query Localization!",
      "authors": [
        "Yuheng Chen",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "knowledge",
        "assumption",
        "localization",
        "storage",
        "limitations",
        "store",
        "query",
        "modification",
        "accomplished",
        "enter"
      ],
      "summary": "Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear.The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the **Knowledge Localization (KL)** assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption has two limitations: first, it may be too rigid regarding knowledge storage, and second, it neglects the role of the attention module in knowledge expression. In this paper, we first re-examine the KL assumption and demonstrate that its limitations do indeed exist. To address these, we then present two new findings, each targeting one of the limitations: one focusing on knowledge storage and the other on knowledge expression.We summarize these findings as **Query Localization** assumption and argue that the KL assumption can be viewed as a simplification of the QL assumption. Based on QL assumption, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification, further validating our new assumption. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously confirm our conclusions. Code will be made public soon.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tfyHbvFZ0K"
        ],
        "venue": [
          "/venue/tfyHbvFZ0K@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tfyHbvFZ0K"
        ],
        "detail": [
          "https://openreview.net/forum?id=tfyHbvFZ0K"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Knowledge Localization: Mission Not Accomplished? Enter Query Localization! [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Yuheng Chen , Pengfei Cao , Yubo Chen , Kang Liu , Jun Zhao Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear.The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the **Knowledge Localization (KL)** assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption has two limitations: first, it may be too rigid regarding knowledge storage, and second, it neglects the role of the attention module in knowledge expression. In this paper, we first re-examine the KL assumption and demonstrate that its limitations do indeed exist. To address these, we then present two new findings, each targeting one of the limitations: one focusing on knowledge storage and the other on knowledge expression.We summarize these findings as **Query Localization** assumption and argue that the KL assumption can be viewed as a simplification of the QL assumption. Based on QL assumption, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification, further validating our new assumption. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously confirm our conclusions. Code will be made public soon. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "04qx93Viwj@OpenReview",
      "index": 350,
      "title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
      "authors": [
        "Jacob Morrison",
        "Clara Na",
        "Jared Fernandez",
        "Tim Dettmers",
        "Emma Strubell",
        "Jesse Dodge"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "environmental",
        "impact",
        "textbf",
        "water",
        "developers",
        "usage",
        "development",
        "creating",
        "hardware",
        "sim"
      ],
      "summary": "As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 7 billion active parameters, trained on up to 5 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released 270 metric tons 270 metric tons of carbon emissions, equivalent to powering about 53 homes in the United States for one year, and consumed 1.137 million liters of water 1.137 million liters of water , equivalent to about 10 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to ∼ ∼ 80 80 % of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between ∼ ∼ 15% and ∼ ∼ 85% of our hardware's maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=04qx93Viwj"
        ],
        "venue": [
          "/venue/04qx93Viwj@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=04qx93Viwj"
        ],
        "detail": [
          "https://openreview.net/forum?id=04qx93Viwj"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Holistically Evaluating the Environmental Impact of Creating Language Models [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Jacob Morrison , Clara Na , Jared Fernandez , Tim Dettmers , Emma Strubell , Jesse Dodge As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 7 billion active parameters, trained on up to 5 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released 270 metric tons 270 metric tons of carbon emissions, equivalent to powering about 53 homes in the United States for one year, and consumed 1.137 million liters of water 1.137 million liters of water , equivalent to about 10 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to ∼ ∼ 80 80 % of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between ∼ ∼ 15% and ∼ ∼ 85% of our hardware's maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "FtX6oAW7Dd@OpenReview",
      "index": 351,
      "title": "PLENCH: Realistic Evaluation of Deep Partial-Label Learning Algorithms",
      "authors": [
        "Wei Wang",
        "Dong-Dong Wu",
        "Jindong Wang",
        "Gang Niu",
        "Min-Ling Zhang",
        "Masashi Sugiyama"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pll",
        "plench",
        "algorithms",
        "label",
        "partial",
        "systematically",
        "evaluation",
        "realistic",
        "developed",
        "selection"
      ],
      "summary": "Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We systematically investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=FtX6oAW7Dd"
        ],
        "venue": [
          "/venue/FtX6oAW7Dd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=FtX6oAW7Dd"
        ],
        "detail": [
          "https://openreview.net/forum?id=FtX6oAW7Dd"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "PLENCH: Realistic Evaluation of Deep Partial-Label Learning Algorithms [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Wei Wang , Dong-Dong Wu , Jindong Wang , Gang Niu , Min-Ling Zhang , Masashi Sugiyama Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We systematically investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "lgsyLSsDRe@OpenReview",
      "index": 352,
      "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
      "authors": [
        "Chankyu Lee",
        "Rajarshi Roy",
        "Mengyao Xu",
        "Jonathan Raiman",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Wei Ping"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "embedding",
        "mteb",
        "retrieval",
        "embed",
        "training",
        "contrastive",
        "llms",
        "curated",
        "2024",
        "generalist"
      ],
      "summary": "Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed- v1 model secured the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), across 56 embedding tasks. NV-Embed-v2 has reclaimed and maintained the top spot on MTEB since August 30, 2024, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=lgsyLSsDRe"
        ],
        "venue": [
          "/venue/lgsyLSsDRe@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=lgsyLSsDRe"
        ],
        "detail": [
          "https://openreview.net/forum?id=lgsyLSsDRe"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Chankyu Lee , Rajarshi Roy , Mengyao Xu , Jonathan Raiman , Mohammad Shoeybi , Bryan Catanzaro , Wei Ping Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed- v1 model secured the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), across 56 embedding tasks. NV-Embed-v2 has reclaimed and maintained the top spot on MTEB since August 30, 2024, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "3cgMU3TyyE@OpenReview",
      "index": 353,
      "title": "Broaden your SCOPE! Efficient Conversation Planning for LLMs with Semantic Space",
      "authors": [
        "Zhiliang Chen",
        "Xinyuan Niu",
        "Chuan-Sheng Foo",
        "Bryan Kian Hsiang Low"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "conversation",
        "planning",
        "conversations",
        "scope",
        "llm",
        "semantic",
        "turn",
        "llms",
        "broaden",
        "response"
      ],
      "summary": "Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This gives the advantage of allowing the optimal LLM response to be selected at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=3cgMU3TyyE"
        ],
        "venue": [
          "/venue/3cgMU3TyyE@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=3cgMU3TyyE"
        ],
        "detail": [
          "https://openreview.net/forum?id=3cgMU3TyyE"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 5
      },
      "raw_excerpt": "Broaden your SCOPE! Efficient Conversation Planning for LLMs with Semantic Space [PDF 2 ] [Copy] [Kimi 5 ] [REL] Authors : Zhiliang Chen , Xinyuan Niu , Chuan-Sheng Foo , Bryan Kian Hsiang Low Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This gives the advantage of allowing the optimal LLM response to be selected at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "rySLejeB1k@OpenReview",
      "index": 354,
      "title": "Emergent Orientation Maps —— Mechanisms, Coding Efficiency and Robustness",
      "authors": [
        "Haixin Zhong",
        "Haoyu Wang",
        "Wei Dai",
        "Yuchao Huang",
        "Mingyi Huang",
        "Rubin Wang",
        "Anna Roe",
        "Yuguo Yu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pepper",
        "pinwheel",
        "orientation",
        "coding",
        "neuronal",
        "salt",
        "mammals",
        "visual",
        "spiking",
        "retinotopy"
      ],
      "summary": "Extensive experimental studies have shown that in lower mammals, neuronal orientation preference in the primary visual cortex is organized in a disordered \"salt-and-pepper\" pattern. In contrast, higher-order mammals display a continuous variation in orientation preference, forming structured pinwheel-like patterns. Despite these observations, the spiking mechanisms underlying the emergence of these distinct topological structures and their functional roles in visual processing remain poorly understood. To address this, we developed a self-evolving spiking neural network model with Hebbian plasticity, trained using physiological parameters characteristic of rodents, cats, and primates, including retinotopy, neuronal morphology, and connectivity patterns. Our results identify critical factors, such as the degree of input visual field overlap, neuronal density, and the balance between localized connectivity and long-range competition, that determine the emergence of either salt-and-pepper or pinwheel-like topologies. Furthermore, we demonstrate that pinwheel structures exhibit lower wiring costs and enhanced sparse coding capabilities compared to salt-and-pepper organizations. They also maintain greater coding robustness against noise in naturalistic visual stimuli. These findings suggest that such topological structures confer significant computational advantages in visual processing and highlight their potential application in the design of brain-inspired deep learning networks and algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rySLejeB1k"
        ],
        "venue": [
          "/venue/rySLejeB1k@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rySLejeB1k"
        ],
        "detail": [
          "https://openreview.net/forum?id=rySLejeB1k"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 1
      },
      "raw_excerpt": "Emergent Orientation Maps —— Mechanisms, Coding Efficiency and Robustness [PDF 1 ] [Copy] [Kimi 1 ] [REL] Authors : Haixin Zhong , Haoyu Wang , Wei Dai , Yuchao Huang , Mingyi Huang , Rubin Wang , Anna Roe , Yuguo Yu Extensive experimental studies have shown that in lower mammals, neuronal orientation preference in the primary visual cortex is organized in a disordered \"salt-and-pepper\" pattern. In contrast, higher-order mammals display a continuous variation in orientation preference, forming structured pinwheel-like patterns. Despite these observations, the spiking mechanisms underlying the emergence of these distinct topological structures and their functional roles in visual processing remain poorly understood. To address this, we developed a self-evolving spiking neural network model with Hebbian plasticity, trained using physiological parameters characteristic of rodents, cats, and primates, including retinotopy, neuronal morphology, and connectivity patterns. Our results identify critical factors, such as the degree of input visual field overlap, neuronal density, and the balance between localized connectivity and long-range competition, that determine the emergence of either salt-and-pepper or pinwheel-like topologies. Furthermore, we demonstrate that pinwheel structures exhibit lower wiring costs and enhanced sparse coding capabilities compared to salt-and-pepper organizations. They also maintain greater coding robustness against noise in naturalistic visual stimuli. These findings suggest that such topological structures confer significant computational advantages in visual processing and highlight their potential application in the design of brain-inspired deep learning networks and algorithms. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "6NNA0MxhCH@OpenReview",
      "index": 355,
      "title": "Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions",
      "authors": [
        "Sarah Wiegreffe",
        "Oyvind Tafjord",
        "Yonatan Belinkov",
        "Hanna Hajishirzi",
        "Ashish Sabharwal"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "answer",
        "mcqa",
        "choice",
        "formatted",
        "vocabulary",
        "symbol",
        "ace",
        "assemble",
        "lms",
        "layers"
      ],
      "summary": "Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that logit differences between answer choice tokens continue to grow over the course of training.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=6NNA0MxhCH"
        ],
        "venue": [
          "/venue/6NNA0MxhCH@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=6NNA0MxhCH"
        ],
        "detail": [
          "https://openreview.net/forum?id=6NNA0MxhCH"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 2
      },
      "raw_excerpt": "Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions [PDF 3 ] [Copy] [Kimi 2 ] [REL] Authors : Sarah Wiegreffe , Oyvind Tafjord , Yonatan Belinkov , Hanna Hajishirzi , Ashish Sabharwal Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that logit differences between answer choice tokens continue to grow over the course of training. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ixMBnOhFGd@OpenReview",
      "index": 356,
      "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction",
      "authors": [
        "Lu Dai",
        "Yijie Xu",
        "Jinhui Ye",
        "Hao Liu",
        "Hui Xiong"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "retrieval",
        "seper",
        "rag",
        "perplexity",
        "utility",
        "generation",
        "lens",
        "semantic",
        "retrieved",
        "obscures"
      ],
      "summary": "Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose *Semantic Perplexity (SePer)*, a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ixMBnOhFGd"
        ],
        "venue": [
          "/venue/ixMBnOhFGd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ixMBnOhFGd"
        ],
        "detail": [
          "https://openreview.net/forum?id=ixMBnOhFGd"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 8
      },
      "raw_excerpt": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction [PDF 4 ] [Copy] [Kimi 8 ] [REL] Authors : Lu Dai , Yijie Xu , Jinhui Ye , Hao Liu , Hui Xiong Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose *Semantic Perplexity (SePer)*, a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "OdnqG1fYpo@OpenReview",
      "index": 357,
      "title": "Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation",
      "authors": [
        "Qing Wu",
        "Chenhe Du",
        "Xuanyu Tian",
        "Jingyi Yu",
        "Yuyao Zhang",
        "Hongjiang Wei"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "moner",
        "moco",
        "mri",
        "motion",
        "undersampled",
        "inr",
        "radial",
        "iwuqing",
        "correction",
        "unsupervised"
      ],
      "summary": "Motion correction (MoCo) in radial MRI is a particularly challenging problem due to the unpredictability of subject movement. Current state-of-the-art (SOTA) MoCo algorithms often rely on extensive high-quality MR images to pre-train neural networks, which constrains the solution space and leads to outstanding image reconstruction results. However, the need for large-scale datasets significantly increases costs and limits model generalization. In this work, we propose Moner, an unsupervised MoCo method that jointly reconstructs artifact-free MR images and estimates accurate motion from undersampled, rigid motion-corrupted k-space data, without requiring any training data. Our core idea is to leverage the continuous prior of implicit neural representation (INR) to constrain this ill-posed inverse problem, facilitating optimal solutions. Specifically, we integrate a quasi-static motion model into the INR, granting its ability to correct subject's motion. To stabilize model optimization, we reformulate radial MRI reconstruction as a back-projection problem using the Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash encoding strategy, significantly enhancing MoCo accuracy. Experiments on multiple MRI datasets show our Moner achieves performance comparable to SOTA MoCo techniques on in-domain data, while demonstrating significant improvements on out-of-domain data. The code is available at: https://github.com/iwuqing/Moner",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OdnqG1fYpo"
        ],
        "venue": [
          "/venue/OdnqG1fYpo@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OdnqG1fYpo"
        ],
        "detail": [
          "https://openreview.net/forum?id=OdnqG1fYpo"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Qing Wu , Chenhe Du , Xuanyu Tian , Jingyi Yu , Yuyao Zhang , Hongjiang Wei Motion correction (MoCo) in radial MRI is a particularly challenging problem due to the unpredictability of subject movement. Current state-of-the-art (SOTA) MoCo algorithms often rely on extensive high-quality MR images to pre-train neural networks, which constrains the solution space and leads to outstanding image reconstruction results. However, the need for large-scale datasets significantly increases costs and limits model generalization. In this work, we propose Moner, an unsupervised MoCo method that jointly reconstructs artifact-free MR images and estimates accurate motion from undersampled, rigid motion-corrupted k-space data, without requiring any training data. Our core idea is to leverage the continuous prior of implicit neural representation (INR) to constrain this ill-posed inverse problem, facilitating optimal solutions. Specifically, we integrate a quasi-static motion model into the INR, granting its ability to correct subject's motion. To stabilize model optimization, we reformulate radial MRI reconstruction as a back-projection problem using the Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash encoding strategy, significantly enhancing MoCo accuracy. Experiments on multiple MRI datasets show our Moner achieves performance comparable to SOTA MoCo techniques on in-domain data, while demonstrating significant improvements on out-of-domain data. The code is available at: https://github.com/iwuqing/Moner Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ztzZDzgfrh@OpenReview",
      "index": 358,
      "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
      "authors": [
        "Zhongxiang Sun",
        "Xiaoxue Zang",
        "Kai Zheng",
        "Jun Xu",
        "Xiao Zhang",
        "Weijie Yu",
        "Yang Song",
        "Han Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "hallucinations",
        "redeep",
        "rag",
        "knowledge",
        "ffns",
        "retrieved",
        "hallucination",
        "external",
        "parametric",
        "copying"
      ],
      "summary": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the **Knowledge FFNs** in LLMs overemphasize parametric knowledge in the residual stream, while **Copying Heads** fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose **ReDeEP**, a novel method that detects hallucinations by decoupling LLM’s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ztzZDzgfrh"
        ],
        "venue": [
          "/venue/ztzZDzgfrh@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ztzZDzgfrh"
        ],
        "detail": [
          "https://openreview.net/forum?id=ztzZDzgfrh"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 12
      },
      "raw_excerpt": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability [PDF 13 ] [Copy] [Kimi 12 ] [REL] Authors : Zhongxiang Sun , Xiaoxue Zang , Kai Zheng , Jun Xu , Xiao Zhang , Weijie Yu , Yang Song , Han Li Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the **Knowledge FFNs** in LLMs overemphasize parametric knowledge in the residual stream, while **Copying Heads** fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose **ReDeEP**, a novel method that detects hallucinations by decoupling LLM’s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "cfKZ5VrhXt@OpenReview",
      "index": 359,
      "title": "Online Preference Alignment for Language Models via Count-based Exploration",
      "authors": [
        "Chenjia Bai",
        "Yang Zhang",
        "Shuang Qiu",
        "Qiaosheng Zhang",
        "Kang Xu",
        "Xuelong Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "rlhf",
        "copo",
        "preference",
        "online",
        "exploration",
        "count",
        "reward",
        "llm",
        "ucb",
        "alignment"
      ],
      "summary": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cfKZ5VrhXt"
        ],
        "venue": [
          "/venue/cfKZ5VrhXt@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cfKZ5VrhXt"
        ],
        "detail": [
          "https://openreview.net/forum?id=cfKZ5VrhXt"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 4
      },
      "raw_excerpt": "Online Preference Alignment for Language Models via Count-based Exploration [PDF 3 ] [Copy] [Kimi 4 ] [REL] Authors : Chenjia Bai , Yang Zhang , Shuang Qiu , Qiaosheng Zhang , Kang Xu , Xuelong Li Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "gye2U9uNXx@OpenReview",
      "index": 360,
      "title": "Uncovering Gaps in How Humans and LLMs Interpret Subjective Language",
      "authors": [
        "Erik Jones",
        "Arjun Patrawala",
        "Jacob Steinhardt"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "thesaurus",
        "llm",
        "instruct",
        "subjective",
        "enthusiastic",
        "humans",
        "operational",
        "semantics",
        "ted",
        "llms"
      ],
      "summary": "Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an *enthusiastic* blogpost, while developers might train models to be *helpful* and *harmless* using LLM-based edits. The LLM's *operational semantics* of such subjective phrases---how it adjusts its behavior when each phrase is included in the prompt---thus dictates how aligned it is with human intent. In this work, we uncover instances of *misalignment* between LLMs' actual operational semantics and what humans expect. Our method, TED (thesaurus error detector), first constructs a thesaurus that captures whether two phrases have similar operational semantics according to the LLM. It then elicits failures by unearthing disagreements between this thesaurus and a reference semantic thesaurus. TED routinely produces surprising instances of misalignment; for example, Mistral 7B Instruct produces more *harassing* outputs when it edits text to be *witty*, and Llama 3 8B Instruct produces *dishonest* articles when instructed to make the articles *enthusiastic*. Our results demonstrate that we can uncover unexpected LLM behavior by characterizing relationships between abstract concepts, rather than supervising individual outputs directly.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gye2U9uNXx"
        ],
        "venue": [
          "/venue/gye2U9uNXx@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gye2U9uNXx"
        ],
        "detail": [
          "https://openreview.net/forum?id=gye2U9uNXx"
        ]
      },
      "scores": {
        "pdf": 1,
        "kimi": 2
      },
      "raw_excerpt": "Uncovering Gaps in How Humans and LLMs Interpret Subjective Language [PDF 1 ] [Copy] [Kimi 2 ] [REL] Authors : Erik Jones , Arjun Patrawala , Jacob Steinhardt Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an *enthusiastic* blogpost, while developers might train models to be *helpful* and *harmless* using LLM-based edits. The LLM's *operational semantics* of such subjective phrases---how it adjusts its behavior when each phrase is included in the prompt---thus dictates how aligned it is with human intent. In this work, we uncover instances of *misalignment* between LLMs' actual operational semantics and what humans expect. Our method, TED (thesaurus error detector), first constructs a thesaurus that captures whether two phrases have similar operational semantics according to the LLM. It then elicits failures by unearthing disagreements between this thesaurus and a reference semantic thesaurus. TED routinely produces surprising instances of misalignment; for example, Mistral 7B Instruct produces more *harassing* outputs when it edits text to be *witty*, and Llama 3 8B Instruct produces *dishonest* articles when instructed to make the articles *enthusiastic*. Our results demonstrate that we can uncover unexpected LLM behavior by characterizing relationships between abstract concepts, rather than supervising individual outputs directly. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "KZgo2YQbhc@OpenReview",
      "index": 361,
      "title": "PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction",
      "authors": [
        "Shangyu Chen",
        "Zizheng Pan",
        "Jianfei Cai",
        "Dinh Phung"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "para",
        "personalizing",
        "target",
        "rank",
        "text",
        "t2i",
        "personalization",
        "diffusionmodel",
        "trainingdata",
        "aligning"
      ],
      "summary": "Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal-lenging as it typically struggles to make an appropriate trade-off between its trainingdata distribution and the target distribution, i.e., learning a novel concept with only afew target images to achieve personalization (aligning with the personalized target)while preserving text editability (aligning with diverse text prompts). In this paper,we propose PaRa, an effective and efficient Parameter Rank Reduction approachfor T2I model personalization by explicitly controlling the rank of the diffusionmodel parameters to restrict its initial diverse generation space into a small andwell-balanced target space. Our design is motivated by the fact that taming a T2Imodel toward a novel concept such as a specific art style implies a small generationspace. To this end, by reducing the rank of model parameters during finetuning, wecan effectively constrain the space of the denoising sampling trajectories towardsthe target. With comprehensive experiments, we show that PaRa achieves greatadvantages over existing finetuning approaches on single/multi-subject generationas well as single-image editing. Notably, compared to the prevailing fine-tuningtechnique LoRA, PaRa achieves better parameter efficiency (2× fewer learnableparameters) and much better target image alignment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KZgo2YQbhc"
        ],
        "venue": [
          "/venue/KZgo2YQbhc@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KZgo2YQbhc"
        ],
        "detail": [
          "https://openreview.net/forum?id=KZgo2YQbhc"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Shangyu Chen , Zizheng Pan , Jianfei Cai , Dinh Phung Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal-lenging as it typically struggles to make an appropriate trade-off between its trainingdata distribution and the target distribution, i.e., learning a novel concept with only afew target images to achieve personalization (aligning with the personalized target)while preserving text editability (aligning with diverse text prompts). In this paper,we propose PaRa, an effective and efficient Parameter Rank Reduction approachfor T2I model personalization by explicitly controlling the rank of the diffusionmodel parameters to restrict its initial diverse generation space into a small andwell-balanced target space. Our design is motivated by the fact that taming a T2Imodel toward a novel concept such as a specific art style implies a small generationspace. To this end, by reducing the rank of model parameters during finetuning, wecan effectively constrain the space of the denoising sampling trajectories towardsthe target. With comprehensive experiments, we show that PaRa achieves greatadvantages over existing finetuning approaches on single/multi-subject generationas well as single-image editing. Notably, compared to the prevailing fine-tuningtechnique LoRA, PaRa achieves better parameter efficiency (2× fewer learnableparameters) and much better target image alignment. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "i8dYPGdB1C@OpenReview",
      "index": 362,
      "title": "Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency",
      "authors": [
        "Qixin ZHANG",
        "Zongqi Wan",
        "Yu Yang",
        "Li Shen",
        "Dacheng Tao"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "osma",
        "submodular",
        "textbf",
        "osg",
        "approximation",
        "frac",
        "algorithm",
        "multi",
        "communication",
        "projection"
      ],
      "summary": "Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm, are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a MA-OSMA MA-OSMA algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, MA-OSMA MA-OSMA leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in MA-OSMA MA-OSMA , we also introduce a projection-free MA-OSEA MA-OSEA algorithm, which effectively utilizes the KL divergence by mixing a uniform distribution. Theoretically, we confirm that both algorithms achieve a regret bound of O ˜ ( C T T 1 − β − − − √ ) O ~ ( C T T 1 − β ) against a ( 1 − e − c c ) ( 1 − e − c c ) -approximation to the best comparator in hindsight, where C T C T is the deviation of maximizer sequence, β β is the spectral gap of the network and c c is the joint curvature of submodular objectives. This result significantly improves the ( 1 1 + c ) ( 1 1 + c ) -approximation provided by the state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness of our proposed algorithms through simulation-based multi-target tracking.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=i8dYPGdB1C"
        ],
        "venue": [
          "/venue/i8dYPGdB1C@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=i8dYPGdB1C"
        ],
        "detail": [
          "https://openreview.net/forum?id=i8dYPGdB1C"
        ]
      },
      "scores": {
        "pdf": null,
        "kimi": 3
      },
      "raw_excerpt": "Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency [PDF ] [Copy] [Kimi 3 ] [REL] Authors : Qixin ZHANG , Zongqi Wan , Yu Yang , Li Shen , Dacheng Tao Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm, are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a MA-OSMA MA-OSMA algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, MA-OSMA MA-OSMA leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in MA-OSMA MA-OSMA , we also introduce a projection-free MA-OSEA MA-OSEA algorithm, which effectively utilizes the KL divergence by mixing a uniform distribution. Theoretically, we confirm that both algorithms achieve a regret bound of O ˜ ( C T T 1 − β − − − √ ) O ~ ( C T T 1 − β ) against a ( 1 − e − c c ) ( 1 − e − c c ) -approximation to the best comparator in hindsight, where C T C T is the deviation of maximizer sequence, β β is the spectral gap of the network and c c is the joint curvature of submodular objectives. This result significantly improves the ( 1 1 + c ) ( 1 1 + c ) -approximation provided by the state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness of our proposed algorithms through simulation-based multi-target tracking. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Fs9EabmQrJ@OpenReview",
      "index": 363,
      "title": "EmbedLLM: Learning Compact Representations of Large Language Models",
      "authors": [
        "Richard Zhuang",
        "Tianhao Wu",
        "Zhaojin Wen",
        "Andrew Li",
        "Jiantao Jiao",
        "Kannan Ramchandran"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "embedllm",
        "representations",
        "language",
        "routing",
        "models",
        "embedder",
        "compact",
        "downstream",
        "llms",
        "facilitate"
      ],
      "summary": "With hundreds of thousands of language models available on Huggingface today, efficiently evaluating and utilizing these models across various downstream tasks has become increasingly critical. Many existing methods repeatedly learn task-specific representations of Large Language Models (LLMs), which leads to inefficiencies in both time and computational resources. To address this, we propose EmbedLLM, a framework designed to learn compact vector representations of LLMs that facilitate downstream applications involving many models, such as model routing. We introduce an encoder-decoder approach for learning such embedding, along with a systematic framework to evaluate their effectiveness. Empirical results show that EmbedLLM outperforms prior methods in model routing. Additionally, we demonstrate that our method can forecast a model's performance on multiple benchmarks, without incurring additional inference cost. Extensive probing experiments validate that the learned embeddings capture key model characteristics, e.g. whether the model is specialized for coding tasks, even without being explicitly trained on them. We open source our dataset, code and embedder to facilitate further research and application.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Fs9EabmQrJ"
        ],
        "venue": [
          "/venue/Fs9EabmQrJ@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Fs9EabmQrJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=Fs9EabmQrJ"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "EmbedLLM: Learning Compact Representations of Large Language Models [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Richard Zhuang , Tianhao Wu , Zhaojin Wen , Andrew Li , Jiantao Jiao , Kannan Ramchandran With hundreds of thousands of language models available on Huggingface today, efficiently evaluating and utilizing these models across various downstream tasks has become increasingly critical. Many existing methods repeatedly learn task-specific representations of Large Language Models (LLMs), which leads to inefficiencies in both time and computational resources. To address this, we propose EmbedLLM, a framework designed to learn compact vector representations of LLMs that facilitate downstream applications involving many models, such as model routing. We introduce an encoder-decoder approach for learning such embedding, along with a systematic framework to evaluate their effectiveness. Empirical results show that EmbedLLM outperforms prior methods in model routing. Additionally, we demonstrate that our method can forecast a model's performance on multiple benchmarks, without incurring additional inference cost. Extensive probing experiments validate that the learned embeddings capture key model characteristics, e.g. whether the model is specialized for coding tasks, even without being explicitly trained on them. We open source our dataset, code and embedder to facilitate further research and application. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "rxVvRBgqmS@OpenReview",
      "index": 364,
      "title": "PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance",
      "authors": [
        "Qijun Gan",
        "Song Wang",
        "Shengtao Wu",
        "Jianke Zhu"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pianomotion10m",
        "piano",
        "presses",
        "hand",
        "music",
        "audios",
        "dataset",
        "motion",
        "movements",
        "playing"
      ],
      "summary": "Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed at https://github.com/PianoMotion10M/PianoMotion10M.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rxVvRBgqmS"
        ],
        "venue": [
          "/venue/rxVvRBgqmS@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rxVvRBgqmS"
        ],
        "detail": [
          "https://openreview.net/forum?id=rxVvRBgqmS"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": null
      },
      "raw_excerpt": "PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance [PDF 3 ] [Copy] [Kimi ] [REL] Authors : Qijun Gan , Song Wang , Shengtao Wu , Jianke Zhu Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed at https://github.com/PianoMotion10M/PianoMotion10M. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "ALzTQUgW8a@OpenReview",
      "index": 365,
      "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
      "authors": [
        "Zhuoming Chen",
        "Ranajoy Sadhukhan",
        "Zihao Ye",
        "Yang Zhou",
        "Jianyu Zhang",
        "Niklas Nolte",
        "Yuandong Tian",
        "Matthijs Douze",
        "Leon Bottou",
        "Zhihao Jia",
        "Beidi Chen"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "magicpig",
        "attention",
        "lsh",
        "topk",
        "llm",
        "sparse",
        "sampling",
        "110ms",
        "computation",
        "approximation"
      ],
      "summary": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, now becomes a bottleneck. Leveraging the common insight that attention is sparse, various dynamic sparse or TopK-based attention approximation methods have been proposed. In this paper, we first show that TopK attention itself suffers from a quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on CPU, which allows to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by 1.9 ∼ 3.9 × 1.9 ∼ 3.9 × across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ALzTQUgW8a"
        ],
        "venue": [
          "/venue/ALzTQUgW8a@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ALzTQUgW8a"
        ],
        "detail": [
          "https://openreview.net/forum?id=ALzTQUgW8a"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "MagicPIG: LSH Sampling for Efficient LLM Generation [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : Zhuoming Chen , Ranajoy Sadhukhan , Zihao Ye , Yang Zhou , Jianyu Zhang , Niklas Nolte , Yuandong Tian , Matthijs Douze , Leon Bottou , Zhihao Jia , Beidi Chen Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, now becomes a bottleneck. Leveraging the common insight that attention is sparse, various dynamic sparse or TopK-based attention approximation methods have been proposed. In this paper, we first show that TopK attention itself suffers from a quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on CPU, which allows to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by 1.9 ∼ 3.9 × 1.9 ∼ 3.9 × across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "dGVZwyq5tV@OpenReview",
      "index": 366,
      "title": "Training-Free Activation Sparsity in Large Language Models",
      "authors": [
        "James Liu",
        "Pragaash Ponnusamy",
        "Tianle Cai",
        "placeholder",
        "Yoon Kim",
        "Ben Athiwaratkun"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sparsity",
        "teal",
        "activation",
        "llama",
        "ctivation",
        "fre",
        "training",
        "language",
        "raining",
        "mistral"
      ],
      "summary": "Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL (**T**raining-Fre**e** **A**ctivation Sparsity in **L**LMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50\\% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40\\% and 50\\% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=dGVZwyq5tV"
        ],
        "venue": [
          "/venue/dGVZwyq5tV@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=dGVZwyq5tV"
        ],
        "detail": [
          "https://openreview.net/forum?id=dGVZwyq5tV"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 7
      },
      "raw_excerpt": "Training-Free Activation Sparsity in Large Language Models [PDF 7 ] [Copy] [Kimi 7 ] [REL] Authors : James Liu , Pragaash Ponnusamy , Tianle Cai , placeholder , Yoon Kim , Ben Athiwaratkun Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL (**T**raining-Fre**e** **A**ctivation Sparsity in **L**LMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50\\% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40\\% and 50\\% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "kam84eEmub@OpenReview",
      "index": 367,
      "title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation",
      "authors": [
        "Mufei Li",
        "Viraj Shitole",
        "Eli Chien",
        "Changhai Man",
        "Zhaodong Wang",
        "Srinivas",
        "Ying Zhang",
        "Tushar Krishna",
        "Pan Li"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "layerdag",
        "dags",
        "dependencies",
        "autoregressive",
        "benchmarking",
        "acyclic",
        "computing",
        "dag",
        "layerwise",
        "directed"
      ],
      "summary": "Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kam84eEmub"
        ],
        "venue": [
          "/venue/kam84eEmub@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kam84eEmub"
        ],
        "detail": [
          "https://openreview.net/forum?id=kam84eEmub"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": null
      },
      "raw_excerpt": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation [PDF 4 ] [Copy] [Kimi ] [REL] Authors : Mufei Li , Viraj Shitole , Eli Chien , Changhai Man , Zhaodong Wang , Srinivas , Ying Zhang , Tushar Krishna , Pan Li Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "fxv0FfmDAg@OpenReview",
      "index": 368,
      "title": "DRoP: Distributionally Robust Data Pruning",
      "authors": [
        "Artem Vysogorets",
        "Kartik Ahuja",
        "Julia Kempe"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "pruning",
        "drop",
        "distributionally",
        "data",
        "robust",
        "uninformative",
        "performance",
        "tolerable",
        "exceptionally",
        "hungry"
      ],
      "summary": "In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. We present theoretical analysis of the classification risk in a mixture of Gaussians to argue that choosing appropriate class pruning ratios, coupled with random pruning within classes has potential to improve worst-class performance. We thus propose DRoP, a distributionally robust approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving distributional robustness at a tolerable drop of average performance as we prune more from the datasets.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fxv0FfmDAg"
        ],
        "venue": [
          "/venue/fxv0FfmDAg@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fxv0FfmDAg"
        ],
        "detail": [
          "https://openreview.net/forum?id=fxv0FfmDAg"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 2
      },
      "raw_excerpt": "DRoP: Distributionally Robust Data Pruning [PDF 2 ] [Copy] [Kimi 2 ] [REL] Authors : Artem Vysogorets , Kartik Ahuja , Julia Kempe In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. We present theoretical analysis of the classification risk in a mixture of Gaussians to argue that choosing appropriate class pruning ratios, coupled with random pruning within classes has potential to improve worst-class performance. We thus propose DRoP, a distributionally robust approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving distributional robustness at a tolerable drop of average performance as we prune more from the datasets. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "NQqJPPCesd@OpenReview",
      "index": 369,
      "title": "Towards Marginal Fairness Sliced Wasserstein Barycenter",
      "authors": [
        "Khai Nguyen",
        "Hai Nguyen",
        "Nhat Ho"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "sliced",
        "barycenter",
        "mfswb",
        "swb",
        "marginal",
        "fairness",
        "wasserstein",
        "marginals",
        "surrogate",
        "slicing"
      ],
      "summary": "The Sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NQqJPPCesd"
        ],
        "venue": [
          "/venue/NQqJPPCesd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NQqJPPCesd"
        ],
        "detail": [
          "https://openreview.net/forum?id=NQqJPPCesd"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Towards Marginal Fairness Sliced Wasserstein Barycenter [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Khai Nguyen , Hai Nguyen , Nhat Ho The Sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "UqYNPyotxL@OpenReview",
      "index": 370,
      "title": "Linear Mode Connectivity in Differentiable Tree Ensembles",
      "authors": [
        "Ryuichi Kanoh",
        "Mahito Sugiyama"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "lmc",
        "invariances",
        "tree",
        "invariance",
        "connectivity",
        "differentiable",
        "ensembles",
        "permutation",
        "mode",
        "achieving"
      ],
      "summary": "Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UqYNPyotxL"
        ],
        "venue": [
          "/venue/UqYNPyotxL@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UqYNPyotxL"
        ],
        "detail": [
          "https://openreview.net/forum?id=UqYNPyotxL"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 1
      },
      "raw_excerpt": "Linear Mode Connectivity in Differentiable Tree Ensembles [PDF 2 ] [Copy] [Kimi 1 ] [REL] Authors : Ryuichi Kanoh , Mahito Sugiyama Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "pPyJyeLriR@OpenReview",
      "index": 371,
      "title": "Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier",
      "authors": [
        "Yi",
        "Zhewei Wei"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "unlearning",
        "scalegun",
        "certified",
        "graph",
        "node",
        "propagation",
        "scalable",
        "edge",
        "certifiable",
        "embeddings"
      ],
      "summary": "Graph unlearning has emerged as a pivotal research area for ensuring privacy protection, given the widespread adoption of Graph Neural Networks (GNNs) in applications involving sensitive user data. Among existing studies, certified graph unlearning is distinguished by providing robust privacy guarantees. However, current certified graph unlearning methods are impractical for large-scale graphs because they necessitate the costly re-computation of graph propagation for each unlearning request. Although numerous scalable techniques have been developed to accelerate graph propagation for GNNs, their integration into certified graph unlearning remains uncertain as these scalable approaches introduce approximation errors into node embeddings. In contrast, certified graph unlearning demands bounded model error on exact node embeddings to maintain its certified guarantee. To address this challenge, we present ScaleGUN, the first approach to scale certified graph unlearning to billion-edge graphs. ScaleGUN integrates the approximate graph propagation technique into certified graph unlearning, offering certified guarantees for three unlearning scenarios: node feature, edge and node unlearning. Extensive experiments on real-world datasets demonstrate the efficiency and unlearning efficacy of ScaleGUN. Remarkably, ScaleGUN accomplishes ( ϵ , δ ) = ( 1 , 10 − 4 ) ( ϵ , δ ) = ( 1 , 10 − 4 ) certified unlearning on the billion-edge graph ogbn-papers100M in 20 seconds for a 5,000 random edge removal request -- of which only 5 seconds are required for updating the node embeddings -- compared to 1.91 hours for retraining and 1.89 hours for re-propagation. Our code is available at https://anonymous.4open.science/r/ScaleGUN-5921.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=pPyJyeLriR"
        ],
        "venue": [
          "/venue/pPyJyeLriR@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=pPyJyeLriR"
        ],
        "detail": [
          "https://openreview.net/forum?id=pPyJyeLriR"
        ]
      },
      "scores": {
        "pdf": 3,
        "kimi": 1
      },
      "raw_excerpt": "Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier [PDF 3 ] [Copy] [Kimi 1 ] [REL] Authors : Yi , Zhewei Wei Graph unlearning has emerged as a pivotal research area for ensuring privacy protection, given the widespread adoption of Graph Neural Networks (GNNs) in applications involving sensitive user data. Among existing studies, certified graph unlearning is distinguished by providing robust privacy guarantees. However, current certified graph unlearning methods are impractical for large-scale graphs because they necessitate the costly re-computation of graph propagation for each unlearning request. Although numerous scalable techniques have been developed to accelerate graph propagation for GNNs, their integration into certified graph unlearning remains uncertain as these scalable approaches introduce approximation errors into node embeddings. In contrast, certified graph unlearning demands bounded model error on exact node embeddings to maintain its certified guarantee. To address this challenge, we present ScaleGUN, the first approach to scale certified graph unlearning to billion-edge graphs. ScaleGUN integrates the approximate graph propagation technique into certified graph unlearning, offering certified guarantees for three unlearning scenarios: node feature, edge and node unlearning. Extensive experiments on real-world datasets demonstrate the efficiency and unlearning efficacy of ScaleGUN. Remarkably, ScaleGUN accomplishes ( ϵ , δ ) = ( 1 , 10 − 4 ) ( ϵ , δ ) = ( 1 , 10 − 4 ) certified unlearning on the billion-edge graph ogbn-papers100M in 20 seconds for a 5,000 random edge removal request -- of which only 5 seconds are required for updating the node embeddings -- compared to 1.91 hours for retraining and 1.89 hours for re-propagation. Our code is available at https://anonymous.4open.science/r/ScaleGUN-5921. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "SjufxrSOYd@OpenReview",
      "index": 372,
      "title": "Invariant Graphon Networks: Approximation and Cut Distance",
      "authors": [
        "Daniel Herbst",
        "Stefanie Jegelka"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "iwns",
        "graphon",
        "mpnns",
        "ign",
        "igns",
        "graphons",
        "cut",
        "invariant",
        "transferability",
        "gnns"
      ],
      "summary": "Graph limit models, like graphons for limits of dense graphs, have recently been used as a tool to study size transferability of graph neural networks (GNNs). While most existing literature focuses on message passing GNNs (MPNNs), we attend to *Invariant Graph Networks* (IGNs), a powerful alternative GNN architecture. In this work, we generalize IGNs to graphons, introducing *Invariant Graphon Networks (IWNs)* which are defined using a subset of the IGN basis corresponding to bounded linear operators. Even with this restricted basis, we show that IWNs of order k + 1 k + 1 are at least as powerful as the k k -dimensional Weisfeiler-Leman (WL) test for graphon-signals and we establish universal approximation results for graphon-signals in L p L p distances using *signal-weighted homomorphism densities*. This significantly extends the prior work of Cai & Wang (2022), showing that IWNs—a subset of their *IGN-small*—retain effectively the same expressivity as the full IGN basis in the limit. In contrast to their approach, our blueprint of IWNs also aligns better with the geometry of graphon space, for example facilitating comparability to MPNNs. We also highlight that, unlike other GNN architectures such as MPNNs, IWNs are *discontinuous with respect to cut distance*, which causes their lack of convergence and is inherently tied to the definition of k k -WL. Yet, their transferability remains comparable to MPNNs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=SjufxrSOYd"
        ],
        "venue": [
          "/venue/SjufxrSOYd@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=SjufxrSOYd"
        ],
        "detail": [
          "https://openreview.net/forum?id=SjufxrSOYd"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "Invariant Graphon Networks: Approximation and Cut Distance [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Daniel Herbst , Stefanie Jegelka Graph limit models, like graphons for limits of dense graphs, have recently been used as a tool to study size transferability of graph neural networks (GNNs). While most existing literature focuses on message passing GNNs (MPNNs), we attend to *Invariant Graph Networks* (IGNs), a powerful alternative GNN architecture. In this work, we generalize IGNs to graphons, introducing *Invariant Graphon Networks (IWNs)* which are defined using a subset of the IGN basis corresponding to bounded linear operators. Even with this restricted basis, we show that IWNs of order k + 1 k + 1 are at least as powerful as the k k -dimensional Weisfeiler-Leman (WL) test for graphon-signals and we establish universal approximation results for graphon-signals in L p L p distances using *signal-weighted homomorphism densities*. This significantly extends the prior work of Cai & Wang (2022), showing that IWNs—a subset of their *IGN-small*—retain effectively the same expressivity as the full IGN basis in the limit. In contrast to their approach, our blueprint of IWNs also aligns better with the geometry of graphon space, for example facilitating comparability to MPNNs. We also highlight that, unlike other GNN architectures such as MPNNs, IWNs are *discontinuous with respect to cut distance*, which causes their lack of convergence and is inherently tied to the definition of k k -WL. Yet, their transferability remains comparable to MPNNs. Subject : ICLR.2025 - Spotlight"
    },
    {
      "paper_id": "Bk13Qfu8Ru@OpenReview",
      "index": 373,
      "title": "Severing Spurious Correlations with Data Pruning",
      "authors": [
        "Varun Mulchandani",
        "Jung-Eun Kim"
      ],
      "subjects": [
        "ICLR.2025 - Spotlight"
      ],
      "keywords": [
        "spurious",
        "correlations",
        "pruning",
        "severing",
        "settings",
        "signal",
        "samples",
        "information",
        "data",
        "malfunction"
      ],
      "summary": "Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the formation of such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also learn that spurious correlations are formed primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Bk13Qfu8Ru"
        ],
        "venue": [
          "/venue/Bk13Qfu8Ru@OpenReview",
          "/venue/ICLR.2025?group=Spotlight"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Bk13Qfu8Ru"
        ],
        "detail": [
          "https://openreview.net/forum?id=Bk13Qfu8Ru"
        ]
      },
      "scores": {
        "pdf": 2,
        "kimi": 3
      },
      "raw_excerpt": "Severing Spurious Correlations with Data Pruning [PDF 2 ] [Copy] [Kimi 3 ] [REL] Authors : Varun Mulchandani , Jung-Eun Kim Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the formation of such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also learn that spurious correlations are formed primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable. Subject : ICLR.2025 - Spotlight"
    }
  ]
}