<html><head>
    <title>CVPR.2025 - Highlight | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="The list of accepted papers for CVPR.2025 - Highlight, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="/static/flatpickr/dist/flatpickr.min.css?v=4.6.13">
    <link rel="stylesheet" href="/static/style.css?v=1.5.1.6">
<script src="https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em 0.7em;; position: relative; display: inline-block!important;; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none; box-sizing: content-box}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_test.mjx-test-display {display: table!important}
.MathJax_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_test.mjx-test-default {display: block!important; clear: both}
.MathJax_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.MathJax_em_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60em}
.mjx-test-inline .MathJax_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.9') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.9') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_Typewriter; src: url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff?V=2.7.9') format('woff'), url('https://papers.cool/static/MathJax-2.7.9/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf?V=2.7.9') format('opentype')}
</style></head>
<body id="venue"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <h1 class="notranslate">CVPR.2025 - Highlight</h1>
    <p class="info notranslate">
        <span class="shortcut sort-it" title="sort by reading stars" onclick="paperSort('stars')"><i class="fa fa-star"></i></span>
        <span class="shortcut sort-it" title="sort by your preference" onclick="paperSort('prefer')"><i class="fa fa-heart"></i></span>
        <span class="shortcut feed-it" title="open feed link" onclick="openFeed()"><i class="fa fa-rss"></i></span> |
        Total: 388
    </p>
    <div class="papers">
        <div id="Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="panel paper" keywords="chroma,foreground,tkg,background,generation,diffusion,key,content,images,fine">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model_CVPR_2025_paper.html" target="_blank" title="1/388"><span class="index notranslate">#1</span></a>
                <a id="title-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="title-link" href="/venue/Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" target="_blank">TKG-DM: Training-free Chroma Key Content Generation Diffusion Model</a>
                <a id="pdf-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF">141</sup>]</a>
                <a id="copy-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF">90</sup>]</a>
                <a id="rel-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryugo Morita" target="_blank">Ryugo Morita</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stanislav Frolov" target="_blank">Stanislav Frolov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Bernhard Moser" target="_blank">Brian Bernhard Moser</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takahiro Shirakawa" target="_blank">Takahiro Shirakawa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ko Watanabe" target="_blank">Ko Watanabe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andreas Dengel" target="_blank">Andreas Dengel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinjia Zhou" target="_blank">Jinjia Zhou</a>
            </p>
            <p id="summary-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="summary">Diffusion models have enabled the generation of high-quality images with a strong focus on realism and textual fidelity. Yet, large-scale text-to-image models, such as Stable Diffusion, struggle to generate images where foreground objects are placed over a chroma key background, limiting their ability to separate foreground and background elements without fine-tuning. To address this limitation, we present a novel Training-Free Chroma Key Content Generation Diffusion Model (TKG-DM), which optimizes the initial random noise to produce images with foreground objects on a specifiable color background. Our proposed method is the first to explore the manipulation of the color aspects in initial noise for controlled background generation, enabling precise separation of foreground and background without fine-tuning. Extensive experiments demonstrate that our training-free method outperforms existing methods in both qualitative and quantitative evaluations, matching or surpassing fine-tuned models. Finally, we successfully extend it to other tasks (e.g., consistency models and text-to-video), highlighting its transformative potential across various generative applications where independent control of foreground and background is crucial.</p>
            <p id="subjects-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" onclick="foldPdfKimi('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="panel paper" keywords="shot,adaptation,pretraining,multimodal,representations,context,zero,test,significantly,across">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Roth_Context-Aware_Multimodal_Pretraining_CVPR_2025_paper.html" target="_blank" title="2/388"><span class="index notranslate">#2</span></a>
                <a id="title-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="title-link" href="/venue/Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" target="_blank">Context-Aware Multimodal Pretraining</a>
                <a id="pdf-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Roth_Context-Aware_Multimodal_Pretraining_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF">104</sup>]</a>
                <a id="copy-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF">56</sup>]</a>
                <a id="rel-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Karsten Roth" target="_blank">Karsten Roth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeynep Akata" target="_blank">Zeynep Akata</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dima Damen" target="_blank">Dima Damen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivana Balazevic" target="_blank">Ivana Balazevic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olivier J. Henaff" target="_blank">Olivier J. Henaff</a>
            </p>
            <p id="summary-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="summary">Large-scale multimodal representation learning successfully optimizes for zero-shot transfer at test time. Yet the standard pretraining paradigm (contrastive learning on large amounts of image-text data) does not explicitly encourage representations to support few-shot adaptation. In this work, we propose a simple, but carefully designed extension to multimodal pretraining which enables representations to accommodate additional context. Using this objective, we show that vision-language models can be trained to exhibit significantly increased few-shot adaptation: across 21 downstream tasks, we find up to four-fold improvements in test-time sample efficiency, and average few-shot adaptation gains of over 5\%, while retaining zero-shot generalization performance across model scales and training durations. In particular, equipped with simple, training-free, metric-based adaptation mechanisms, our representations surpass significantly more complex optimization-based adaptation schemes.</p>
            <p id="subjects-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" onclick="foldPdfKimi('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="panel paper" keywords="srgb,raw,pre,aodraw,detection,object,conditions,diverse,weather,training">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_RAW_Object_Detection_in_Diverse_Conditions_CVPR_2025_paper.html" target="_blank" title="3/388"><span class="index notranslate">#3</span></a>
                <a id="title-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="title-link" href="/venue/Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" target="_blank">Towards RAW Object Detection in Diverse Conditions</a>
                <a id="pdf-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Towards_RAW_Object_Detection_in_Diverse_Conditions_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF">69</sup>]</a>
                <a id="copy-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF">31</sup>]</a>
                <a id="rel-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhong-Yu Li" target="_blank">Zhong-Yu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Jin" target="_blank">Xin Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo-Yuan Sun" target="_blank">Bo-Yuan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun-Le Guo" target="_blank">Chun-Le Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Ming Cheng" target="_blank">Ming-Ming Cheng</a>
            </p>
            <p id="summary-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="summary">Existing object detection methods often consider sRGB input, which was compressed from RAW data using ISP originally designed for visualization. However, such compression might lose crucial information for detection, especially under complex light and weather conditions. We introduce the AODRaw dataset, which offers 7,785 high-resolution real RAW images with 135,601 annotated instances spanning 62 categories, capturing a broad range of indoor and outdoor scenes under 9 distinct light and weather conditions. Based on AODRaw that supports RAW and sRGB object detection, we provide a comprehensive benchmark for evaluating current detection methods. We find that sRGB pre-training constrains the potential of RAW object detection due to the domain gap between sRGB and RAW, prompting us to directly pre-train on the RAW domain. However, it is harder for RAW pre-training to learn rich representations than sRGB pre-training due to the camera noise. To assist RAW pre-training, we distill the knowledge from an off-the-shelf model pre-trained on the sRGB domain. As a result, we achieve substantial improvements under diverse and adverse conditions without relying on extra pre-processing modules. The code and dataset will be made publicly available.</p>
            <p id="subjects-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" onclick="foldPdfKimi('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="panel paper" keywords="pretraining,imagined,imaginefsl,shot,images,synthetic,supervised,base,self,clip">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based_CVPR_2025_paper.html" target="_blank" title="4/388"><span class="index notranslate">#4</span></a>
                <a id="title-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="title-link" href="/venue/Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" target="_blank">ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning</a>
                <a id="pdf-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF">56</sup>]</a>
                <a id="copy-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF">19</sup>]</a>
                <a id="rel-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyuan Yang" target="_blank">Haoyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoou Li" target="_blank">Xiaoou Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaming Lv" target="_blank">Jiaming Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianjun Cheng" target="_blank">Xianjun Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qilong Wang" target="_blank">Qilong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peihua Li" target="_blank">Peihua Li</a>
            </p>
            <p id="summary-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="summary">Adapting CLIP models for few-shot recognition has recently attracted significant attention. Despite considerable progress, these adaptations remain hindered by the pervasive challenge of data scarcity. Text-to-image models, capable of generating abundant photorealistic labeled images, offer a promising solution. However, existing approaches treat synthetic images merely as complements to real images, rather than as standalone knowledge repositories stemming from distinct foundation models. To overcome this limitation, we reconceptualize synthetic images as an *imagined base set*, i.e., a unique, large-scale synthetic dataset encompassing diverse concepts. We introduce a novel CLIP adaptation methodology called *ImagineFSL*, involving pretraining on the imagined base set followed by fine-tuning on downstream few-shot tasks. We find that, compared to no pretraining, both supervised and self-supervised pretraining are beneficial, with the latter providing better performance. Building on this finding, we propose an improved self-supervised method tailored for few-shot scenarios, enhancing the transferability of representations from synthetic to real image domains. Additionally, we present an image generation pipeline that employs chain-of-thought and in-context learning techniques, harnessing foundation models to automatically generate diverse, realistic images. Our methods are validated across eleven datasets, consistently outperforming state-of-the-art methods by substantial margins.</p>
            <p id="subjects-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" onclick="foldPdfKimi('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="panel paper" keywords="sinusoidal,frequencies,training,networks,tuning,neural,robust,inrs,capacity,tuner">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks_CVPR_2025_paper.html" target="_blank" title="5/388"><span class="index notranslate">#5</span></a>
                <a id="title-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="title-link" href="/venue/Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" target="_blank">Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks</a>
                <a id="pdf-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF">37</sup>]</a>
                <a id="copy-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF">18</sup>]</a>
                <a id="rel-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tiago Novello" target="_blank">Tiago Novello</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Diana Aldana" target="_blank">Diana Aldana</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andre Araujo" target="_blank">Andre Araujo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luiz Velho" target="_blank">Luiz Velho</a>
            </p>
            <p id="summary-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="summary">Sinusoidal neural networks have been shown effective as implicit neural representations (INRs) of low-dimensional signals, due to their smoothness and high representation capacity. However, initializing and training them remain empirical tasks which lack on deeper understanding to guide the learning process. To fill this gap, our work introduces a theoretical framework that explains the capacity property of sinusoidal networks and offers robust control mechanisms for initialization and training. Our analysis is based on a novel amplitude-phase expansion of the sinusoidal multilayer perceptron, showing how its layer compositions produce a large number of new frequencies expressed as integer combinations of the input frequencies. This relationship can be directly used to initialize the input neurons, as a form of spectral sampling, and to bound the network's spectrum while training. Our method, referred to as TUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and convergence of sinusoidal INR training, leading to detailed reconstructions, while preventing overfitting.</p>
            <p id="subjects-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" onclick="foldPdfKimi('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="panel paper" keywords="lip,speech,perceptually,talking,perceptual,synchronization,head,mesh,movements,representation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation_CVPR_2025_paper.html" target="_blank" title="6/388"><span class="index notranslate">#6</span></a>
                <a id="title-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="title-link" href="/venue/Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" target="_blank">Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</a>
                <a id="pdf-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF">23</sup>]</a>
                <a id="copy-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF">14</sup>]</a>
                <a id="rel-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lee Chae-Yeon" target="_blank">Lee Chae-Yeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oh Hyun-Bin" target="_blank">Oh Hyun-Bin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han EunGi" target="_blank">Han EunGi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kim Sung-Bin" target="_blank">Kim Sung-Bin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Suekyeong Nam" target="_blank">Suekyeong Nam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tae-Hyun Oh" target="_blank">Tae-Hyun Oh</a>
            </p>
            <p id="summary-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="summary">Recent advancements in speech-driven 3D talking head generation have achieved impressive advance in lip synchronization. However, existing models still fall short in capturing a perceptual alignment between diverse speech characteristics and lip movements. In this work, we define essential criteriatemporal synchronization, lip readability, and expressiveness for perceptually accurate lip movements in response to speech signals. We also introduce a speech-mesh synchronized representation that captures the intricate correspondence between speech and facial mesh. We plug in this representation as a perceptual loss to guide lip movements, ensuring they are perceptually aligned with the given speech. Additionally, we utilize this representation as a perceptual metric and introduce two other physically-grounded lip synchronization metrics to evaluate these three criteria. Experiments demonstrate that training 3D talking head models with our perceptual loss significantly enhances all three aspects of perceptually accurate lip synchronization. Codes will be released if accepted.</p>
            <p id="subjects-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" onclick="foldPdfKimi('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="panel paper" keywords="head,listening,motion,generation,256,512,expressiveness,realistic,hybrid,videos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling_CVPR_2025_paper.html" target="_blank" title="7/388"><span class="index notranslate">#7</span></a>
                <a id="title-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="title-link" href="/venue/Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" target="_blank">Diffusion-based Realistic Listening Head Generation via Hybrid Motion Modeling</a>
                <a id="pdf-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF">30</sup>]</a>
                <a id="copy-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF">18</sup>]</a>
                <a id="rel-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yinuo Wang" target="_blank">Yinuo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanbo Fan" target="_blank">Yanbo Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Wang" target="_blank">Xuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guo Yu" target="_blank">Guo Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Wang" target="_blank">Fei Wang</a>
            </p>
            <p id="summary-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="summary">Listening head generation aims to synthesize non-verbal responsive listening head videos that naturally react to a certain speaker, for which, both realistic head movements, expressive facial expressions, and high visual qualities are expected. Previous approaches typically follow a two-stage pipeline that first generates intermediate 3D motion signals such as 3DMM coefficients, and then synthesizes the videos by deterministic rendering, suffering from limited motion expressiveness and low visual quality (eg, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;256&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;256&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 5.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.586em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.48em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mn" id="MathJax-Span-3" style="font-family: MathJax_Main;">256</span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main; padding-left: 0.211em;"></span><span class="mn" id="MathJax-Span-5" style="font-family: MathJax_Main; padding-left: 0.211em;">256</span><span class="mo" id="MathJax-Span-6" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>256</mn><mo></mo><mn>256</mn><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">256 \times 256)</script>. In this work, we propose a novel listening head generation method that harnesses the generative capabilities of the diffusion model for both motion generation and high-quality rendering. Crucially, we propose an effective hybrid motion modeling module that addresses training difficulties caused by the scarcity of listening head data while preserving the intricate details that may be lost in explicit motion representations. We further develop a tailored control guidance for head pose and facial expression, by integrating their intrinsic motion characteristics. Our method enables high-fidelity video generation with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;512&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;512&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7" style="width: 5.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1004.17em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-8"><span class="mn" id="MathJax-Span-9" style="font-family: MathJax_Main;">512</span><span class="mo" id="MathJax-Span-10" style="font-family: MathJax_Main; padding-left: 0.211em;"></span><span class="mn" id="MathJax-Span-11" style="font-family: MathJax_Main; padding-left: 0.211em;">512</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>512</mn><mo></mo><mn>512</mn></math></span></span><script type="math/tex" id="MathJax-Element-2">512\times 512</script> resolution and delivers vivid listener motion feedback. We conduct comprehensive experiments and obtain superior performance in terms of both visual quality and motion expressiveness compared to existing methods.</p>
            <p id="subjects-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="panel paper" keywords="unsafe,content,unlearning,hyperbolic,vision,language,safe,safety,entailment,clip">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models_CVPR_2025_paper.html" target="_blank" title="8/388"><span class="index notranslate">#8</span></a>
                <a id="title-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" target="_blank">Hyperbolic Safety-Aware Vision-Language Models</a>
                <a id="pdf-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF">39</sup>]</a>
                <a id="copy-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF">22</sup>]</a>
                <a id="rel-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tobia Poppi" target="_blank">Tobia Poppi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tejaswi Kasarla" target="_blank">Tejaswi Kasarla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Mettes" target="_blank">Pascal Mettes</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Baraldi" target="_blank">Lorenzo Baraldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rita Cucchiara" target="_blank">Rita Cucchiara</a>
            </p>
            <p id="summary-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="summary">Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the models knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling  ineffective in standard vision-language models due to their reliance on Euclidean embeddings  endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition, but also establishes a more adaptable and interpretable framework for content moderation in vision-language models.</p>
            <p id="subjects-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="panel paper" keywords="xlrs,bench,mllms,ultra,8500,resolution,imagery,multimodal,remote,capabilities">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote_CVPR_2025_paper.html" target="_blank" title="9/388"><span class="index notranslate">#9</span></a>
                <a id="title-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="title-link" href="/venue/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" target="_blank">XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?</a>
                <a id="pdf-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF">40</sup>]</a>
                <a id="copy-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF">20</sup>]</a>
                <a id="rel-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fengxiang Wang" target="_blank">Fengxiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongzhen Wang" target="_blank">Hongzhen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zonghao Guo" target="_blank">Zonghao Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Wang" target="_blank">Di Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulin Wang" target="_blank">Yulin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingshuo Chen" target="_blank">Mingshuo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Ma" target="_blank">Qiang Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Long Lan" target="_blank">Long Lan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjing Yang" target="_blank">Wenjing Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Zhang" target="_blank">Jing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Liu" target="_blank">Zhiyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maosong Sun" target="_blank">Maosong Sun</a>
            </p>
            <p id="summary-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="summary">The astonishing breakthrough of multimodal large language models (MLLMs) has necessitated new benchmarks to quantitatively assess their capabilities, reveal their limitations, and indicate future research directions. However, this is challenging in the context of remote sensing (RS), since the imagery features ultra-high resolution that incorporates extremely complex semantic relationships. Existing benchmarks usually adopt notably smaller image sizes than real-world RS scenarios, suffer from limited annotation quality, and consider insufficient dimensions of evaluation. To address these issues, we present XLRS-Bench: a comprehensive benchmark for evaluating the perception and reasoning capabilities of MLLMs in ultra-high-resolution RS scenarios. XLRS-Bench boasts the largest average image size (8500<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-12" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-13"><span class="mo" id="MathJax-Span-14" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-3">\times</script>8500) observed thus far, with all evaluation samples meticulously annotated manually, assisted by a novel semi-automatic captioner on ultra-high-resolution RS images. On top of the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 6 kinds of perceptual abilities and 4 kinds of reasoning capabilities, with a primary emphasis on advanced cognitive processes that facilitate real-world decision-making and the capture of spatiotemporal changes. The results of both general and RS-focused MLLMs on XLRS-Bench indicate that further efforts are needed to enhance their performance in real RS scenarios. We will open source XLRS-Bench to support further research of developing more powerful MLLMs for RS.</p>
            <p id="subjects-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" onclick="foldPdfKimi('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="panel paper" keywords="personalized,language,personal,vision,retrieval,regularized,deepfashion2,rank,recognize,fido">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates_CVPR_2025_paper.html" target="_blank" title="10/388"><span class="index notranslate">#10</span></a>
                <a id="title-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="title-link" href="/venue/Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" target="_blank">Improving Personalized Search with Regularized Low-Rank Parameter Updates</a>
                <a id="pdf-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF">32</sup>]</a>
                <a id="copy-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fiona Ryan" target="_blank">Fiona Ryan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Josef Sivic" target="_blank">Josef Sivic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Caba Heilbron" target="_blank">Fabian Caba Heilbron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Judy Hoffman" target="_blank">Judy Hoffman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James M. Rehg" target="_blank">James M. Rehg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bryan Russell" target="_blank">Bryan Russell</a>
            </p>
            <p id="summary-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="summary">Personalized vision-language retrieval seeks to recognize new concepts (e.g. "my dog Fido'') from only a few examples. This task is challenging because it requires not only learning a new concept from a few images, but also integrating the personal and general knowledge together to recognize the concept in different contexts. In this paper, we show how to effectively adapt the internal representation of a vision-language dual encoder model for personalized vision-language retrieval. We find that regularized low-rank adaption of a small set of parameters in the language encoder's final layer serves as a highly effective alternative to textual inversion for recognizing the personal concept while preserving general knowledge. Additionally, we explore strategies for combining parameters of multiple learned personal concepts, finding that parameter addition is effective. To evaluate how well general knowledge is preserved in a finetuned representation, we introduce a metric that measures image retrieval accuracy based on captions generated by a vision language model (VLM). Our approach achieves state-of-the-art accuracy on two benchmarks for personalized image retrieval with natural language queries -- DeepFashion2 and ConConChi -- outperforming the prior art by 4%-22% on personal retrievals.</p>
            <p id="subjects-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" onclick="foldPdfKimi('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="panel paper" keywords="denoising,n3dnet,diffractive,image,nonlinear,deep,network,midd,termed,120k">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising_CVPR_2025_paper.html" target="_blank" title="11/388"><span class="index notranslate">#11</span></a>
                <a id="title-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="title-link" href="/venue/Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" target="_blank">All-Optical Nonlinear Diffractive Deep Network for Ultrafast Image Denoising</a>
                <a id="pdf-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF">34</sup>]</a>
                <a id="copy-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF">18</sup>]</a>
                <a id="rel-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoling Zhou" target="_blank">Xiaoling Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhemg Lee" target="_blank">Zhemg Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Ye" target="_blank">Wei Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Xie" target="_blank">Rui Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbo Zhang" target="_blank">Wenbo Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanju Peng" target="_blank">Guanju Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongze Li" target="_blank">Zongze Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shikun Zhang" target="_blank">Shikun Zhang</a>
            </p>
            <p id="summary-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="summary">Image denoising poses a significant challenge in image processing, aiming to remove noise and artifacts from input images. However, current denoising algorithms implemented on electronic chips frequently encounter latency issues and demand substantial computational resources. In this paper, we introduce an all-optical Nonlinear Diffractive Denoising Deep Network (N3DNet) for image denoising at the speed of light. Initially, we incorporate an image encoding and pre-denoising module into the Diffractive Deep Neural Network and integrate a nonlinear activation function, termed the phase exponential linear function, after each diffractive layer, thereby boosting the network's nonlinear modeling and denoising capabilities. Subsequently, we devise a new reinforcement learning algorithm called regularization-assisted deep Q-network to optimize N3DNet. Finally, leveraging 3D printing techniques, we fabricate N3DNet using the trained parameters and construct a physical experimental system for real-world applications. A new benchmark dataset, termed MIDD, is constructed for mode image denoising, comprising 120K pairs of noisy/noise-free images captured from real fiber communication systems across various transmission lengths. Through extensive simulation and real experiments, we validate that N3DNet outperforms both traditional and deep learning-based denoising approaches across various datasets. Remarkably, its processing speed is nearly 3,800 times faster than electronic chip-based methods.</p>
            <p id="subjects-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="panel paper" keywords="click,ntclick,segmentation,clicks,interactive,precise,tolerant,user,masks,textit">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks_CVPR_2025_paper.html" target="_blank" title="12/388"><span class="index notranslate">#12</span></a>
                <a id="title-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="title-link" href="/venue/Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" target="_blank">NTClick: Achieving Precise Interactive Segmentation With Noise-tolerant Clicks</a>
                <a id="pdf-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyi Zhang" target="_blank">Chenyi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ting Liu" target="_blank">Ting Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochao Qu" target="_blank">Xiaochao Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luoqi Liu" target="_blank">Luoqi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Zhao" target="_blank">Yao Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunchao Wei" target="_blank">Yunchao Wei</a>
            </p>
            <p id="summary-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="summary">Interactive segmentation is a pivotal task in computer vision, focused on predicting precise masks with minimal user input. Although the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;click&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-15" style="width: 2.346em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.98em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-16"><span class="texatom" id="MathJax-Span-17"><span class="mrow" id="MathJax-Span-18"><span class="mtext" id="MathJax-Span-19" style="font-family: MathJax_Main-italic;">click</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">click</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4"> \textit{click}</script> has recently become the most prevalent form of interaction due to its flexibility and efficiency, its advantages diminish as the complexity and details of target objects increase because it's time-consuming and user-unfriendly to precisely locate and click on narrow, fine regions. To tackle this problem, we propose NTClick, a powerful click-based interactive segmentation method capable of predicting accurate masks even with imprecise user clicks when dealing with intricate targets. We first introduce a novel interaction form called <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext class=&quot;MJX-tex-mathit&quot; mathvariant=&quot;italic&quot;&gt;Noist-tolerant Click&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-20" style="width: 10.211em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.492em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1008.54em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-21"><span class="texatom" id="MathJax-Span-22"><span class="mrow" id="MathJax-Span-23"><span class="mtext" id="MathJax-Span-24" style="font-family: MathJax_Main-italic;">Noist-tolerant Click</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext class="MJX-tex-mathit" mathvariant="italic">Noist-tolerant Click</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-5"> \textit{Noist-tolerant Click}</script>, a type of click that does not require user's precise localization when selecting fine regions. Then, we design a two-stage workflow, consisting of an Explicit Coarse Perception network for initial estimation and a High Resolution Refinement network for final classification. Quantitative results across extensive datasets demonstrate that NTClick not only maintains an efficient and flexible interaction mode but also significantly outperforms existing methods in segmentation accuracy.</p>
            <p id="subjects-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="panel paper" keywords="optimization,lic,distortion,balanced,compression,rate,learned,updates,objective,moo">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression_CVPR_2025_paper.html" target="_blank" title="13/388"><span class="index notranslate">#13</span></a>
                <a id="title-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" target="_blank">Balanced Rate-Distortion Optimization in Learned Image Compression</a>
                <a id="pdf-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF">20</sup>]</a>
                <a id="copy-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yichi Zhang" target="_blank">Yichi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Duan" target="_blank">Zhihao Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuning Huang" target="_blank">Yuning Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengqing Zhu" target="_blank">Fengqing Zhu</a>
            </p>
            <p id="summary-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="summary">Learned image compression (LIC) using deep learning architectures has seen significant advancements, yet standard rate-distortion (R-D) optimization often encounters imbalanced updates due to diverse gradients of the rate and distortion objectives. This imbalance can lead to suboptimal optimization, where one objective dominates, thereby reducing overall compression efficiency. To address this challenge, we reformulate R-D optimization as a multi-objective optimization (MOO) problem and introduce two balanced R-D optimization strategies that adaptively adjust gradient updates to achieve more equitable improvements in both rate and distortion. The first proposed strategy utilizes a coarse-to-fine gradient descent approach along standard R-D optimization trajectories, making it particularly suitable for training LIC models from scratch. The second proposed strategy analytically addresses the reformulated optimization as a quadratic programming problem with an equality constraint, which is ideal for fine-tuning existing models. Experimental results demonstrate that both proposed methods enhance the R-D performance of LIC models, achieving around a 2\% BD-Rate reduction with acceptable additional training cost, leading to a more balanced and efficient optimization process. The code will be made publicly available.</p>
            <p id="subjects-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="panel paper" keywords="egoallo,hand,frame,motion,ego,sensed,estimation,head,allocentric,body">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World_CVPR_2025_paper.html" target="_blank" title="14/388"><span class="index notranslate">#14</span></a>
                <a id="title-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="title-link" href="/venue/Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" target="_blank">Estimating Body and Hand Motion in an Ego-sensed World</a>
                <a id="pdf-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Brent Yi" target="_blank">Brent Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vickie Ye" target="_blank">Vickie Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maya Zheng" target="_blank">Maya Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunqi Li" target="_blank">Yunqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lea Mller" target="_blank">Lea Mller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Pavlakos" target="_blank">Georgios Pavlakos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Ma" target="_blank">Yi Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jitendra Malik" target="_blank">Jitendra Malik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angjoo Kanazawa" target="_blank">Angjoo Kanazawa</a>
            </p>
            <p id="summary-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="summary">We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%.</p>
            <p id="subjects-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" onclick="foldPdfKimi('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="panel paper" keywords="view,viewpoint,instructional,best,informative,multi,supervising,video,camera,language">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View_CVPR_2025_paper.html" target="_blank" title="15/388"><span class="index notranslate">#15</span></a>
                <a id="title-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="title-link" href="/venue/Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" target="_blank">Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</a>
                <a id="pdf-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF">19</sup>]</a>
                <a id="copy-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sagnik Majumder" target="_blank">Sagnik Majumder</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tushar Nagarajan" target="_blank">Tushar Nagarajan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziad Al-Halah" target="_blank">Ziad Al-Halah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Reina Pradhan" target="_blank">Reina Pradhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristen Grauman" target="_blank">Kristen Grauman</a>
            </p>
            <p id="summary-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="summary">Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive best-view" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view videono language or camera posesand returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation.</p>
            <p id="subjects-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" onclick="foldPdfKimi('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="panel paper" keywords="ood,object,counts,shifts,detectors,oodg,mllms,distributional,grounding,generalization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under_CVPR_2025_paper.html" target="_blank" title="16/388"><span class="index notranslate">#16</span></a>
                <a id="title-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="title-link" href="/venue/Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" target="_blank">COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts</a>
                <a id="pdf-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiansheng Li" target="_blank">Jiansheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingxuan Zhang" target="_blank">Xingxuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zou" target="_blank">Hao Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yige Guo" target="_blank">Yige Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renzhe Xu" target="_blank">Renzhe Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilong Liu" target="_blank">Yilong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuzhao Zhu" target="_blank">Chuzhao Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue He" target="_blank">Yue He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Cui" target="_blank">Peng Cui</a>
            </p>
            <p id="summary-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="summary">Current object detectors often suffer significant performance degradation in real-world applications when encountering distributional shifts, posing serious risks in high-stakes domains such as autonomous driving and medical diagnosis. Consequently, the out-of-distribution (OOD) generalization capability of object detectors has garnered increasing attention from researchers. Despite this growing interest, there remains a lack of a large-scale, comprehensive dataset and evaluation benchmark with fine-grained annotations tailored to assess the OOD generalization on more intricate tasks like object detection and grounding. To address this gap, we introduce COUNTS, a large-scale OOD dataset with object-level annotations. COUNTS encompasses 14 natural distributional shifts, over 222K samples, and more than 1,196K labeled bounding boxes. Leveraging COUNTS, we introduce two novel benchmarks: O(OD) and OODG. OODOD is designed to comprehensively evaluate the OOD generalization capabilities of object detectors by utilizing controlled distribution shifts between training and testing data. OODG, on the other hand, aims to assess the OOD generalization of grounding abilities in multimodal large language models (MLLMs). Our findings reveal that, while large models and extensive pre-training data substantially enhance performance in in-distribution (IID) scenarios, significant limitations and opportunities for improvement persist in OOD contexts for both object detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o and Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope COUNTS facilitates advancements in the development and assessment of robust object detectors and MLLMs capable of maintaining high performance under distributional shifts.</p>
            <p id="subjects-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" onclick="foldPdfKimi('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="panel paper" keywords="textbf,esc,knowledge,nowledge,erasing,forgetting,rasing,apprehensive,oncept,concerns">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion_CVPR_2025_paper.html" target="_blank" title="17/388"><span class="index notranslate">#17</span></a>
                <a id="title-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="title-link" href="/venue/Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" target="_blank">ESC: Erasing Space Concept for Knowledge Deletion</a>
                <a id="pdf-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF">22</sup>]</a>
                <a id="copy-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tae-Young Lee" target="_blank">Tae-Young Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sundong Park" target="_blank">Sundong Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minwoo Jeon" target="_blank">Minwoo Jeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyoseok Hwang" target="_blank">Hyoseok Hwang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gyeong-Moon Park" target="_blank">Gyeong-Moon Park</a>
            </p>
            <p id="summary-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="summary">As concerns regarding privacy in deep learning continue to grow, individuals are increasingly apprehensive about the potential exploitation of their personal knowledge in trained models. Despite several research efforts to address this, they often fail to consider the real-world demand from users for complete knowledge erasure. Furthermore, our investigation reveals that existing methods have a risk of leaking personal knowledge through embedding features. To address these issues, we introduce a novel concept of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;K&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-25" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-26"><span class="texatom" id="MathJax-Span-27"><span class="mrow" id="MathJax-Span-28"><span class="mtext" id="MathJax-Span-29" style="font-family: MathJax_Main-bold;">K</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">K</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-6">\textbf{K}</script>nowledge <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;D&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-30" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-31"><span class="texatom" id="MathJax-Span-32"><span class="mrow" id="MathJax-Span-33"><span class="mtext" id="MathJax-Span-34" style="font-family: MathJax_Main-bold;">D</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">D</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7">\textbf{D}</script>eletion (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;KD&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-35" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.72em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-36"><span class="texatom" id="MathJax-Span-37"><span class="mrow" id="MathJax-Span-38"><span class="mtext" id="MathJax-Span-39" style="font-family: MathJax_Main-bold;">KD</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">KD</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-8">\textbf{KD}</script>), an advanced task that considers both concerns, and provides an appropriate metric, named <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;K&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-40" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-41"><span class="texatom" id="MathJax-Span-42"><span class="mrow" id="MathJax-Span-43"><span class="mtext" id="MathJax-Span-44" style="font-family: MathJax_Main-bold;">K</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">K</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-9">\textbf{K}</script>nowledge <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;R&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-45" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.89em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-46"><span class="texatom" id="MathJax-Span-47"><span class="mrow" id="MathJax-Span-48"><span class="mtext" id="MathJax-Span-49" style="font-family: MathJax_Main-bold;">R</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">R</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-10">\textbf{R}</script>etention score (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;KR&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-50" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.77em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-51"><span class="texatom" id="MathJax-Span-52"><span class="mrow" id="MathJax-Span-53"><span class="mtext" id="MathJax-Span-54" style="font-family: MathJax_Main-bold;">KR</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">KR</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-11">\textbf{KR}</script>), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;E&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-55" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-56"><span class="texatom" id="MathJax-Span-57"><span class="mrow" id="MathJax-Span-58"><span class="mtext" id="MathJax-Span-59" style="font-family: MathJax_Main-bold;">E</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">E</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-12">\textbf{E}</script>rasing <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;S&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-60" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-61"><span class="texatom" id="MathJax-Span-62"><span class="mrow" id="MathJax-Span-63"><span class="mtext" id="MathJax-Span-64" style="font-family: MathJax_Main-bold;">S</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">S</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-13">\textbf{S}</script>pace <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;C&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-65" style="width: 1.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-66"><span class="texatom" id="MathJax-Span-67"><span class="mrow" id="MathJax-Span-68"><span class="mtext" id="MathJax-Span-69" style="font-family: MathJax_Main-bold;">C</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">C</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-14">\textbf{C}</script>oncept (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;ESC&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-70" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.19em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-71"><span class="texatom" id="MathJax-Span-72"><span class="mrow" id="MathJax-Span-73"><span class="mtext" id="MathJax-Span-74" style="font-family: MathJax_Main-bold;">ESC</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">ESC</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-15">\textbf{ESC}</script>), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;ESC&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-75" style="width: 2.711em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.19em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-76"><span class="texatom" id="MathJax-Span-77"><span class="mrow" id="MathJax-Span-78"><span class="mtext" id="MathJax-Span-79" style="font-family: MathJax_Main-bold;">ESC</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">ESC</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-16">\textbf{ESC}</script> with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;T&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-80" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-81"><span class="texatom" id="MathJax-Span-82"><span class="mrow" id="MathJax-Span-83"><span class="mtext" id="MathJax-Span-84" style="font-family: MathJax_Main-bold;">T</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">T</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17">\textbf{T}</script>raining (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;ESC-T&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-85" style="width: 4.065em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.34em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-86"><span class="texatom" id="MathJax-Span-87"><span class="mrow" id="MathJax-Span-88"><span class="mtext" id="MathJax-Span-89" style="font-family: MathJax_Main-bold;">ESC-T</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">ESC-T</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-18">\textbf{ESC-T}</script>), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD. Our extensive experiments on various datasets and models demonstrate that our proposed methods achieve the fastest and state-of-the-art performance. Notably, our methods are applicable to diverse forgetting scenarios, such as facial domain setting, demonstrating the generalizability of our methods.</p>
            <p id="subjects-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" onclick="foldPdfKimi('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="panel paper" keywords="panoptic,unsupervised,centric,scene,segmentation,training,scenes,pseudo,complex,object">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation_CVPR_2025_paper.html" target="_blank" title="18/388"><span class="index notranslate">#18</span></a>
                <a id="title-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" target="_blank">Scene-Centric Unsupervised Panoptic Segmentation</a>
                <a id="pdf-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF">31</sup>]</a>
                <a id="copy-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Oliver Hahn" target="_blank">Oliver Hahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Reich" target="_blank">Christoph Reich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikita Araslanov" target="_blank">Nikita Araslanov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Cremers" target="_blank">Daniel Cremers</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Rupprecht" target="_blank">Christian Rupprecht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Roth" target="_blank">Stefan Roth</a>
            </p>
            <p id="summary-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="summary">Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.</p>
            <p id="subjects-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="panel paper" keywords="blendshapes,3dmm,rgbavatar,blendshape,avatars,reconstruction,gaussian,head,reduced,fly">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars_CVPR_2025_paper.html" target="_blank" title="19/388"><span class="index notranslate">#19</span></a>
                <a id="title-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="title-link" href="/venue/Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" target="_blank">RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars</a>
                <a id="pdf-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF">19</sup>]</a>
                <a id="copy-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Linzhou Li" target="_blank">Linzhou Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yumeng Li" target="_blank">Yumeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanlin Weng" target="_blank">Yanlin Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youyi Zheng" target="_blank">Youyi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhou" target="_blank">Kun Zhou</a>
            </p>
            <p id="summary-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="summary">We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for reconstructing photorealistic, animatable head avatars at speeds sufficient for on-the-fly reconstruction. Unlike prior approaches that utilize linear bases from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to a compact set of blendshape bases. The learned compact base composition effectively captures essential facial details for specific individuals, and does not rely on the fixed base composition weights of 3DMM, leading to enhanced reconstruction quality and higher efficiency. To further expedite the reconstruction process, we develop a novel color initialization estimation method and a batch-parallel Gaussian rasterization process, achieving state-of-the-art quality with training throughput of about 630 images per second. Moreover, we propose a local-global sampling strategy that enables direct on-the-fly reconstruction, immediately reconstructing the model as video streams in real time while achieving quality comparable to offline settings.</p>
            <p id="subjects-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" onclick="foldPdfKimi('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="panel paper" keywords="mitracker,view,tracking,mvot,object,mvtrack,multi,integration,gmtd,234k">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking_CVPR_2025_paper.html" target="_blank" title="20/388"><span class="index notranslate">#20</span></a>
                <a id="title-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="title-link" href="/venue/Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" target="_blank">MITracker: Multi-View Integration for Visual Object Tracking</a>
                <a id="pdf-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF">29</sup>]</a>
                <a id="copy-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mengjie Xu" target="_blank">Mengjie Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yitao Zhu" target="_blank">Yitao Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haotian Jiang" target="_blank">Haotian Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaming Li" target="_blank">Jiaming Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenrong Shen" target="_blank">Zhenrong Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Wang" target="_blank">Sheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haolin Huang" target="_blank">Haolin Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyu Wang" target="_blank">Xinyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Zhang" target="_blank">Han Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Yang" target="_blank">Qing Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Wang" target="_blank">Qian Wang</a>
            </p>
            <p id="summary-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="summary">Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a birds eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance.</p>
            <p id="subjects-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" onclick="foldPdfKimi('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="panel paper" keywords="videos,see3d,creation,visual,got,video,scale,priors,pose,generation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ma_You_See_it_You_Got_it_Learning_3D_Creation_on_CVPR_2025_paper.html" target="_blank" title="21/388"><span class="index notranslate">#21</span></a>
                <a id="title-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="title-link" href="/venue/Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" target="_blank">You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale</a>
                <a id="pdf-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_You_See_it_You_Got_it_Learning_3D_Creation_on_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF">25</sup>]</a>
                <a id="copy-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Baorui Ma" target="_blank">Baorui Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huachen Gao" target="_blank">Huachen Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoge Deng" target="_blank">Haoge Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengxiong Luo" target="_blank">Zhengxiong Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejun Huang" target="_blank">Tiejun Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lulu Tang" target="_blank">Lulu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlong Wang" target="_blank">Xinlong Wang</a>
            </p>
            <p id="summary-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="summary">Recent 3D generation models typically rely on limited-scale 3D `gold-labels' or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, a visual-conditional multi-view diffusion model trained on large-scale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data --- You See it, You Got it. To achieve this, we first scale up the training data using a proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in a high-quality, richly diverse, large-scale dataset of multi-view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - a purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce a novel visual-conditional 3D generation framework by integrating See3D into a warping-based pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on cost-effective and scalable video data, achieves notable zero-shot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Additionally, our model naturally supports other image-conditioned 3D creation tasks, such as 3D editing, without further fine-tuning.</p>
            <p id="subjects-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" onclick="foldPdfKimi('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="panel paper" keywords="object,human,affordance,parsing,objects,interactanything,shot,synthesis,feedback,unseen">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and_CVPR_2025_paper.html" target="_blank" title="22/388"><span class="index notranslate">#22</span></a>
                <a id="title-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="title-link" href="/venue/Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" target="_blank">InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing</a>
                <a id="pdf-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF">23</sup>]</a>
                <a id="copy-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinlu Zhang" target="_blank">Jinlu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixin Chen" target="_blank">Yixin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zan Wang" target="_blank">Zan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Yang" target="_blank">Jie Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Wang" target="_blank">Yizhou Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Huang" target="_blank">Siyuan Huang</a>
            </p>
            <p id="summary-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="summary">Recent advances in 3D human-centric generation have made significant progress. However, existing methods still struggle with generating novel Human-Object Interactions (HOIs), particularly for open-set objects. We identify three main challenges of this task: precise human object relation reasoning, adaptive affordance parsing for unseen objects, and realistic human pose synthesis that aligns with the description and 3D object geometry. In this work, we propose a novel zero-shot 3D HOI generation framework, leveraging the knowledge from large-scale pretrained models without training from specific datasets. More specifically, we first generate an initial human pose by sampling multiple hypotheses through multi-view SDS based on the input text and object geometry. We then utilize a pre-trained 2D image diffusion model to parse unseen objects and extract contact points, avoiding the limitations imposed by existing 3D asset knowledge. Finally, we introduce a detailed optimization to generate fine-grained, precise and natural interaction, enforcing realistic 3D contact between the involved body parts, including hands in grasp, and 3D object. This is achieved by distilling relational feedback from LLMs to capture detailed human-object relations from the text inputs. Extensive experiments validate the effectiveness of our approach compared to prior work, particularly in terms of the fine-grained nature of interactions and the ability to handle open-set 3D objects.</p>
            <p id="subjects-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="panel paper" keywords="anomaly,zsad,reasoning,onevision,mllms,visual,multimodal,detection,shot,125k">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language_CVPR_2025_paper.html" target="_blank" title="23/388"><span class="index notranslate">#23</span></a>
                <a id="title-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="title-link" href="/venue/Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" target="_blank">Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models</a>
                <a id="pdf-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF">45</sup>]</a>
                <a id="copy-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF">15</sup>]</a>
                <a id="rel-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacong Xu" target="_blank">Jiacong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shao-Yuan Lo" target="_blank">Shao-Yuan Lo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bardia Safaei" target="_blank">Bardia Safaei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vishal M. Patel" target="_blank">Vishal M. Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Isht Dwivedi" target="_blank">Isht Dwivedi</a>
            </p>
            <p id="summary-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="summary">Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in anomaly detection and reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&amp;R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning, based on LLaVA-OneVision. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens for its LLM. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Furthermore, extensions to medical and 3D anomaly reasoning are provided for future study.</p>
            <p id="subjects-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" onclick="foldPdfKimi('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="panel paper" keywords="4real,videoscore,video,viewpoint,frames,synchronization,dust3r,stream,updates,layer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion_CVPR_2025_paper.html" target="_blank" title="24/388"><span class="index notranslate">#24</span></a>
                <a id="title-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" target="_blank">4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion</a>
                <a id="pdf-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF">21</sup>]</a>
                <a id="copy-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoyang Wang" target="_blank">Chaoyang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiye Zhuang" target="_blank">Peiye Zhuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuan Duc Ngo" target="_blank">Tuan Duc Ngo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Willi Menapace" target="_blank">Willi Menapace</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aliaksandr Siarohin" target="_blank">Aliaksandr Siarohin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Vasilkovsky" target="_blank">Michael Vasilkovsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivan Skorokhodov" target="_blank">Ivan Skorokhodov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Tulyakov" target="_blank">Sergey Tulyakov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Wonka" target="_blank">Peter Wonka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hsin-Ying Lee" target="_blank">Hsin-Ying Lee</a>
            </p>
            <p id="summary-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="summary">We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a newly designed synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization.This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore, GIM-Confidence, and Dust3R-Confidence).</p>
            <p id="subjects-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
        <div id="Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="panel paper" keywords="pruning,sparsity,icp,tuning,fine,mid,levels,compensation,full,immediate">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity_CVPR_2025_paper.html" target="_blank" title="25/388"><span class="index notranslate">#25</span></a>
                <a id="title-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="title-link" href="/venue/Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" target="_blank">ICP: Immediate Compensation Pruning for Mid-to-high Sparsity</a>
                <a id="pdf-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Luo" target="_blank">Xin Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueming Fu" target="_blank">Xueming Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihang Jiang" target="_blank">Zihang Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=S. Kevin Zhou" target="_blank">S. Kevin Zhou</a>
            </p>
            <p id="summary-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="summary">The increasing adoption of large-scale models under 7 billion parameters in both language and vision domains enables inference tasks on a single consumer-grade GPU but makes fine-tuning models of this scale, especially 7B models, challenging. This limits the applicability of pruning methods that require full fine-tuning. Meanwhile, pruning methods that do not require fine-tuning perform well at low sparsity levels (10%-50%) but struggle at mid-to-high sparsity levels (50%-70%), where the error behaves equivalently to that of semi-structured pruning. To address these issues, this paper introduces ICP, which finds a balance between full fine-tuning and zero fine-tuning. First, Sparsity Rearrange is used to reorganize the predefined sparsity levels, followed by Block-wise Compensate Pruning, which alternates pruning and compensation on the models backbone, fully utilizing inference results while avoiding full model fine-tuning. Experiments show that ICP improves performance at mid-to-high sparsity levels compared to baselines, with only a slight increase in pruning time and no additional peak memory overhead.</p>
            <p id="subjects-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" onclick="foldPdfKimi('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF', this)" class="hr hr-fold">
        </div>
    <div id="Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="panel paper" keywords="i2v,motion,controllability,extrapolating,video,inject,decoupling,condition,textual,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier_CVPR_2025_paper.html" target="_blank" title="26/388"><span class="index notranslate">#26</span></a>
                <a id="title-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="title-link" href="/venue/Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" target="_blank">Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</a>
                <a id="pdf-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF">30</sup>]</a>
                <a id="copy-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF">16</sup>]</a>
                <a id="rel-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Tian" target="_blank">Jie Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoye Qu" target="_blank">Xiaoye Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyi Lu" target="_blank">Zhenyi Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Wei" target="_blank">Wei Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sichen Liu" target="_blank">Sichen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Cheng" target="_blank">Yu Cheng</a>
            </p>
            <p id="summary-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="summary">Image-to-Video (I2V) generation aims to synthesize a video clip according to a given image and condition (e.g., text). The key challenge of this task lies in simultaneously generating natural motions while preserving the original appearance of the images.However, current I2V diffusion models (I2V-DMs) often produce videos with limited motion degrees or exhibit uncontrollable motion that conflicts with the textual condition. In this paper, we propose a novel Extrapolating and Decoupling framework to mitigate these issues. Specifically, our framework consists of three separate stages:(1) Starting with a base I2V-DM, we explicitly inject the textual condition into the temporal module using a lightweight, learnable adapter and fine-tune the integrated model to improve motion controllability. (2) We introduce a training-free extrapolation strategy to amplify the dynamic range of the motion, effectively reversing the fine-tuning process to enhance the motion degree significantly.(3) With the above two-stage models excelling in motion controllability and motion degree, we decouple the relevant parameters associated with each type of motion ability and inject them into the base I2V-DM. Since the I2V-DM handles different levels of motion controllability and dynamics at various denoising time steps, we adjust the motion-aware parameters accordingly over time. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of our framework over existing methods.</p>
            <p id="subjects-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" onclick="foldPdfKimi('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="panel paper" keywords="mllms,fireplace,geometric,reasoning,placements,sense,placement,scene,common,object">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D_CVPR_2025_paper.html" target="_blank" title="27/388"><span class="index notranslate">#27</span></a>
                <a id="title-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="title-link" href="/venue/Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" target="_blank">FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement</a>
                <a id="pdf-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF">17</sup>]</a>
                <a id="copy-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ian Huang" target="_blank">Ian Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanan Bao" target="_blank">Yanan Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karen Truong" target="_blank">Karen Truong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Howard Zhou" target="_blank">Howard Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cordelia Schmid" target="_blank">Cordelia Schmid</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas Guibas" target="_blank">Leonidas Guibas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alireza Fathi" target="_blank">Alireza Fathi</a>
            </p>
            <p id="summary-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="summary">Scene generation with 3D assets presents a complex challenge, requiring both high-level semantic understanding and low-level geometric reasoning. While Multimodal Large Language Models (MLLMs) excel at semantic tasks, their application to 3D scene generation is hindered by their limited grounding on 3D geometry. In this paper, we investigate how to best work with MLLMs in an object placement task. Towards this goal, we introduce a novel framework, FirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the extraction of relevant geometric details from the 3D scene, (2) constructing and solving geometric constraints on the extracted low-level geometry, and (3) pruning for final placements that conform to common sense. By combining geometric reasoning with real-world understanding of MLLMs, our method can propose object placements that satisfy both geometric constraints as well as high-level semantic common-sense considerations. Our experiments show that these capabilities allow our method to place objects more effectively in complex scenes with intricate geometry, surpassing the quality of prior work.</p>
            <p id="subjects-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" onclick="foldPdfKimi('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="panel paper" keywords="edits,triplane,editing,aware,triplanes,reference,image,gans,eg3d,latent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes_CVPR_2025_paper.html" target="_blank" title="28/388"><span class="index notranslate">#28</span></a>
                <a id="title-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="title-link" href="/venue/Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" target="_blank">Reference-Based 3D-Aware Image Editing with Triplanes</a>
                <a id="pdf-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF">20</sup>]</a>
                <a id="copy-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bahri Batuhan Bilecen" target="_blank">Bahri Batuhan Bilecen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yigit Yalin" target="_blank">Yigit Yalin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Yu" target="_blank">Ning Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aysegul Dundar" target="_blank">Aysegul Dundar</a>
            </p>
            <p id="summary-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="summary">Generative Adversarial Networks (GANs) have emerged as powerful tools for high-quality image generation and real image editing by manipulating their latent spaces. Recent advancements in GANs include 3D-aware models such as EG3D, which feature efficient triplane-based architectures capable of reconstructing 3D geometry from single images. However, limited attention has been given to providing an integrated framework for 3D-aware, high-quality, reference-based image editing. This study addresses this gap by exploring and demonstrating the effectiveness of the triplane space for advanced reference-based edits. Our novel approach integrates encoding, automatic localization, spatial disentanglement of triplane features, and fusion learning to achieve the desired edits. Additionally, our framework demonstrates versatility and robustness across various domains, extending its effectiveness to animal face edits, partially stylized edits like cartoon faces, full-body clothing edits, and 360-degree head edits. Our method shows state-of-the-art performance over relevant latent direction, text, and image-guided 2D and 3D-aware diffusion and GAN methods, both qualitatively and quantitatively.</p>
            <p id="subjects-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" onclick="foldPdfKimi('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="panel paper" keywords="splatting,convexes,3dgs,3dcs,primitives,radiance,gaussians,rasterizer,convex,rendering">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes_CVPR_2025_paper.html" target="_blank" title="29/388"><span class="index notranslate">#29</span></a>
                <a id="title-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="title-link" href="/venue/Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" target="_blank">3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes</a>
                <a id="pdf-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF">24</sup>]</a>
                <a id="copy-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF">11</sup>]</a>
                <a id="rel-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Held" target="_blank">Jan Held</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renaud Vandeghen" target="_blank">Renaud Vandeghen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abdullah Hamdi" target="_blank">Abdullah Hamdi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adrien Deliege" target="_blank">Adrien Deliege</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anthony Cioppa" target="_blank">Anthony Cioppa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Silvio Giancola" target="_blank">Silvio Giancola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bernard Ghanem" target="_blank">Bernard Ghanem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Van Droogenbroeck" target="_blank">Marc Van Droogenbroeck</a>
            </p>
            <p id="summary-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="summary">Recent advances in radiance field reconstruction, such as 3D Gaussian Splatting (3DGS), have achieved high-quality novel view synthesis and fast rendering by representing scenes with compositions of Gaussian primitives. However, 3D Gaussians present several limitations for scene reconstruction. Accurately capturing hard edges is challenging without significantly increasing the number of Gaussians, creating a large memory footprint. Moreover, they struggle to represent flat surfaces, as they are diffused in space. Without hand-crafted regularizers, they tend to disperse irregularly around the actual surface. To circumvent these issues, we introduce a novel method, named 3D Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for modeling geometrically-meaningful radiance fields from multi-view images. Smooth convex shapes offer greater flexibility than Gaussians, allowing for a better representation of 3D scenes with hard edges and dense volumes using fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and Temples, and Deep Blending. Specifically, our method attains an improvement of up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high rendering speeds and reducing the number of required primitives. Our results highlight the potential of 3D Convex Splatting to become the new standard for high-quality scene reconstruction and novel view synthesis. We will publicly release our code and CUDA rasterizer.</p>
            <p id="subjects-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" onclick="foldPdfKimi('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="panel paper" keywords="arcpro,architectural,programs,abstraction,abstractions,feedforward,structured,sparse,decoder,points">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points_CVPR_2025_paper.html" target="_blank" title="30/388"><span class="index notranslate">#30</span></a>
                <a id="title-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="title-link" href="/venue/Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" target="_blank">ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points</a>
                <a id="pdf-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qirui Huang" target="_blank">Qirui Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runze Zhang" target="_blank">Runze Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kangjun Liu" target="_blank">Kangjun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minglun Gong" target="_blank">Minglun Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhang" target="_blank">Hao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Huang" target="_blank">Hui Huang</a>
            </p>
            <p id="summary-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="summary">We introduce ArcPro, a novel learning framework built on architectural programs to recover structured 3D abstractions from highly sparse and low-quality point clouds. Specifically, we design a domain-specific language (DSL) to hierarchically represent building structures as a program, which can be efficiently converted into a mesh. We bridge feedforward and inverse procedural modeling by using a feedforward process for training data synthesis, allowing the network to make reverse predictions. We train an encoder-decoder on the points-program pairs to establish a mapping from unstructured point clouds to architectural programs, where a 3D convolutional encoder extracts point cloud features and a transformer decoder autoregressively predicts the programs in a tokenized form. Inference by our method is highly efficient and produces plausible and faithful 3D abstractions. Comprehensive experiments demonstrate that ArcPro outperforms both traditional architectural proxy reconstruction and learning-based abstraction methods. We further explore its potential when working with multi-view image and natural language inputs.</p>
            <p id="subjects-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" onclick="foldPdfKimi('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="panel paper" keywords="vdms,generating,vdm,genvdm,displacement,maps,image,vector,stamps,details">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image_CVPR_2025_paper.html" target="_blank" title="31/388"><span class="index notranslate">#31</span></a>
                <a id="title-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="title-link" href="/venue/Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" target="_blank">GenVDM: Generating Vector Displacement Maps From a Single Image</a>
                <a id="pdf-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuezhi Yang" target="_blank">Yuezhi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qimin Chen" target="_blank">Qimin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vladimir G. Kim" target="_blank">Vladimir G. Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddhartha Chaudhuri" target="_blank">Siddhartha Chaudhuri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixing Huang" target="_blank">Qixing Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqin Chen" target="_blank">Zhiqin Chen</a>
            </p>
            <p id="summary-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="summary">We introduce the first method for generating Vector Displacement Maps (VDMs): parameterized, detailed geometric stamps commonly used in 3D modeling. Given a single input image, our method first generates multi-view normal maps and then reconstructs a VDM from the normals via a novel reconstruction pipeline. We also propose an efficient algorithm for extracting VDMs from 3D objects, and present the first academic VDM dataset. Compared to existing 3D generative models focusing on complete shapes, we focus on generating parts that can be seamlessly attached to shape surfaces. The method gives artists rich control over adding geometric details to a 3D shape. Experiments demonstrate that our approach outperforms existing baselines. Generating VDMs offers additional benefits, such as using 2D image editing to customize and refine 3D details.</p>
            <p id="subjects-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" onclick="foldPdfKimi('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="panel paper" keywords="crossover,scene,modal,object,modality,modalities,alignment,cross,3rscan,understanding">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment_CVPR_2025_paper.html" target="_blank" title="32/388"><span class="index notranslate">#32</span></a>
                <a id="title-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="title-link" href="/venue/Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" target="_blank">CrossOver: 3D Scene Cross-Modal Alignment</a>
                <a id="pdf-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF">21</sup>]</a>
                <a id="copy-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF">10</sup>]</a>
                <a id="rel-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sayan Deb Sarkar" target="_blank">Sayan Deb Sarkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ondrej Miksik" target="_blank">Ondrej Miksik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Barath" target="_blank">Daniel Barath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Iro Armeni" target="_blank">Iro Armeni</a>
            </p>
            <p id="summary-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="summary">Multi-modal 3D object understanding has gained significant attention, yet current approaches often rely on rigid object-level modality alignment or assume complete data availability across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require paired data for every object instance, CrossOver learns a unified, modality-agnostic embedding space for scenes by aligning modalitiesRGB images, point clouds, CAD models, floorplans, and text descriptionswithout explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting CrossOver's adaptability for real-world applications in 3D scene understanding.</p>
            <p id="subjects-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" onclick="foldPdfKimi('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="panel paper" keywords="hallucination,video,tokens,mash,scene,llms,vlm,spatial,temporal,action">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations_CVPR_2025_paper.html" target="_blank" title="33/388"><span class="index notranslate">#33</span></a>
                <a id="title-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="title-link" href="/venue/Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" target="_blank">MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations</a>
                <a id="pdf-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kyungho Bae" target="_blank">Kyungho Bae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinhyung Kim" target="_blank">Jinhyung Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sihaeng Lee" target="_blank">Sihaeng Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soonyoung Lee" target="_blank">Soonyoung Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gunhee Lee" target="_blank">Gunhee Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinwoo Choi" target="_blank">Jinwoo Choi</a>
            </p>
            <p id="summary-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="summary">In this work, we tackle action-scene hallucination in Video Large Language Models (Video-LLMs), where models incorrectly predict actions based on the scene context or scenes based on observed actions. We observe that existing Video-LLMs often suffer from action-scene hallucination due to two main factors. First, existing Video-LLMs intermingle spatial and temporal features by applying an attention operation across all tokens. Second, they use the standard Rotary Position Embedding (RoPE), which causes the text tokens to overemphasize certain types of tokens depending on their sequential orders. To address these issues, we introduce MASH-VLM, Mitigating Action-Scene Hallucination in Video-LLMs through disentangled spatial-temporal representations. Our approach includes two key innovations: (1) DST-attention, a novel attention mechanism that disentangles the spatial and temporal tokens within the LLM by using masked attention to restrict direct interactions between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the dimensionality of the positional IDs, allowing the spatial and temporal tokens to maintain balanced positions relative to the text tokens. To evaluate the action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as on existing video understanding benchmarks.</p>
            <p id="subjects-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" onclick="foldPdfKimi('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="panel paper" keywords="planarsplatting,reconstruction,indoor,surface,planar,scenes,scannet,minutes,splatting,plane">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes_CVPR_2025_paper.html" target="_blank" title="34/388"><span class="index notranslate">#34</span></a>
                <a id="title-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="title-link" href="/venue/Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" target="_blank">PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes</a>
                <a id="pdf-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF">18</sup>]</a>
                <a id="copy-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Tan" target="_blank">Bin Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Yu" target="_blank">Rui Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Shen" target="_blank">Yujun Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Xue" target="_blank">Nan Xue</a>
            </p>
            <p id="summary-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="summary">This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images. We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps. As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction. Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy. Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method. We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future. The code of our CUDA implementation will be publicly available.</p>
            <p id="subjects-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" onclick="foldPdfKimi('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="panel paper" keywords="multimodal,vision,encoders,aimv2,pre,autoregressive,training,thermore,siglip,fur">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders_CVPR_2025_paper.html" target="_blank" title="35/388"><span class="index notranslate">#35</span></a>
                <a id="title-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="title-link" href="/venue/Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" target="_blank">Multimodal Autoregressive Pre-training of Large Vision Encoders</a>
                <a id="pdf-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF">28</sup>]</a>
                <a id="copy-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Enrico Fini" target="_blank">Enrico Fini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mustafa Shukor" target="_blank">Mustafa Shukor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiujun Li" target="_blank">Xiujun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Dufter" target="_blank">Philipp Dufter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Klein" target="_blank">Michal Klein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Haldimann" target="_blank">David Haldimann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai Aitharaju" target="_blank">Sai Aitharaju</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Victor G. Turrisi da Costa" target="_blank">Victor G. Turrisi da Costa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Louis Bthune" target="_blank">Louis Bthune</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Gan" target="_blank">Zhe Gan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Toshev" target="_blank">Alexander Toshev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcin Eichner" target="_blank">Marcin Eichner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moin Nabi" target="_blank">Moin Nabi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinfei Yang" target="_blank">Yinfei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua Susskind" target="_blank">Joshua Susskind</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alaaeldin El-Nouby" target="_blank">Alaaeldin El-Nouby</a>
            </p>
            <p id="summary-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="summary">We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Fur- thermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal im- age understanding across diverse settings.</p>
            <p id="subjects-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" onclick="foldPdfKimi('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="panel paper" keywords="photography,camera,consistent,generative,scene,24mm,70mm,realistic,images,generator">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis_CVPR_2025_paper.html" target="_blank" title="36/388"><span class="index notranslate">#36</span></a>
                <a id="title-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="title-link" href="/venue/Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" target="_blank">Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis</a>
                <a id="pdf-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF">17</sup>]</a>
                <a id="copy-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Yuan" target="_blank">Yu Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xijun Wang" target="_blank">Xijun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yichen Sheng" target="_blank">Yichen Sheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prateek Chennuri" target="_blank">Prateek Chennuri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingguang Zhang" target="_blank">Xingguang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stanley Chan" target="_blank">Stanley Chan</a>
            </p>
            <p id="summary-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="summary">Image generation today can produce somewhat realistic images from text prompts. However, if one asks the generator to synthesize a particular camera setting such as creating different fields of view using a 24mm lens versus a 70mm lens, the generator will not be able to interpret and generate scene-consistent images. This limitation not only hinders the adoption of generative tools in photography applications but also exemplifies a broader issue of bridging the gap between the data-driven models and the physical world. In this paper, we introduce the concept of Generative Photography, a framework designed to control camera intrinsic settings during content generation. The core innovation of this work are the concepts of Dimensionality Lifting and Contrastive Camera Learning, which achieve continuous and consistent transitions for different camera settings. Experimental results show that our method produces significantly more scene-consistent photorealistic images than state-of-the-art models such as Stable Diffusion 3 and FLUX.</p>
            <p id="subjects-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" onclick="foldPdfKimi('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="panel paper" keywords="safety,mllms,rules,mllm,human,images,unsafe,judgment,content,querying">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling_CVPR_2025_paper.html" target="_blank" title="37/388"><span class="index notranslate">#37</span></a>
                <a id="title-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="title-link" href="/venue/Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" target="_blank">MLLM-as-a-Judge for Image Safety without Human Labeling</a>
                <a id="pdf-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF">24</sup>]</a>
                <a id="copy-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF">12</sup>]</a>
                <a id="rel-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenting Wang" target="_blank">Zhenting Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuming Hu" target="_blank">Shuming Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyu Zhao" target="_blank">Shiyu Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaowen Lin" target="_blank">Xiaowen Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Juefei-Xu" target="_blank">Felix Juefei-Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuowei Li" target="_blank">Zhuowei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ligong Han" target="_blank">Ligong Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harihar Subramanyam" target="_blank">Harihar Subramanyam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Chen" target="_blank">Li Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfa Chen" target="_blank">Jianfa Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Jiang" target="_blank">Nan Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjuan Lyu" target="_blank">Lingjuan Lyu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiqing Ma" target="_blank">Shiqing Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dimitris N. Metaxas" target="_blank">Dimitris N. Metaxas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ankit Jain" target="_blank">Ankit Jain</a>
            </p>
            <p id="summary-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="summary">Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.</p>
            <p id="subjects-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" onclick="foldPdfKimi('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="panel paper" keywords="homography,sshnet,ihn,estimation,unsupervised,split,cross,network,modal,localtrans">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split_CVPR_2025_paper.html" target="_blank" title="38/388"><span class="index notranslate">#38</span></a>
                <a id="title-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="title-link" href="/venue/Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" target="_blank">SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization</a>
                <a id="pdf-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junchen Yu" target="_blank">Junchen Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Si-Yuan Cao" target="_blank">Si-Yuan Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runmin Zhang" target="_blank">Runmin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenghao Zhang" target="_blank">Chenghao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhu Yu" target="_blank">Zhu Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shujie Chen" target="_blank">Shujie Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bailin Yang" target="_blank">Bailin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui-Liang Shen" target="_blank">Hui-Liang Shen</a>
            </p>
            <p id="summary-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="summary">We propose a novel unsupervised cross-modal homography estimation learning framework, named Split Supervised Homography estimation Network (SSHNet). SSHNet redefines the unsupervised cross-modal homography estimation into two supervised sub-problems, each addressed by its specialized network: a homography estimation network and a modality transfer network. To realize stable training, we introduce an effective split optimization strategy to train each network separately within its respective sub-problem. We also formulate an extra homography feature space supervision to enhance feature consistency, further boosting the estimation accuracy. Moreover, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. The training stability of SSHNet enables its cooperation with various homography estimation architectures. Experiments reveal that the SSHNet using IHN as homography estimation network, namely SSHNet-IHN, outperforms previous unsupervised approaches by a significant margin. Even compared to supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4\% and 85.8\% mean average corner errors (MACEs) reduction on the challenging OPT-SAR dataset. The source code is provided in the supplementary material.</p>
            <p id="subjects-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" onclick="foldPdfKimi('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="panel paper" keywords="streaming,frame,reconstruction,instant,gaussian,generalizable,motion,gaussians,stream,accumulation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene_CVPR_2025_paper.html" target="_blank" title="39/388"><span class="index notranslate">#39</span></a>
                <a id="title-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="title-link" href="/venue/Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" target="_blank">Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting</a>
                <a id="pdf-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinbo Yan" target="_blank">Jinbo Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Peng" target="_blank">Rui Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyan Wang" target="_blank">Zhiyan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luyang Tang" target="_blank">Luyang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayu Yang" target="_blank">Jiayu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Liang" target="_blank">Jie Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Wu" target="_blank">Jiahao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronggang Wang" target="_blank">Ronggang Wang</a>
            </p>
            <p id="summary-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="summary">Building Free-Viewpoint Videos in a streaming manner offers the advantage of rapid responsiveness compared to offline training methods, greatly enhancing user experience. However, current streaming approaches face challenges of high per-frame reconstruction time (10s+) and error accumulation, limiting their broader application. In this paper, we propose Instant Gaussian Stream (IGS), a fast and generalizable streaming framework, to address these issues. First, we introduce a generalized Anchor-driven Gaussian Motion Network, which projects multi-view 2D motion features into 3D space, using anchor points to drive the motion of all Gaussians. This generalized Network generates the motion of Gaussians for each target frame in the time required for a single inference. Second, we propose a Key-frame-guided Streaming Strategy that refines each key frame, enabling accurate reconstruction of temporally complex scenes while mitigating error accumulation. We conducted extensive in-domain and cross-domain evaluations, demonstrating that our approach can achieve streaming with a average per-frame reconstruction time of 2s+, alongside a enhancement in view synthesis quality.</p>
            <p id="subjects-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" onclick="foldPdfKimi('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="panel paper" keywords="alm,lmms,languages,bench,culturally,diverse,cultural,multimodal,celebrations,evaluating">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages_CVPR_2025_paper.html" target="_blank" title="40/388"><span class="index notranslate">#40</span></a>
                <a id="title-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="title-link" href="/venue/Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" target="_blank">All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</a>
                <a id="pdf-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ashmal Vayani" target="_blank">Ashmal Vayani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dinura Dissanayake" target="_blank">Dinura Dissanayake</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hasindri Watawana" target="_blank">Hasindri Watawana</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noor Ahsan" target="_blank">Noor Ahsan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nevasini Sasikumar" target="_blank">Nevasini Sasikumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Omkar Thawakar" target="_blank">Omkar Thawakar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Henok Biadglign Ademtew" target="_blank">Henok Biadglign Ademtew</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yahya Hmaiti" target="_blank">Yahya Hmaiti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amandeep Kumar" target="_blank">Amandeep Kumar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kartik Kukreja" target="_blank">Kartik Kukreja</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mykola Maslych" target="_blank">Mykola Maslych</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wafa Al Ghallabi" target="_blank">Wafa Al Ghallabi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mihail Minkov Mihaylov" target="_blank">Mihail Minkov Mihaylov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Qin" target="_blank">Chao Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abdelrahman M. Shaker" target="_blank">Abdelrahman M. Shaker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mike Zhang" target="_blank">Mike Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mahardika Krisna Ihsani" target="_blank">Mahardika Krisna Ihsani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amiel Gian Esplana" target="_blank">Amiel Gian Esplana</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Monil Gokani" target="_blank">Monil Gokani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shachar Mirkin" target="_blank">Shachar Mirkin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harsh Singh" target="_blank">Harsh Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ashay Srivastava" target="_blank">Ashay Srivastava</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Endre Hamerlik" target="_blank">Endre Hamerlik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fathinah Asma Izzati" target="_blank">Fathinah Asma Izzati</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fadillah Adamsyah Maani" target="_blank">Fadillah Adamsyah Maani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sebastian Cavada" target="_blank">Sebastian Cavada</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jenny Chim" target="_blank">Jenny Chim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohit Gupta" target="_blank">Rohit Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanjay Manjunath" target="_blank">Sanjay Manjunath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kamila Zhumakhanova" target="_blank">Kamila Zhumakhanova</a><span style="display:none">,
                <a class="author notranslate" href="https://www.google.com/search?q=Feno Heriniaina Rabevohitra" target="_blank">Feno Heriniaina Rabevohitra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azril Hafizi Amirudin" target="_blank">Azril Hafizi Amirudin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Ridzuan" target="_blank">Muhammad Ridzuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniya Najiha Abdul Kareem" target="_blank">Daniya Najiha Abdul Kareem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ketan Pravin More" target="_blank">Ketan Pravin More</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kunyang Li" target="_blank">Kunyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pramesh Shakya" target="_blank">Pramesh Shakya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Saad" target="_blank">Muhammad Saad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amirpouya Ghasemaghaei" target="_blank">Amirpouya Ghasemaghaei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amirbek Djanibekov" target="_blank">Amirbek Djanibekov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dilshod Azizov" target="_blank">Dilshod Azizov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Branislava Jankovic" target="_blank">Branislava Jankovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naman Bhatia" target="_blank">Naman Bhatia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alvaro Cabrera" target="_blank">Alvaro Cabrera</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Johan Obando-Ceron" target="_blank">Johan Obando-Ceron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olympiah Otieno" target="_blank">Olympiah Otieno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Febian Farestam" target="_blank">Febian Farestam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muztoba Rabbani" target="_blank">Muztoba Rabbani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanoojan Ballah" target="_blank">Sanoojan Ballah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Santosh Sanjeev" target="_blank">Santosh Sanjeev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abduragim Shtanchaev" target="_blank">Abduragim Shtanchaev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maheen Fatima" target="_blank">Maheen Fatima</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thao Nguyen" target="_blank">Thao Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amrin Kareem" target="_blank">Amrin Kareem</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Toluwani Aremu" target="_blank">Toluwani Aremu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nathan Augusto Zacarias Xavier" target="_blank">Nathan Augusto Zacarias Xavier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amit Bhatkal" target="_blank">Amit Bhatkal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hawau Olamide Toyin" target="_blank">Hawau Olamide Toyin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aman Chadha" target="_blank">Aman Chadha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hisham Cholakkal" target="_blank">Hisham Cholakkal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rao Muhammad Anwer" target="_blank">Rao Muhammad Anwer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Felsberg" target="_blank">Michael Felsberg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jorma Laaksonen" target="_blank">Jorma Laaksonen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thamar Solorio" target="_blank">Thamar Solorio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Monojit Choudhury" target="_blank">Monojit Choudhury</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivan Laptev" target="_blank">Ivan Laptev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mubarak Shah" target="_blank">Mubarak Shah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Salman Khan" target="_blank">Salman Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fahad Shahbaz Khan" target="_blank">Fahad Shahbaz Khan</a></span>
                <a class="notranslate" onclick="toggleAuthorList(this, 'et al. (39 additional authors not shown)')">et al. (39 additional authors not shown)</a>
            </p>
            <p id="summary-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="summary">Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in multimodal research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including True/False, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a models ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark will be publicly released.</p>
            <p id="subjects-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" onclick="foldPdfKimi('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="panel paper" keywords="dust3r,reconstruction,stereo,collections,must3r,view,pointmaps,multi,pairs,coordinate">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction_CVPR_2025_paper.html" target="_blank" title="41/388"><span class="index notranslate">#41</span></a>
                <a id="title-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="title-link" href="/venue/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" target="_blank">MUSt3R: Multi-view Network for Stereo 3D Reconstruction</a>
                <a id="pdf-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yohann Cabon" target="_blank">Yohann Cabon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lucas Stoffl" target="_blank">Lucas Stoffl</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonid Antsfeld" target="_blank">Leonid Antsfeld</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriela Csurka" target="_blank">Gabriela Csurka</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Chidlovskii" target="_blank">Boris Chidlovskii</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jerome Revaud" target="_blank">Jerome Revaud</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vincent Leroy" target="_blank">Vincent Leroy</a>
            </p>
            <p id="summary-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="summary">DUSt3R introduced a novel paradigm in geometric computer vision by proposing a model that can provide dense and unconstrained Stereo 3D Reconstruction of arbitrary image collections with no prior information about camera calibration nor viewpoint poses. Under the hood, however, DUSt3R processes image pairs, regressing local 3D reconstructions that need to be aligned in a global coordinate system. The number of pairs, growing quadratically, is an inherent limitation that becomes especially concerning for robust and fast optimization in the case of large image collections. In this paper, we propose an extension of DUSt3R from pairs to multiple views, that addresses all aforementioned concerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction, or MUNSt3R, that modifies the DUSt3R architecture by making it symmetric and extending it to directly predict 3D structure for all views in a common coordinate frame. Second, we entail the model with a multi-layer memory mechanism which allows to reduce the computational complexity and to scale the reconstruction to large collections, inferring 3D pointmaps at high frame-rates with limited added complexity. The framework is designed to perform 3D reconstruction both offline and online, and hence can be seamlessly applied to SfM and SLAM scenarios showing state-of-the-art performance on various 3D downstream tasks, including uncalibrated Visual Odometry, relative camera pose, 3D reconstruction and multi-view depth estimation.</p>
            <p id="subjects-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" onclick="foldPdfKimi('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="panel paper" keywords="matcha,photorealism,charts,surfels,gaussians,geometry,mesh,surface,atlas,views">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism_CVPR_2025_paper.html" target="_blank" title="42/388"><span class="index notranslate">#42</span></a>
                <a id="title-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="title-link" href="/venue/Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" target="_blank">MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views</a>
                <a id="pdf-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Antoine Guedon" target="_blank">Antoine Guedon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomoki Ichikawa" target="_blank">Tomoki Ichikawa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kohei Yamashita" target="_blank">Kohei Yamashita</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ko Nishino" target="_blank">Ko Nishino</a>
            </p>
            <p id="summary-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="summary">We present a novel appearance model that simultaneously realizes explicit high-quality 3D surface mesh recovery and photorealistic novel view synthesis from sparse view samples. Our key idea is to model the underlying scene geometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians). MAtCha distills high-frequency scene surface details from an off-the-shelf monocular depth estimator and refines it through Gaussian surfel rendering. The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering and crisp geometry of a mesh model, \ie, two seemingly contradicting goals in a single model. At the core of MAtCha lies a novel neural deformation model and a structure loss that preserve the fine surface details distilled from learned monocular depths while addressing their fundamental scale ambiguities. Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time. We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, and robotics that require explicit geometry in addition to photorealism.</p>
            <p id="subjects-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" onclick="foldPdfKimi('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="panel paper" keywords="cad,caddreamer,generation,view,primitive,primitives,single,diffusion,semantics,watertight">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images_CVPR_2025_paper.html" target="_blank" title="43/388"><span class="index notranslate">#43</span></a>
                <a id="title-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="title-link" href="/venue/Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" target="_blank">CADDreamer: CAD Object Generation from Single-view Images</a>
                <a id="pdf-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Li" target="_blank">Yuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Lin" target="_blank">Cheng Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Liu" target="_blank">Yuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoxiao Long" target="_blank">Xiaoxiao Long</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxu Zhang" target="_blank">Chenxu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ningna Wang" target="_blank">Ningna Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Li" target="_blank">Xin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenping Wang" target="_blank">Wenping Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohu Guo" target="_blank">Xiaohu Guo</a>
            </p>
            <p id="summary-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="summary">The field of diffusion-based 3D generation has experienced tremendous progress in recent times. However, existing 3D generative models often produce overly dense and unstructured meshes, which are in stark contrast to the compact, structured and clear-edged CAD models created by human modelers. We introduce CADDreamer, a novel method for generating CAD objects from a single image. This method proposes a primitive-aware multi-view diffusion model, which perceives both local geometry and high-level structural semantics during the generation process. We encode primitive semantics into the color domain, and enforce the strong priors in pre-trained diffusion models to align with the well-defined primitives. As a result, we can infer multi-view normal maps and semantic maps from a single image, thereby reconstructing a mesh with primitive labels. Correspondingly, we propose a set of fitting and optimization methods to deal with the inevitable noise and distortion in generated primitives, ultimately producing a complete and seamless Boundary Representation (B-rep) of a Computer-Aided Design (CAD) model. Experimental results demonstrate that our method can effectively recover high-quality CAD objects from single-view images. Compared to existing 3D generation methods, the models produced by CADDreamer are compact in representation, clear in structure, sharp in boundaries, and watertight in topology.</p>
            <p id="subjects-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" onclick="foldPdfKimi('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="panel paper" keywords="dyna,human,lifelike,animations,expressive,dynamic,animation,facial,control,animating">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation_CVPR_2025_paper.html" target="_blank" title="44/388"><span class="index notranslate">#44</span></a>
                <a id="title-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="title-link" href="/venue/Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" target="_blank">X-Dyna: Expressive Dynamic Human Image Animation</a>
                <a id="pdf-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Di Chang" target="_blank">Di Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyi Xu" target="_blank">Hongyi Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=You Xie" target="_blank">You Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yipeng Gao" target="_blank">Yipeng Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengfei Kuang" target="_blank">Zhengfei Kuang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengqu Cai" target="_blank">Shengqu Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxu Zhang" target="_blank">Chenxu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoxian Song" target="_blank">Guoxian Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Wang" target="_blank">Chao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yichun Shi" target="_blank">Yichun Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyuan Chen" target="_blank">Zeyuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijie Zhou" target="_blank">Shijie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linjie Luo" target="_blank">Linjie Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gordon Wetzstein" target="_blank">Gordon Wetzstein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Soleymani" target="_blank">Mohammad Soleymani</a>
            </p>
            <p id="summary-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="summary">We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key factors underlying the loss of dynamic details, enhancing the lifelike qualities of human video animations.At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos.Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations.</p>
            <p id="subjects-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" onclick="foldPdfKimi('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="panel paper" keywords="causal,visual,editing,attributes,controllable,representation,spurious,learning,representations,intervention">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing_CVPR_2025_paper.html" target="_blank" title="45/388"><span class="index notranslate">#45</span></a>
                <a id="title-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="title-link" href="/venue/Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" target="_blank">Visual Representation Learning through Causal Intervention for Controllable Image Editing</a>
                <a id="pdf-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF">22</sup>]</a>
                <a id="copy-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shanshan Huang" target="_blank">Shanshan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoxuan Li" target="_blank">Haoxuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunyuan Zheng" target="_blank">Chunyuan Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Wang" target="_blank">Lei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guorui Liao" target="_blank">Guorui Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhili Gong" target="_blank">Zhili Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huayi Yang" target="_blank">Huayi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Liu" target="_blank">Li Liu</a>
            </p>
            <p id="summary-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="summary">A key challenge for controllable image editing is the fact that visual attributes with semantic meanings are not always independent of each other, resulting in spurious correlations in model training. However,most existing methods ignore such issue, leading to biased causal representations learning and unintended changes to unrelated features in the edited images.This leads us to present a diffusion-based causal representation learning framework called CIDiffuser that employs structural causal models (SCMs) to capture causal representations of visual attributes to address the spurious correlation.The framework first adopts a semanticencoder to decompose the representation into the target part, which includes visual attributes of interest to the user, and the ``other" part.We then introduce a direct causal effect learning module to capture the total direct causal effect between the potential outcomes before and after intervening on the visual attributes.In addition, a diffusion-based learning strategy is designed to optimize the representation learning process.Empirical evaluations on two benchmark datasets and one in-house dataset suggest our approach significantly outperforms the state-of-the-art methods, enabling controllable image editing by modifying learned visual representations.</p>
            <p id="subjects-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" onclick="foldPdfKimi('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="panel paper" keywords="appearance,illumination,relighting,images,variation,multiview,reconstructing,taken,extreme,objects">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation_CVPR_2025_paper.html" target="_blank" title="46/388"><span class="index notranslate">#46</span></a>
                <a id="title-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="title-link" href="/venue/Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" target="_blank">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</a>
                <a id="pdf-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hadi Alzayer" target="_blank">Hadi Alzayer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Henzler" target="_blank">Philipp Henzler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonathan T. Barron" target="_blank">Jonathan T. Barron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jia-Bin Huang" target="_blank">Jia-Bin Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pratul P. Srinivasan" target="_blank">Pratul P. Srinivasan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dor Verbin" target="_blank">Dor Verbin</a>
            </p>
            <p id="summary-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="summary">Reconstructing the geometry and appearance of objects from photographs taken in different environments is difficult as the illumination and therefore the object appearance vary across captured images. This is particularly challenging for more specular objects whose appearance strongly depends on the viewing direction. Some prior approaches model appearance variation across images using a per-image embedding vector, while others use physically-based rendering to recover the materials and per-image illumination. Such approaches fail at faithfully recovering view-dependent appearance given significant variation in input illumination and tend to produce mostly diffuse results. We present an approach that reconstructs objects from images taken under different illuminations by first relighting the images under a single reference illumination with a multiview relighting diffusion model and then reconstructing the object's geometry and appearance with a radiance field architecture that is robust to the small remaining inconsistencies among the relit images. We validate our proposed approach on both simulated and real datasets and demonstrate that it greatly outperforms existing techniques at reconstructing high-fidelity appearance from images taken under extreme illumination variation. Moreover, our approach is particularly effective at recovering view-dependent ``shiny'' appearance which cannot be reconstructed by prior methods.</p>
            <p id="subjects-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" onclick="foldPdfKimi('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="panel paper" keywords="garments,garment,gaussians,mesh,pgc,details,cloth,pose,reconstruct,poses">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose_CVPR_2025_paper.html" target="_blank" title="47/388"><span class="index notranslate">#47</span></a>
                <a id="title-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="title-link" href="/venue/Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" target="_blank">PGC: Physics-Based Gaussian Cloth from a Single Pose</a>
                <a id="pdf-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Michelle Guo" target="_blank">Michelle Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matt Jen-Yuan Chiang" target="_blank">Matt Jen-Yuan Chiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Igor Santesteban" target="_blank">Igor Santesteban</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikolaos Sarafianos" target="_blank">Nikolaos Sarafianos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hsiao-yu Chen" target="_blank">Hsiao-yu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oshri Halimi" target="_blank">Oshri Halimi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alja Boi" target="_blank">Alja Boi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunsuke Saito" target="_blank">Shunsuke Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=C. Karen Liu" target="_blank">C. Karen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tuur Stuyck" target="_blank">Tuur Stuyck</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Egor Larionov" target="_blank">Egor Larionov</a>
            </p>
            <p id="summary-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="summary">We introduce a novel approach to reconstruct simulation-ready garments with intricate appearance. Despite recent advancements, existing methods often struggle to balance the need for accurate garment reconstruction with the ability to generalize to new poses and body shapes or require large amounts of data to achieve this. In contrast, our method only requires a multi-view capture of a single static frame. We represent garments as hybrid mesh-embedded 3D Gaussian splats (or simply Gaussians), where the Gaussians capture near-field shading and high-frequency details, while the mesh encodes far-field albedo and optimized reflectance parameters. We achieve novel pose generalization by exploiting the mesh from our hybrid approach, enabling physics-based simulation and surface rendering techniques, while also capturing fine details with Gaussians that accurately reconstruct garment details. Our optimized garments can be used for simulating garments on novel poses, and garment relighting.</p>
            <p id="subjects-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" onclick="foldPdfKimi('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="panel paper" keywords="classifiers,adversarial,metamers,doppelgngers,mistakes,doppelgangers,hyper,robustness,sensitive,humans">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kamberov_Doppelgangers_and_Adversarial_Vulnerability_CVPR_2025_paper.html" target="_blank" title="48/388"><span class="index notranslate">#48</span></a>
                <a id="title-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="title-link" href="/venue/Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" target="_blank">Doppelgangers and Adversarial Vulnerability</a>
                <a id="pdf-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kamberov_Doppelgangers_and_Adversarial_Vulnerability_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=George Kamberov" target="_blank">George Kamberov</a>
            </p>
            <p id="summary-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="summary">Many machine learning (ML) classifiers are claimed to outperform humans, but they still make mistakes that humans do not. The most notorious examples of such mistakes are adversarial visual metamers. This paper aims to define and investigate the phenomenon of adversarial Doppelgngers (AD), which includes adversarial visual metamers, and to compare the performance and robustness of ML classifiers to human performance.We find that AD are inputs that are close to each other with respect to a perceptual metric defined in this paper, and show that AD are qualitatively different from the usual adversarial examples. The vast majority of classifiers are vulnerable to AD and robustness-accuracy trade-offs may not improve them. Some classification problems may not admit any AD robust classifiers because the underlying classes are ambiguous. We provide criteria that can be used to determine whether a classification problem is well defined or not; describe of an AD robust classifiers structure and attributes; introduce and explore the notions of conceptual entropy and regions of conceptual ambiguity for classifiers that are vulnerable to AD attacks, along with methods to bound the AD fooling rate of an attack. We define the notion of classifiers that exhibit hyper-sensitive behavior, that is, classifiers whose only mistakes are adversarial Doppelgngers. Improving the AD robustness of hyper-sensitive classifiers proving accuracy. We identify conditions guaranteeing that all classifiers with sufficiently high accuracy are hyper-sensitive.</p>
            <p id="subjects-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" onclick="foldPdfKimi('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="panel paper" keywords="estimation,aware,correspondence,pose,structure,co3d,relative,keypoint,object,feature">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation_CVPR_2025_paper.html" target="_blank" title="49/388"><span class="index notranslate">#49</span></a>
                <a id="title-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="title-link" href="/venue/Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" target="_blank">Structure-Aware Correspondence Learning for Relative Pose Estimation</a>
                <a id="pdf-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF">19</sup>]</a>
                <a id="copy-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yihan Chen" target="_blank">Yihan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenfei Yang" target="_blank">Wenfei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Ren" target="_blank">Huan Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shifeng Zhang" target="_blank">Shifeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianzhu Zhang" target="_blank">Tianzhu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Wu" target="_blank">Feng Wu</a>
            </p>
            <p id="summary-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="summary">Relative pose estimation provides a promising way for achieving object-agnostic pose estimation. Despite the success of existing 3D correspondence-based methods, the reliance on explicit feature matching suffers from small overlaps in visible regions and unreliable feature estimation for invisible regions. Inspired by humans' ability to assemble two object parts that have small or no overlapping regions by considering object structure, we propose a novel Structure-Aware Correspondence Learning method for Relative Pose Estimation, which consists of two key modules. First, a structure-aware keypoint extraction module is designed to locate a set of kepoints that can represent the structure of objects with different shapes and appearance, under the guidance of a keypoint based image reconstruction loss. Second, a structure-aware correspondence estimation module is designed to model the intra-image and inter-image relationships between keypoints to extract structure-aware features for correspondence estimation. By jointly leveraging these two modules, the proposed method can naturally estimate 3D-3D correspondences for unseen objects without explicit feature matching for precise relative pose estimation. Experimental results on the CO3D, Objaverse and LineMOD datasets demonstrate that the proposed method significantly outperforms prior methods, i.e., with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mn&gt;5.7&lt;/mn&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-90" style="width: 2.086em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1001.72em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-91"><span class="msubsup" id="MathJax-Span-92"><span style="display: inline-block; position: relative; width: 1.721em; height: 0px;"><span style="position: absolute; clip: rect(1.305em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mn" id="MathJax-Span-93" style="font-family: MathJax_Main;">5.7</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span><span style="position: absolute; top: -2.549em; left: 1.305em;"><span class="mo" id="MathJax-Span-94" style="font-size: 70.7%; font-family: MathJax_Main;"></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>5.7</mn><mo></mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-19">5.7^\circ</script> reduction in mean angular error on the CO3D dataset.</p>
            <p id="subjects-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="panel paper" keywords="flash3d,ptv3,locality,flashattention,psh,gpu,tiling,scaling,attention,mechanism">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality_CVPR_2025_paper.html" target="_blank" title="50/388"><span class="index notranslate">#50</span></a>
                <a id="title-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="title-link" href="/venue/Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" target="_blank">Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality</a>
                <a id="pdf-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Liyan Chen" target="_blank">Liyan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gregory P. Meyer" target="_blank">Gregory P. Meyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zaiwei Zhang" target="_blank">Zaiwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric M. Wolff" target="_blank">Eric M. Wolff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Vernaza" target="_blank">Paul Vernaza</a>
            </p>
            <p id="summary-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="summary">Recent efforts recognize the power of scale in 3D learning (e.g. PTv3) and attention mechanisms (e.g. FlashAttention).However, current point cloud backbones fail to holistically unify geometric locality, attention mechanisms, and GPU architectures in one view.In this paper, we introduce <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;Flash3D Transformer&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-95" style="width: 12.815em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1010.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-96"><span class="texatom" id="MathJax-Span-97"><span class="mrow" id="MathJax-Span-98"><span class="mtext" id="MathJax-Span-99" style="font-family: MathJax_Main-bold;">Flash3D Transformer</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">Flash3D Transformer</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-20">\textbf{Flash3D Transformer}</script>, which aligns geometric locality and GPU tiling through a principled locality mechanism based on Perfect Spatial Hashing (PSH).The common alignment with GPU tiling naturally fuses our PSH locality mechanism with FlashAttention at negligible extra cost.This mechanism affords flexible design choices throughout the backbone that result in superior downstream task results.Flash3D outperforms state-of-the-art PTv3 results on benchmark datasets, delivering a 2.25x speed increase and 2.4x memory efficiency boost. This efficiency enables scaling to wider attention scopes and larger models without additional overhead.Such scaling allows Flash3D to achieve even higher task accuracies than PTv3 under the same compute budget.</p>
            <p id="subjects-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="panel paper" keywords="hyperspectral,spectral,event,imaging,bandwidth,resolution,camera,dynamic,illumination,temporal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera_CVPR_2025_paper.html" target="_blank" title="51/388"><span class="index notranslate">#51</span></a>
                <a id="title-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="title-link" href="/venue/Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" target="_blank">Active Hyperspectral Imaging Using an Event Camera</a>
                <a id="pdf-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bohan Yu" target="_blank">Bohan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinxiu Liang" target="_blank">Jinxiu Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuofeng Wang" target="_blank">Zhuofeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Fan" target="_blank">Bin Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Art Subpa-asa" target="_blank">Art Subpa-asa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imari Sato" target="_blank">Imari Sato</a>
            </p>
            <p id="summary-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="summary">Hyperspectral imaging plays a critical role in numerous scientific and industrial fields. Conventional hyperspectral imaging systems often struggle with the trade-off between capture speed, spectral resolution, and bandwidth, particularly in dynamic environments. In this work, we present a novel event-based active hyperspectral imaging system designed for real-time capture with low bandwidth in dynamic scenes. By combining an event camera with a dynamic illumination strategy, our system achieves unprecedented temporal resolution while maintaining high spectral fidelity, all at a fraction of the bandwidth requirements of traditional systems. Unlike basis-based methods that sacrifice spectral resolution for efficiency, our approach enables continuous spectral sampling through an innovative ``sweeping rainbow" illumination pattern synchronized with a rotating mirror array. The key insight is leveraging the sparse, asynchronous nature of event cameras to encode spectral variations as temporal contrasts, effectively transforming the spectral reconstruction problem into a series of geometric constraints. Extensive evaluations on both synthetic and real data demonstrate that our system outperforms state-of-the-art methods in temporal resolution while maintaining competitive spectral reconstruction quality.</p>
            <p id="subjects-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" onclick="foldPdfKimi('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="assets,parts,part,view,reconstruction,object,occlusions,partgen,diffusion,meaningful">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="52/388"><span class="index notranslate">#52</span></a>
                <a id="title-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" target="_blank">PartGen: Part-level 3D Generation and Reconstruction with Multi-view Diffusion Models</a>
                <a id="pdf-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Minghao Chen" target="_blank">Minghao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Roman Shapovalov" target="_blank">Roman Shapovalov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Iro Laina" target="_blank">Iro Laina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tom Monnier" target="_blank">Tom Monnier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyuan Wang" target="_blank">Jianyuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Novotny" target="_blank">David Novotny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>
            </p>
            <p id="summary-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="summary">Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure.However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce ParGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing.</p>
            <p id="subjects-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="panel paper" keywords="slam,mast3r,dense,camera,pointmap,monocular,reconstruction,system,matching,priors">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors_CVPR_2025_paper.html" target="_blank" title="53/388"><span class="index notranslate">#53</span></a>
                <a id="title-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="title-link" href="/venue/Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" target="_blank">MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</a>
                <a id="pdf-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Riku Murai" target="_blank">Riku Murai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric Dexheimer" target="_blank">Eric Dexheimer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew J. Davison" target="_blank">Andrew J. Davison</a>
            </p>
            <p id="summary-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="summary">We present a real-time monocular dense SLAM system designed bottom-up from MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this strong prior, our system is robust on in-the-wild video sequences despite making no assumption on a fixed or parametric camera model beyond a unique camera centre. We introduce efficient methods for pointmap matching, camera tracking and local fusion, graph construction and loop closure, and second-order global optimisation. With known calibration, a simple modification to the system achieves state-of-the-art performance across various benchmarks. Altogether, we propose a plug-and-play monocular SLAM system capable of producing globally-consistent poses and dense geometry while operating at 15 FPS.</p>
            <p id="subjects-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" onclick="foldPdfKimi('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="panel paper" keywords="continuation,homotopy,problem,solution,simulator,pairs,resectioning,online,starting,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for_CVPR_2025_paper.html" target="_blank" title="54/388"><span class="index notranslate">#54</span></a>
                <a id="title-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" target="_blank">Simulator HC: Regression-based Online Simulation of Starting Problem-Solution Pairs for Homotopy Continuation in Geometric Vision</a>
                <a id="pdf-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyue Zhang" target="_blank">Xinyue Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zijia Dai" target="_blank">Zijia Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanting Xu" target="_blank">Wanting Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurent Kneip" target="_blank">Laurent Kneip</a>
            </p>
            <p id="summary-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="summary">While automatically generated polynomial elimination templates have sparked great progress in the field of 3D computer vision, there remain many problems for which the degree of the constraints or the number of unknowns leads to intractability. In recent years, homotopy continuation has been introduced as a plausible alternative. However, the method currently depends on expensive parallel tracking of all possible solutions in the complex domain, or a classification network for starting problem-solution pairs trained over a limited set of real-world examples. Our innovation lies in a novel approach to finding solution-problem pairs, where we only need to predict a rough initial solution, with the corresponding problem generated by an online simulator. Subsequently, homotopy continuation is applied to track that single solution back to the original problem. We apply this elegant combination to generalized camera resectioning, and also introduce a new solution to the challenging generalized relative pose and scale problem. As demonstrated, the proposed method successfully compensates the raw error committed by the regressor alone, and leads to state-of-the-art efficiency and success rates.</p>
            <p id="subjects-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="panel paper" keywords="eventpsr,reflectance,surface,normal,hdr,stereo,event,photometric,camera,cameras">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using_CVPR_2025_paper.html" target="_blank" title="55/388"><span class="index notranslate">#55</span></a>
                <a id="title-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="title-link" href="/venue/Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" target="_blank">EventPSR: Surface Normal and Reflectance Estimation from Photometric Stereo Using an Event Camera</a>
                <a id="pdf-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bohan Yu" target="_blank">Bohan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Han" target="_blank">Jin Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Imari Sato" target="_blank">Imari Sato</a>
            </p>
            <p id="summary-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="summary">Simultaneously acquisition of the surface normal and reflectance parameters is a crucial but challenging technique in the field of computer vision and graphics. It requires capturing multiple high dynamic range (HDR) images in existing methods using frame-based cameras. In this paper, we propose EventPSR, the first work to recover surface normal and reflectance parameters (e.g., metallic and roughness) simultaneously using an event camera. Compared with the existing methods based on photometric stereo or neural radiance fields, EventPSR is a robust and efficient approach that works consistently with different materials. Thanks to the extremely high temporal resolution and high dynamic range coverage of event cameras, EventPSR can recover accurate surface normal and reflectance of objects with various materials in 10 seconds. Extensive experiments on both synthetic data and real objects show that compared with existing methods using more than 100 HDR images, EventPSR recovers comparable surface normal and reflectance parameters with only about 30% of the data rate.</p>
            <p id="subjects-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" onclick="foldPdfKimi('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="panel paper" keywords="reconstruction,maps,dualpm,pose,dualpms,point,shape,dust3r,canonical,generalizing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose_CVPR_2025_paper.html" target="_blank" title="56/388"><span class="index notranslate">#56</span></a>
                <a id="title-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="title-link" href="/venue/Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" target="_blank">DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction</a>
                <a id="pdf-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Kaye" target="_blank">Ben Kaye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Jakab" target="_blank">Tomas Jakab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangzhe Wu" target="_blank">Shangzhe Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Ruprecht" target="_blank">Christian Ruprecht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>
            </p>
            <p id="summary-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="summary">The choice of data representation is a key factor in the success of deep learning in geometric tasks. For instance, DUSt3R has recently introduced the concept of viewpoint-invariant point maps, generalizing depth prediction, and showing that one can reduce all the key problems in the 3D reconstruction of static scenes to predicting such point maps. In this paper, we develop an analogous concept for a very different problem, namely, the reconstruction of the 3D shape and pose of deformable objects. To this end, we introduce the Dual Point Maps (DualPM), where a pair of point maps is extracted from the same image, one associating pixels to their 3D locations on the object, and the other to a canonical version of the object at rest pose. We also extend point maps to amodal reconstruction, seeing through self-occlusions to obtain the complete shape of the object. We show that 3D reconstruction and 3D pose estimation reduce to the prediction of the DualPMs. We demonstrate empirically that this representation is a good target for a deep network to predict; specifically, we consider modelling horses, showing that DualPMs can be trained purely on 3D synthetic data, consisting of a single model of a horse, while generalizing very well to real images. With this, we improve by a large margin previous methods for the 3D analysis and reconstruction of this type of objects.</p>
            <p id="subjects-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" onclick="foldPdfKimi('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="panel paper" keywords="inpainting,asuka,hue,generative,inpainted,context,color,image,rectified,misato">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving_CVPR_2025_paper.html" target="_blank" title="57/388"><span class="index notranslate">#57</span></a>
                <a id="title-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="title-link" href="/venue/Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" target="_blank">Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency</a>
                <a id="pdf-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF">18</sup>]</a>
                <a id="copy-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yikai Wang" target="_blank">Yikai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenjie Cao" target="_blank">Chenjie Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junqiu Yu" target="_blank">Junqiu Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Fan" target="_blank">Ke Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyang Xue" target="_blank">Xiangyang Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Fu" target="_blank">Yanwei Fu</a>
            </p>
            <p id="summary-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="summary">Recent advances in image inpainting increasingly use generative models to handle large irregular masks. However, these models can create unrealistic inpainted images due to two main issues: (1) Context Instability: Even with unmasked areas as context, generative models may still generate arbitrary objects in the masked region that dont align with the rest of the image. (2) Hue Inconsistency: Inpainted regions often have color shifts that causes a smeared appearance, reducing image quality.Retraining the generative model could help solve these issues, but its costly since state-of-the-art latent-based diffusion and rectified flow models require a three-stage training process: training a VAE, training a generative U-Net or transformer, and fine-tuning for inpainting.Instead, this paper proposes a post-processing approach, dubbed as ASUKA (Aligned Stable inpainting with UnKnown Areas prior), to improve inpainting models. To address context instability, we leverage a Masked Auto-Encoder (MAE) for reconstruction-based priors. This strengthens context alignment while maintaining the model's generation capabilities. To address hue inconsistency, we propose a specialized VAE decoder that treats latent-to-image decoding as a local harmonization task, significantly reducing color shifts for hue-consistent inpainting. We validate ASUKA on SD 1.5 and FLUX inpainting variants using the Places2 benchmark and MISATO, our proposed diverse collection of datasets. Results show that ASUKA improves context stability and hue consistency over standard diffusion and rectified flow models and other inpainting methods. Code, model, and dataset will be released.</p>
            <p id="subjects-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="panel paper" keywords="scene,language,scenes,words,embeddings,visual,identity,entity,renderers,programs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings_CVPR_2025_paper.html" target="_blank" title="58/388"><span class="index notranslate">#58</span></a>
                <a id="title-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="title-link" href="/venue/Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" target="_blank">The Scene Language: Representing Scenes with Programs, Words, and Embeddings</a>
                <a id="pdf-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunzhi Zhang" target="_blank">Yunzhi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zizhang Li" target="_blank">Zizhang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matt Zhou" target="_blank">Matt Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangzhe Wu" target="_blank">Shangzhe Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>
            </p>
            <p id="summary-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="summary">We introduce the Scene Language, a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. It represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. This representation can be inferred from pre-trained language models via a training-free inference technique, given text or image inputs. The resulting scene can be rendered into images using traditional, neural, or hybrid graphics renderers. Together, this forms an automated system for high-quality 3D and 4D scene generation. Compared with existing representations like scene graphs, our proposed Scene Language generates complex scenes with higher fidelity, while explicitly modeling the scene structures to enable precise control and editing.</p>
            <p id="subjects-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="panel paper" keywords="gaze,lle,encoders,person,target,scene,estimation,dinov2,streamlines,feature">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders_CVPR_2025_paper.html" target="_blank" title="59/388"><span class="index notranslate">#59</span></a>
                <a id="title-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="title-link" href="/venue/Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" target="_blank">Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</a>
                <a id="pdf-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fiona Ryan" target="_blank">Fiona Ryan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ajay Bati" target="_blank">Ajay Bati</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangmin Lee" target="_blank">Sangmin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Bolya" target="_blank">Daniel Bolya</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Judy Hoffman" target="_blank">Judy Hoffman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James M. Rehg" target="_blank">James M. Rehg</a>
            </p>
            <p id="summary-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="summary">We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a persons gaze target requires reasoning both about the persons appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines for gaze target estimation that carefully fuse features from separate scene encoders, head encoders, and auxiliary models for signals like depth and pose. Motivated by the success of general-purpose feature extractors on a variety of visual tasks, we propose Gaze-LLE, a novel transformer framework that streamlines gaze target estimation by leveraging features from a frozen DINOv2 encoder. We extract a single feature representation for the scene, and apply a person-specific positional prompt to decode gaze with a lightweight module. We demonstrate state-of-the-art performance across several gaze benchmarks and provide extensive analysis to validate our design choices.</p>
            <p id="subjects-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" onclick="foldPdfKimi('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="panel paper" keywords="lidar,diffuse,rgb,handheld,scanning,blurred,lidars,scene,sharper,flight">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with_CVPR_2025_paper.html" target="_blank" title="60/388"><span class="index notranslate">#60</span></a>
                <a id="title-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="title-link" href="/venue/Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" target="_blank">Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB</a>
                <a id="pdf-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nikhil Behari" target="_blank">Nikhil Behari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aaron Young" target="_blank">Aaron Young</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siddharth Somasundaram" target="_blank">Siddharth Somasundaram</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tzofi Klinghoffer" target="_blank">Tzofi Klinghoffer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akshat Dave" target="_blank">Akshat Dave</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ramesh Raskar" target="_blank">Ramesh Raskar</a>
            </p>
            <p id="summary-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="summary">3D surface reconstruction is essential across applications of virtual reality, robotics, and mobile scanning. However, RGB-based reconstruction often fails in low-texture, low-light, and low-albedo scenes. Handheld LiDARs, now common on mobile devices, aim to address these challenges by capturing depth information from time-of-flight measurements of a coarse grid of projected dots. Yet, these sparse LiDARs struggle with scene coverage on limited input views, leaving large gaps in depth information. In this work, we propose using an alternative class of "blurred" LiDAR that emits a diffuse flash, greatly improving scene coverage but introducing spatial ambiguity from mixed time-of-flight measurements across a wide field of view. To handle these ambiguities, we propose leveraging the complementary strengths of diffuse LiDAR with RGB. We introduce a Gaussian surfel-based rendering framework with a scene-adaptive loss function that dynamically balances RGB and diffuse LiDAR signals. We demonstrate that, surprisingly, diffuse LiDAR can outperform traditional sparse LiDAR, enabling robust 3D scanning with accurate color and geometry estimation in challenging environments.</p>
            <p id="subjects-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" onclick="foldPdfKimi('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="panel paper" keywords="foundhand,hand,keypoints,hands,articulation,synthesizing,repose,malformed,scale,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation_CVPR_2025_paper.html" target="_blank" title="61/388"><span class="index notranslate">#61</span></a>
                <a id="title-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="title-link" href="/venue/Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" target="_blank">FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation</a>
                <a id="pdf-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kefan Chen" target="_blank">Kefan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaerin Min" target="_blank">Chaerin Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linguang Zhang" target="_blank">Linguang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shreyas Hampali" target="_blank">Shreyas Hampali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cem Keskin" target="_blank">Cem Keskin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Srinath Sridhar" target="_blank">Srinath Sridhar</a>
            </p>
            <p id="summary-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="summary">Despite remarkable progress in image generation models, generating realistic hands remains a persistent challenge due to their complex articulation, varying viewpoints, and frequent occlusions. We present FoundHand, a large-scale domain-specific diffusion model for synthesizing single and dual hand images. To train our model, we introduce FoundHand-10M, a large-scale hand dataset with 2D keypoints and segmentation mask annotations. Our insight is to use 2D hand keypoints as a universal representation that encodes both hand articulation and camera viewpoint. FoundHand learns from image pairs to capture physically plausible hand articulations, natively enables precise control through 2D keypoints, and supports appearance control. Our model exhibits core capabilities that include the ability to repose hands, transfer hand appearance, and even synthesize novel views. This leads to zero-shot capabilities for fixing malformed hands in previously generated images, or synthesizing hand video sequences. We present extensive experiments and evaluations that demonstrate state-of-the-art performance of our method.</p>
            <p id="subjects-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="panel paper" keywords="talking,taoavatar,lifelike,body,avatars,splatting,3dgs,full,avatar,devices">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via_CVPR_2025_paper.html" target="_blank" title="62/388"><span class="index notranslate">#62</span></a>
                <a id="title-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="title-link" href="/venue/Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" target="_blank">TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting</a>
                <a id="pdf-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianchuan Chen" target="_blank">Jianchuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingchuan Hu" target="_blank">Jingchuan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaige Wang" target="_blank">Gaige Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhonghua Jiang" target="_blank">Zhonghua Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiansong Zhou" target="_blank">Tiansong Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwen Chen" target="_blank">Zhiwen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengfei Lv" target="_blank">Chengfei Lv</a>
            </p>
            <p id="summary-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="summary">Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.</p>
            <p id="subjects-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" onclick="foldPdfKimi('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="panel paper" keywords="label,online,shift,adaptation,regret,settings,universal,offline,meets,consistent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal_CVPR_2025_paper.html" target="_blank" title="63/388"><span class="index notranslate">#63</span></a>
                <a id="title-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="title-link" href="/venue/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" target="_blank">Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret</a>
                <a id="pdf-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yucong Dai" target="_blank">Yucong Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shilin Gu" target="_blank">Shilin Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruidong Fan" target="_blank">Ruidong Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Xu" target="_blank">Chao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenping Hou" target="_blank">Chenping Hou</a>
            </p>
            <p id="summary-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="summary">Label shift, which investigates the adaptation of label distributions between the fixed source and target domains, has attracted significant research interests and broad applications in offline settings. In real-world scenarios, however, data often arrives as a continuous stream. Addressing label shift in online learning settings is paramount. Existing strategies, which tailor traditional offline label shift techniques to online settings, have degraded performance due to the inconsistent estimation of label distributions and violation of convex assumption for theoretical guarantee. In this paper, we propose a novel method to ensure consistent adaptation to online label shift. We construct a new convex risk estimator that is pivotal for both online optimization and theoretical analysis. Furthermore, we enhance an optimistic online algorithm as the base learner and refine the classifier using an ensemble method. Theoretically, we derive a universal dynamic regret which achieves minimax optimal. Extensive experiments on both real-world datasets and human motion task demonstrate the superiority of our method comparing existing methods.</p>
            <p id="subjects-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" onclick="foldPdfKimi('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="panel paper" keywords="pre,interactions,training,dnns,methods,mechanism,strength,encoded,common,unified">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D_CVPR_2025_paper.html" target="_blank" title="64/388"><span class="index notranslate">#64</span></a>
                <a id="title-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="title-link" href="/venue/Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" target="_blank">A Unified Approach to Interpreting Self-supervised Pre-training Methods for 3D Point Clouds via Interactions</a>
                <a id="pdf-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Li" target="_blank">Qiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Ruan" target="_blank">Jian Ruan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fanghao Wu" target="_blank">Fanghao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuchi Chen" target="_blank">Yuchi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihua Wei" target="_blank">Zhihua Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Shen" target="_blank">Wen Shen</a>
            </p>
            <p id="summary-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="summary">Recently, many self-supervised pre-training methods have been proposed to improve the performance of deep neural networks (DNNs) for 3D point clouds processing. However, the common mechanism underlying the effectiveness of different pre-training methods remains unclear. In this paper, we use game-theoretic interactions as a unified approach to explore the common mechanism of pre-training methods. Specifically, we decompose the output score of a DNN into the sum of numerous effects of interactions, with each interaction representing a distinct 3D substructure of the input point cloud. Based on the decomposed interactions, we draw the following conclusions. (1) The common mechanism across different pre-training methods is that they enhance the strength of high-order interactions encoded by DNNs, which represent complex and global 3D structures, while reducing the strength of low-order interactions, which represent simple and local 3D structures. (2) Sufficient pre-training and adequate fine-tuning data for downstream tasks further reinforce the mechanism described above. (3) Pre-training methods carry a potential risk of reducing the transferability of features encoded by DNNs. Inspired by the observed common mechanism, we propose a new method to directly enhance the strength of high-order interactions and reduce the strength of low-order interactions encoded by DNNs, improving performance without the need for pre-training on large-scale datasets. Experiments show that our method achieves performance comparable to traditional pre-training methods.</p>
            <p id="subjects-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" onclick="foldPdfKimi('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="panel paper" keywords="lpm,splatting,opacity,densification,points,point,zones,management,reset,spacetimegs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management_CVPR_2025_paper.html" target="_blank" title="65/388"><span class="index notranslate">#65</span></a>
                <a id="title-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="title-link" href="/venue/Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" target="_blank">Improving Gaussian Splatting with Localized Points Management</a>
                <a id="pdf-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haosen Yang" target="_blank">Haosen Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenhao Zhang" target="_blank">Chenhao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqing Wang" target="_blank">Wenqing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Volino" target="_blank">Marco Volino</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adrian Hilton" target="_blank">Adrian Hilton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Zhang" target="_blank">Li Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiatian Zhu" target="_blank">Xiatian Zhu</a>
            </p>
            <p id="summary-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="summary">Point management is critical for optimizing 3D Gaussian Splatting models, as point initiation (e.g., via structure from motion) is often distributionally inappropriate. Typically, Adaptive Density Control (ADC) algorithm is adopted, leveraging view-averaged gradient magnitude thresholding for point densification, opacity thresholding for pruning, and regular all-points opacity reset. We reveal that this strategy is limited in tackling intricate/special image regions (e.g., transparent) due to inability of identifying all 3D zones requiring point densification, and lacking an appropriate mechanism to handle ill-conditioned points with negative impacts (e.g., occlusion due to false high opacity).To address these limitations, we propose a Localized Point Management(LPM) strategy, capable of identifying those error-contributing zones in greatest need for both point addition and geometry calibration. Zone identification is achieved by leveraging the underlying multiview geometry constraints, subject to image rendering errors. We apply point densification in the identified zones and then reset the opacity of the points in front of these regions, creating a new opportunity to correct poorly conditioned points. Serving as a versatile plugin, LPM can be seamlessly integrated into existing static 3D and dynamic 4D Gaussian Splatting models with minimal additional cost.Experimental evaluations validate the efficacy of our LPM in boosting a variety of existing 3D/4D models both quantitatively and qualitatively. Notably, LPM improves both static 3DGS and dynamic SpaceTimeGS to achieve state-of-the-art rendering quality while retaining real-time speeds, excelling on challenging datasets such as Tanks &amp; Temples and the Neural 3D Video dataset.</p>
            <p id="subjects-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" onclick="foldPdfKimi('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="panel paper" keywords="facial,electromyography,muscle,semg,eifer,activity,expression,occlusion,recordings,mimicry">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis_CVPR_2025_paper.html" target="_blank" title="66/388"><span class="index notranslate">#66</span></a>
                <a id="title-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="title-link" href="/venue/Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" target="_blank">Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis</a>
                <a id="pdf-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tim Bchner" target="_blank">Tim Bchner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Anders" target="_blank">Christoph Anders</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Orlando Guntinas-Lichius" target="_blank">Orlando Guntinas-Lichius</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joachim Denzler" target="_blank">Joachim Denzler</a>
            </p>
            <p id="summary-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="summary">The relationship between muscle activity and resulting facial expressions is crucial for various fields, including psychology, medicine, and entertainment.The synchronous recording of facial mimicry and muscular activity via surface electromyography (sEMG) provides a unique window into these complex dynamics.Unfortunately, existing methods for facial analysis cannot handle electrode occlusion, rendering them ineffective.Even with occlusion-free reference images of the same person, variations in expression intensity and execution are unmatchable.Our electromyography-informed facial expression reconstruction (EIFER) approach is a novel method to restore faces under sEMG occlusion faithfully in an adversarial manner.We decouple facial geometry and visual appearance (e.g., skin texture, lighting, electrodes) by combining a 3D Morphable Model (3DMM) with neural unpaired image-to-image translation via reference recordings.Then, EIFER learns a bidirectional mapping between 3DMM expression parameters and muscle activity, establishing correspondence between the two domains. We validate the effectiveness of our approach through experiments on a dataset of synchronized sEMG recordings and facial mimicry, demonstrating faithful geometry and appearance reconstruction.Further, we synthesize expressions based on muscle activity and how observed expressions can predict dynamic muscle activity.Consequently, EIFER introduces a new paradigm for facial electromyography, which could be extended to other forms of multi-modal face recordings.</p>
            <p id="subjects-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" onclick="foldPdfKimi('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="panel paper" keywords="skills,basketball,ismimic,interaction,skillmimic,diverse,demonstrations,learn,skill,layup">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations_CVPR_2025_paper.html" target="_blank" title="67/388"><span class="index notranslate">#67</span></a>
                <a id="title-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="title-link" href="/venue/Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" target="_blank">SkillMimic: Learning Basketball Interaction Skills from Demonstrations</a>
                <a id="pdf-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yinhuai Wang" target="_blank">Yinhuai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qihan Zhao" target="_blank">Qihan Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runyi Yu" target="_blank">Runyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hok Wai Tsui" target="_blank">Hok Wai Tsui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ailing Zeng" target="_blank">Ailing Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Lin" target="_blank">Jing Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengyi Luo" target="_blank">Zhengyi Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiwen Yu" target="_blank">Jiwen Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiu Li" target="_blank">Xiu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifeng Chen" target="_blank">Qifeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Zhang" target="_blank">Jian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Zhang" target="_blank">Lei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Tan" target="_blank">Ping Tan</a>
            </p>
            <p id="summary-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="summary">Traditional reinforcement learning methods for interaction skills rely on labor-intensive, manually designed rewards that do not generalize well across different skills. Inspired by how humans learn from demonstrations, we propose ISMimic, the first data-driven approach that Mimics both human and ball motions to learn diverse Interaction Skills, e.g., a wide variety of challenging basketball skills. ISMimic employs a unified configuration to learn diverse interaction skills from human-ball motion datasets, with skill diversity and generalization improving as the dataset grows. This approach allows training a single policy to learn multiple interaction skills and allows smooth skill switching. The interaction skills acquired by ISMimic can be easily reused by a high-level controller to accomplish high-level tasks. To evaluate our approach, we introduce two basketball datasets that collectively contain about 35 minutes of diverse basketball skills. Experiments show that our method can effectively acquire various reusable basketball skills including diverse styles of dribbling, layup, and shooting. Video results and 3D visualization are available at https://ismimic.github.io</p>
            <p id="subjects-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" onclick="foldPdfKimi('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="backdoor,uibdiffusion,imperceptible,dms,attack,universal,triggers,diffusion,trigger,samplers">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="68/388"><span class="index notranslate">#68</span></a>
                <a id="title-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" target="_blank">UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models</a>
                <a id="pdf-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuning Han" target="_blank">Yuning Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingyin Zhao" target="_blank">Bingyin Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Chu" target="_blank">Rui Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Luo" target="_blank">Feng Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Biplab Sikdar" target="_blank">Biplab Sikdar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingjie Lao" target="_blank">Yingjie Lao</a>
            </p>
            <p id="summary-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="summary">Recent studies show that diffusion models (DMs) are vulnerable to backdoor attacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray box and eyeglasses) that contain evident patterns, rendering remarkable attack effects yet easy detection upon human inspection and defensive algorithms. While it is possible to improve stealthiness by reducing the strength of the backdoor, doing so can significantly compromise its generality and effectiveness. In this paper, we propose UIBDiffusion, the universal imperceptible backdoor attack for diffusion models, which allows us to achieve superior attack and generation performance while evading state-of-the-art defenses. We propose a novel trigger generation approach based on universal adversarial perturbations (UAPs) and reveal that such perturbations, which are initially devised for fooling pre-trained discriminative models, can be adapted as potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on multiple types of DMs with different kinds of samplers across various datasets and targets. Experimental results demonstrate that UIBDiffusion brings three advantages: 1) Universality, the imperceptible trigger is universal (i.e., image and model agnostic) where a single trigger is effective to any images and all diffusion models with different samplers; 2) Utility, it achieves comparable generation quality (e.g., FID) and even better attack success rate (i.e., ASR) at low poison rates compared to the prior works; and 3) Undetectability, UIBDiffusion is plausible to human perception and can bypass Elijah and TERD, the SOTA defenses against backdoors for DMs. We will release our backdoor triggers and code.</p>
            <p id="subjects-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="panel paper" keywords="pseudo,npp,ncp,noisy,view,labels,roll,label,clustering,mvc">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy_CVPR_2025_paper.html" target="_blank" title="69/388"><span class="index notranslate">#69</span></a>
                <a id="title-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="title-link" href="/venue/Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" target="_blank">ROLL: Robust Noisy Pseudo-label Learning for Multi-View Clustering with Noisy Correspondence</a>
                <a id="pdf-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Sun" target="_blank">Yuan Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongxiang Li" target="_blank">Yongxiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenwen Ren" target="_blank">Zhenwen Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guiduo Duan" target="_blank">Guiduo Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dezhong Peng" target="_blank">Dezhong Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Hu" target="_blank">Peng Hu</a>
            </p>
            <p id="summary-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="summary">Multi-view clustering (MVC) aims to exploit complementary information from diverse views to enhance clustering performance. Since pseudo-labels can provide additional semantic information, many MVC methods have been proposed to guide unsupervised multi-view learning through pseudo-labels. These methods implicitly assume that the predicted pseudo-labels are predicted correctly. However, due to the challenges in training a flawless unsupervised model, this assumption can be easily violated, thereby leading to the Noisy Pseudo-label Problem (NPP). Moreover, these existing approaches typically rely on the assumption of perfect cross-view alignment. In practice, it is frequently compromised due to noise or sensor differences, thereby resulting in the Noisy Correspondence Problem (NCP). Based on the above observations, we reveal and study unsupervised multi-view learning under NPP and NCP. To this end, we propose Robust Noisy Pseudo-label Learning (ROLL) to prevent the overfitting problem caused by both NPP and NCP. Specifically, we first adopt traditional contrastive learning to warm up the model, thereby generating the pseudo-labels in a self-supervised manner. Afterward, we propose noise-tolerance pseudo-label learning to deal with the noise in the predicted pseudo-labels, thereby embracing the robustness against NPP. To further mitigate the overfitting problem, we present robust multi-view contrastive learning to mitigate the negative impact of NCP. Extensive experiments on five multi-view datasets demonstrate the superior clustering performance of our ROLL compared to 11 state-of-the-art methods.</p>
            <p id="subjects-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" onclick="foldPdfKimi('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="panel paper" keywords="hallucination,octopus,contrastive,decoding,expansibility,alleviating,dynamic,lvlms,strategies,deployability">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding_CVPR_2025_paper.html" target="_blank" title="70/388"><span class="index notranslate">#70</span></a>
                <a id="title-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="title-link" href="/venue/Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" target="_blank">Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding</a>
                <a id="pdf-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF">18</sup>]</a>
                <a id="copy-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Suo" target="_blank">Wei Suo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lijun Zhang" target="_blank">Lijun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengyang Sun" target="_blank">Mengyang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Yuanbo Wu" target="_blank">Lin Yuanbo Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Wang" target="_blank">Peng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanning Zhang" target="_blank">Yanning Zhang</a>
            </p>
            <p id="summary-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="summary">Large Vision-Language Models (LVLMs) have obtained impressive performance in visual content understanding and multi-modal reasoning. Unfortunately, these large models suffer from serious hallucination problems and tend to generate fabricated responses. Recently, several Contrastive Decoding (CD) strategies have been proposed to alleviate hallucination by introducing disturbed inputs. Although great progress has been made, these CD strategies mostly apply a one-size-fits-all approach for all input conditions. In this paper, we revisit this process through extensive experiments. Related results show that hallucination causes are hybrid and each generative step faces a unique hallucination challenge. Leveraging these meaningful insights, we introduce a simple yet effective Octopus-like framework that enables the model to adaptively identify hallucination types and create a dynamic CD workflow. Our Octopus framework not only outperforms existing methods across four benchmarks but also demonstrates excellent deployability and expansibility. Our code will be released.</p>
            <p id="subjects-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" onclick="foldPdfKimi('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="panel paper" keywords="soundvista,sound,microphones,acoustic,scene,viewpoint,ambient,binding,reference,embeddings">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding_CVPR_2025_paper.html" target="_blank" title="71/388"><span class="index notranslate">#71</span></a>
                <a id="title-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="title-link" href="/venue/Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" target="_blank">SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding</a>
                <a id="pdf-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mingfei Chen" target="_blank">Mingfei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Israel D. Gebru" target="_blank">Israel D. Gebru</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ishwarya Ananthabhotla" target="_blank">Ishwarya Ananthabhotla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Richardt" target="_blank">Christian Richardt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dejan Markovic" target="_blank">Dejan Markovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jake Sandakly" target="_blank">Jake Sandakly</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Steven Krenn" target="_blank">Steven Krenn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Todd Keebler" target="_blank">Todd Keebler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eli Shlizerman" target="_blank">Eli Shlizerman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Richard" target="_blank">Alexander Richard</a>
            </p>
            <p id="summary-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="summary">We introduce SoundVista, a method to generate the ambient sound of an arbitrary scene at novel viewpoints. Given a pre-acquired recording of the scene from sparsely distributed microphones, SoundVista can synthesize the sound of that scene from an unseen target viewpoint. The method learns the underlying acoustic transfer function that relates the signals acquired at the distributed microphones to the signal at the target viewpoint, using a limited number of known recordings. Unlike existing works, our method does not require constraints or prior knowledge of sound source details. Moreover, our method efficiently adapts to diverse room layouts, reference microphone configurations and unseen environments. To enable this, we introduce a visual-acoustic binding module that learns visual embeddings linked with local acoustic properties from panoramic RGB and depth data. We first leverage these embeddings to optimize the placement of reference microphones in any given scene. During synthesis, we leverage multiple embeddings extracted from reference locations to get adaptive weights for their contribution, conditioned on target viewpoint. We benchmark the task on both publicly available data and real-world settings. We demonstrate significant improvements over existing methods.</p>
            <p id="subjects-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" onclick="foldPdfKimi('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="panel paper" keywords="reconstruction,plane,wild,planar,indoor,outdoor,generalizability,offset,single,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image_CVPR_2025_paper.html" target="_blank" title="72/388"><span class="index notranslate">#72</span></a>
                <a id="title-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="title-link" href="/venue/Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" target="_blank">Towards In-the-wild 3D Plane Reconstruction from a Single Image</a>
                <a id="pdf-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiachen Liu" target="_blank">Jiachen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Yu" target="_blank">Rui Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sili Chen" target="_blank">Sili Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sharon X. Huang" target="_blank">Sharon X. Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengkai Guo" target="_blank">Hengkai Guo</a>
            </p>
            <p id="summary-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="summary">3D plane reconstruction from a single image is a crucial yet challenging topic in 3D computer vision. Previous state-of-the-art (SOTA) methods have focused on training their system on a single dataset from either indoor or outdoor domain, limiting their generalizability across diverse testing data. In this work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based model targeting zero-shot 3D plane detection and reconstruction from a single image, over diverse domains and environments. To enable data-driving models on multiple domains, we have curated a large-scale (over 14 datasets and 560,000 images), high-resolution, densely-annotated planar benchmark from various indoor and outdoor scenes. To address the challenge of achieving desirable planar geometry on multi-dataset training, we propose to disentangle the representation of plane normal and offset, and employ an exemplar-guided, classification-then-regression paradigm to learn plane and offset respectively. Additionally, we employ advanced backbones as image encoder, and present an effective pixel-geometry-enhanced plane embedding module to further facilitate planar reconstruction. Extensive experiments across multiple zero-shot evaluation datasets have demonstrated that our approach significantly outperforms previous methods on both reconstruction accuracy and generalizability, especially over in-the-wild data. We will release all of the labeled data, code and models upon the acceptance of this paper.</p>
            <p id="subjects-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="panel paper" keywords="t2icount,text,counting,sensitivity,fsc147,shot,denosing,cross,understanding,zero">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting_CVPR_2025_paper.html" target="_blank" title="73/388"><span class="index notranslate">#73</span></a>
                <a id="title-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="title-link" href="/venue/Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" target="_blank">T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting</a>
                <a id="pdf-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Qian" target="_blank">Yifei Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongliang Guo" target="_blank">Zhongliang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Deng" target="_blank">Bowen Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun Tong Lei" target="_blank">Chun Tong Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Zhao" target="_blank">Shuai Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chun Pong Lau" target="_blank">Chun Pong Lau</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaopeng Hong" target="_blank">Xiaopeng Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael P. Pound" target="_blank">Michael P. Pound</a>
            </p>
            <p id="summary-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="summary">Zero-shot object counting aims to count instances of arbitrary object categories specified by text descriptions. Existing methods typically rely on vision-language models like CLIP, but often exhibit limited sensitivity to text prompts. We present T2ICount, a one-step diffusion-based framework that leverages rich prior knowledge and fine-grained visual understanding from pretrained diffusion models. While one-step denoising ensures efficiency, it leads to weakened text sensitivity. To address this challenge, we propose a Hierarchical Semantic Correction Module that progressively refines text-image feature alignment, and a Representational Regional Coherence Loss that provides reliable supervision signals by leveraging the cross-attention maps extracted from the denosing U-Net. Furthermore, we observe that current benchmarks mainly focus on majority objects in images, potentially masking models' text sensitivity. To address this, we contribute a challenging re-annotated subset of FSC147 for better evaluation of text-guided counting ability. Extensive experiments demonstrate that our method achieves superior performance across different benchmarks. Code will be made publicly available.</p>
            <p id="subjects-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" onclick="foldPdfKimi('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="panel paper" keywords="referencenet,reference,image,tfcustom,frequency,feature,features,generation,aware,denoising">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance_CVPR_2025_paper.html" target="_blank" title="74/388"><span class="index notranslate">#74</span></a>
                <a id="title-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="title-link" href="/venue/Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" target="_blank">TFCustom: Customized Image Generation with Time-Aware Frequency Feature Guidance</a>
                <a id="pdf-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mushui Liu" target="_blank">Mushui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong She" target="_blank">Dong She</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingxuan Pang" target="_blank">Jingxuan Pang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qihan Huang" target="_blank">Qihan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacheng Ying" target="_blank">Jiacheng Ying</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanggui He" target="_blank">Wanggui He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanlei Hou" target="_blank">Yuanlei Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siming Fu" target="_blank">Siming Fu</a>
            </p>
            <p id="summary-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="summary">Subject-driven image personalization has seen notable advancements, especially with the advent of the ReferenceNet paradigm. ReferenceNet excels in integrating image reference features, making it highly applicable in creative and commercial settings. However, current implementations of ReferenceNet primarily operate as latent-level feature extractors, which limit their potential. This constraint hinders the provision of appropriate features to the denoising backbone across different timesteps, leading to suboptimal image consistency. In this paper, we revisit the extraction of reference features and propose TFCustom, a model framework designed to focus on reference image features at different temporal steps and frequency levels. Specifically, we firstly propose synchronized ReferenceNet to extract reference image features while simultaneously optimizing noise injection and denoising for the reference image. We also propose a time-aware frequency feature refinement module that leverages high- and low-frequency filters, combined with time embeddings, to adaptively select the degree of reference feature injection. Additionally, to enhance the similarity between reference objects and the generated image, we introduce a novel reward-based loss that encourages greater alignment between the reference and generated images. Experimental results demonstrate state-of-the-art performance in both multi-object and single-object reference generation, with significant improvements in texture and textual detail generation over existing methods.</p>
            <p id="subjects-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" onclick="foldPdfKimi('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="panel paper" keywords="segmentation,ambissl,medical,decoders,diverse,image,ambiguity,pruned,annotation,labeled">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation_CVPR_2025_paper.html" target="_blank" title="75/388"><span class="index notranslate">#75</span></a>
                <a id="title-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" target="_blank">Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation</a>
                <a id="pdf-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF">33</sup>]</a>
                <a id="copy-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Suruchi Kumari" target="_blank">Suruchi Kumari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pravendra Singh" target="_blank">Pravendra Singh</a>
            </p>
            <p id="summary-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="summary">Despite the remarkable progress of deep learning-based methods in medical image segmentation, their use in clinical practice remains limited for two main reasons. First, obtaining a large medical dataset with precise annotations to train segmentation models is challenging. Secondly, most current segmentation techniques generate a single deterministic segmentation mask for each image. However, in real-world scenarios, there is often significant uncertainty regarding what defines the ``correct" segmentation, and various expert annotators might provide different segmentations for the same image. To tackle both of these problems, we propose Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation (AmbiSSL). AmbiSSL combines a small amount of multi-annotator labeled data and a large set of unlabeled data to generate diverse and plausible segmentation maps. Our method consists of three key components: (1) The Diverse Pseudo-Label Generation (DPG) module utilizes multiple decoders, created by performing randomized pruning on the original backbone decoder. These pruned decoders enable the generation of a diverse pseudo-label set; (2) a Semi-Supervised Latent Distribution Learning (SSLDL) module constructs acommon latent space by utilizing both ground truth annotations andpseudo-label set; and (3) a Cross-Decoder Supervision (CDS) module, which enables pruned decoders to guide each others learning. We evaluated the proposed method on two publicly available datasets. Extensive experiments demonstrate that AmbiSSL can generate diverse segmentation maps using only a small amount of labeled data and abundant unlabeled data, offering a more practical solution for medical image segmentation by reducing reliance on large labeled datasets.</p>
            <p id="subjects-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="panel paper" keywords="motion,mvlift,animal,truth,poses,supervision,athletic,lifting,world,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion_CVPR_2025_paper.html" target="_blank" title="76/388"><span class="index notranslate">#76</span></a>
                <a id="title-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" target="_blank">Lifting Motion to the 3D World via 2D Diffusion</a>
                <a id="pdf-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaman Li" target="_blank">Jiaman Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=C. Karen Liu" target="_blank">C. Karen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>
            </p>
            <p id="summary-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="summary">Estimating 3D motion from 2D observations is a long-standing research challenge. Prior work typically requires training on datasets containing ground truth 3D motions, limiting their applicability to activities well-represented in existing motion capture data. This dependency particularly hinders generalization to out-of-distribution scenarios or subjects where collecting 3D ground truth is challenging, such as complex athletic movements or animal motion. We introduce MVLift, a novel approach to predict global 3D motion---including both joint rotations and root trajectories in the world coordinate system---using only 2D pose sequences for training. Our multi-stage framework leverages 2D motion diffusion models to progressively generate consistent 2D pose sequences across multiple views, a key step in recovering accurate global 3D motion. MVLift generalizes across various domains, including human poses, human-object interactions, and animal poses. Despite not requiring 3D supervision, it outperforms prior work on five datasets, including those methods that require 3D supervision.</p>
            <p id="subjects-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="panel paper" keywords="lmm,textbf,scene,inst3d,modal,understanding,instance,aware,tuning,multi">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html" target="_blank" title="77/388"><span class="index notranslate">#77</span></a>
                <a id="title-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="title-link" href="/venue/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" target="_blank">Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning</a>
                <a id="pdf-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanxun Yu" target="_blank">Hanxun Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentong Li" target="_blank">Wentong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Wang" target="_blank">Song Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junbo Chen" target="_blank">Junbo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianke Zhu" target="_blank">Jianke Zhu</a>
            </p>
            <p id="summary-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="summary">Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;Inst&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-100" style="width: 2.398em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.93em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-101"><span class="texatom" id="MathJax-Span-102"><span class="mrow" id="MathJax-Span-103"><span class="mtext" id="MathJax-Span-104" style="font-family: MathJax_Main-bold;">Inst</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">Inst</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-21">\textbf{Inst}</script>ance-aware <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;3D&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-105" style="width: 1.773em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-106"><span class="texatom" id="MathJax-Span-107"><span class="mrow" id="MathJax-Span-108"><span class="mtext" id="MathJax-Span-109" style="font-family: MathJax_Main-bold;">3D</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">3D</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-22">\textbf{3D}</script> <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;L&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-110" style="width: 0.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-111"><span class="texatom" id="MathJax-Span-112"><span class="mrow" id="MathJax-Span-113"><span class="mtext" id="MathJax-Span-114" style="font-family: MathJax_Main-bold;">L</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">L</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-23">\textbf{L}</script>arge <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;M&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-115" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-116"><span class="texatom" id="MathJax-Span-117"><span class="mrow" id="MathJax-Span-118"><span class="mtext" id="MathJax-Span-119" style="font-family: MathJax_Main-bold;">M</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">M</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-24">\textbf{M}</script>ulti-modal <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;M&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-120" style="width: 1.357em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-121"><span class="texatom" id="MathJax-Span-122"><span class="mrow" id="MathJax-Span-123"><span class="mtext" id="MathJax-Span-124" style="font-family: MathJax_Main-bold;">M</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">M</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-25">\textbf{M}</script>odel (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Our full implementation will be publicly available.</p>
            <p id="subjects-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" onclick="foldPdfKimi('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="panel paper" keywords="npcs,articulated,rgb,cap,parts,part,poses,rgbd,net,pose">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation_CVPR_2025_paper.html" target="_blank" title="78/388"><span class="index notranslate">#78</span></a>
                <a id="title-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="title-link" href="/venue/Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" target="_blank">CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image</a>
                <a id="pdf-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingshun Huang" target="_blank">Jingshun Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haitao Lin" target="_blank">Haitao Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Wang" target="_blank">Tianyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Fu" target="_blank">Yanwei Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyang Xue" target="_blank">Xiangyang Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Zhu" target="_blank">Yi Zhu</a>
            </p>
            <p id="summary-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="summary">This paper tackles category-level pose estimation of articulated objects in robotic manipulation tasks and introduces a new benchmark dataset. While recent methods estimate part poses and sizes at the category level, they often rely on geometric cues and complex multi-stage pipelines that first segment parts from the point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation for 6D poses. These approaches overlook dense semantic cues from RGB images, leading to suboptimal accuracy, particularly for objects with small parts. To address these limitations, we propose a single-stage Network, CAP-Net, for estimating the 6D poses and sizes of Categorical Articulated Parts. This method combines RGB-D features to generate instance segmentation and NPCS representations for each part in an end-to-end manner. CAP-Net uses a unified network to simultaneously predict point-wise class labels, centroid offsets, and NPCS maps. A clustering algorithm then groups points of the same predicted class based on their estimated centroid distances to isolate each part. Finally, the NPCS region of each part is aligned with the point cloud to recover its final pose and size.To bridge the sim-to-real domain gap, we introduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date, featuring photorealistic RGB images and depth noise simulated from real sensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our method significantly outperforms the state-of-the-art approach. Real-world deployments of our model in robotic tasks underscore its robustness and exceptional sim-to-real transfer capabilities, confirming its substantial practical utility.</p>
            <p id="subjects-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" onclick="foldPdfKimi('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="panel paper" keywords="pippo,view,multi,resolution,image,generation,single,model,camera,consistency">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image_CVPR_2025_paper.html" target="_blank" title="79/388"><span class="index notranslate">#79</span></a>
                <a id="title-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="title-link" href="/venue/Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" target="_blank">Pippo: High-Resolution Multi-View Humans from a Single Image</a>
                <a id="pdf-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yash Kant" target="_blank">Yash Kant</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ethan Weber" target="_blank">Ethan Weber</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Kyu Kim" target="_blank">Jin Kyu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rawal Khirodkar" target="_blank">Rawal Khirodkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Su Zhaoen" target="_blank">Su Zhaoen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julieta Martinez" target="_blank">Julieta Martinez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Igor Gilitschenski" target="_blank">Igor Gilitschenski</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunsuke Saito" target="_blank">Shunsuke Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Timur Bagautdinov" target="_blank">Timur Bagautdinov</a>
            </p>
            <p id="summary-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="summary">We present Pippo, a generative model capable of producing a dense set of high-resolution (1K) multi-view images of a person from a single photo. Our approach does not require any parametric model fitting or camera parameters of the input image, and generalizes to arbitrary identities with diverse clothing and hair styles. Pippo is a multi-view diffusion transformer trained in multiple stages. First, we pretrain the model on a billion-scale human-centric image dataset. Second, we train the model on studio data to generate many low-resolution consistent views conditioned on a coarse camera and an input image. Finally, we fine-tune the model on high-resolution data for multi-view generation with minimal placement controls, further improving consistency. This training strategy allows us to retain the generalizability from the large-scale pretraining while enabling high-resolution multi-view synthesis. We investigate several key architecture design choices for multi-view generation with diffusion transformers for precise view and identity control. Using a newly introduced 3D consistency metric, we demonstrate that Pippo outperforms existing approaches on multi-view human generation from a single image.</p>
            <p id="subjects-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" onclick="foldPdfKimi('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="panel paper" keywords="motion,latent,diffusion,generation,energymogen,energy,spectrums,text,semantic,compositional">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in_CVPR_2025_paper.html" target="_blank" title="80/388"><span class="index notranslate">#80</span></a>
                <a id="title-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="title-link" href="/venue/Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" target="_blank">EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space</a>
                <a id="pdf-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianrong Zhang" target="_blank">Jianrong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hehe Fan" target="_blank">Hehe Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="summary">Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task. Our implementation will be released.</p>
            <p id="subjects-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="panel paper" keywords="attack,adversarial,trajectory,frame,point,attacks,perturbations,robustness,prediction,enduring">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving_CVPR_2025_paper.html" target="_blank" title="81/388"><span class="index notranslate">#81</span></a>
                <a id="title-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="title-link" href="/venue/Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" target="_blank">Enduring, Efficient and Robust Trajectory Prediction Attack in Autonomous Driving via Optimization-Driven Multi-Frame Perturbation Framework</a>
                <a id="pdf-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yu" target="_blank">Yi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weizhen Han" target="_blank">Weizhen Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Libing Wu" target="_blank">Libing Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingyi Liu" target="_blank">Bingyi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enshu Wang" target="_blank">Enshu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuangzhuang Zhang" target="_blank">Zhuangzhuang Zhang</a>
            </p>
            <p id="summary-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="summary">Trajectory prediction plays a crucial role in autonomous driving systems, and exploring its vulnerability has garnered widespread attention. However, existing trajectory prediction attack methods often rely on single-point attacks to make efficient perturbations. This limits their applications in real-world scenarios due to the transient nature of single-point attacks, their susceptibility to filtration, and the uncertainty regarding the deployment environment. To address these challenges, this paper proposes a novel LiDAR-induced attack framework to impose multi-frame attacks by optimization-driven adversarial location search, achieving endurance, efficiency, and robustness. This framework strategically places objects near the adversarial vehicle to implement an attack and introduces three key innovations. First, successive state perturbations are generated using a multi-frame single-point attack strategy, effectively misleading trajectory predictions over extended time horizons. Second, we efficiently optimize adversarial objects' locations through three specialized loss functions to achieve desired perturbations. Lastly, we improve robustness by treating the adversarial object as a point without size constraints during the location search phase and reduce dependence on both the specific attack point and the adversarial object's properties. Extensive experiments confirm the superior performance and robustness of our framework.</p>
            <p id="subjects-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" onclick="foldPdfKimi('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="panel paper" keywords="baggage,sting,security,bee,ray,stcray,threats,scans,threat,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection_CVPR_2025_paper.html" target="_blank" title="82/388"><span class="index notranslate">#82</span></a>
                <a id="title-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="title-link" href="/venue/Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" target="_blank">STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection</a>
                <a id="pdf-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Divya Velayudhan" target="_blank">Divya Velayudhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abdelfatah Ahmed" target="_blank">Abdelfatah Ahmed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohamad Alansari" target="_blank">Mohamad Alansari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neha Gour" target="_blank">Neha Gour</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abderaouf Behouch" target="_blank">Abderaouf Behouch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taimur Hassan" target="_blank">Taimur Hassan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Syed Talal Wasim" target="_blank">Syed Talal Wasim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nabil Maalej" target="_blank">Nabil Maalej</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muzammal Naseer" target="_blank">Muzammal Naseer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juergen Gall" target="_blank">Juergen Gall</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammed Bennamoun" target="_blank">Mohammed Bennamoun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ernesto Damiani" target="_blank">Ernesto Damiani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naoufel Werghi" target="_blank">Naoufel Werghi</a>
            </p>
            <p id="summary-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="summary">Advancements in Computer-Aided Screening (CAS) systems are essential for improving the detection of security threats in X-ray baggage scans. However, current datasets are limited in representing real-world, sophisticated threats and concealment tactics, and existing approaches are constrained by a closed-set paradigm with predefined labels. To address these challenges, we introduce STCray, the first multimodal X-ray baggage security dataset, comprising 46,642 image-caption paired scans across 21 threat categories, generated using an X-ray scanner for airport security. STCray is meticulously developed with our specialized protocol that ensures domain-aware, coherent captions, that lead to the multi-modal instruction following data in X-ray baggage security. This allows us to train a domain-aware visual AI assistant named STING-BEE that supports a range of vision-language tasks, including scene comprehension, referring threat localization, visual grounding, and visual question answering (VQA), establishing novel baselines for multi-modal learning in X-ray baggage security. Further, STING-BEE shows state-of-the-art generalization in cross-domain settings. Our code, data, and pre-trained models will be made publicly available.</p>
            <p id="subjects-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" onclick="foldPdfKimi('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="panel paper" keywords="grounding,florence,refcoco,dino,object,visual,foundation,attribution,coco,lvis">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search_CVPR_2025_paper.html" target="_blank" title="83/388"><span class="index notranslate">#83</span></a>
                <a id="title-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="title-link" href="/venue/Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" target="_blank">Interpreting Object-level Foundation Models via Visual Precision Search</a>
                <a id="pdf-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruoyu Chen" target="_blank">Ruoyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Liang" target="_blank">Siyuan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingzhi Li" target="_blank">Jingzhi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiming Liu" target="_blank">Shiming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maosen Li" target="_blank">Maosen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Huang" target="_blank">Zhen Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hua Zhang" target="_blank">Hua Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaochun Cao" target="_blank">Xiaochun Cao</a>
            </p>
            <p id="summary-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="summary">Advances in multimodal pre-training have propelled object-level foundation models, such as Grounding DINO and Florence-2, in tasks like visual grounding and object detection. However, interpreting these models decisions has grown increasingly challenging. Existing interpretable attribution methods for object-level task interpretation have notable limitations: (1) gradient-based methods lack precise localization due to visual-textual fusion in foundation models, and (2) perturbation-based methods produce noisy saliency maps, limiting fine-grained interpretability. To address these, we propose a Visual Precision Search method that generates accurate attribution maps with fewer regions. Our method bypasses internal model parameters to overcome attribution issues from multimodal fusion, dividing inputs into sparse sub-regions and using consistency and collaboration scores to accurately identify critical decision-making regions. We also conducted a theoretical analysis of the boundary guarantees and scope of applicability of our method. Experiments on RefCOCO, MS COCO, and LVIS show our approach enhances object-level task interpretability over SOTA for Grounding DINO and Florence-2 across various evaluation metrics, with faithfulness gains of 23.7\%, 31.6\%, and 20.1\% on MS COCO, LVIS, and RefCOCO for Grounding DINO, and 102.9\% and 66.9\% on MS COCO and RefCOCO for Florence-2. Additionally, our method can interpret failures in visual grounding and object detection tasks, surpassing existing methods across multiple evaluation metrics.</p>
            <p id="subjects-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="panel paper" keywords="readout,photon,multiplexing,multiplexed,detector,reconstruction,photons,readouts,estimator,arrays">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays_CVPR_2025_paper.html" target="_blank" title="84/388"><span class="index notranslate">#84</span></a>
                <a id="title-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="title-link" href="/venue/Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" target="_blank">Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays</a>
                <a id="pdf-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shashwath Bharadwaj" target="_blank">Shashwath Bharadwaj</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruangrawee Kitichotkul" target="_blank">Ruangrawee Kitichotkul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Akshay Agarwal" target="_blank">Akshay Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vivek K Goyal" target="_blank">Vivek K Goyal</a>
            </p>
            <p id="summary-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="summary">Readout multiplexing is a promising solution to overcome hardware limitations and data bottlenecks in imaging with single-photon detectors. Conventional multiplexed readout processing creates an upper bound on photon counts at a very fine time scale, where measurements with multiple detected photons must either be discarded or allowed to introduce significant bias. We formulate multiphoton coincidence resolution as an inverse imaging problem and introduce a solution framework to probabilistically resolve the spatial locations of photon incidences. Specifically, we develop a theoretical abstraction of row--column multiplexing and a model of photon events that make readouts ambiguous. Using this, we propose a novel estimator that spatially resolves up to four coincident photons. Our estimator achieves a 3 to 4 dB increase in the peak signal-to-noise ratio of image reconstruction compared to traditional methods at higher incidence photon fluxes. Additionally, this method achieves a ~4 reduction in the required number of readout frames to achieve the same mean-squared error as other methods. Finally, our solution matches the Cramer-Rao bound for detection probability estimation for a wider range of incident flux values compared to conventional methods. While demonstrated for a specific detector type and readout architecture, this method can be extended to more general multiplexing with different detector models.</p>
            <p id="subjects-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" onclick="foldPdfKimi('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="panel paper" keywords="tracktention,temporal,attend,layer,consistency,video,videos,object,motion,leveraging">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better_CVPR_2025_paper.html" target="_blank" title="85/388"><span class="index notranslate">#85</span></a>
                <a id="title-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="title-link" href="/venue/Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" target="_blank">Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better</a>
                <a id="pdf-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zihang Lai" target="_blank">Zihang Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Vedaldi" target="_blank">Andrea Vedaldi</a>
            </p>
            <p id="summary-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="summary">Temporal consistency is critical in video prediction. Traditional methods, such as temporal attention mechanisms and 3D convolutions, often struggle with significant object movements and fail to capture long-range temporal dependencies in dynamic scenes. To address these limitations, we propose the Tracktention Layer, a novel architectural component that explicitly integrates motion information using point tracks  sequences of corresponding points across frames. By incorporating these motion cues, the Tracktention Layer enhances temporal alignment and effectively handles complex object motions, maintaining consistent feature representations over time. Our approach is computationally efficient and can be seamlessly integrated into existing models, such as Vision Transformers, with minimal modification. Empirical evaluations on standard video estimation benchmarks demonstrate that models augmented with the Tracktention Layer exhibit significantly improved temporal consistency compared to baseline models.</p>
            <p id="subjects-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" onclick="foldPdfKimi('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="panel paper" keywords="unprojected,rendering,texture,view,sparse,textures,geometry,rgb,appearance,deformation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double_CVPR_2025_paper.html" target="_blank" title="86/388"><span class="index notranslate">#86</span></a>
                <a id="title-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="title-link" href="/venue/Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" target="_blank">Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures</a>
                <a id="pdf-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guoxing Sun" target="_blank">Guoxing Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Dabral" target="_blank">Rishabh Dabral</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Heming Zhu" target="_blank">Heming Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Fua" target="_blank">Pascal Fua</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Theobalt" target="_blank">Christian Theobalt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Habermann" target="_blank">Marc Habermann</a>
            </p>
            <p id="summary-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="summary">Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or they ignore relevant sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address this issue, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods.</p>
            <p id="subjects-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" onclick="foldPdfKimi('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="panel paper" keywords="grids,metricgrids,grid,elementary,metric,nonlinear,hash,representations,decoder,based">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit_CVPR_2025_paper.html" target="_blank" title="87/388"><span class="index notranslate">#87</span></a>
                <a id="title-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="title-link" href="/venue/Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" target="_blank">MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation</a>
                <a id="pdf-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shu Wang" target="_blank">Shu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanbo Gao" target="_blank">Yanbo Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai Li" target="_blank">Shuai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chong Lv" target="_blank">Chong Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xun Cai" target="_blank">Xun Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuankun Li" target="_blank">Chuankun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Yuan" target="_blank">Hui Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinglin Zhang" target="_blank">Jinglin Zhang</a>
            </p>
            <p id="summary-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="summary">This paper presents MetricGrids, a novel grid-based neural representation that combines elementary metric grids in various metric spaces to approximate complex nonlinear signals. While grid-based representations are widely adopted for their efficiency and scalability, the existing feature grids with linear indexing for continuous-space points can only provide degenerate linear latent space representations, and such representations cannot be adequately compensated to represent complex nonlinear signals by the following compact decoder. To address this problem while keeping the simplicity of a regular grid structure, our approach builds upon the standard grid-based paradigm by constructing multiple elementary metric grids as high-order terms to approximate complex nonlinearities, following the Taylor expansion principle. Furthermore, we enhance model compactness with hash encoding based on different sparsities of the grids to prevent detrimental hash collisions, and a high-order extrapolation decoder to reduce explicit grid storage requirements. experimental results on both 2D and 3D reconstructions demonstrate the superior fitting and rendering accuracy of the proposed method across diverse signal types, validating its robustness and generalizability.</p>
            <p id="subjects-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" onclick="foldPdfKimi('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="panel paper" keywords="video,pose,estimation,generative,models,dust3r,overlap,trained,scenes,diverse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Can_Generative_Video_Models_Help_Pose_Estimation_CVPR_2025_paper.html" target="_blank" title="88/388"><span class="index notranslate">#88</span></a>
                <a id="title-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="title-link" href="/venue/Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" target="_blank">Can Generative Video Models Help Pose Estimation?</a>
                <a id="pdf-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_Can_Generative_Video_Models_Help_Pose_Estimation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruojin Cai" target="_blank">Ruojin Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Y. Zhang" target="_blank">Jason Y. Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philipp Henzler" target="_blank">Philipp Henzler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqi Li" target="_blank">Zhengqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ricardo Martin-Brualla" target="_blank">Ricardo Martin-Brualla</a>
            </p>
            <p id="summary-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="summary">Pairwise pose estimation from images with little or no overlap is an open challenge in computer vision. Existing methods, even those trained on large-scale datasets, struggle in these scenarios due to the lack of identifiable correspondences or visual overlap. Inspired by the human ability to infer spatial relationships from diverse scenes, we propose a novel approach that leverages the rich priors encoded within pre-trained generative video models. We propose to use a video model to hallucinate intermediate frames between two input images, effectively creating a dense, visual transition, which significantly simplifies the problem of pose estimation.Since current video models can still produce implausible motion or inconsistent geometry, we introduce a self-consistency score that evaluates the consistency of pose predictions from sampled videos.We demonstrate that our approach generalizes among three state-of-the-art video models and show consistent improvements over the state-of-the-art DUSt3R baseline on four diverse datasets encompassing indoor, outdoor, and object-centric scenes.Our findings suggest a promising avenue for improving pose estimation models by leveraging large generative models trained on vast amounts of video data, which is more readily available than 3D data.</p>
            <p id="subjects-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" onclick="foldPdfKimi('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="panel paper" keywords="composition,picd,photographic,mllms,understand,aesthetic,dataset,embed,embedding,specialized">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image_CVPR_2025_paper.html" target="_blank" title="89/388"><span class="index notranslate">#89</span></a>
                <a id="title-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="title-link" href="/venue/Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" target="_blank">Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding</a>
                <a id="pdf-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoran Zhao" target="_blank">Zhaoran Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Lu" target="_blank">Peng Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anran Zhang" target="_blank">Anran Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peipei Li" target="_blank">Peipei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xia Li" target="_blank">Xia Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuannan Liu" target="_blank">Xuannan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Hu" target="_blank">Yang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyi Chen" target="_blank">Shiyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liwei Wang" target="_blank">Liwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Guo" target="_blank">Wenhao Guo</a>
            </p>
            <p id="summary-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="summary">With the rapid growth of social media and digital photography, visually appealing images have become essential for effective communication and emotional engagement. Among the factors influencing aesthetic appeal, compositionthe arrangement of visual elements within a frameplays a crucial role. In recent years, specialized models for photographic composition have achieved impressive results across various aesthetic tasks. Meanwhile, rapidly advancing multimodal large language models (MLLMs) have excelled in several visual perception tasks. However, their ability to embed and understand compositional information remains underexplored, primarily due to the lack of suitable evaluation datasets. To address this gap, we introduce the Photographic Image Composition Dataset (PICD), a large-scale dataset consisting of 36,857 images categorized into 24 composition categories across 355 diverse scenes. We demonstrate the advantages of PICD over existing datasets in terms of data scale, composition category, label quality, and scene diversity. Building on PICD, we establish benchmarks to evaluate the composition embedding capabilities of specialized models and the compositional understanding ability of MLLMs. To enable efficient and effective evaluation, we propose a novel Composition Discrimination Accuracy (CDA) metric. Our evaluation highlights the limitations of current models and provides insights into directions for improving their ability to embed and understand composition.</p>
            <p id="subjects-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" onclick="foldPdfKimi('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="panel paper" keywords="meshes,lightweight,meshing,mesh,sdfs,curvature,fidelity,reconstruction,clouds,surface">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds_CVPR_2025_paper.html" target="_blank" title="90/388"><span class="index notranslate">#90</span></a>
                <a id="title-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="title-link" href="/venue/Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" target="_blank">High-Fidelity Lightweight Mesh Reconstruction from Point Clouds</a>
                <a id="pdf-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Zhang" target="_blank">Chen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Wang" target="_blank">Wentao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ximeng Li" target="_blank">Ximeng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyao Liao" target="_blank">Xinyao Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanjuan Su" target="_blank">Wanjuan Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbing Tao" target="_blank">Wenbing Tao</a>
            </p>
            <p id="summary-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="summary">Recently, learning signed distance functions (SDFs) from point clouds has become popular for reconstruction. To ensure accuracy, most methods require using high-resolution Marching Cubes for surface extraction. However, this results in redundant mesh elements, making the mesh inconvenient to use. To solve the problem, we propose an adaptive meshing method to extract resolution-adaptive meshes based on surface curvature, enabling the recovery of high-fidelity lightweight meshes. Specifically, we first use point-based representation to perceive implicit surfaces and calculate surface curvature. A vertex generator is designed to produce curvature-adaptive vertices with any specified number on the implicit surface, preserving the overall structure and high-curvature features. Then we develop a Delaunay meshing algorithm to generate meshes from vertices, ensuring geometric fidelity and correct topology. In addition, to obtain accurate SDFs for adaptive meshing and achieve better lightweight reconstruction, we design a hybrid representation combining feature grid and feature tri-plane for better detail capture. Experiments demonstrate that our method can generate high-quality lightweight meshes from point clouds. Compared with methods from various categories, our approach achieves superior results, especially in capturing more details with fewer elements.</p>
            <p id="subjects-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="panel paper" keywords="unireal,editing,generation,tasks,image,universal,frames,videos,world,consistency">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics_CVPR_2025_paper.html" target="_blank" title="91/388"><span class="index notranslate">#91</span></a>
                <a id="title-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="title-link" href="/venue/Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" target="_blank">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</a>
                <a id="pdf-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Chen" target="_blank">Xi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhifei Zhang" target="_blank">Zhifei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Zhang" target="_blank">He Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqian Zhou" target="_blank">Yuqian Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soo Ye Kim" target="_blank">Soo Ye Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Liu" target="_blank">Qing Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yijun Li" target="_blank">Yijun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianming Zhang" target="_blank">Jianming Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nanxuan Zhao" target="_blank">Nanxuan Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yilin Wang" target="_blank">Yilin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Ding" target="_blank">Hui Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Lin" target="_blank">Zhe Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengshuang Zhao" target="_blank">Hengshuang Zhao</a>
            </p>
            <p id="summary-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="summary">We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation models that effectively balance consistency and variation across frames, we propose a unifying approach that treats image-level tasks as discontinuous video generation. Specifically, we treat varying numbers of input and output images as frames, enabling seamless support for tasks such as image generation, editing, composition, etc. Although designed for image-level tasks, we leverage videos as a scalable source for universal supervision. UniReal learns world dynamics from large-scale videos, demonstrating advanced capability in handling shadows, reflections, pose variation, and object interaction, while also exhibiting emergent capability for novel applications.</p>
            <p id="subjects-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" onclick="foldPdfKimi('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="panel paper" keywords="diffusiondrive,diffusion,driving,end,policy,denoising,truncated,steps,navsim,autonomous">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving_CVPR_2025_paper.html" target="_blank" title="92/388"><span class="index notranslate">#92</span></a>
                <a id="title-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="title-link" href="/venue/Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" target="_blank">DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving</a>
                <a id="pdf-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bencheng Liao" target="_blank">Bencheng Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaoyu Chen" target="_blank">Shaoyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoran Yin" target="_blank">Haoran Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Jiang" target="_blank">Bo Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Wang" target="_blank">Cheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sixu Yan" target="_blank">Sixu Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinbang Zhang" target="_blank">Xinbang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyu Li" target="_blank">Xiangyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Zhang" target="_blank">Ying Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Zhang" target="_blank">Qian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinggang Wang" target="_blank">Xinggang Wang</a>
            </p>
            <p id="summary-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="summary">Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-125" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-126"><span class="mn" id="MathJax-Span-127" style="font-family: MathJax_Main;">10</span><span class="mo" id="MathJax-Span-128" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>10</mn><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-26">10\times</script> reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-129" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-130"><span class="mn" id="MathJax-Span-131" style="font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-27">2</script> steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;88.1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-132" style="width: 2.138em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.72em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-133"><span class="mn" id="MathJax-Span-134" style="font-family: MathJax_Main;">88.1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>88.1</mn></math></span></span><script type="math/tex" id="MathJax-Element-28">88.1</script> PDMS without bells and whistles, setting a new record, while running at a real-time speed of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;45&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-135" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-136"><span class="mn" id="MathJax-Span-137" style="font-family: MathJax_Main;">45</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>45</mn></math></span></span><script type="math/tex" id="MathJax-Element-29">45</script> FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available for future research.</p>
            <p id="subjects-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" onclick="foldPdfKimi('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="panel paper" keywords="canopy,open,resolution,height,km,imagery,satellite,computer,vision,access">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html" target="_blank" title="93/388"><span class="index notranslate">#93</span></a>
                <a id="title-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="title-link" href="/venue/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" target="_blank">Open-Canopy: Towards Very High Resolution Forest Monitoring</a>
                <a id="pdf-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fajwel Fogel" target="_blank">Fajwel Fogel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yohann Perron" target="_blank">Yohann Perron</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikola Besic" target="_blank">Nikola Besic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurent Saint-Andr" target="_blank">Laurent Saint-Andr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Agns Pellissier-Tanon" target="_blank">Agns Pellissier-Tanon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Schwartz" target="_blank">Martin Schwartz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Boudras" target="_blank">Thomas Boudras</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ibrahim Fayad" target="_blank">Ibrahim Fayad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre d'Aspremont" target="_blank">Alexandre d'Aspremont</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Loic Landrieu" target="_blank">Loic Landrieu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philippe Ciais" target="_blank">Philippe Ciais</a>
            </p>
            <p id="summary-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="summary">Estimating canopy height and its changes at meter resolution from satellite imagery is a significant challenge in computer vision with critical environmental applications.However, the lack of open-access datasets at this resolution hinders the reproducibility and evaluation of models. We introduce Open-Canopy, the first open-access, country-scale benchmark for very high-resolution (1.5 m) canopy height estimation, covering over 87,000 km across France with 1.5 m resolution satellite imagery and aerial LiDAR data.Additionally, we present Open-Canopy-, a benchmark for canopy height change detection between images from different years at tree levela challenging task for current computer vision models. We evaluate state-of-the-art architectures on these benchmarks, highlighting significant challenges and opportunities for improvement. Our datasets and code are publicly available at [URL].</p>
            <p id="subjects-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" onclick="foldPdfKimi('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="panel paper" keywords="textual,visual,representations,captions,smartclip,clip,radford2021learning,aligning,misalignment,modular">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html" target="_blank" title="94/388"><span class="index notranslate">#94</span></a>
                <a id="title-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="title-link" href="/venue/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" target="_blank">SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</a>
                <a id="pdf-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF">24</sup>]</a>
                <a id="copy-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaoan Xie" target="_blank">Shaoan Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjing Lingjing" target="_blank">Lingjing Lingjing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujia Zheng" target="_blank">Yujia Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Yao" target="_blank">Yu Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Tang" target="_blank">Zeyu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eric P. Xing" target="_blank">Eric P. Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangyi Chen" target="_blank">Guangyi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhang" target="_blank">Kun Zhang</a>
            </p>
            <p id="summary-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="summary">Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning} has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning.However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard.On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts -- ultimately limiting its generalization on certain downstream tasks involving short prompts.In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only \emph{preserve} cross-modal semantic information in its entirety but also \emph{disentangle} visual representations to capture fine-grained textual concepts. Building on this foundation, we introduce \ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory.</p>
            <p id="subjects-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" onclick="foldPdfKimi('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="panel paper" keywords="colorization,manganinja,reference,precise,line,art,thoughtful,character,color,specializes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following_CVPR_2025_paper.html" target="_blank" title="95/388"><span class="index notranslate">#95</span></a>
                <a id="title-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="title-link" href="/venue/Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" target="_blank">MangaNinja: Line Art Colorization with Precise Reference Following</a>
                <a id="pdf-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiheng Liu" target="_blank">Zhiheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ka Leong Cheng" target="_blank">Ka Leong Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Chen" target="_blank">Xi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Xiao" target="_blank">Jie Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Ouyang" target="_blank">Hao Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Zhu" target="_blank">Kai Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Liu" target="_blank">Yu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Shen" target="_blank">Yujun Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifeng Chen" target="_blank">Qifeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Luo" target="_blank">Ping Luo</a>
            </p>
            <p id="summary-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="summary">Derived from diffusion models, MangaNinja specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases (*e.g.*, extreme poses and shadows), cross-character colorization, multi-reference harmonization, *etc.*, beyond the reach of existing algorithms.</p>
            <p id="subjects-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" onclick="foldPdfKimi('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="panel paper" keywords="sam2,rvos,video,streaming,segmentation,samwise,infusing,tracking,wisdom,object">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation_CVPR_2025_paper.html" target="_blank" title="96/388"><span class="index notranslate">#96</span></a>
                <a id="title-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" target="_blank">SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation</a>
                <a id="pdf-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Claudia Cuttano" target="_blank">Claudia Cuttano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriele Trivigno" target="_blank">Gabriele Trivigno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gabriele Rosi" target="_blank">Gabriele Rosi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carlo Masone" target="_blank">Carlo Masone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giuseppe Averta" target="_blank">Giuseppe Averta</a>
            </p>
            <p id="summary-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="summary">Referring Video Object Segmentation (RVOS) relies on natural language expressions to segment an object in a video clip.Existing methods restrict reasoning either to independent short clips, losing global context, or process the entire video offline, impairing their application in a streaming fashion. In this work, we aim to surpass these limitations and design an RVOS method capable of effectively operating in streaming-like scenarios while retaining contextual information from past frames. We build upon the Segment-Anything 2 (SAM2) model, that provides robust segmentation and tracking capabilities and is naturally suited for streaming processing. We make SAM2 wiser, by empowering it with natural language understanding and explicit temporal modeling at the feature extraction stage, without fine-tuning its weights, and without outsourcing modality interaction to external models. To this end, we introduce a novel adapter module that injects temporal information and multi-modal cues in the feature extraction process. We further reveal the phenomenon of tracking bias in SAM2 and propose a learnable module to adjust its tracking focus when the current frame features suggest a new object more aligned with the caption. Our proposed method, \ours, achieves state-of-the-art across various benchmarks, by adding a negligible overhead of just 4.2 M parameters.</p>
            <p id="subjects-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="panel paper" keywords="slam3r,pointmaps,reconstruction,rgb,monocular,dense,scene,real,regresses,videos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos_CVPR_2025_paper.html" target="_blank" title="97/388"><span class="index notranslate">#97</span></a>
                <a id="title-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="title-link" href="/venue/Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" target="_blank">SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</a>
                <a id="pdf-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzheng Liu" target="_blank">Yuzheng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyan Dong" target="_blank">Siyan Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuzhe Wang" target="_blank">Shuzhe Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingda Yin" target="_blank">Yingda Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanchao Yang" target="_blank">Yanchao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingnan Fan" target="_blank">Qingnan Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoquan Chen" target="_blank">Baoquan Chen</a>
            </p>
            <p id="summary-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="summary">In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R provides an end-to-end solution by seamlessly integrating local 3D reconstruction and global coordinate registration through feed-forward neural networks. Given a video input, the system first converts it into overlapping clips using a sliding window mechanism. Unlike traditional pose optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB images and then progressively aligns and deforms these local pointmaps to create a globally consistent scene reconstruction - all without explicitly solving any camera parameters. Experiments across datasets consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy and completeness while maintaining real-time performance at 20+ FPS. Upon acceptance, we will release our code to support further research.</p>
            <p id="subjects-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" onclick="foldPdfKimi('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="panel paper" keywords="distortion,panorama,nfov,panoramas,clip,textbf,panodecouple,distort,completion,visual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Panorama_Generation_From_NFoV_Image_Done_Right_CVPR_2025_paper.html" target="_blank" title="98/388"><span class="index notranslate">#98</span></a>
                <a id="title-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="title-link" href="/venue/Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" target="_blank">Panorama Generation From NFoV Image Done Right</a>
                <a id="pdf-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_Panorama_Generation_From_NFoV_Image_Done_Right_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dian Zheng" target="_blank">Dian Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Zhang" target="_blank">Cheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao-Ming Wu" target="_blank">Xiao-Ming Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cao Li" target="_blank">Cao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengfei Lv" target="_blank">Chengfei Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian-Fang Hu" target="_blank">Jian-Fang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Shi Zheng" target="_blank">Wei-Shi Zheng</a>
            </p>
            <p id="summary-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="summary">Generating 360-degree panoramas from narrow field of view (NFoV) image is a promising computer vision task for Virtual Reality (VR) applications. Existing methods mostly assess the generated panoramas with InceptionNet or CLIP based metrics, which tend to perceive the image quality and is <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;not suitable for evaluating the distortion&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-138" style="width: 23.909em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1019.9em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-139"><span class="texatom" id="MathJax-Span-140"><span class="mrow" id="MathJax-Span-141"><span class="mtext" id="MathJax-Span-142" style="font-family: MathJax_Main-bold;">not suitable for evaluating the distortion</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">not suitable for evaluating the distortion</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-30">\textbf{not suitable for evaluating the distortion}</script>. In this work, we first propose a distortion-specific CLIP, named Distort-CLIP to accurately evaluate the panorama distortion and discover the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;``visual cheating''&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-143" style="width: 11.044em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1009.12em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-144"><span class="texatom" id="MathJax-Span-145"><span class="mrow" id="MathJax-Span-146"><span class="mtext" id="MathJax-Span-147" style="font-family: MathJax_Main-bold;">``visual cheating''</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">``visual cheating''</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-31">\textbf{``visual cheating''}</script> phenomenon in previous works (i.e., tending to improve the visual results by sacrificing distortion accuracy). This phenomenon arises because prior methods employ a single network to learn the distinct panorama distortion and content completion at once, which leads the model to prioritize optimizing the latter. To address the phenomenon, we propose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;PanoDecouple&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-148" style="width: 8.648em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.19em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1007.14em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-149"><span class="texatom" id="MathJax-Span-150"><span class="mrow" id="MathJax-Span-151"><span class="mtext" id="MathJax-Span-152" style="font-family: MathJax_Main-bold;">PanoDecouple</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">PanoDecouple</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-32">\textbf{PanoDecouple}</script>, a decoupled diffusion model framework, which decouples the panorama generation into distortion guidance and content completion, aiming to generate panoramas with both accurate distortion and visual appeal. Specifically, we design a DistortNet for distortion guidance by imposing panorama-specific distortion prior and a modified condition registration mechanism; and a ContentNet for content completion by imposing perspective image information. Additionally, a distortion correction loss function with Distort-CLIP is introduced to constrain the distortion explicitly. The extensive experiments validate that PanoDecouple surpasses existing methods both in distortion and visual metrics.</p>
            <p id="subjects-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" onclick="foldPdfKimi('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="panel paper" keywords="contrastive,memory,tile,loss,batch,gpu,a800,80gb,strategy,similarity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy_CVPR_2025_paper.html" target="_blank" title="99/388"><span class="index notranslate">#99</span></a>
                <a id="title-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="title-link" href="/venue/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" target="_blank">Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy</a>
                <a id="pdf-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zesen Cheng" target="_blank">Zesen Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Zhang" target="_blank">Hang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kehan Li" target="_blank">Kehan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sicong Leng" target="_blank">Sicong Leng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqiang Hu" target="_blank">Zhiqiang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Wu" target="_blank">Fei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deli Zhao" target="_blank">Deli Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Li" target="_blank">Xin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lidong Bing" target="_blank">Lidong Bing</a>
            </p>
            <p id="summary-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="summary">Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, the full instantiation of the similarity matrix demands substantial GPU memory, making large batch training highly resource-intensive. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into small blocks, avoiding full materialization of the similarity matrix. Additionally, we introduce a multi-level tiling implementation to leverage the hierarchical structure of distributed systems, using ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method significantly reduces GPU memory usage in contrastive loss. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M using only 8 A800 80GB GPUs, without sacrificing accuracy. Compared to state-of-the-art memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.</p>
            <p id="subjects-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" onclick="foldPdfKimi('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="panel paper" keywords="material,anything,lighting,diffusion,masks,objects,switcher,materials,object,confidence">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion_CVPR_2025_paper.html" target="_blank" title="100/388"><span class="index notranslate">#100</span></a>
                <a id="title-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" target="_blank">Material Anything: Generating Materials for Any 3D Object via Diffusion</a>
                <a id="pdf-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Huang" target="_blank">Xin Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengfei Wang" target="_blank">Tengfei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Liu" target="_blank">Ziwei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qing Wang" target="_blank">Qing Wang</a>
            </p>
            <p id="summary-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="summary">We present **Material Anything**, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.</p>
            <p id="subjects-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="panel paper" keywords="generative,finc,sample,types,clustering,models,produced,fourier,differential,differently">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach_CVPR_2025_paper.html" target="_blank" title="101/388"><span class="index notranslate">#101</span></a>
                <a id="title-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" target="_blank">Unveiling Differences in Generative Models: A Scalable Differential Clustering Approach</a>
                <a id="pdf-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingwei Zhang" target="_blank">Jingwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Jalali" target="_blank">Mohammad Jalali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheuk Ting Li" target="_blank">Cheuk Ting Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Farzan Farnia" target="_blank">Farzan Farnia</a>
            </p>
            <p id="summary-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="summary">An interpretable comparison of generative models requires the identification of sample types produced differently by each of the involved models. While several quantitative scores have been proposed in the literature to rank different generative models, such score-based evaluations do not reveal the nuanced differences between the generative models in capturing various sample types. In this work, we attempt to solve a differential clustering problem to detect sample types expressed differently by two generative models. To solve the differential clustering problem, we propose a method called Fourier-based Identification of Novel Clusters (FINC) to identify modes produced by a generative model with a higher frequency in comparison to a reference distribution. FINC provides a scalable stochastic algorithm based on random Fourier features to estimate the eigenspace of kernel covariance matrices of two generative models and utilize the principal eigendirections to detect the sample types present more dominantly in each model. We demonstrate the application of the FINC method to large-scale computer vision datasets and generative model frameworks. Our numerical results suggest the scalability of the developed Fourier-based method in highlighting the sample types produced with different frequencies by widely-used generative models.</p>
            <p id="subjects-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="panel paper" keywords="generation,autoregressive,tokens,parallel,dependencies,visual,parallelized,token,sequential,speedup">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Parallelized_Autoregressive_Visual_Generation_CVPR_2025_paper.html" target="_blank" title="102/388"><span class="index notranslate">#102</span></a>
                <a id="title-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="title-link" href="/venue/Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" target="_blank">Parallelized Autoregressive Visual Generation</a>
                <a id="pdf-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Parallelized_Autoregressive_Visual_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF">21</sup>]</a>
                <a id="copy-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqing Wang" target="_blank">Yuqing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhuai Ren" target="_blank">Shuhuai Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijie Lin" target="_blank">Zhijie Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujin Han" target="_blank">Yujin Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyuan Guo" target="_blank">Haoyuan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenheng Yang" target="_blank">Zhenheng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Difan Zou" target="_blank">Difan Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiashi Feng" target="_blank">Jiashi Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xihui Liu" target="_blank">Xihui Liu</a>
            </p>
            <p id="summary-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="summary">Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process.In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling.Our key insight is that the feasibility of parallel generation is closely tied to visual token dependencies - while tokens with weak dependencies can be generated in parallel, adjacent tokens with strong dependencies are hard to generate together, as independent sampling of strongly correlated tokens may lead to inconsistent decisions.Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Specifically, we first generate initial tokens in each region sequentially to establish the global structure, then enable parallel generation across distant regions while maintaining sequential generation within each region. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-153" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-154"><span class="mo" id="MathJax-Span-155" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-33">\times</script> speedup with comparable quality and up to 9.5<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-156" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-157"><span class="mo" id="MathJax-Span-158" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-34">\times</script> speedup with minimal quality degradation across both image and video generation tasks.We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling.</p>
            <p id="subjects-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="panel paper" keywords="seedvr,restoration,video,diffusion,transformer,generic,seeding,window,degradations,world">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration_CVPR_2025_paper.html" target="_blank" title="103/388"><span class="index notranslate">#103</span></a>
                <a id="title-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="title-link" href="/venue/Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" target="_blank">SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</a>
                <a id="pdf-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianyi Wang" target="_blank">Jianyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijie Lin" target="_blank">Zhijie Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meng Wei" target="_blank">Meng Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Zhao" target="_blank">Yang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ceyuan Yang" target="_blank">Ceyuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Change Loy" target="_blank">Chen Change Loy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Jiang" target="_blank">Lu Jiang</a>
            </p>
            <p id="summary-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="summary">Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtext&gt;SeedVR&lt;/mtext&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-159" style="width: 4.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.492em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.49em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-160"><span class="mtext" id="MathJax-Span-161" style="font-family: MathJax_Main;">SeedVR</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>SeedVR</mtext></math></span></span><script type="math/tex" id="MathJax-Element-35">\text{SeedVR}</script>, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.</p>
            <p id="subjects-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" onclick="foldPdfKimi('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="panel paper" keywords="continual,kac,arnold,kan,kolmogorov,learning,classifier,tasks,classifiers,rbf">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning_CVPR_2025_paper.html" target="_blank" title="104/388"><span class="index notranslate">#104</span></a>
                <a id="title-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="title-link" href="/venue/Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" target="_blank">KAC: Kolmogorov-Arnold Classifier for Continual Learning</a>
                <a id="pdf-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF">18</sup>]</a>
                <a id="copy-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yusong Hu" target="_blank">Yusong Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Liang" target="_blank">Zichen Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Yang" target="_blank">Fei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qibin Hou" target="_blank">Qibin Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xialei Liu" target="_blank">Xialei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Ming Cheng" target="_blank">Ming-Ming Cheng</a>
            </p>
            <p id="summary-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="summary">Continual learning requires models to train continuously across consecutive tasks without forgetting. Most existing methods utilize linear classifiers, which struggle to maintain a stable classification space while learning new tasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in preserving learning stability during simple continual regression tasks, we set out to explore their potential in more complex continual learning scenarios. In this paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel classifier developed for continual learning based on the KAN structure. We delve into the impact of KAN's spline functions and introduce Radial Basis Functions (RBF) for improved compatibility with continual learning. We replace linear classifiers with KAC in several recent approaches and conduct experiments across various continual learning benchmarks, all of which demonstrate performance improvements, highlighting the effectiveness and robustness of KAC in continual learning.</p>
            <p id="subjects-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" onclick="foldPdfKimi('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="panel paper" keywords="paa,aesthetics,physique,personalized,assessment,gaa,physiqueaa,exemplification,individual,aesthetic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An_CVPR_2025_paper.html" target="_blank" title="105/388"><span class="index notranslate">#105</span></a>
                <a id="title-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="title-link" href="/venue/Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" target="_blank">Rethinking Personalized Aesthetics Assessment: Employing Physique Aesthetics Assessment as An Exemplification</a>
                <a id="pdf-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haobin Zhong" target="_blank">Haobin Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuai He" target="_blank">Shuai He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anlong Ming" target="_blank">Anlong Ming</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huadong Ma" target="_blank">Huadong Ma</a>
            </p>
            <p id="summary-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="summary">The Personalized Aesthetics Assessment (PAA) aims to accurately predict an individual's unique perception of aesthetics. With the surging demand for customization, PAA enables applications to generate personalized outcomes by aligning with individual aesthetic preferences. The prevailing PAA paradigm involves two stages: pre-training and fine-tuning, but it faces three inherent challenges: 1) The model is pre-trained using datasets of the Generic Aesthetics Assessment (GAA), but the collective preferences of GAA lead to conflicts in individualized aesthetic predictions. 2) The scope and stage of personalized surveys are related to both the user and the assessed object; however, the prevailing personalized surveys fail to adequately address assessed objects' characteristics. 3) During application usage, the cumulative multimodal feedback from an individual holds great value that should be considered for improving the PAA model but unfortunately attracts insufficient attention. To address the aforementioned challenges, we introduce a new PAA paradigm called PAA+, which is structured into three distinct stages: pre-training, fine-tuning, and domain-incremental learning. Furthermore, to better reflect individual differences, we employ a familiar and intuitive application, physique aesthetics assessment (PhysiqueAA), to validate the PAA+ paradigm. We propose a dataset called PhysiqueAA50K, consisting of over 50,000 fully annotated physique images. Furthermore, we develop a PhysiqueAA framework (PhysiqueFrame) and conduct a large-scale benchmark, achieving state-of-the-art (SOTA) performance. Our research is expected to provide an innovative roadmap and application for the PAA community. The code and dataset are available in the supplementary.</p>
            <p id="subjects-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" onclick="foldPdfKimi('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="panel paper" keywords="style,editing,editor,object,centric,text,patch,loss,tmps,directional">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Park_Style-Editor_Text-driven_Object-centric_Style_Editing_CVPR_2025_paper.html" target="_blank" title="106/388"><span class="index notranslate">#106</span></a>
                <a id="title-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="title-link" href="/venue/Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" target="_blank">Style-Editor: Text-driven Object-centric Style Editing</a>
                <a id="pdf-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Park_Style-Editor_Text-driven_Object-centric_Style_Editing_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jihun Park" target="_blank">Jihun Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jongmin Gim" target="_blank">Jongmin Gim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyoungmin Lee" target="_blank">Kyoungmin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seunghun Lee" target="_blank">Seunghun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunghoon Im" target="_blank">Sunghoon Im</a>
            </p>
            <p id="summary-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="summary">We present Text-driven object-centric style editing model named Style-Editor, a novel method that guides style editing at an object-centric level using textual inputs.The core of Style-Editor is our Patch-wise Co-Directional (PCD) loss, meticulously designed for precise object-centric editing that are closely aligned with the input text. This loss combines a patch directional loss for text-guided style direction and a patch distribution consistency loss for even CLIP embedding distribution across object regions. It ensures a seamless and harmonious style editing across object regions.Key to our method are the Text-Matched Patch Selection (TMPS) and Pre-fixed Region Selection (PRS) modules for identifying object locations via text, eliminating the need for segmentation masks. Lastly, we introduce an Adaptive Background Preservation (ABP) loss to maintain the original style and structural essence of the images background. This loss is applied to dynamically identified background areas.Extensive experiments underline the effectiveness of our approach in creating visually coherent and textually aligned style editing.</p>
            <p id="subjects-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" onclick="foldPdfKimi('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="panel paper" keywords="omnimatte,video,generative,decompose,layers,shadows,occluded,reflections,decompositions,videos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers_CVPR_2025_paper.html" target="_blank" title="107/388"><span class="index notranslate">#107</span></a>
                <a id="title-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="title-link" href="/venue/Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" target="_blank">Generative Omnimatte: Learning to Decompose Video into Layers</a>
                <a id="pdf-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yao-Chih Lee" target="_blank">Yao-Chih Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erika Lu" target="_blank">Erika Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sarah Rumbley" target="_blank">Sarah Rumbley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michal Geyer" target="_blank">Michal Geyer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jia-Bin Huang" target="_blank">Jia-Bin Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tali Dekel" target="_blank">Tali Dekel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Forrester Cole" target="_blank">Forrester Cole</a>
            </p>
            <p id="summary-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="summary">Given a video and a set of input object masks, an omnimatte method aims to decompose the video into semantically meaningful layers containing individual objects along with their associated effects, such as shadows and reflections.Existing omnimatte methods assume a static background or accurate pose and depth estimation and produce poor decompositions when these assumptions are violated. Furthermore, due to the lack of generative prior on natural videos, existing methods cannot complete dynamic occluded regions.We present a novel generative layered video decomposition framework to address the omnimatte problem. Our method does not assume a stationary scene or require camera pose or depth information and produces clean, complete layers, including convincing completions of occluded dynamic regions. Our core idea is to train a video diffusion model to identify and remove scene effects caused by a specific object. We show that this model can be finetuned from an existing video inpainting model with a small, carefully curated dataset, anddemonstrate high-quality decompositions and editing results for a wide range of casually captured videos containing soft shadows, glossy reflections, splashing water, and more.</p>
            <p id="subjects-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" onclick="foldPdfKimi('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="panel paper" keywords="domain,annealing,generalization,gradients,gga,conflicts,training,minima,guided,eludes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ballas_Gradient-Guided_Annealing_for_Domain_Generalization_CVPR_2025_paper.html" target="_blank" title="108/388"><span class="index notranslate">#108</span></a>
                <a id="title-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="title-link" href="/venue/Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" target="_blank">Gradient-Guided Annealing for Domain Generalization</a>
                <a id="pdf-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ballas_Gradient-Guided_Annealing_for_Domain_Generalization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF">19</sup>]</a>
                <a id="copy-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aristotelis Ballas" target="_blank">Aristotelis Ballas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christos Diou" target="_blank">Christos Diou</a>
            </p>
            <p id="summary-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="summary">Domain Generalization (DG) research has gained considerable traction as of late, since the ability to generalize to unseen data distributions is a requirement that eludes even state-of-the-art training algorithms. In this paper we observe that the initial iterations of model training play a key role in domain generalization effectiveness, since the loss landscape may be significantly different across the training and test distributions, contrary to the case of i.i.d. data. Conflicts between gradients of the loss components of each domain lead the optimization procedure to undesirable local minima that do not capture the domain-invariant features of the target classes. We propose alleviating domain conflicts in model optimization, by iteratively annealing the parameters of a model in the early stages of training and searching for points where gradients align between domains. By discovering a set of parameter values where gradients are updated towards the same direction for each data distribution present in the training set, the proposed Gradient-Guided Annealing (GGA) algorithm encourages models to seek out minima that exhibit improved robustness against domain shifts. The efficacy of GGA is evaluated on five widely accepted and challenging image classification domain generalization benchmarks, where its use alone is able to establish highly competitive or even state-of-the-art performance. Moreover, when combined with previously proposed domain-generalization algorithms it is able to consistently improve their effectiveness by significant margins.</p>
            <p id="subjects-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" onclick="foldPdfKimi('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="panel paper" keywords="manivideo,object,occlusion,hand,manipulation,generalizable,video,dexterous,mlo,grasping">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping_CVPR_2025_paper.html" target="_blank" title="109/388"><span class="index notranslate">#109</span></a>
                <a id="title-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="title-link" href="/venue/Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" target="_blank">ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping</a>
                <a id="pdf-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Youxin Pang" target="_blank">Youxin Pang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruizhi Shao" target="_blank">Ruizhi Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Zhang" target="_blank">Jiajun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanzhang Tu" target="_blank">Hanzhang Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Liu" target="_blank">Yun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyao Zhou" target="_blank">Boyao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongwen Zhang" target="_blank">Hongwen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yebin Liu" target="_blank">Yebin Liu</a>
            </p>
            <p id="summary-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="summary">In this paper, we introduce ManiVideo, a novel method for generating consistent and temporally coherent bimanual hand-object manipulation videos from given motion sequences of hands and objects. The core idea of ManiVideo is the construction of a multi-layer occlusion (MLO) representation that learns 3D occlusion relationships from occlusion-free normal maps and occlusion confidence maps. By embedding the MLO structure into the UNet in two forms, the model enhances the 3D consistency of dexterous hand-object manipulation. To further achieve the generalizable grasping of objects, we integrate Objaverse, a large-scale 3D object dataset, to address the scarcity of video data, thereby facilitating the learning of extensive object consistency. Additionally, we propose an innovative training strategy that effectively integrates multiple datasets, supporting downstream tasks such as human-centric hand-object manipulation video generation. Through extensive experiments, we demonstrate that our approach not only achieves video generation with plausible hand-object interaction and generalizable objects, but also outperforms existing SOTA methods.</p>
            <p id="subjects-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" onclick="foldPdfKimi('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="panel paper" keywords="spatial,mpdrive,marker,visual,perception,coordinates,autonomous,driving,vqa,prompt">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous_CVPR_2025_paper.html" target="_blank" title="110/388"><span class="index notranslate">#110</span></a>
                <a id="title-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="title-link" href="/venue/Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" target="_blank">MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving</a>
                <a id="pdf-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Zhang" target="_blank">Zhiyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofan Li" target="_blank">Xiaofan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Xu" target="_blank">Zhihao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenjie Peng" target="_blank">Wenjie Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zijian Zhou" target="_blank">Zijian Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miaojing Shi" target="_blank">Miaojing Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuangping Huang" target="_blank">Shuangping Huang</a>
            </p>
            <p id="summary-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="summary">Autonomous driving visual question answering (AD-VQA) aims to answer questions related to perception, prediction, and planning based on given driving scene images, heavily relying on the model's spatial perception capabilities.Previous works typically express spatial comprehension through textual representations of spatial coordinates, resulting in semantic gaps between visual coordinate representations and textual descriptions.This oversight hinders the accurate transmission of spatial information and increases the expressive burden.To address this, we propose Marker-based Prompt Learning framework (MPDrive), which transforms spatial coordinates into concise visual markers, ensuring linguistic consistency and enhancing the accuracy of visual perception and spatial expression in AD-VQA.Specifically, MPDrive converts complex spatial coordinates into text-based visual marker predictions, simplifying the expression of spatial information for autonomous decision-making.Moreover, we introduce visual marker images as conditional inputs and integrate object-level fine-grained features to further enhance multi-level spatial perception abilities.Extensive experiments on the DriveLM and CODA-LM datasets show that MPDrive performs at state-of-the-art levels, particularly in cases requiring sophisticated spatial understanding.</p>
            <p id="subjects-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="panel paper" keywords="urban,micromobility,autonomous,agents,robots,bench,robot,simulation,scalable,efficiency">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation_CVPR_2025_paper.html" target="_blank" title="111/388"><span class="index notranslate">#111</span></a>
                <a id="title-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="title-link" href="/venue/Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" target="_blank">Towards Autonomous Micromobility through Scalable Urban Simulation</a>
                <a id="pdf-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wayne Wu" target="_blank">Wayne Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Honglin He" target="_blank">Honglin He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoyuan Zhang" target="_blank">Chaoyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jack He" target="_blank">Jack He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seth Z. Zhao" target="_blank">Seth Z. Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Gong" target="_blank">Ran Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanyi Li" target="_blank">Quanyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bolei Zhou" target="_blank">Bolei Zhou</a>
            </p>
            <p id="summary-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="summary">Micromobility, which utilizes lightweight devices moving in urban public spaces - such as delivery robots and electric wheelchairs - emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM -- a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH -- a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's unique strengths and limitations. This work will be open-sourced and under sustainable maintenance to foster future research in autonomous micromobility.</p>
            <p id="subjects-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" onclick="foldPdfKimi('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="panel paper" keywords="orderless,rendering,gradients,histogram,differentiable,locally,derivatives,images,inverse,parameters">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering_CVPR_2025_paper.html" target="_blank" title="112/388"><span class="index notranslate">#112</span></a>
                <a id="title-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="title-link" href="/venue/Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" target="_blank">Locally Orderless Images for Optimization in Differentiable Rendering</a>
                <a id="pdf-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ishit Mehta" target="_blank">Ishit Mehta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Manmohan Chandraker" target="_blank">Manmohan Chandraker</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ravi Ramamoorthi" target="_blank">Ravi Ramamoorthi</a>
            </p>
            <p id="summary-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="summary">Problems in differentiable rendering often involve optimizing scene parameters that cause motion in image space. The gradients for such parameters tend to be sparse, leading to poor convergence. While existing methods address this sparsity through proxy gradients such as topological derivatives or lagrangian derivatives, they make simplifying assumptions about rendering. Multi-resolution image pyramids offer an alternative approach but prove unreliable in practice. We introduce a method that uses locally orderless images --- where each pixel maps to a histogram of intensities that preserves local variations in appearance. Using an inverse rendering objective that minimizes histogram distance, our method extends support for sparsely defined image gradients and recovers optimal parameters. We validate our method on various inverse problems using both synthetic and real data.</p>
            <p id="subjects-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" onclick="foldPdfKimi('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="panel paper" keywords="modal,image,scans,pre,modality,multi,training,medical,cross,mri">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis_CVPR_2025_paper.html" target="_blank" title="113/388"><span class="index notranslate">#113</span></a>
                <a id="title-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="title-link" href="/venue/Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" target="_blank">Multi-modal Vision Pre-training for Medical Image Analysis</a>
                <a id="pdf-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF">20</sup>]</a>
                <a id="copy-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohao Rui" target="_blank">Shaohao Rui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingzhi Chen" target="_blank">Lingzhi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Tang" target="_blank">Zhenyu Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lilong Wang" target="_blank">Lilong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mianxin Liu" target="_blank">Mianxin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaoting Zhang" target="_blank">Shaoting Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaosong Wang" target="_blank">Xiaosong Wang</a>
            </p>
            <p id="summary-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="summary">Self-supervised learning has greatly facilitated medical image analysis by suppressing the training data requirement for real-world applications. Current paradigms predominantly rely on self-supervision within uni-modal image data, thereby neglecting the inter-modal correlations essential for effective learning of cross-modal image representations. This limitation is particularly significant for naturally grouped multi-modal data, e.g., multi-parametric MRI scans for a patient undergoing various functional imaging protocols in the same study. To bridge this gap, we conduct a novel multi-modal image pre-training with three proxy tasks to facilitate the learning of cross-modality representations and correlations using multi-modal brain MRI scans (over 2.4 million images in 16,022 scans of 3,755 patients), i.e., cross-modal image reconstruction, modality-aware contrastive learning, and modality template distillation. To demonstrate the generalizability of our pre-trained model, we conduct extensive experiments on various benchmarks with ten downstream tasks. The superior performance of our method is reported in comparison to state-of-the-art pre-training methods, with Dice Score improvement of 0.28\%-14.47\% across six segmentation benchmarks and a consistent accuracy boost of 0.65\%-18.07\% in four individual image classification tasks.</p>
            <p id="subjects-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" onclick="foldPdfKimi('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="panel paper" keywords="sacb,registration,convolution,spatial,kernels,net,awareness,feature,spatially,sacbs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration_CVPR_2025_paper.html" target="_blank" title="114/388"><span class="index notranslate">#114</span></a>
                <a id="title-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="title-link" href="/venue/Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" target="_blank">SACB-Net: Spatial-awareness Convolutions for Medical Image Registration</a>
                <a id="pdf-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF">16</sup>]</a>
                <a id="copy-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinxing Cheng" target="_blank">Xinxing Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyang Zhang" target="_blank">Tianyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Lu" target="_blank">Wenqi Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingjie Meng" target="_blank">Qingjie Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alejandro F. Frangi" target="_blank">Alejandro F. Frangi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinming Duan" target="_blank">Jinming Duan</a>
            </p>
            <p id="summary-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="summary">Deep learning-based image registration methods have shown state-of-the-art performance and rapid inference speeds.Despite these advances, many existing approaches fall short in capturing spatially varying information in non-local regions of feature maps due to the reliance on spatially-shared convolution kernels. This limitation leads to suboptimal estimation of deformation fields. In this paper, we propose a 3D Spatial-Awareness Convolution Block (SACB) to enhance the spatial information within feature representations. Our SACB estimates the spatial clusters within feature maps by leveraging feature similarity and subsequently parameterizes the adaptive convolution kernels across diverse regions. This adaptive mechanism generates the convolution kernels (weights and biases) tailored to spatial variations, thereby enabling the network to effectively capture spatially varying information. Building on SACB, we introduce a pyramid flow estimator (named SACB-Net) that integrates SACBs to facilitate multi-scale flow composition, particularly addressing large deformations. Experimental results on the brain IXI and LPBA datasets as well as Abdomen CT datasets demonstrate the effectiveness of SACB and the superiority of SACB-Net over the state-of-the-art learning-based registration methods. The code will be made publicly available.</p>
            <p id="subjects-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" onclick="foldPdfKimi('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kaneko_Structure_from_Collision@CVPR2025@CVF" class="panel paper" keywords="sfc,structure,nerf,invisible,internal,collision,optima,volume,visible,3dgs">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kaneko_Structure_from_Collision_CVPR_2025_paper.html" target="_blank" title="115/388"><span class="index notranslate">#115</span></a>
                <a id="title-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="title-link" href="/venue/Kaneko_Structure_from_Collision@CVPR2025@CVF" target="_blank">Structure from Collision</a>
                <a id="pdf-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kaneko_Structure_from_Collision@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kaneko_Structure_from_Collision_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kaneko_Structure_from_Collision@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kaneko_Structure_from_Collision@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kaneko_Structure_from_Collision@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kaneko_Structure_from_Collision@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kaneko_Structure_from_Collision@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Author</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Takuhiro Kaneko" target="_blank">Takuhiro Kaneko</a>
            </p>
            <p id="summary-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="summary">Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have made accurate estimation of the 3D structure from multiview images possible. However, this capability is limited to estimating the visible external structure, and it is still difficult to identify the invisible internal structure hidden behind the surface. To overcome this limitation, we address a new task called structure from collision (SfC), which aims to estimate the structure (including the invisible internal one) of an object from the appearance changes at collision. To solve this task, we propose a novel model called SfC-NeRF, which optimizes the invisible internal structure (i.e., internal volume density) of the object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and key-frame constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing, i.e., searching for the global optima by repeatedly reducing and expanding the volume. Extensive experiments on 60 cases involving diverse structures (i.e., various cavity shapes, locations, and sizes) and various material properties reveal the properties of SfC and demonstrate the effectiveness of the proposed SfC-NeRF.</p>
            <p id="subjects-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kaneko_Structure_from_Collision@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kaneko_Structure_from_Collision@CVPR2025@CVF" onclick="foldPdfKimi('Kaneko_Structure_from_Collision@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="panel paper" keywords="uni4d,dynamic,visual,modeling,video,understanding,pretrained,models,unifying,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html" target="_blank" title="116/388"><span class="index notranslate">#116</span></a>
                <a id="title-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="title-link" href="/venue/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" target="_blank">Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video</a>
                <a id="pdf-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=David Yifan Yao" target="_blank">David Yifan Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert J. Zhai" target="_blank">Albert J. Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shenlong Wang" target="_blank">Shenlong Wang</a>
            </p>
            <p id="summary-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="summary">This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing large visual models for 4D understanding.</p>
            <p id="subjects-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" onclick="foldPdfKimi('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="panel paper" keywords="domain,ucdsl,continual,mpl,prototype,unsupervised,shift,domains,representations,specific">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling_CVPR_2025_paper.html" target="_blank" title="117/388"><span class="index notranslate">#117</span></a>
                <a id="title-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="title-link" href="/venue/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" target="_blank">Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling</a>
                <a id="pdf-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haopeng Sun" target="_blank">Haopeng Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingwei Zhang" target="_blank">Yingwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lumin Xu" target="_blank">Lumin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Jin" target="_blank">Sheng Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Luo" target="_blank">Ping Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chen Qian" target="_blank">Chen Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Liu" target="_blank">Wentao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiqiang Chen" target="_blank">Yiqiang Chen</a>
            </p>
            <p id="summary-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="summary">In real-world applications, deep neural networks may encounter constantly changing environments, where the test data originates from continually shifting unlabeled target domains. This problem, known as Unsupervised Continual Domain Shift Learning (UCDSL), poses practical difficulties. Existing methods for UCDSL aim to learn domain-invariant representations for all target domains. However, due to the existence of adaptivity gap, the invariant representation may theoretically lead to large joint errors. To overcome the limitation, we propose a novel UCDSL method, called Multi-Prototype Modeling (MPM). Our model comprises two key components: (1) Multi-Prototype Learning (MPL) for acquiring domain-specific representations using multiple domain-specific prototypes. MPL achieves domain-specific error minimization instead of enforcing feature alignment across different domains. (2) Bi-Level Graph Enhancer (BiGE) for enhancing domain-level and category-level representations, resulting in more accurate predictions. We provide theoretical and empirical analysis to demonstrate the effectiveness of our proposed method. We evaluate our approach on multiple benchmark datasets and show that our model surpasses state-of-the-art methods across all datasets, highlighting its effectiveness and robustness in handling unsupervised continual domain shift learning. Codes will be publicly accessible.</p>
            <p id="subjects-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" onclick="foldPdfKimi('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="panel paper" keywords="scene,anticipation,stsg,spatio,graph,temporal,generation,unbiased,robust,vidsgg">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.html" target="_blank" title="118/388"><span class="index notranslate">#118</span></a>
                <a id="title-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="title-link" href="/venue/Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" target="_blank">Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation</a>
                <a id="pdf-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rohith Peddi" target="_blank">Rohith Peddi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saurabh Saurabh" target="_blank">Saurabh Saurabh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ayush Abhay Shrivastava" target="_blank">Ayush Abhay Shrivastava</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Parag Singla" target="_blank">Parag Singla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vibhav Gogate" target="_blank">Vibhav Gogate</a>
            </p>
            <p id="summary-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="summary">Spatio-Temporal Scene Graphs (STSGs) provide a concise and expressive representation of dynamic scenes by modelling objects and their evolving relationships over time. However, real-world visual relationships often exhibit a long-tailed distribution, causing existing methods for tasks like Video Scene Graph Generation (VidSGG) and Scene Graph Anticipation (SGA) to produce biased scene graphs. To this end, we propose \textbf{ImparTail}, a novel training framework that leverages curriculum learning and loss masking to mitigate bias in the generation and anticipation of spatio-temporal scene graphs. Our approach gradually decreases the dominance of the head relationship classes during training and focuses more on tail classes, leading to more balanced training. Furthermore, we introduce two new tasksRobust Spatio-Temporal Scene Graph Generation and Robust Scene Graph Anticipationdesigned to evaluate the robustness of STSG models against distribution shifts. Extensive experiments on the Action Genome dataset demonstrate that our framework significantly enhances the unbiased performance and robustness of STSG models compared to existing methods.</p>
            <p id="subjects-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" onclick="foldPdfKimi('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="panel paper" keywords="sevc,nvcs,temporal,references,contexts,latent,spatial,video,prior,augmented">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding_CVPR_2025_paper.html" target="_blank" title="119/388"><span class="index notranslate">#119</span></a>
                <a id="title-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="title-link" href="/venue/Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" target="_blank">Augmented Deep Contexts for Spatially Embedded Video Coding</a>
                <a id="pdf-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Bian" target="_blank">Yifan Bian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuanbo Tang" target="_blank">Chuanbo Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Li" target="_blank">Li Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Liu" target="_blank">Dong Liu</a>
            </p>
            <p id="summary-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="summary">Most Neural Video Codecs (NVCs) only employ temporal references to generate temporal-only contexts and latent prior. These temporal-only NVCs fail to handle large motions or emerging objects due to limited contexts and misaligned latent prior. To relieve the limitations, we propose a Spatially Embedded Video Codec (SEVC), in which the low-resolution video is compressed for spatial references. Firstly, our SEVC leverages both spatial and temporal references to generate augmented motion vectors and hybrid spatial-temporal contexts. Secondly, to address the misalignment issue in latent prior and enrich the prior information, we introduce a spatial-guided latent prior augmented by multiple temporal latent representations. At last, we design a joint spatial-temporal optimization to learn quality-adaptive bit allocation for spatial references, further boosting rate-distortion performance. Experimental results show that our SEVC effectively alleviates the limitations in handling large motions or emerging objects, and also reduces 11.9\% more bitrate than the previous state-of-the-art NVC while providing an additional low-resolution bitstream.</p>
            <p id="subjects-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" onclick="foldPdfKimi('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="panel paper" keywords="mllms,moe,experts,continual,answering,visual,knowledge,multimodal,rmoe,language">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts_CVPR_2025_paper.html" target="_blank" title="120/388"><span class="index notranslate">#120</span></a>
                <a id="title-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="title-link" href="/venue/Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" target="_blank">CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering</a>
                <a id="pdf-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Huai" target="_blank">Tianyu Huai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Zhou" target="_blank">Jie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingjiao Wu" target="_blank">Xingjiao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qin Chen" target="_blank">Qin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingchun Bai" target="_blank">Qingchun Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ze Zhou" target="_blank">Ze Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang He" target="_blank">Liang He</a>
            </p>
            <p id="summary-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="summary">Multimodal large language models (MLLMs) have garnered widespread attention from researchers due to their remarkable understanding and generation capabilities in visual language tasks (e.g., visual question answering). However, the rapid pace of knowledge updates in the real world makes offline training of MLLMs costly, and when faced with non-stationary data streams, MLLMs suffer from catastrophic forgetting during learning. In this paper, we propose an MLLMs-based dual momentum Mixture-of-Experts (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;CL-MoE&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-162" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.13em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-163"><span class="texatom" id="MathJax-Span-164"><span class="mrow" id="MathJax-Span-165"><span class="mtext" id="MathJax-Span-166" style="font-family: MathJax_Typewriter;">CL-MoE</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">CL-MoE</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-36">\texttt{CL-MoE}</script>) framework for continual visual question answering. We integrate MLLMs with continual learning to utilize the rich commonsense knowledge in LLMs.We introduce a Dual-Router MoE (RMoE) to select the global and local experts using task-level and instance-level routers, to robustly assign weights to the experts most appropriate for the task. Then, we design a dynamic Momentum MoE (MMoE) to update the parameters of experts dynamically based on the relationships between the experts and tasks, so that the model can absorb new knowledge while maintaining existing knowledge. The extensive experimental results indicate that our method achieves state-of-the-art performance on 10 VQA tasks, proving the effectiveness of our approach. The codes and weights will be released on GitHub.</p>
            <p id="subjects-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" onclick="foldPdfKimi('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="panel paper" keywords="hot3d,egocentric,hand,objects,view,hands,tracking,multi,object,umetrack">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View_CVPR_2025_paper.html" target="_blank" title="121/388"><span class="index notranslate">#121</span></a>
                <a id="title-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="title-link" href="/venue/Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" target="_blank">HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</a>
                <a id="pdf-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Prithviraj Banerjee" target="_blank">Prithviraj Banerjee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sindi Shkodrani" target="_blank">Sindi Shkodrani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pierre Moulon" target="_blank">Pierre Moulon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shreyas Hampali" target="_blank">Shreyas Hampali</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangchen Han" target="_blank">Shangchen Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Zhang" target="_blank">Fan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linguang Zhang" target="_blank">Linguang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jade Fountain" target="_blank">Jade Fountain</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edward Miller" target="_blank">Edward Miller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Selen Basol" target="_blank">Selen Basol</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Newcombe" target="_blank">Richard Newcombe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robert Wang" target="_blank">Robert Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Julian Engel" target="_blank">Jakob Julian Engel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Hodan" target="_blank">Tomas Hodan</a>
            </p>
            <p id="summary-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="summary">We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects. In addition to simple pick-up/observe/put-down actions, HOT3D contains scenarios resembling typical actions in a kitchen, office, and living room environment. The dataset is recorded by two head-mounted devices from Meta: Project Aria, a research prototype of light-weight AR/AI glasses, and Quest 3, a production VR headset sold in millions of units. Ground-truth poses were obtained by a professional motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.</p>
            <p id="subjects-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" onclick="foldPdfKimi('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="panel paper" keywords="meshgen,pbr,auto,augmentation,meshes,texture,render,encoder,mesh,textures">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative_CVPR_2025_paper.html" target="_blank" title="122/388"><span class="index notranslate">#122</span></a>
                <a id="title-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="title-link" href="/venue/Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" target="_blank">MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation</a>
                <a id="pdf-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zilong Chen" target="_blank">Zilong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yikai Wang" target="_blank">Yikai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqiang Sun" target="_blank">Wenqiang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Wang" target="_blank">Feng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiwen Chen" target="_blank">Yiwen Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaping Liu" target="_blank">Huaping Liu</a>
            </p>
            <p id="summary-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="summary">In this paper, we introduce MeshGen, an advanced image-to-3D pipeline that generates high-quality 3D meshes with detailed geometry and physically based rendering (PBR) textures. Addressing the challenges faced by existing 3D native diffusion models, such as suboptimal auto-encoder performance, limited controllability, poor generalization, and inconsistent image-based PBR texturing, MeshGen employs several key innovations to overcome these limitations. We pioneer a render-enhanced point-to-shape auto-encoder that compresses meshes into a compact latent space, by designing perceptual optimization with ray-based regularization. This ensures that the 3D shapes are accurately represented and reconstructed to preserve geometric details within the latent space. To address data scarcity and image-shape misalignment, we further propose geometric augmentation and generative rendering augmentation techniques, which enhance the model's controllability and generalization ability, allowing it to perform well even with limited public datasets. For texture generation, MeshGen employs a reference attention-based multi-view ControlNet for consistent appearance synthesis. This is further complemented by our multi-view PBR decomposer that estimates PBR components and a UV inpainter that fills invisible areas, ensuring a seamless and consistent texture across the 3D mesh. Our extensive experiments demonstrate that MeshGen largely outperforms previous methods in both shape and texture generation, setting a new standard for the quality of 3D meshes generated with PBR textures.</p>
            <p id="subjects-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" onclick="foldPdfKimi('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="panel paper" keywords="coser,multiview,dense,text,views,view,generator,consistent,neighbor,scores">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation_CVPR_2025_paper.html" target="_blank" title="123/388"><span class="index notranslate">#123</span></a>
                <a id="title-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="title-link" href="/venue/Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" target="_blank">CoSER: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation</a>
                <a id="pdf-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bonan Li" target="_blank">Bonan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zicheng Zhang" target="_blank">Zicheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyi Yang" target="_blank">Xingyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchao Wang" target="_blank">Xinchao Wang</a>
            </p>
            <p id="summary-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="summary">Generating dense multiview images from text prompts is crucial for creating high-fidelity 3D assets. Nevertheless, existing methods struggle with space-view correspondences, resulting in sparse and low-quality outputs. In this paper, we introduce CoSER, a novel consistent dense Multiview Text-to-Image Generator for Text-to-3D, achieving both efficiency and quality by meticulously learning neighbor-view coherence and further alleviating ambiguity through the swift traversal of all views. For achieving neighbor-view consistency, each viewpoint densely interacts with adjacent viewpoints to perceive the global spatial structure, and aggregates information along motion paths explicitly defined by physical principles to refine details. To further enhance cross-view consistency and alleviate content drift, CoSER rapidly scan all views in spiral bidirectional manner to aware holistic information and then scores each point based on semantic material. Subsequently, we conduct weighted down-sampling along the spatial dimension based on scores, thereby facilitating prominent information fusion across all views with lightweight computation. Technically, the core module is built by integrating the attention mechanism with a selective state space model, exploiting the robust learning capabilities of the former and the low overhead of the latter. Extensive evaluation shows that CoSER is capable of producing dense, high-fidelity, content-consistent multiview images that can be flexibly integrated into various 3D generation models. Code will be released upon acceptance.</p>
            <p id="subjects-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" onclick="foldPdfKimi('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="panel paper" keywords="subspace,peft,attention,tuning,filter,coeff,head,tunable,tune,transformers">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large_CVPR_2025_paper.html" target="_blank" title="124/388"><span class="index notranslate">#124</span></a>
                <a id="title-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="title-link" href="/venue/Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" target="_blank">Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models</a>
                <a id="pdf-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Miao" target="_blank">Zichen Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Chen" target="_blank">Wei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Qiu" target="_blank">Qiang Qiu</a>
            </p>
            <p id="summary-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="summary">Transformer-based large pre-trained models have shown remarkable generalization ability, and various parameter-efficient fine-tuning (PEFT) methods have been proposed to customize these models on downstream tasks with minimal computational and memory budgets. Previous PEFT methods are primarily designed from a tensor-decomposition perspective that tries to effectively tune the linear transformation by finding the smallest subset of parameters to train. Our study adopts an orthogonal view by representing the attention operation as a graph convolution and formulating the multi-head attention maps as a convolutional filter subspace, with each attention map as a subspace element. In this paper, we propose to tune the large pre-trained transformers by learning a small set of combination coefficients that construct a more expressive filter subspace from the original multi-head attention maps. We show analytically and experimentally that the tuned filter subspace can effectively expand the feature space of the multi-head attention and further enhance the capacity of transformers. We further stabilize the fine-tuning with a residual parameterization of the tunable subspace coefficients, and enhance the generalization with a regularization design by directly applying dropout on the tunable coefficient during training. The tunable coefficients take a tiny number of parameters and can be combined with previous PEFT methods in a plug-and-play manner. Extensive experiments show that our approach achieves superior performances than PEFT baselines with neglectable additional parameters.</p>
            <p id="subjects-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" onclick="foldPdfKimi('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="panel paper" keywords="olympus,router,tasks,mllms,chained,computer,vision,universal,routing,delegates">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks_CVPR_2025_paper.html" target="_blank" title="125/388"><span class="index notranslate">#125</span></a>
                <a id="title-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="title-link" href="/venue/Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" target="_blank">Olympus: A Universal Task Router for Computer Vision Tasks</a>
                <a id="pdf-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanze Lin" target="_blank">Yuanze Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunsheng Li" target="_blank">Yunsheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongdong Chen" target="_blank">Dongdong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijian Xu" target="_blank">Weijian Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ronald Clark" target="_blank">Ronald Clark</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Philip Torr" target="_blank">Philip Torr</a>
            </p>
            <p id="summary-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="summary">We introduce Olympus, a new approach that transforms Multimodal Large Language Models (MLLMs) into a unified framework capable of handling a wide array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates over 20 specialized tasks across images, videos, and 3D objects to dedicated modules. This instruction-based routing enables complex workflows through chained actions without the need for training heavy generative models. Olympus easily integrates with existing MLLMs, expanding their capabilities with comparable performance. Experimental results demonstrate that Olympus achieves an average routing accuracy of 94.75% across 20 tasks and precision of 91.82% in chained action scenarios, showcasing its effectiveness as a universal task router that can solve a diverse range of computer vision tasks.</p>
            <p id="subjects-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" onclick="foldPdfKimi('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="panel paper" keywords="3dgs,volumetrically,rasterization,splatting,rendering,transmittance,rasterizer,fewer,tomography,consistent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization_CVPR_2025_paper.html" target="_blank" title="126/388"><span class="index notranslate">#126</span></a>
                <a id="title-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="title-link" href="/venue/Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" target="_blank">Volumetrically Consistent 3D Gaussian Rasterization</a>
                <a id="pdf-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chinmay Talegaonkar" target="_blank">Chinmay Talegaonkar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yash Belhe" target="_blank">Yash Belhe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ravi Ramamoorthi" target="_blank">Ravi Ramamoorthi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Antipa" target="_blank">Nicholas Antipa</a>
            </p>
            <p id="summary-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="summary">Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds.However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy.We show that splatting and its approximations are unnecessary, even within a rasterizer;we instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically.We use this analytic transmittance to derive more physically accurate alpha values than 3DGS, which can directly be used within their framework. The result is a method that more closely follows the volume rendering equation (similar to ray tracing) while enjoying the speed benefits of rasterization. Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS.This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS).Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points.</p>
            <p id="subjects-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" onclick="foldPdfKimi('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="panel paper" keywords="canonicalization,object,objaverse,semantic,category,framework,canonical,shot,consistency,samples">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency_CVPR_2025_paper.html" target="_blank" title="127/388"><span class="index notranslate">#127</span></a>
                <a id="title-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="title-link" href="/venue/Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" target="_blank">One-shot 3D Object Canonicalization based on Geometric and Semantic Consistency</a>
                <a id="pdf-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Li Jin" target="_blank">Li Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujie Wang" target="_blank">Yujie Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenzheng Chen" target="_blank">Wenzheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiyu Dai" target="_blank">Qiyu Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingzhe Gao" target="_blank">Qingzhe Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueying Qin" target="_blank">Xueying Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoquan Chen" target="_blank">Baoquan Chen</a>
            </p>
            <p id="summary-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="summary">3D object canonicalization is a fundamental task, essential for a variety of downstream tasks. Existing methods rely on either cumbersome manual processes or priors learned from extensive, per-category training samples. Real-world datasets, however, often exhibit long-tail distributions, challenging existing learning-based methods, especially in categories with limited samples. We address this by introducing the first one-shot category-level object canonicalization framework, requiring only a single canonical model as a reference (the "prior model") for each category. To canonicalize any object, our framework first extracts semantic cues with large language models (LLMs) and vision-language models (VLMs) to establish correspondences with the prior model. We introduce a novel loss function to enforce geometric and semantic consistency, aligning object orientations precisely despite significant shape variations. Moreover, we adopt a support-plane strategy to reduce search space for initial poses and utilize a semantic relationship map to select the canonical pose from multiple hypotheses. Extensive experiments on multiple datasets demonstrate that our framework achieves state-of-the-art performance and validate key design choices. Using our framework, we create the Canonical Objaverse Dataset (COD), canonicalizing 33K samples in the Objaverse-LVIS dataset, underscoring the effectiveness of our framework on handling large-scale datasets.</p>
            <p id="subjects-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" onclick="foldPdfKimi('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="panel paper" keywords="manipulation,trajectories,egocentric,6dof,vision,exo,action,hot3d,generating,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric_CVPR_2025_paper.html" target="_blank" title="128/388"><span class="index notranslate">#128</span></a>
                <a id="title-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="title-link" href="/venue/Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" target="_blank">Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision</a>
                <a id="pdf-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tomoya Yoshida" target="_blank">Tomoya Yoshida</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhei Kurita" target="_blank">Shuhei Kurita</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taichi Nishimura" target="_blank">Taichi Nishimura</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shinsuke Mori" target="_blank">Shinsuke Mori</a>
            </p>
            <p id="summary-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="summary">Learning to use tools or objects in common scenes, particularly handling them in various ways as instructed, is a key challenge for developing interactive robots. Training models to generate such manipulation trajectories requires a large and diverse collection of detailed manipulation demonstrations for various objects, which is nearly unfeasible to gather at scale. In this paper, we propose a framework that leverages large-scale ego- and exo-centric video datasets --- constructed globally with substantial effort --- of Exo-Ego4D to extract diverse manipulation trajectories at scale. From these extracted trajectories with the associated textual action description, we develop trajectory generation models based on visual and point cloud-based language models. In the recently proposed egocentric vision-based in-a-quality trajectory dataset of HOT3D, we confirmed that our models successfully generate valid object trajectories, establishing a training dataset and baseline models for the novel task of generating 6DoF manipulation trajectories from action descriptions in egocentric vision. Our dataset and code is available upon acceptance.</p>
            <p id="subjects-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" onclick="foldPdfKimi('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="panel paper" keywords="volumetric,reconstruction,diffusion,posterior,monoplanar,light,view,fields,single,transport">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D_CVPR_2025_paper.html" target="_blank" title="129/388"><span class="index notranslate">#129</span></a>
                <a id="title-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="title-link" href="/venue/Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" target="_blank">Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes</a>
                <a id="pdf-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ludwic Leonard" target="_blank">Ludwic Leonard</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nils Thurey" target="_blank">Nils Thurey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rdiger Westermann" target="_blank">Rdiger Westermann</a>
            </p>
            <p id="summary-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="summary">We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality.</p>
            <p id="subjects-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" onclick="foldPdfKimi('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="panel paper" keywords="veu,bench,vid,editing,video,textbf,llms,understanding,tasks,nderstanding">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.html" target="_blank" title="130/388"><span class="index notranslate">#130</span></a>
                <a id="title-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="title-link" href="/venue/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" target="_blank">VEU-Bench: Towards Comprehensive Understanding of Video Editing</a>
                <a id="pdf-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bozheng Li" target="_blank">Bozheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongliang Wu" target="_blank">Yongliang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Lu" target="_blank">Yi Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiashuo Yu" target="_blank">Jiashuo Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Licheng Tang" target="_blank">Licheng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawang Cao" target="_blank">Jiawang Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqing Zhu" target="_blank">Wenqing Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuyang Sun" target="_blank">Yuyang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jay Wu" target="_blank">Jay Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbo Zhu" target="_blank">Wenbo Zhu</a>
            </p>
            <p id="summary-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="summary">Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (\textbf{V}ideo \textbf{E}diting \textbf{U}nderstanding \textbf{Bench}mark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars\footnote{Named after the Academy Awards.}, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3\% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3\% across nine reasoning tasks. The code and data will be made available.</p>
            <p id="subjects-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" onclick="foldPdfKimi('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="panel paper" keywords="modal,class,alignment,probability,representation,anchor,multi,generative,modalities,modality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning_CVPR_2025_paper.html" target="_blank" title="131/388"><span class="index notranslate">#131</span></a>
                <a id="title-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="title-link" href="/venue/Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" target="_blank">Generative Modeling of Class Probability for Multi-Modal Representation Learning</a>
                <a id="pdf-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=JungKyoo Shin" target="_blank">JungKyoo Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bumsoo Kim" target="_blank">Bumsoo Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eunwoo Kim" target="_blank">Eunwoo Kim</a>
            </p>
            <p id="summary-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="summary">Multi-modal understanding plays a crucial role in artificial intelligence by enabling models to jointly interpret inputs from different modalities. However, conventional approaches such as contrastive learning often struggle with modality discrepancies, leading to potential misalignments. In this paper, we propose a novel class anchor alignment approach that leverages class probability distributions for multi-modal representation learning. Our method, Class-anchor-ALigned generative Modeling (CALM), encodes class anchors as prompts to generate and align class probability distributions for each modality, enabling more flexible alignment. Furthermore, we introduce a cross-modal probabilistic variational autoencoder to model uncertainty in the alignment, enhancing the ability to capture deeper relationships between modalities and data variations. Extensive experiments on four benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, especially in out-of-domain evaluations. This highlights its superior generalization capabilities in multi-modal representation learning.</p>
            <p id="subjects-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" onclick="foldPdfKimi('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="panel paper" keywords="immersive,volumetric,videos,imvid,capture,multi,engagement,stimulate,view,dof">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement_CVPR_2025_paper.html" target="_blank" title="132/388"><span class="index notranslate">#132</span></a>
                <a id="title-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="title-link" href="/venue/Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" target="_blank">ImViD: Immersive Volumetric Videos for Enhanced VR Engagement</a>
                <a id="pdf-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengxian Yang" target="_blank">Zhengxian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shi Pan" target="_blank">Shi Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengqi Wang" target="_blank">Shengqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoxiang Wang" target="_blank">Haoxiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Lin" target="_blank">Li Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guanjun Li" target="_blank">Guanjun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqi Wen" target="_blank">Zhengqi Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Borong Lin" target="_blank">Borong Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianhua Tao" target="_blank">Jianhua Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Yu" target="_blank">Tao Yu</a>
            </p>
            <p id="summary-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="summary">User engagement is greatly enhanced by fully immersive multimodal experiences that combine visual and auditory stimuli. Consequently, the next frontier in VR/AR technologies lies in immersive volumetric videos with complete scene capture, large 6-DoF interactive space, Multi-modal feedback, and high resolution\&amp;frame-rate contents. To stimulate the reconstruction of immersive volumetric videos, we introduce **ImViD**, a multi-view, multi-modal dataset featuring complete space-oriented data capture and various indoor/outdoor scenarios. Our capture rig supports multi-view video-audio capture while on the move, a capability absent in existing datasets, which significantly enhances the completeness, flexibility, and efficiency of data capture. The captured multi-view videos (with synchronized audios) are in 5K resolution at 60FPS, lasting from 1-5 minutes, and include rich foreground-background elements, and complex dynamics. We benchmark existing methods using our dataset and establish a base pipeline for constructing immersive volumetric videos from multi-view audiovisual inputs for 6-DoF multimodal immersive VR experiences. The benchmark and the reconstruction and interaction results demonstrate the effectiveness of our dataset and baseline method, which we believe will stimulate future research on immersive volumetric video production.</p>
            <p id="subjects-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" onclick="foldPdfKimi('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="panel paper" keywords="sora,generalizable,singular,components,domain,peft,decomposed,soma,minor,value">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable_CVPR_2025_paper.html" target="_blank" title="133/388"><span class="index notranslate">#133</span></a>
                <a id="title-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="title-link" href="/venue/Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" target="_blank">SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning</a>
                <a id="pdf-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF">13</sup>]</a>
                <a id="copy-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seokju Yun" target="_blank">Seokju Yun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seunghye Chae" target="_blank">Seunghye Chae</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongheon Lee" target="_blank">Dongheon Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngmin Ro" target="_blank">Youngmin Ro</a>
            </p>
            <p id="summary-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="summary">Domain generalization (DG) aims to adapt a model using one or multiple source domains to ensure robust performance in unseen target domains. Recently, Parameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising results in the context of DG problem. Nevertheless, existing PEFT methods still struggle to strike a balance between preserving generalizable components of the pre-trained model and learning task-specific features. To gain insights into the distribution of generalizable components, we begin by analyzing the pre-trained weights through the lens of singular value decomposition. Building on these insights, we introduce Singular Value Decomposed Low-Rank Adaptation (SoRA), an approach that selectively tunes minor singular components while keeping the residual parts frozen. SoRA effectively retains the generalization ability of the pre-trained model while efficiently acquiring task-specific skills. Furthermore, we freeze domain-generalizable blocks and employ an annealing weight decay strategy, thereby achieving an optimal balance in the delicate trade-off between generalizability and discriminability. SoRA attains state-of-the-art results on multiple benchmarks that span both domain generalized semantic segmentation to object detection. In addition, our methods introduce no additional inference overhead or regularization loss, maintain compatibility with any backbone or head, and are designed to be versatile, allowing easy integration into a wide range of tasks.The code will be open-sourced for reproducibility.</p>
            <p id="subjects-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" onclick="foldPdfKimi('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="panel paper" keywords="memory,activation,tta,surgeon,adaptation,terminals,accuracy,sparsity,test,dynamic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity_CVPR_2025_paper.html" target="_blank" title="134/388"><span class="index notranslate">#134</span></a>
                <a id="title-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="title-link" href="/venue/Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" target="_blank">SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity</a>
                <a id="pdf-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Ma" target="_blank">Ke Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Tang" target="_blank">Jiaqi Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Guo" target="_blank">Bin Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Dang" target="_blank">Fan Dang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sicong Liu" target="_blank">Sicong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhui Zhu" target="_blank">Zhui Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Wu" target="_blank">Lei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Fang" target="_blank">Cheng Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying-Cong Chen" target="_blank">Ying-Cong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiwen Yu" target="_blank">Zhiwen Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhao Liu" target="_blank">Yunhao Liu</a>
            </p>
            <p id="summary-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="summary">Despite the growing integration of deep models into mobile and embedded terminals, the accuracy of these models often declines significantly during inference due to various deployment interferences. Test-time adaptation (TTA) has emerged as an effective strategy to improve the performance of deep models by adapting them to unlabeled target data online. Yet, the significant memory cost, particularly in memory-constrained IoT terminals, impedes the effective deployment of most backward-propagation-based TTA methods. To tackle memory constraints, we introduce SURGEON, a method that substantially reduces memory cost while preserving comparable accuracy improvements during fully test-time adaptation (FTTA) without relying on specific network architectures or modifications to the original training procedure. Specifically, we propose a novel dynamic activation sparsity strategy that directly prunes activations at layer-specific dynamic ratios, allowing for flexible control of learning ability and memory cost in a data-sensitive manner during adaptation. Among this, two metrics, Gradient Importance and Layer Activation Memory, are considered to determine the layer-wise activation pruning ratios, reflecting accuracy contribution and memory efficiency, respectively. Experimentally, our method surpasses previous TTA baselines by not only reducing memory usage but also achieving superior accuracy, delivering SOTA performance across diverse datasets, network architectures, and tasks.</p>
            <p id="subjects-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" onclick="foldPdfKimi('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="panel paper" keywords="diffusion,pathways,removal,erase,eradiff,object,sra,artifacts,rectifying,optimization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways_CVPR_2025_paper.html" target="_blank" title="135/388"><span class="index notranslate">#135</span></a>
                <a id="title-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="title-link" href="/venue/Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" target="_blank">Erase Diffusion: Empowering Object Removal Through Calibrating Diffusion Pathways</a>
                <a id="pdf-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Liu" target="_blank">Yi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhou" target="_blank">Hao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benlei Cui" target="_blank">Benlei Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxiang Shang" target="_blank">Wenxiang Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Lin" target="_blank">Ran Lin</a>
            </p>
            <p id="summary-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="summary">Erase inpainting, or object removal, aims to precisely remove target objects within masked regions while preserving the overall consistency of the surrounding content. Despite diffusion-based methods have made significant strides in the field of image inpainting, challenges remain regarding the emergence of unexpected objects or artifacts. We assert that the inexact diffusion pathways established by existing standard optimization paradigms constrain the efficacy of object removal. To tackle these challenges, we propose a novel Erase Diffusion, termed EraDiff, aimed at unleashing the potential power of standard diffusion in the context of object removal. In contrast to standard diffusion, the EraDiff adapts both the optimization paradigm and the network to improve the coherence and elimination of the erasure results.We first introduce a Chain-Rectifying Optimization (CRO) paradigm, a sophisticated diffusion process specifically designed to align with the objectives of erasure. This paradigm establishes innovative diffusion transition pathways that simulate the gradual elimination of objects during optimization, allowing the model to accurately capture the intent of object removal. Furthermore, to mitigate deviations caused by artifacts during the sampling pathways, we develop a simple yet effective Self-Rectifying Attention (SRA) mechanism. The SRA calibrates the sampling pathways by altering self-attention activation, allowing the model to effectively bypass artifacts while further enhancing the coherence of the generated content. With this design, our proposed EraDiff achieves state-of-the-art performance on the OpenImages V5 dataset and demonstrates significant superiority in real-world scenarios.</p>
            <p id="subjects-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="panel paper" keywords="diffraction,optical,lego,objects,subwavelength,limit,opticalnet,imaging,dataset,collaboration">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html" target="_blank" title="136/388"><span class="index notranslate">#136</span></a>
                <a id="title-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="title-link" href="/venue/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" target="_blank">OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit</a>
                <a id="pdf-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Benquan Wang" target="_blank">Benquan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruyi An" target="_blank">Ruyi An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin-Kyu So" target="_blank">Jin-Kyu So</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergei Kurdiumov" target="_blank">Sergei Kurdiumov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eng Aik Chan" target="_blank">Eng Aik Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giorgio Adamo" target="_blank">Giorgio Adamo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Peng" target="_blank">Yuhan Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yewen Li" target="_blank">Yewen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo An" target="_blank">Bo An</a>
            </p>
            <p id="summary-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="summary">Observing objects of small size has always been a charming pursuit of human beings.However, due to the physical phenomenon of diffraction, the optical resolution is restricted to approximately half the wavelength of light, which impedes the observation of subwavelength objects, typically smaller than 200 nm. This constrains its application in numerous scientific and industrial fields that aim to observe objects beyond the diffraction limit, such as native state coronavirus inspection.Fortunately, deep learning methods have shown remarkable potential in uncovering underlying patterns within data, promising to overcome the diffraction limit by revealing the mapping pattern between diffraction images and their corresponding ground truth object localization images. However, the absence of suitable datasets has hindered progress in this field - collecting high-quality optical data of subwavelength objects is very challenging as these objects are inherently invisible under conventional microscopy, making it impossible to perform standard visual calibration and drift correction. Therefore, in collaboration with top optical scientists, we provide the first general optical imaging dataset based on the "LEGO" concept for addressing the diffraction limit. Drawing an analogy to the modular construction of the LEGO blocks, we construct a comprehensive optical imaging dataset comprising subwavelength fundamental elements, *i.e.*, small square units that can be assembled into larger and more complex objects of any shape. We then frame the task as an image-to-image translation task and evaluate various vision backbone methods. Experimental results validate our "LEGO" concept, demonstrating that models trained on basic square units can effectively generalize to realistic, more complex unseen objects. Most importantly, by highlighting this underexplored AI-for-science area and its potential, we aspire to advance optical science by fostering collaboration with the vision and machine learning communities.</p>
            <p id="subjects-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" onclick="foldPdfKimi('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="panel paper" keywords="mammalps,video,wildlife,ecology,monitoring,swiss,behavior,animal,wild,mammals">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals_CVPR_2025_paper.html" target="_blank" title="137/388"><span class="index notranslate">#137</span></a>
                <a id="title-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="title-link" href="/venue/Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" target="_blank">MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps</a>
                <a id="pdf-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Valentin Gabeff" target="_blank">Valentin Gabeff</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haozhe Qi" target="_blank">Haozhe Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brendan Flaherty" target="_blank">Brendan Flaherty</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gencer Sumbul" target="_blank">Gencer Sumbul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Mathis" target="_blank">Alexander Mathis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Devis Tuia" target="_blank">Devis Tuia</a>
            </p>
            <p id="summary-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="summary">Monitoring wildlife is essential for ecology and especially in light of the increasing human impact on ecosystems. Camera traps have emerged as habitat-centric sensors enabling the study of wildlife-environment interactions at scale with minimal disturbance. While computer vision models are becoming more powerful for general video understanding tasks, they struggle comparatively with camera trap videos. This gap in terms of performance and applicability can be partly attributed to the lack of annotated video datasets. To advance research in wild animal behavior monitoring we present MammAlps, a multimodal and multi-view dataset of wildlife behavior monitoring from 9 camera-traps in the Swiss National Park. MammAlps contains over 14 hours of video with audio, 2D segmentation maps and 8.5 hours of individual tracks densely labeled for species and behavior. Behaviors were annotated at two levels of complexity: actions representing simple behaviors and high-level activities. Based on 6,135 single animal clips, we propose the first hierarchical and multimodal animal behavior recognition benchmark using audio, video and reference scene segmentation maps as inputs. To enable future ecology research, we also propose a second benchmark aiming at identifying activities, species, number of individuals and meteorological conditions from 397 multi-view and long-term ecological events, including false positive triggers. We advocate that both tasks are complementary and contribute to bridging the gap between machine learning and ecology. Code and data will be made accessible.</p>
            <p id="subjects-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" onclick="foldPdfKimi('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="panel paper" keywords="ugp,registration,generalization,lidar,cloud,nuscenes,kitti,unlocking,cross,scenes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration_CVPR_2025_paper.html" target="_blank" title="138/388"><span class="index notranslate">#138</span></a>
                <a id="title-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="title-link" href="/venue/Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" target="_blank">Unlocking Generalization Power in LiDAR Point Cloud Registration</a>
                <a id="pdf-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenxuan Zeng" target="_blank">Zhenxuan Zeng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiao Wu" target="_blank">Qiao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiyu Zhang" target="_blank">Xiyu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Yuanbo Wu" target="_blank">Lin Yuanbo Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pei An" target="_blank">Pei An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Yang" target="_blank">Jiaqi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ji Wang" target="_blank">Ji Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Wang" target="_blank">Peng Wang</a>
            </p>
            <p id="summary-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="summary">In real-world environments, a LiDAR point cloud registration method with robust generalization capabilities (across varying distances and datasets) is crucial for ensuring safety in autonomous driving and other LiDAR-based applications. However, current methods fall short in achieving this level of generalization. To address these limitations, we propose UGP, a pruned framework designed to enhance generalization power for LiDAR point cloud registration. The core insight in UGP is the elimination of cross-attention mechanisms to improve generalization, allowing the network to concentrate on intra-frame feature extraction. Additionally, we introduce a progressive self-attention module to reduce ambiguity in large-scale scenes and integrate Birds Eye View (BEV) features to incorporate semantic information about scene elements. Together, these enhancements significantly boost the networks generalization performance. We validated our approach through various generalization experiments in multiple outdoor scenes. In cross-distance generalization experiments on KITTI and nuScenes, UGP achieved state-of-the-art mean Registration Recall rates of 94.5\% and 91.4\%, respectively. In cross-dataset generalization from nuScenes to KITTI, UGP achieved a state-of-the-art mean Registration Recall of 90.9\%.</p>
            <p id="subjects-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" onclick="foldPdfKimi('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="panel paper" keywords="overfitted,image,compression,wasserstein,distortion,hific,cheap,quality,dists,good">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion_CVPR_2025_paper.html" target="_blank" title="139/388"><span class="index notranslate">#139</span></a>
                <a id="title-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="title-link" href="/venue/Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" target="_blank">Good, Cheap, and Fast: Overfitted Image Compression with Wasserstein Distortion</a>
                <a id="pdf-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jona Ball" target="_blank">Jona Ball</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Versari" target="_blank">Luca Versari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emilien Dupont" target="_blank">Emilien Dupont</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunjik Kim" target="_blank">Hyunjik Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matthias Bauer" target="_blank">Matthias Bauer</a>
            </p>
            <p id="summary-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="summary">Inspired by the success of generative image models, recent work on learned image compression increasingly focuses on better probabilistic models of the natural image distribution, leading to excellent image quality. This, however, comes at the expense of a computational complexity that is several orders of magnitude higher than today's commercial codecs, and thus prohibitive for most practical applications. With this paper, we demonstrate that by focusing on modeling visual perception rather than the data distribution, we can achieve a very good trade-off between visual quality and bit rate similar to "generative" compression models such as HiFiC, while requiring less than 1% of the multiplyaccumulate operations (MACs) for decompression. We do this by optimizing C3, an overfitted image codec, for Wasserstein Distortion (WD), and evaluating the image reconstructions with a human rater study. The study also reveals that WD outperforms other perceptual quality metrics such as LPIPS, DISTS, and MS-SSIM, both as an optimization objective and as a predictor of human ratings, achieving over 94% Pearson correlation with Elo scores.</p>
            <p id="subjects-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" onclick="foldPdfKimi('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="panel paper" keywords="3dgs,guidance,splatting,sequences,diffusion,scene,grounding,sparse,modeling,occluded">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian_CVPR_2025_paper.html" target="_blank" title="140/388"><span class="index notranslate">#140</span></a>
                <a id="title-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="title-link" href="/venue/Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" target="_blank">Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs</a>
                <a id="pdf-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yingji Zhong" target="_blank">Yingji Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Li" target="_blank">Zhihao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dave Zhenyu Chen" target="_blank">Dave Zhenyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lanqing Hong" target="_blank">Lanqing Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Xu" target="_blank">Dan Xu</a>
            </p>
            <p id="summary-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="summary">Despite recent successes in novel view synthesis using 3D Gaussian Splatting (3DGS), modeling scenes with sparse inputs remains a challenge. In this work, we address two critical yet overlooked issues in real-world sparse-input modeling: extrapolation and occlusion. To tackle these issues, we propose to use a reconstruction by generation pipeline that leverages learned priors from video diffusion models to provide plausible interpretations for regions outside the field of view or occluded. However, the generated sequences exhibit inconsistencies that do not fully benefit subsequent 3DGS modeling. To address the challenge of inconsistency, we introduce a novel scene-grounding guidance based on rendered sequences from an optimized 3DGS, which tames the diffusion model to generate consistent sequences. This guidance is training-free and does not require any fine-tuning of the diffusion model. To facilitate holistic scene modeling, we also propose a trajectory initialization method. It effectively identifies regions that are outside the field of view and occluded. We further design a scheme tailored for 3DGS optimization with generated sequences. Experiments demonstrate that our method significantly improves upon the baseline and achieves state-of-the-art performance on challenging benchmarks.</p>
            <p id="subjects-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" onclick="foldPdfKimi('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="panel paper" keywords="static,reconstruction,scenes,dynamic,unified,events,cameras,reconstruct,videos,event">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events_CVPR_2025_paper.html" target="_blank" title="141/388"><span class="index notranslate">#141</span></a>
                <a id="title-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="title-link" href="/venue/Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" target="_blank">Unified Reconstruction of Static and Dynamic Scenes from Events</a>
                <a id="pdf-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qiyao Gao" target="_blank">Qiyao Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiqi Duan" target="_blank">Peiqi Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyue Lou" target="_blank">Hanyue Lou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minggui Teng" target="_blank">Minggui Teng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziqi Cai" target="_blank">Ziqi Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Chen" target="_blank">Xu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>
            </p>
            <p id="summary-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="summary">This paper addresses the challenge that current event-based video reconstruction methods cannot produce static background information. Recent research has uncovered the potential of event cameras in capturing static scenes. Nonetheless, image quality deteriorates due to noise interference and detail loss, failing to provide reliable background information. We propose a two-stage reconstruction strategy to address these challenges and reconstruct static scene images comparable to frame cameras. Building on this, we introduce the URSEE framework, the first unified framework designed for reconstructing motion videos with static backgrounds. This framework includes a parallel channel that can simultaneously process static and dynamic events, and a network module designed to reconstruct videos encompassing both static and dynamic scenes in an end-to-end manner. We also collect a real-captured dataset for static reconstruction, containing both indoor and outdoor scenes. Comparison results indicate that the proposed approach achieves state-of-the-art reconstruction results on both synthetic and real data.</p>
            <p id="subjects-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" onclick="foldPdfKimi('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="panel paper" keywords="clothing,clothed,loose,dresses,challenging,freecloth,human,skirts,disjointed,generation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling_CVPR_2025_paper.html" target="_blank" title="142/388"><span class="index notranslate">#142</span></a>
                <a id="title-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="title-link" href="/venue/Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" target="_blank">FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling</a>
                <a id="pdf-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Ye" target="_blank">Hang Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoxuan Ma" target="_blank">Xiaoxuan Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai Ci" target="_blank">Hai Ci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Zhu" target="_blank">Wentao Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yizhou Wang" target="_blank">Yizhou Wang</a>
            </p>
            <p id="summary-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="summary">Achieving realistic animated human avatars requires accurate modeling of pose-dependent clothing deformations. Existing learning-based methods heavily rely on the Linear Blend Skinning (LBS) of minimally-clothed human models like SMPL to model deformation. However, these methods struggle to handle loose clothing, such as long dresses, where the canonicalization process becomes ill-defined when the clothing is far from the body, leading to disjointed and fragmented results. To overcome this limitation, we propose a novel hybrid framework to model challenging clothed humans. Our core idea is to use dedicated strategies to model different regions, depending on whether they are close to or distant from the body. This free-form generation paradigm brings enhanced flexibility and expressiveness to our hybrid framework, enabling it to capture the intricate geometric details of challenging loose clothing, such as skirts and dresses. Experimental results on the benchmark dataset featuring loose clothing demonstrate that our method achieves state-of-the-art performance with superior visual fidelity and realism, particularly in the most challenging cases.</p>
            <p id="subjects-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" onclick="foldPdfKimi('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="panel paper" keywords="wish,instance,segmentation,heterogeneous,weakly,labels,weak,tags,supervised,label">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels_CVPR_2025_paper.html" target="_blank" title="143/388"><span class="index notranslate">#143</span></a>
                <a id="title-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="title-link" href="/venue/Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" target="_blank">WISH: Weakly Supervised Instance Segmentation using Heterogeneous Labels</a>
                <a id="pdf-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hyeokjun Kweon" target="_blank">Hyeokjun Kweon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuk-Jin Yoon" target="_blank">Kuk-Jin Yoon</a>
            </p>
            <p id="summary-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="summary">Instance segmentation traditionally relies on dense pixel-level annotations, making it costly and labor-intensive. To alleviate this burden, weakly supervised instance segmentation utilizes cost-effective weak labels, such as image-level tags, points, and bounding boxes. However, existing approaches typically focus on a single type of weak label, overlooking the cost-efficiency potential of combining multiple types. In this paper, we introduce WISH, a novel heterogeneous framework for weakly supervised instance segmentation that integrates diverse weak label types within a single model. WISH unifies heterogeneous labels by leveraging SAMs prompt latent space through a multi-stage matching strategy, effectively compensating for the lack of spatial information in class tags. Extensive experiments on Pascal VOC and COCO demonstrate that our framework not only surpasses existing homogeneous weak supervision methods but also achieves superior results in heterogeneous settings with equivalent annotation costs.</p>
            <p id="subjects-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" onclick="foldPdfKimi('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="panel paper" keywords="change3d,change,captioning,video,frames,detection,images,tasks,extractors,reconceptualizes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling_CVPR_2025_paper.html" target="_blank" title="144/388"><span class="index notranslate">#144</span></a>
                <a id="title-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="title-link" href="/venue/Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" target="_blank">Change3D: Revisiting Change Detection and Captioning from A Video Modeling Perspective</a>
                <a id="pdf-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Duowang Zhu" target="_blank">Duowang Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohu Huang" target="_blank">Xiaohu Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyan Huang" target="_blank">Haiyan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhou" target="_blank">Hao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenfeng Shao" target="_blank">Zhenfeng Shao</a>
            </p>
            <p id="summary-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="summary">In this paper, we present Change3D, a framework that reconceptualizes the change detection and captioning tasks through video modeling. Recent methods have achieved remarkable success by regarding each pair of bi-temporal images as separate frames. They employ a shared-weight image encoder to extract spatial features and then use a change extractor to capture differences between the two images. However, image feature encoding, being a task-agnostic process, cannot attend to changed regions effectively. Furthermore, different change extractors designed for various change detection and captioning tasks make it difficult to have a unified framework. To tackle these challenges, Change3D regards the bi-temporal images as comprising two frames akin to a tiny video. By integrating learnable perception frames between the bi-temporal images, a video encoder enables the perception frames to interact with the images directly and perceive their differences. Therefore, we can get rid of the intricate change extractors, providing a unified framework for different change detection and captioning tasks. We verify Change3D on multiple tasks, encompassing change detection (including binary change detection, semantic change detection, and building damage assessment) and change captioning, across eight standard benchmarks. Without bells and whistles, this simple yet effective framework can achieve superior performance with an ultra-light video model comprising only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-167" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-168"><span class="mo" id="MathJax-Span-169" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-37">\sim</script>6\%-13\% of the parameters and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-170" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-171"><span class="mo" id="MathJax-Span-172" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-38">\sim</script>8\%-34\% of the FLOPs compared to state-of-the-art methods. We hope that Change3D could be an alternative to 2D-based models and facilitate future research. Code and models will be made publicly available.</p>
            <p id="subjects-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" onclick="foldPdfKimi('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="panel paper" keywords="lvq,lattice,multirate,vector,quantization,rate,shortcomings,compression,domain,adaptive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization_CVPR_2025_paper.html" target="_blank" title="145/388"><span class="index notranslate">#145</span></a>
                <a id="title-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="title-link" href="/venue/Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" target="_blank">Multirate Neural Image Compression with Adaptive Lattice Vector Quantization</a>
                <a id="pdf-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Xu" target="_blank">Hao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolin Wu" target="_blank">Xiaolin Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Zhang" target="_blank">Xi Zhang</a>
            </p>
            <p id="summary-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="summary">Recent research has explored integrating lattice vector quantization (LVQ) into learned image compression models. Due to its more efficient Voronoi covering of vector space than scalar quantization (SQ), LVQ achieves better rate-distortion (R-D) performance than SQ, while still retaining the low complexity advantage of SQ. However, existing LVQ-based methods have two shortcomings: 1) lack of a multirate coding mode, hence incapable to operate at different rates; 2) the use of a fixed lattice basis, hence nonadaptive to changing source distributions. To overcome these shortcomings, we propose a novel adaptive LVQ method, which is the first among LVQ-based methods to achieve both rate and domain adaptations. By scaling the lattice basis vector, our method can adjust the density of lattice points to achieve various bit rate targets, achieving superior R-D performance to current SQ-based variable rate models. Additionally, by using a learned invertible linear transformation between two different input domains, we can reshape the predefined lattice cell to better represent the target domain, further improving the R-D performance. To our knowledge, this paper represents the first attempt to propose a unified solution for rate adaptation and domain adaptation through quantizer design.</p>
            <p id="subjects-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" onclick="foldPdfKimi('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="panel paper" keywords="psg,scene,panoptic,generation,visual,scarcity,vocabulary,rich,annotations,graph">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual_CVPR_2025_paper.html" target="_blank" title="146/388"><span class="index notranslate">#146</span></a>
                <a id="title-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="title-link" href="/venue/Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" target="_blank">Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene</a>
                <a id="pdf-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shengqiong Wu" target="_blank">Shengqiong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Fei" target="_blank">Hao Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingkang Yang" target="_blank">Jingkang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangtai Li" target="_blank">Xiangtai Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Juncheng Li" target="_blank">Juncheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tat-seng Chua" target="_blank">Tat-seng Chua</a>
            </p>
            <p id="summary-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="summary">The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever representation for comprehensively modeling the dynamic 4D visual real world. Unfortunately, current pioneering 4D-PSG research can largely suffer from data scarcity issues severely, as well as the resulting out-of-vocabulary problems; also, the pipeline nature of the benchmark generation method can lead to suboptimal performance. To address these challenges, this paper investigates a novel framework for 4D-PSG generation that leverages rich 2D visual scene annotations to enhance 4D scene learning. First, we introduce a 4D Large Language Model (4D-LLM) integrated with a 3D mask decoder for end-to-end generation of 4D-PSG. A chained SG inference mechanism is further designed to exploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive object and relation labels iteratively. Most importantly, we propose a 2D-to-4D visual scene transfer learning framework, where a spatial-temporal scene transcending strategy effectively transfers dimension-invariant features from abundant 2D SG annotations to 4D scenes, effectively compensating for data scarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate that we strikingly outperform baseline models by an average of 14.62%, highlighting the effectiveness of our method.</p>
            <p id="subjects-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" onclick="foldPdfKimi('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="panel paper" keywords="mueller,ellipsometer,event,qwps,matrix,ellipsometric,scenes,imaging,ellipsometers,light">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging_CVPR_2025_paper.html" target="_blank" title="147/388"><span class="index notranslate">#147</span></a>
                <a id="title-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="title-link" href="/venue/Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" target="_blank">Event Ellipsometer: Event-based Mueller-Matrix Video Imaging</a>
                <a id="pdf-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ryota Maeda" target="_blank">Ryota Maeda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunseong Moon" target="_blank">Yunseong Moon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seung-Hwan Baek" target="_blank">Seung-Hwan Baek</a>
            </p>
            <p id="summary-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="summary">Light-matter interactions modify both the intensity and polarization state of light. Changes in polarization, represented by a Mueller matrix, encode detailed scene information. Existing optical ellipsometers capture Mueller-matrix images; however, they are often limited to static scenes due to long acquisition times. Here, we introduce Event Ellipsometer, a method for acquiring Mueller-matrix images of dynamic scenes. Our imaging system employs fast-rotating quarter-wave plates (QWPs) in front of a light source and an event camera that asynchronously captures intensity changes induced by the rotating QWPs. We develop an ellipsometric-event image formation model, a calibration method, and an ellipsometric-event reconstruction method. We experimentally demonstrate that Event Ellipsometer enables Mueller-matrix imaging at 30fps, extending ellipsometry to dynamic scenes.</p>
            <p id="subjects-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" onclick="foldPdfKimi('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="panel paper" keywords="animatable,rigging,characters,animation,shapes,ready,make,skeleton,authoring,manual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters_CVPR_2025_paper.html" target="_blank" title="148/388"><span class="index notranslate">#148</span></a>
                <a id="title-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="title-link" href="/venue/Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" target="_blank">Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters</a>
                <a id="pdf-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyang Guo" target="_blank">Zhiyang Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinxu Xiang" target="_blank">Jinxu Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Ma" target="_blank">Kai Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wengang Zhou" target="_blank">Wengang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Houqiang Li" target="_blank">Houqiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran Zhang" target="_blank">Ran Zhang</a>
            </p>
            <p id="summary-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="summary">3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach generates animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is typically limited to realistic human shapes. To address these issues, we present Make-It-Animatable, a novel data-driven method to make any 3D humanoid model ready for character animation in less than one second, regardless of its shapes and poses. Our unified framework generates high-quality blend weights, bones, and pose transformations. By incorporating a particle-based shape autoencoder, our approach supports various 3D representations, including meshes and 3D Gaussian splats. Additionally, we employ a coarse-to-fine representation and a structure-aware modeling strategy to ensure both accuracy and robustness, even for characters with non-standard skeleton structures. We conducted extensive experiments to validate our framework's effectiveness. Compared to existing methods, our approach demonstrates significant improvements in both quality and speed. The source code will be made publicly available.</p>
            <p id="subjects-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" onclick="foldPdfKimi('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="panel paper" keywords="sound,binaural,localization,egomotion,ego,supervising,motion,videos,audio,wild">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion_CVPR_2025_paper.html" target="_blank" title="149/388"><span class="index notranslate">#149</span></a>
                <a id="title-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="title-link" href="/venue/Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" target="_blank">Supervising Sound Localization by In-the-wild Egomotion</a>
                <a id="pdf-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anna Min" target="_blank">Anna Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyang Chen" target="_blank">Ziyang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Zhao" target="_blank">Hang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Owens" target="_blank">Andrew Owens</a>
            </p>
            <p id="summary-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="summary">We present a method for learning binaural sound localization from ego-motion in videos. When the camera moves in a video, the direction of sound sources will change along with it. We train an audio model to predict sound directions that are consistent with visual estimates of camera motion, which we obtain using methods from multi-view geometry. This provides a weak but plentiful form of supervision that we combine with traditional binaural cues. To evaluate this idea, we propose a dataset of real-world audio-visual videos with ego-motion. We show that our model can successfully learn from this real-world data, and that it obtains strong performance on sound localization tasks.</p>
            <p id="subjects-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" onclick="foldPdfKimi('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="panel paper" keywords="vcr,appearance,object,instance,segmentation,clr,view,open,world,awareness">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation_CVPR_2025_paper.html" target="_blank" title="150/388"><span class="index notranslate">#150</span></a>
                <a id="title-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" target="_blank">v-CLR: View-Consistent Learning for Open-World Instance Segmentation</a>
                <a id="pdf-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chang-Bin Zhang" target="_blank">Chang-Bin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinhong Ni" target="_blank">Jinhong Ni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujie Zhong" target="_blank">Yujie Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Han" target="_blank">Kai Han</a>
            </p>
            <p id="summary-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="summary">In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, e.g., texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called View-Consistent leaRning (VCR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In VCR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the image's underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our VCR on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance.</p>
            <p id="subjects-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="panel paper" keywords="udfs,gaussians,unsigned,splatting,surfaces,gaussianudf,digitalizing,supervision,inferring,distance">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting_CVPR_2025_paper.html" target="_blank" title="151/388"><span class="index notranslate">#151</span></a>
                <a id="title-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="title-link" href="/venue/Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" target="_blank">GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting</a>
                <a id="pdf-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shujuan Li" target="_blank">Shujuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Shen Liu" target="_blank">Yu-Shen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhizhong Han" target="_blank">Zhizhong Han</a>
            </p>
            <p id="summary-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="summary">Reconstructing open surfaces from multi-view images is vital in digitalizing complex objects in daily life. A widely used strategy is to learn unsigned distance functions (UDFs) by checking if their appearance conforms to the image observations through neural rendering. However, it is still hard to learn the continuous and implicit UDF representations through 3D Gaussians splatting (3DGS) due to the discrete and explicit scene representations, i.e., 3D Gaussians. To resolve this issue, we propose a novel approach to bridge the gap between 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D Gaussian planes on surfaces, and then, leverage the self-supervision and gradient-based inference to supervise unsigned distances in both near and far area to surfaces. To this end, we introduce novel constraints and strategies to constrain the learning of 2D Gaussians to pursue more stable optimization and more reliable self-supervision, addressing the challenges brought by complicated gradient field on or near the zero level set of UDFs. We report numerical and visual comparisons with the state-of-the-art on widely used benchmarks and real data to show our advantages in terms of accuracy, efficiency, completeness, and sharpness of reconstructed open surfaces with boundaries.</p>
            <p id="subjects-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" onclick="foldPdfKimi('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="panel paper" keywords="llmdet,vocabulary,captions,open,language,image,caption,supervision,level,detector">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of_CVPR_2025_paper.html" target="_blank" title="152/388"><span class="index notranslate">#152</span></a>
                <a id="title-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="title-link" href="/venue/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" target="_blank">LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models</a>
                <a id="pdf-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shenghao Fu" target="_blank">Shenghao Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qize Yang" target="_blank">Qize Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qijie Mo" target="_blank">Qijie Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junkai Yan" target="_blank">Junkai Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xihan Wei" target="_blank">Xihan Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingke Meng" target="_blank">Jingke Meng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohua Xie" target="_blank">Xiaohua Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Shi Zheng" target="_blank">Wei-Shi Zheng</a>
            </p>
            <p id="summary-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="summary">Recent open-vocabulary detectors achieve promising performance with abundant region-level annotated data. In this work, we show that an open-vocabulary detector co-training with a large language model by generating image-level detailed captions for each image can further improve performance. To achieve the goal, we first collect a dataset, GroundingCap-1M, wherein each image is accompanied by associated grounding labels and an image-level detailed caption. With this dataset, we finetune an open-vocabulary detector with training objectives including a standard grounding loss and a caption generation loss. We take advantage of a large language model to generate both region-level short captions for each region of interest and image-level long captions for the whole image. Under the supervision of the large language model, the resulting detector, LLMDet, outperforms the baseline by a clear margin, enjoying superior open-vocabulary ability. Further, we show that the improved LLMDet can in turn build a stronger large multi-modal model, achieving mutual benefits. The code, model, and dataset will be available.</p>
            <p id="subjects-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" onclick="foldPdfKimi('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="panel paper" keywords="sail,vision,language,unimodal,alignment,ssl,models,clip,pretrained,multimodal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models_CVPR_2025_paper.html" target="_blank" title="153/388"><span class="index notranslate">#153</span></a>
                <a id="title-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" target="_blank">Assessing and Learning Alignment of Unimodal Vision and Language Models</a>
                <a id="pdf-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Le Zhang" target="_blank">Le Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian Yang" target="_blank">Qian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aishwarya Agrawal" target="_blank">Aishwarya Agrawal</a>
            </p>
            <p id="summary-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="summary">How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this paper, we propose a direct assessment method, inspired by linear probing, to assess vision-language alignment. We identify that the degree of alignment of the SSL vision models depends on their SSL training objective, and we find that the clustering quality of SSL representations has a stronger impact on alignment performance than their linear separability. Next, we introduce Swift Alignment of Image and Language (SAIL), a efficient transfer learning framework that aligns pretrained unimodal vision and language models for downstream vision-language tasks. Since SAIL leverages the strengths of pretrained unimodal models, it requires significantly fewer (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-173" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-174"><span class="mo" id="MathJax-Span-175" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-39">\sim</script>6\%) paired image-text data for the multimodal alignment compared to models like CLIP which are trained from scratch. SAIL training only requires a single A100 GPU, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-176" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-177"><span class="mo" id="MathJax-Span-178" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-40">\sim</script>5 hours of training and can accommodate a batch size up to 32,768. SAIL achieves 73.4\% zero-shot accuracy on ImageNet (vs. CLIP's 72.7\%) and excels in zero-shot retrieval, complex reasoning, and semantic segmentation. Additionally, SAIL improves the language-compatibility of vision encoders that in turn enhance the performance of multimodal large language models.</p>
            <p id="subjects-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="panel paper" keywords="fusion,loss,task,tdfusion,downstream,module,learnable,tasks,objectives,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss_CVPR_2025_paper.html" target="_blank" title="154/388"><span class="index notranslate">#154</span></a>
                <a id="title-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="title-link" href="/venue/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" target="_blank">Task-driven Image Fusion with Learnable Fusion Loss</a>
                <a id="pdf-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF">16</sup>]</a>
                <a id="copy-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haowen Bai" target="_blank">Haowen Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangshe Zhang" target="_blank">Jiangshe Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixiang Zhao" target="_blank">Zixiang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yichen Wu" target="_blank">Yichen Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lilun Deng" target="_blank">Lilun Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yukun Cui" target="_blank">Yukun Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Feng" target="_blank">Tao Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuang Xu" target="_blank">Shuang Xu</a>
            </p>
            <p id="summary-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="summary">Multi-modal image fusion aggregates information from multiple sensor sources, achieving superior visual quality and perceptual characteristics compared to any single source, often enhancing downstream tasks. However, current fusion methods for downstream tasks still use predefined fusion objectives that potentially mismatch the downstream tasks, limiting adaptive guidance and reducing model flexibility.To address this, we propose Task-driven Image Fusion (TDFusion), a fusion framework incorporating a learnable fusion loss guided by task loss. Specifically, our fusion loss includes learnable parameters modeled by a neural network called the loss generation module. This module is supervised by the loss of downstream tasks in a meta-learning manner.The learning objective is to minimize the task loss of the fused images, once the fusion module has been optimized by the fusion loss.Iterative updates between the fusion module and the loss module ensure that the fusion network evolves toward minimizing task loss, guiding the fusion process toward the task objectives.TDFusions training relies solely on the loss of downstream tasks, making it adaptable to any specific task.It can be applied to any architecture of fusion and task networks.Experiments demonstrate TDFusions performance in both fusion and task-related applications, including four public fusion datasets, semantic segmentation, and object detection.The code will be released.</p>
            <p id="subjects-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" onclick="foldPdfKimi('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="panel paper" keywords="distraction,mllms,jailbreaking,texttt,textbf,subimages,gpt,visual,bypass,multimodal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model_CVPR_2025_paper.html" target="_blank" title="155/388"><span class="index notranslate">#155</span></a>
                <a id="title-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="title-link" href="/venue/Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" target="_blank">Distraction is All You Need for Multimodal Large Language Model Jailbreaking</a>
                <a id="pdf-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zuopeng Yang" target="_blank">Zuopeng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiluan Fan" target="_blank">Jiluan Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anli Yan" target="_blank">Anli Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Erdun Gao" target="_blank">Erdun Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Lin" target="_blank">Xin Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Li" target="_blank">Tao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kanghua Mo" target="_blank">Kanghua Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changyu Dong" target="_blank">Changyu Dong</a>
            </p>
            <p id="summary-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="summary">Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;Distraction Hypothesis&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-179" style="width: 13.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.461em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1011.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-180"><span class="texatom" id="MathJax-Span-181"><span class="mrow" id="MathJax-Span-182"><span class="mtext" id="MathJax-Span-183" style="font-family: MathJax_Main-bold;">Distraction Hypothesis</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">Distraction Hypothesis</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-41">\textbf{Distraction Hypothesis}</script>, followed by a novel framework called Contrasting Subimage Distraction Jailbreaking (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-42-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;CS-DJ&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-184" style="width: 4.013em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.336em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.28em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-185"><span class="texatom" id="MathJax-Span-186"><span class="mrow" id="MathJax-Span-187"><span class="mtext" id="MathJax-Span-188" style="font-family: MathJax_Main-bold;">CS-DJ</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">CS-DJ</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-42">\textbf{CS-DJ}</script>), to achieve jailbreaking by disrupting MLLMs alignment through multi-level distraction strategies. CS-DJ consists of two components: structured distraction, achieved through query decomposition that induces a distributional shift by fragmenting harmful prompts into sub-queries, and visual-enhanced distraction, realized by constructing contrasting subimages to disrupt the interactions among visual elements within the model. This dual strategy disperses the models attention, reducing its ability to detect and mitigate harmful content. Extensive experiments across five representative scenarios and four popular closed-source MLLMs, including <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-43-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;GPT-4o-mini&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-189" style="width: 6.982em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1005.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-190"><span class="texatom" id="MathJax-Span-191"><span class="mrow" id="MathJax-Span-192"><span class="mtext" id="MathJax-Span-193" style="font-family: MathJax_Typewriter;">GPT-4o-mini</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">GPT-4o-mini</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-43">\texttt{GPT-4o-mini}</script>, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-44-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;GPT-4o&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-194" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.08em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-195"><span class="texatom" id="MathJax-Span-196"><span class="mrow" id="MathJax-Span-197"><span class="mtext" id="MathJax-Span-198" style="font-family: MathJax_Typewriter;">GPT-4o</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">GPT-4o</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-44">\texttt{GPT-4o}</script>, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-45-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;GPT-4V&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-199" style="width: 3.753em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1003.13em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-200"><span class="texatom" id="MathJax-Span-201"><span class="mrow" id="MathJax-Span-202"><span class="mtext" id="MathJax-Span-203" style="font-family: MathJax_Typewriter;">GPT-4V</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">GPT-4V</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-45">\texttt{GPT-4V}</script>, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-46-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;monospace&quot;&gt;Gemini-1.5-Flash&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-204" style="width: 10.107em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.388em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1008.39em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-205"><span class="texatom" id="MathJax-Span-206"><span class="mrow" id="MathJax-Span-207"><span class="mtext" id="MathJax-Span-208" style="font-family: MathJax_Typewriter;">Gemini-1.5-Flash</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="monospace">Gemini-1.5-Flash</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-46">\texttt{Gemini-1.5-Flash}</script>, demonstrate that CS-DJ achieves average success rates of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-47-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;52.40\%&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-209" style="width: 5.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1004.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-210"><span class="texatom" id="MathJax-Span-211"><span class="mrow" id="MathJax-Span-212"><span class="mtext" id="MathJax-Span-213" style="font-family: MathJax_Main-bold;">52.40\%</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">52.40\%</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-47">\textbf{52.40\%}</script> for the attack success rate and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-48-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;74.10\%&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-214" style="width: 5.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1004.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-215"><span class="texatom" id="MathJax-Span-216"><span class="mrow" id="MathJax-Span-217"><span class="mtext" id="MathJax-Span-218" style="font-family: MathJax_Main-bold;">74.10\%</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">74.10\%</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-48">\textbf{74.10\%}</script> for the ensemble attack success rate. These results reveal the potential of distraction-based approaches to exploit and bypass MLLMs' defenses, offering new insights for attack strategies.</p>
            <p id="subjects-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" onclick="foldPdfKimi('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="panel paper" keywords="retention,unlearning,forgetting,erase,distillation,delete,remaining,logits,decoupled,centric">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any_CVPR_2025_paper.html" target="_blank" title="156/388"><span class="index notranslate">#156</span></a>
                <a id="title-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="title-link" href="/venue/Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" target="_blank">Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks</a>
                <a id="pdf-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Zhou" target="_blank">Yu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dian Zheng" target="_blank">Dian Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qijie Mo" target="_blank">Qijie Mo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renjie Lu" target="_blank">Renjie Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun-Yu Lin" target="_blank">Kun-Yu Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Shi Zheng" target="_blank">Wei-Shi Zheng</a>
            </p>
            <p id="summary-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="summary">In this work, we present DEcoupLEd Distillation To Erase (DELETE), a general and strong unlearning method for any class-centric tasks. To derive this, we first propose a theoretical framework to analyze the general form of unlearning loss and decompose it into forgetting and retention terms. Through the theoretical framework, we point out that a class of previous methods could be mainly formulated as a loss that implicitly optimizes the forgetting term while lacking supervision for the retention term, disturbing the distribution of pre-trained model and struggling to adequately preserve knowledge of the remaining classes.To address it, we refine the retention term using ``dark knowledge and propose a mask distillation unlearning method. By applying a mask to separate forgetting logits from retention logits, our approach optimizes both the forgetting and refined retention components simultaneously, retaining knowledge of the remaining classes while ensuring thorough forgetting of the target class.Without access to the remaining data or intervention (\ie, used in some works), we achieve state-of-the-art performance across various benchmarks. What's more, DELETE is a general solution that can be applied to various downstream tasks, including face recognition, backdoor defense, and semantic segmentation with great performance.</p>
            <p id="subjects-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="panel paper" keywords="information,attribution,coiba,bottleneck,decision,layer,targeted,layers,attributions,comprehensive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision_CVPR_2025_paper.html" target="_blank" title="157/388"><span class="index notranslate">#157</span></a>
                <a id="title-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="title-link" href="/venue/Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" target="_blank">Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers</a>
                <a id="pdf-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jung-Ho Hong" target="_blank">Jung-Ho Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ho-Joong Kim" target="_blank">Ho-Joong Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyu-Sung Jeon" target="_blank">Kyu-Sung Jeon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seong-Whan Lee" target="_blank">Seong-Whan Lee</a>
            </p>
            <p id="summary-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="summary">The feature attribution method reveals the contribution of input variables to the decision-making process to provide an attribution map for explanation. Existing methods grounded on the information bottleneck principle compute information in a specific layer to obtain attributions, compressing the features by injecting noise via a parametric damping ratio. However, the attribution obtained in a specific layer neglects evidence of the decision-making process distributed across layers. In this paper, we introduce a comprehensive information bottleneck (CoIBA), which discovers the relevant information in each targeted layer to explain the decision-making process. Our core idea is applying information bottleneck in multiple targeted layers to estimate the comprehensive information by sharing a parametric damping ratio across the layers. Leveraging this shared ratio complements the over-compressed information to discover the omitted clues of the decision by sharing the relevant information across the targeted layers. We suggest the variational approach to fairly reflect the relevant information of each layer by upper bounding layer-wise information. Therefore, CoIBA guarantees that the discarded activation is unnecessary in every targeted layer to make a decision. The extensive experimental results demonstrate the enhancement in faithfulness of the feature attributions provided by CoIBA.</p>
            <p id="subjects-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" onclick="foldPdfKimi('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="panel paper" keywords="vlm,robotic,manipulation,primitives,omnimanip,centric,level,reasoning,object,interaction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as_CVPR_2025_paper.html" target="_blank" title="158/388"><span class="index notranslate">#158</span></a>
                <a id="title-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="title-link" href="/venue/Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" target="_blank">OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints</a>
                <a id="pdf-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mingjie Pan" target="_blank">Mingjie Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiyao Zhang" target="_blank">Jiyao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianshu Wu" target="_blank">Tianshu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinghao Zhao" target="_blank">Yinghao Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenlong Gao" target="_blank">Wenlong Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Dong" target="_blank">Hao Dong</a>
            </p>
            <p id="summary-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="summary">The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.</p>
            <p id="subjects-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" onclick="foldPdfKimi('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="panel paper" keywords="ppa,group,aggregate,probe,spurious,minority,project,tuning,foundation,biased">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness_CVPR_2025_paper.html" target="_blank" title="159/388"><span class="index notranslate">#159</span></a>
                <a id="title-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="title-link" href="/venue/Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" target="_blank">Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness</a>
                <a id="pdf-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Beier Zhu" target="_blank">Beier Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiequan Cui" target="_blank">Jiequan Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chi Zhang" target="_blank">Chi Zhang</a>
            </p>
            <p id="summary-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="summary">While image-text foundation models have succeeded across diverse downstream tasks, they still face challenges in the presence of spurious correlations between the input and label. To address this issue, we propose a simple three-step approach--Project-Probe-Aggregate (PPA)--that enables parameter-efficient fine-tuning for foundation models without relying on group annotations. Building upon the failure-based debiasing scheme, our method, PPA, improves its two key components: minority samples identification and the robust training algorithm.Specifically, we first train biased classifiers by projecting image features onto the nullspace of class proxies from text encoders. Next, we infer group labels using the biased classifier and probe group targets with prior correction. Finally, we aggregate group weights of each class to produce the debiased classifier. Our theoretical analysis shows that our PPA enhances minority group identification and is Bayes optimal for minimizing the balanced group error, mitigating spurious correlations. Extensive experimental results confirm the effectiveness of our PPA: it outperforms the state-of-the-art by an average worst-group accuracy while requiring less than 0.01% tunable parameters without training group labels.</p>
            <p id="subjects-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" onclick="foldPdfKimi('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="panel paper" keywords="teacache,outputs,timestep,timesteps,cache,caching,differences,model,tells,vbench">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion_CVPR_2025_paper.html" target="_blank" title="160/388"><span class="index notranslate">#160</span></a>
                <a id="title-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" target="_blank">Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</a>
                <a id="pdf-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Liu" target="_blank">Feng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiwei Zhang" target="_blank">Shiwei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofeng Wang" target="_blank">Xiaofeng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujie Wei" target="_blank">Yujie Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haonan Qiu" target="_blank">Haonan Qiu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhong Zhao" target="_blank">Yuzhong Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingya Zhang" target="_blank">Yingya Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixiang Ye" target="_blank">Qixiang Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fang Wan" target="_blank">Fang Wan</a>
            </p>
            <p id="summary-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="summary">As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising.Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps.However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality.In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps.Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost.TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching.Experiments show that TeaCache achieves up to 4.41<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-49-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-219" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-220"><span class="mo" id="MathJax-Span-221" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-49">\times</script> acceleration over Open-Sora-Plan with negligible (-0.07\% Vbench score) degradation of visual quality. Code is enclosed in the supplementary material.</p>
            <p id="subjects-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="panel paper" keywords="depth,anything,video,videos,estimation,temporal,long,super,priors,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html" target="_blank" title="161/388"><span class="index notranslate">#161</span></a>
                <a id="title-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="title-link" href="/venue/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" target="_blank">Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</a>
                <a id="pdf-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sili Chen" target="_blank">Sili Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengkai Guo" target="_blank">Hengkai Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengnan Zhu" target="_blank">Shengnan Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feihu Zhang" target="_blank">Feihu Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zilong Huang" target="_blank">Zilong Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiashi Feng" target="_blank">Jiashi Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingyi Kang" target="_blank">Bingyi Kang</a>
            </p>
            <p id="summary-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="summary">Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.</p>
            <p id="subjects-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="panel paper" keywords="decoupled,interaction,linear,care,transformer,dual,signicant,mobile,attention,efficiency">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction_CVPR_2025_paper.html" target="_blank" title="162/388"><span class="index notranslate">#162</span></a>
                <a id="title-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="title-link" href="/venue/Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" target="_blank">CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction</a>
                <a id="pdf-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Zhou" target="_blank">Yuan Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingshan Xu" target="_blank">Qingshan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiequan Cui" target="_blank">Jiequan Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junbao Zhou" target="_blank">Junbao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Zhang" target="_blank">Jing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richang Hong" target="_blank">Richang Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwang Zhang" target="_blank">Hanwang Zhang</a>
            </p>
            <p id="summary-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="summary">Recently, large efforts have been made to design efficient linear-complexity visual Transformers. However, current linear attention models are generally unsuitable to be deployed in resource-constrained mobile devices, due to suffering from either few efficiency gains or signicant accuracy drops. In this paper, we propose a new deCoupled duAl-interactive lineaR attEntion (CARE) mechanism, revealing that features' decoupling and interaction can fully unleash the power of linear attention. We first propose an asymmetrical feature decoupling strategy that asymmetrically decouples the learning process for local inductive bias and long-range dependencies, thereby preserving sufficient local and global information while effectively enhancing the efficiency of models. Then, a dynamic memory unit is employed to maintain critical information along the network pipeline. Moreover, we design a dual interaction module to effectively facilitate interaction between local inductive bias and long-range information as well as among features at different layers. By adopting a decoupled learning way and fully exploiting complementarity across features, our method can achieve both high efficiency and accuracy. Extensive experiments on ImageNet-1K, COCO, and ADE20K datasets demonstrate the effectiveness of our approach, e.g., achieving <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-50-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;78.4&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;82.1&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0025;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-222" style="width: 5.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1004.85em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-223"><span class="mn" id="MathJax-Span-224" style="font-family: MathJax_Main;">78.4</span><span class="texatom" id="MathJax-Span-225"><span class="mrow" id="MathJax-Span-226"><span class="mo" id="MathJax-Span-227" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-228" style="font-family: MathJax_Main;">82.1</span><span class="mi" id="MathJax-Span-229" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>78.4</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>82.1</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-50">78.4/82.1\%</script> top-1 accuracy on ImagegNet-1K at the cost of only <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-51-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;0.7&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;1.9&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-230" style="width: 3.701em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.076em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1003.02em, 2.555em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-231"><span class="mn" id="MathJax-Span-232" style="font-family: MathJax_Main;">0.7</span><span class="texatom" id="MathJax-Span-233"><span class="mrow" id="MathJax-Span-234"><span class="mo" id="MathJax-Span-235" style="font-family: MathJax_Main;">/</span></span></span><span class="mn" id="MathJax-Span-236" style="font-family: MathJax_Main;">1.9</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.7</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>1.9</mn></math></span></span><script type="math/tex" id="MathJax-Element-51">0.7/1.9</script> GMACs. Codes will be released on github.</p>
            <p id="subjects-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="panel paper" keywords="fun3du,scenes,functionality,task,understanding,segmentation,language,scenefun3d,locate,switch">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes_CVPR_2025_paper.html" target="_blank" title="163/388"><span class="index notranslate">#163</span></a>
                <a id="title-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="title-link" href="/venue/Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" target="_blank">Functionality Understanding and Segmentation in 3D Scenes</a>
                <a id="pdf-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaime Corsetti" target="_blank">Jaime Corsetti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francesco Giuliari" target="_blank">Francesco Giuliari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alice Fasoli" target="_blank">Alice Fasoli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Davide Boscaini" target="_blank">Davide Boscaini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabio Poiesi" target="_blank">Fabio Poiesi</a>
            </p>
            <p id="summary-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="summary">Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like turn on the ceiling light, an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description. To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a vision and language model. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Code will be released publicly.</p>
            <p id="subjects-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" onclick="foldPdfKimi('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="panel paper" keywords="correspondence,cross,completion,view,croco,shot,zero,estimators,supervised,attention">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators_CVPR_2025_paper.html" target="_blank" title="164/388"><span class="index notranslate">#164</span></a>
                <a id="title-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="title-link" href="/venue/An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" target="_blank">Cross-View Completion Models are Zero-shot Correspondence Estimators</a>
                <a id="pdf-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF"></sup>]</a>
                <a id="rel-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Honggyu An" target="_blank">Honggyu An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Hyeon Kim" target="_blank">Jin Hyeon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seonghoon Park" target="_blank">Seonghoon Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewoo Jung" target="_blank">Jaewoo Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jisang Han" target="_blank">Jisang Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sunghwan Hong" target="_blank">Sunghwan Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungryong Kim" target="_blank">Seungryong Kim</a>
            </p>
            <p id="summary-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="summary">In this work, we analyze new aspects of cross-view completion, mainly through the analogy of cross-view completion and traditional self-supervised correspondence learning algorithms. Based on our analysis, we reveal that the cross-attention map of Croco-v2, best reflects this correspondence information compared to other correlations from the encoder or decoder features. We further verify the effectiveness of the cross-attention map by evaluating on both zero-shot and supervised dense geometric correspondence and multi-frame depth estimation.</p>
            <p id="subjects-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" onclick="foldPdfKimi('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="panel paper" keywords="anomaly,vau,video,ats,anomalies,annotations,granularity,holmes,hierarchical,term">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html" target="_blank" title="165/388"><span class="index notranslate">#165</span></a>
                <a id="title-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" target="_blank">Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity</a>
                <a id="pdf-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huaxin Zhang" target="_blank">Huaxin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohao Xu" target="_blank">Xiaohao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Wang" target="_blank">Xiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jialong Zuo" target="_blank">Jialong Zuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaonan Huang" target="_blank">Xiaonan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changxin Gao" target="_blank">Changxin Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shanjun Zhang" target="_blank">Shanjun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Yu" target="_blank">Li Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nong Sang" target="_blank">Nong Sang</a>
            </p>
            <p id="summary-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="summary">How can we enable models to comprehend video anomalies occurring over varying temporal scales and contexts?Traditional Video Anomaly Understanding (VAU) methods focus on frame-level anomaly prediction, often missing the interpretability of complex and diverse real-world anomalies. Recent multimodal approaches leverage visual and textual data but lack hierarchical annotations that capture both short-term and long-term anomalies.To address this challenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical video anomaly understanding across any granularity. We develop a semi-automated annotation engine that efficiently scales high-quality annotations by combining manual video segmentation with recursive free-text annotation using large language models (LLMs). This results in over 70,000 multi-granular annotations organized at clip-level, event-level, and video-level segments.For efficient anomaly detection in long videos, we propose the Anomaly-focused Temporal Sampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to adaptively select frames based on anomaly scores, ensuring that the multimodal LLM concentrates on anomaly-rich regions, which significantly enhances both efficiency and accuracy.Extensive experiments demonstrate that our hierarchical instruction data markedly improves anomaly comprehension. The integrated ATS and visual-language model outperform traditional methods in processing long videos.Our benchmark and model will be publicly available.</p>
            <p id="subjects-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="panel paper" keywords="style,styles,prompt,thought,domain,chain,evolving,along,progressively,dgod">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection_CVPR_2025_paper.html" target="_blank" title="166/388"><span class="index notranslate">#166</span></a>
                <a id="title-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" target="_blank">Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection</a>
                <a id="pdf-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zihao Zhang" target="_blank">Zihao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aming Wu" target="_blank">Aming Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yahong Han" target="_blank">Yahong Han</a>
            </p>
            <p id="summary-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="summary">Recently, a task of Single-Domain Generalized Object Detection (Single-DGOD) is proposed, aiming to generalize a detector to multiple unknown domains never seen before during training. Due to the unavailability of target-domain data, some methods leverage the multimodal capabilities of vision-language models, using textual prompts to estimate cross-domain information, enhancing the model's generalization capability. These methods typically use a single textual prompt, often referred to as the one-step prompt method. However, when dealing with complex styles such as the combination of rain and night, we observe that the performance of the one-step prompt method tends to be relatively weak. The reason may be that many scenes incorporate not just a single style but a combination of multiple styles. The one-step prompt method may not effectively synthesize combined information involving various styles. To address this limitation, we propose a new method, i.e., Style Evolving along Chain-of-Thought, which aims to progressively integrate and expand style information along the chain of thought, enabling the continual evolution of styles. Specifically, by progressively refining style descriptions and guiding the diverse evolution of styles, this approach enables more accurate simulation of various style characteristics and helps the model gradually learn and adapt to subtle differences between styles. Additionally, it exposes the model to a broader range of style features with different data distributions, thereby enhancing its generalization capability in unseen domains. The significant performance gains over five adverse-weather scenarios and the Real to Art benchmark demonstrate the superiorities of our method.</p>
            <p id="subjects-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="panel paper" keywords="edges,graph,sfm,filtering,translation,cameras,outlier,pose,filter,relative">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM_CVPR_2025_paper.html" target="_blank" title="167/388"><span class="index notranslate">#167</span></a>
                <a id="title-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="title-link" href="/venue/Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" target="_blank">Learning to Filter Outlier Edges in Global SfM</a>
                <a id="pdf-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nicole Damblon" target="_blank">Nicole Damblon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Barath" target="_blank">Daniel Barath</a>
            </p>
            <p id="summary-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="summary">This paper introduces a novel approach to improve camera position estimation in global Structure-from-Motion (SfM) frameworks by filtering inaccurate pose graph edges, representing relative translation estimates, before applying translation averaging. In SfM, pose graph vertices represent cameras and edges relative poses (rotation and translation) between cameras. We formulate the edge filtering problem as a vertex filtering in the dual graph - a line graph where the vertices stem from edges in the original graph, and the edges from cameras. Exploiting such a representation, we frame the problem as a binary classification over nodes in the dual graph. To learn such a classification and find outlier edges, we employ a Transformer architecture-based technique. To address the challenge of memory overflow often caused by converting to a line graph, we introduce a clustering-based graph processing approach, enabling the application of our method to arbitrarily large pose graphs. The proposed method outperforms existing relative translation filtering techniques in terms of final camera position accuracy and can be seamlessly integrated with any other filter. The code will be made public.</p>
            <p id="subjects-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" onclick="foldPdfKimi('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="panel paper" keywords="spectre,3dgs,specular,splatting,reflections,highly,surfaces,rays,tracing,gaussian">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by_CVPR_2025_paper.html" target="_blank" title="168/388"><span class="index notranslate">#168</span></a>
                <a id="title-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="title-link" href="/venue/Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" target="_blank">SpecTRe-GS: Modeling Highly Specular Surfaces with Reflected Nearby Objects by Tracing Rays in 3D Gaussian Splatting</a>
                <a id="pdf-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Tang" target="_blank">Jiajun Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Fei" target="_blank">Fan Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Li" target="_blank">Zhihao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Tang" target="_blank">Xiao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiyong Liu" target="_blank">Shiyong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youyu Chen" target="_blank">Youyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binxiao Huang" target="_blank">Binxiao Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Chen" target="_blank">Zhenyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaofei Wu" target="_blank">Xiaofei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boxin Shi" target="_blank">Boxin Shi</a>
            </p>
            <p id="summary-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="summary">3D Gaussian Splatting (3DGS), a recently emerged multi-view 3D reconstruction technique, has shown significant advantages in real-time rendering and explicit editing. However, 3DGS encounters challenges in the accurate modeling of both high-frequency view-dependent appearances and global illumination effects, including inter-reflection. This paper introduces SpecTRe-GS, which addresses these challenges and models highly Specular surfaces that reflect nearby objects through Tracing Rays in 3D Gaussian Splatting. SpecTRe-GS separately models reflections from highly specular and rough surfaces to leverage the distinctions between their reflective properties, integrating an efficient ray tracer within the 3DGS framework for querying secondary rays, thus achieving fast and accurate rendering. Also, it incorporates normal prior guidance and joint geometry optimization at various stages of the training process to enhance geometry reconstruction for undistorted reflections. The proposed SpecTRe-GS demonstrates superior performance compared to existing 3DGS-based methods in capturing highly specular inter-reflections, as confirmed by experiments conducted on both synthetic and real-world scenes. We also showcase the editing applications enabled by the scene decomposition capabilities of SpecTRe-GS.</p>
            <p id="subjects-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" onclick="foldPdfKimi('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="panel paper" keywords="omnidirectional,feed,omnisplat,images,forward,3dgs,splatting,perspective,yin,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with_CVPR_2025_paper.html" target="_blank" title="169/388"><span class="index notranslate">#169</span></a>
                <a id="title-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="title-link" href="/venue/Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" target="_blank">OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities</a>
                <a id="pdf-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Suyoung Lee" target="_blank">Suyoung Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaeyoung Chung" target="_blank">Jaeyoung Chung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kihoon Kim" target="_blank">Kihoon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaeyoo Huh" target="_blank">Jaeyoo Huh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gunhee Lee" target="_blank">Gunhee Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Minsoo Lee" target="_blank">Minsoo Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kyoung Mu Lee" target="_blank">Kyoung Mu Lee</a>
            </p>
            <p id="summary-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="summary">Feed-forward 3D Gaussian Splatting (3DGS) models have gained significant popularity due to their ability to generate scenes immediately without needing per-scene optimization.Although omnidirectional images are getting more popular since they reduce the computation for image stitching to composite a holistic scene, existing feed-forward models are only designed for perspective images.The unique optical properties of omnidirectional images make it difficult for feature encoders to correctly understand the context of the image and make the Gaussian non-uniform in space, which hinders the image quality synthesized from novel views.We propose OmniSplat, a pioneering work for fast feed-forward 3DGS generation from a few omnidirectional images.We introduce Yin-Yang grid and decompose images based on it to reduce the domain gap between omnidirectional and perspective images.The Yin-Yang grid can use the existing CNN structure as it is, but its quasi-uniform characteristic allows the decomposed image to be similar to a perspective image, so it can exploit the strong prior knowledge of the learned feed-forward network.OmniSplat demonstrates higher reconstruction accuracy than existing feed-forward networks trained on perspective images.Furthermore, we enhance the segmentation consistency between omnidirectional images by leveraging attention from the encoder of OmniSplat, providing fast and clean 3DGS editing results.</p>
            <p id="subjects-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" onclick="foldPdfKimi('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="panel paper" keywords="gen3c,camera,video,control,precise,cache,infer,previously,frames,popping">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control_CVPR_2025_paper.html" target="_blank" title="170/388"><span class="index notranslate">#170</span></a>
                <a id="title-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="title-link" href="/venue/Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" target="_blank">GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</a>
                <a id="pdf-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanchi Ren" target="_blank">Xuanchi Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianchang Shen" target="_blank">Tianchang Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui Huang" target="_blank">Jiahui Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Ling" target="_blank">Huan Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Lu" target="_blank">Yifan Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Merlin Nimier-David" target="_blank">Merlin Nimier-David</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Mller" target="_blank">Thomas Mller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Keller" target="_blank">Alexander Keller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanja Fidler" target="_blank">Sanja Fidler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Gao" target="_blank">Jun Gao</a>
            </p>
            <p id="summary-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="summary">We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video.</p>
            <p id="subjects-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" onclick="foldPdfKimi('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="panel paper" keywords="ood,dpu,multimodal,detection,class,updating,samples,prototype,discrepancies,distribution">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection_CVPR_2025_paper.html" target="_blank" title="171/388"><span class="index notranslate">#171</span></a>
                <a id="title-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-link" href="/venue/Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" target="_blank">DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection</a>
                <a id="pdf-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shawn Li" target="_blank">Shawn Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huixian Gong" target="_blank">Huixian Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Dong" target="_blank">Hao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiankai Yang" target="_blank">Tiankai Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengzhong Tu" target="_blank">Zhengzhong Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Zhao" target="_blank">Yue Zhao</a>
            </p>
            <p id="summary-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="summary">Out-of-distribution (OOD) detection is crucial for ensuring the robustness of machine learning models by identifying samples that deviate from the training distribution. While traditional OOD detection has predominantly focused on single-modality inputs, such as images, recent advancements in multimodal models have shown the potential of utilizing multiple modalities (e.g., video, optical flow, audio) to improve detection performance. However, existing approaches often neglect intra-class variability within in-distribution (ID) data, assuming that samples of the same class are perfectly cohesive and consistent. This assumption can lead to performance degradation, especially when prediction discrepancies are indiscriminately amplified across all samples. To address this issue, we propose Dynamic Prototype Updating (DPU), a novel plug-and-play framework for multimodal OOD detection that accounts for intra-class variations. Our method dynamically updates class center representations for each class by measuring the variance of similar samples within each batch, enabling tailored adjustments. This approach allows us to intensify prediction discrepancies based on the updated class centers, thereby enhancing the models robustness and generalization across different modalities. Extensive experiments on two tasks, five datasets, and nine base OOD algorithms demonstrate that DPU significantly improves OOD detection performances, setting a new state-of-the-art in multimodal OOD detection, including improvements up to 80% in Far-OOD detection.To improve accessibility and reproducibility, our code is released anonymously at https://anonymous.4open.science/r/CVPR-9177.</p>
            <p id="subjects-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" onclick="foldPdfKimi('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="panel paper" keywords="symmetry,reflect3d,single,image,detection,generation,strikes,benefit,cohesiveness,visual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation_CVPR_2025_paper.html" target="_blank" title="172/388"><span class="index notranslate">#172</span></a>
                <a id="title-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="title-link" href="/venue/Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" target="_blank">Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation</a>
                <a id="pdf-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Li" target="_blank">Xiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Huang" target="_blank">Zixuan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anh Thai" target="_blank">Anh Thai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James M. Rehg" target="_blank">James M. Rehg</a>
            </p>
            <p id="summary-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="summary">Symmetry is a ubiquitous and fundamental property in the visual world, serving as a critical cue for perception and structure interpretation. This paper investigates the detection of 3D reflection symmetry from a single RGB image, and reveals its significant benefit on single-image 3D generation. We introduce Reflect3D, a scalable, zero-shot symmetry detector capable of robust generalization to diverse and real-world scenarios. Inspired by the success of foundation models, our method scales up symmetry detection with a transformer-based architecture. We also leverage generative priors from multi-view diffusion models to address the inherent ambiguity in single-view symmetry detection. Extensive evaluations on various data sources demonstrate that Reflect3D establishes a new state-of-the-art in single-image symmetry detection. Furthermore, we show the practical benefit of incorporating detected symmetry into single-image 3D generation pipelines through a symmetry-aware optimization process. The integration of symmetry significantly enhances the structural accuracy, cohesiveness, and visual fidelity of the reconstructed 3D geometry and textures, advancing the capabilities of 3D content creation.</p>
            <p id="subjects-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="panel paper" keywords="intrinsic,concepts,ice,extraction,concept,image,t2i,diffusion,interpretative,reliable">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion_CVPR_2025_paper.html" target="_blank" title="173/388"><span class="index notranslate">#173</span></a>
                <a id="title-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" target="_blank">ICE: Intrinsic Concept Extraction from a Single Image via Diffusion Models</a>
                <a id="pdf-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF">14</sup>]</a>
                <a id="copy-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fernando Julio Cendra" target="_blank">Fernando Julio Cendra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Han" target="_blank">Kai Han</a>
            </p>
            <p id="summary-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="summary">The inherent ambiguity in the definition of visual concepts poses significant challenges for modern generative models, like the Text-to-Image (T2I) models based on diffusion models, in accurately learning concepts from the input images. Existing methods lack a systematic framework and interpretative mechanisms, hindering reliable extraction of the underlying intrinsic concepts. To address this challenge, we present ICE, short for Intrinsic Concept Extraction, a novel framework to automatically and systematically extract intrinsic concepts from a single image leveraging a T2I model. ICE consists of two pivotal stages. In the first stage, ICE devises an automatic concept localization module that pinpoints relevant text-based concepts and their corresponding masks within a given image. This critical phase not only streamlines concept initialization but also offers precise guidance for the subsequent analysis. The second stage delves deeper into each identified mask, decomposing concepts into intrinsic components, capturing specific visual characteristics and general components representing broader categories. This decomposition facilitates a more granular understanding by further dissecting concepts into detailed intrinsic attributes such as colour and material. Extensive experiments validate that ICE achieves superior performance on intrinsic concept extraction, enabling reliable and flexible application to downstream tasks like personalized image generation, image editing, and so on. Code and datasets will be made publicly available for research purposes.</p>
            <p id="subjects-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="panel paper" keywords="doppelgangers,sfm,mast3r,disambiguation,scenes,reconstruction,visual,geotag,accuracy,across">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features_CVPR_2025_paper.html" target="_blank" title="174/388"><span class="index notranslate">#174</span></a>
                <a id="title-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="title-link" href="/venue/Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" target="_blank">Doppelgangers++: Improved Visual Disambiguation with Geometric 3D Features</a>
                <a id="pdf-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanbo Xiangli" target="_blank">Yuanbo Xiangli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruojin Cai" target="_blank">Ruojin Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyu Chen" target="_blank">Hanyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeffrey Byrne" target="_blank">Jeffrey Byrne</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Noah Snavely" target="_blank">Noah Snavely</a>
            </p>
            <p id="summary-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="summary">Accurate 3D reconstruction is frequently hindered by visual aliasing, where visually similar but distinct surfaces (aka, doppelgangers), are incorrectly matched. These spurious matches distort the structure-from-motion (SfM) process, leading to misplaced model elements and reduced accuracy. Prior efforts addressed this with CNN classifiers trained on curated datasets, but these approaches struggle to generalize across diverse real-world scenes and can require extensive parameter tuning. In this work, we present Doppelgangers++, a method to enhance doppelganger detection and improve 3D reconstruction accuracy. Our contributions include a diversified training dataset that incorporates geo-tagged images from everyday scenes to expand robustness beyond landmark-based datasets. We further propose a Transformer-based classifier that leverages 3D-aware features from the MASt3R model, achieving superior precision and recall across both in-domain and out-of-domain tests. Doppelgangers++ integrates seamlessly into standard SfM and MASt3R-SfM pipelines, offering efficiency and adaptability across varied scenes. To evaluate SfM accuracy, we introduce an automated, geotag-based method for validating reconstructed models, eliminating the need for manual inspection. Through extensive experiments, we demonstrate that Doppelgangers++ significantly enhances pairwise visual disambiguation and improves 3D reconstruction quality in complex and diverse scenarios.</p>
            <p id="subjects-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" onclick="foldPdfKimi('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="panel paper" keywords="usg,scene,modality,par,semantics,sgs,generation,expressing,universal,end">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Universal_Scene_Graph_Generation_CVPR_2025_paper.html" target="_blank" title="175/388"><span class="index notranslate">#175</span></a>
                <a id="title-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="title-link" href="/venue/Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" target="_blank">Universal Scene Graph Generation</a>
                <a id="pdf-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Universal_Scene_Graph_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shengqiong Wu" target="_blank">Shengqiong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Fei" target="_blank">Hao Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tat-seng Chua" target="_blank">Tat-seng Chua</a>
            </p>
            <p id="summary-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="summary">Scene graph (SG) representations can neatly and efficiently describe scene semantics, which has driven sustained intensive research in SG generation. In the real world, multiple modalities often coexist, with different types, such as images, text, video, and 3D data, expressing distinct characteristics. Unfortunately, current SG research is largely confined to single-modality scene modeling, preventing the full utilization of the complementary strengths of different modality SG representations in depicting holistic scene semantics.To this end, we introduce Universal SG (USG), a novel representation capable of fully characterizing comprehensive semantic scenes from any given combination of modality inputs, encompassing modality-invariant and modality-specific scenes. Further, we tailor a niche-targeting USG parser, USG-Par, which effectively addresses two key bottlenecks of cross-modal object alignment and out-of-domain challenges. We design the USG-Par with modular architecture for end-to-end USG generation, in which we devise an object associator to relieve the modality gap for cross-modal object alignment. Further, we propose a text-centric scene contrasting learning mechanism to mitigate domain imbalances by aligning multimodal objects and relations with textual SGs. Through extensive experiments, we demonstrate that USG offers a stronger capability for expressing scene semantics than standalone SGs, and also that our USG-Par achieves higher efficacy and performance.</p>
            <p id="subjects-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="panel paper" keywords="drone,wild,dronesplat,imagery,reconstruction,splatting,scenes,distractors,static,radiance">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild_CVPR_2025_paper.html" target="_blank" title="176/388"><span class="index notranslate">#176</span></a>
                <a id="title-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="title-link" href="/venue/Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" target="_blank">DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery</a>
                <a id="pdf-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiadong Tang" target="_blank">Jiadong Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Gao" target="_blank">Yu Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dianyi Yang" target="_blank">Dianyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liqi Yan" target="_blank">Liqi Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufeng Yue" target="_blank">Yufeng Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="summary">Drones have become essential tools for reconstructing wild scenes due to their outstanding maneuverability. Recent advances in radiance field methods have achieved remarkable rendering quality, providing a new avenue for 3D reconstruction from drone imagery. However, dynamic distractors in wild environments challenge the static scene assumption in radiance fields, while limited view constraints hinder the accurate capture of underlying scene geometry. To address these challenges, we introduce DroneSplat, a novel framework designed for robust 3D reconstruction from in-the-wild drone imagery. Our method adaptively adjusts masking thresholds by integrating local-global segmentation heuristics with statistical approaches, enabling precise identification and elimination of dynamic distractors in static scenes. We enhance 3D Gaussian Splatting with multi-view stereo predictions and a voxel-guided optimization strategy, supporting high-quality rendering under limited view constraints. For comprehensive evaluation, we provide a drone-captured 3D reconstruction dataset encompassing both dynamic and static scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS and NeRF baselines in handling in-the-wild drone imagery.</p>
            <p id="subjects-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" onclick="foldPdfKimi('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="panel paper" keywords="monster,stereo,monodepth,matching,eth3d,ill,monocular,depth,posed,marry">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power_CVPR_2025_paper.html" target="_blank" title="177/388"><span class="index notranslate">#177</span></a>
                <a id="title-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="title-link" href="/venue/Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" target="_blank">MonSter: Marry Monodepth to Stereo Unleashes Power</a>
                <a id="pdf-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junda Cheng" target="_blank">Junda Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longliang Liu" target="_blank">Longliang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gangwei Xu" target="_blank">Gangwei Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianqi Wang" target="_blank">Xianqi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxing Zhang" target="_blank">Zhaoxing Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Deng" target="_blank">Yong Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinliang Zang" target="_blank">Jinliang Zang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yurui Chen" target="_blank">Yurui Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhipeng Cai" target="_blank">Zhipeng Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Yang" target="_blank">Xin Yang</a>
            </p>
            <p id="summary-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="summary">Stereo matching recovers depth from image correspondences. Existing methods struggle to handle ill-posed regions with limited matching cues, such as occlusions and textureless areas. To address this, we propose MonSter, a novel method that leverages the complementary strengths of monocular depth estimation and stereo matching. MonSter integrates monocular depth and stereo matching into a dual-branch architecture to iteratively improve each other. Confidence-based guidance adaptively selects reliable stereo cues for monodepth scale-shift recovery, and utilizes explicit monocular depth priors to enhance stereo matching at ill-posed regions. Such iterative mutual enhancement enables MonSter to evolve monodepth priors from coarse object-level structures to pixel-level geometry, fully unlocking the potential of stereo matching. As shown in Fig.2, MonSter ranks 1st across five most commonly used leaderboards --- SceneFlow, KITTI 2012, KITTI 2015, Middlebury, and ETH3D. Achieving up to 49.5% improvements over the previous best method (Bad 1.0 on ETH3D). Comprehensive analysis verifies the effectiveness of MonSter in ill-posed regions. In terms of zero-shot generalization, MonSter significantly and consistently outperforms state-of-the-art methods across the board. Code will be released upon acceptance.</p>
            <p id="subjects-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" onclick="foldPdfKimi('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="panel paper" keywords="vlms,galaxy,walker,geometry,understanding,aware,space,reshapes,scale,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding_CVPR_2025_paper.html" target="_blank" title="178/388"><span class="index notranslate">#178</span></a>
                <a id="title-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="title-link" href="/venue/Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" target="_blank">Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding</a>
                <a id="pdf-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Chen" target="_blank">Tianyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingcheng Fu" target="_blank">Xingcheng Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yisen Gao" target="_blank">Yisen Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haodong Qian" target="_blank">Haodong Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuecen Wei" target="_blank">Yuecen Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Yan" target="_blank">Kun Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyi Zhou" target="_blank">Haoyi Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianxin Li" target="_blank">Jianxin Li</a>
            </p>
            <p id="summary-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="summary">Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy-scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation (R scores up to 0.91) and morphology classification tasks (up to +0.17 F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs.</p>
            <p id="subjects-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="panel paper" keywords="egomotion,dof,solvers,event,cameras,full,geometric,translational,rotational,motion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers_CVPR_2025_paper.html" target="_blank" title="179/388"><span class="index notranslate">#179</span></a>
                <a id="title-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="title-link" href="/venue/Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" target="_blank">Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers</a>
                <a id="pdf-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ji Zhao" target="_blank">Ji Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Banglei Guan" target="_blank">Banglei Guan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zibin Liu" target="_blank">Zibin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laurent Kneip" target="_blank">Laurent Kneip</a>
            </p>
            <p id="summary-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="summary">For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU. Thus, they can only recover the translational motion parameters. Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. Our method leverages event manifolds induced by line segments. The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization. Experiments on both synthetic and real-world data demonstrate the effectiveness of our method. The code will be made publicly available.</p>
            <p id="subjects-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" onclick="foldPdfKimi('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="panel paper" keywords="inpo,diffusion,distillation,reparametrized,ddim,model,conditioning,images,knowledge,student">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion_CVPR_2025_paper.html" target="_blank" title="180/388"><span class="index notranslate">#180</span></a>
                <a id="title-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" target="_blank">InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment</a>
                <a id="pdf-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhong Lu" target="_blank">Yunhong Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qichao Wang" target="_blank">Qichao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengyuan Cao" target="_blank">Hengyuan Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xierui Wang" target="_blank">Xierui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyin Xu" target="_blank">Xiaoyin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Zhang" target="_blank">Min Zhang</a>
            </p>
            <p id="summary-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="summary">Diffusion models have emerged as a cornerstone of generative modeling, capable of producing high-quality images through a progressive denoising process. However, their remarkable performance comes with substantial computational costs, driven by large model sizes and the need for multiple sampling steps. Knowledge distillation, a popular approach for model compression, transfers knowledge from a complex teacher model to a simpler student model. While extensively studied for recognition tasks, its application to diffusion modelsespecially for generating unseen concepts absent from training imagesremains relatively unexplored. In this work, we propose a novel approach called random conditioning, which pairs noised images with randomly chosen text conditions to enable efficient, image-free knowledge distillation. By leveraging random conditioning, we show that it is possible to generate unseen concepts not included in the training data. When applied to conditional diffusion model distillation, This method enables the student model to effectively explore the condition space, leading to notable performance gains. Our approach facilitates the resource-efficient deployment of generative diffusion models, broadening their accessibility for both research and practical applications.</p>
            <p id="subjects-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subjects</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Poster" target="_blank">CVPR.2025 - Poster</a>,
                <a class="subject-2" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="panel paper" keywords="egohumans,scene,human,humans,sfm,people,reconstructing,egoexo4d,51m,coordinate">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Reconstructing_People_Places_and_Cameras_CVPR_2025_paper.html" target="_blank" title="181/388"><span class="index notranslate">#181</span></a>
                <a id="title-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="title-link" href="/venue/Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" target="_blank">Reconstructing People, Places, and Cameras</a>
                <a id="pdf-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Muller_Reconstructing_People_Places_and_Cameras_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lea Mller" target="_blank">Lea Mller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongsuk Choi" target="_blank">Hongsuk Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anthony Zhang" target="_blank">Anthony Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brent Yi" target="_blank">Brent Yi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jitendra Malik" target="_blank">Jitendra Malik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Angjoo Kanazawa" target="_blank">Angjoo Kanazawa</a>
            </p>
            <p id="summary-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="summary">We introduce ``Humans and Structure from Motion'', a novel approach for reconstructing multiple people within a metric world coordinate system from a sparse set of images capturing a scene. Our method jointly estimates human body pose, shape, camera positions, and scene structure, capturing the spatial relationships among people and their location in the environment. Unlike existing methods that require calibrated setups, our approach operates with minimal constraints by leveraging the strength of both human body priors and data-driven SfM. By leveraging multi-view geometry, our method is the first work that effectively recovers humans and scene structure without assumptions about human-scene contact. We evaluate our approach on two challenging benchmarks, EgoHumans and EgoExo4D, demonstrating significant improvements in human location estimation within the world coordinate frame (3.51m to 1.04m and 2.9m to 0.56m respectively). Notably, our results also reveal that incorporating human data in the classical SfM task improves camera pose estimation (RRA@15: 0.74 to 0.89 in EgoHumans), when multiple humans are used for correspondence. We will release our code and data.</p>
            <p id="subjects-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" onclick="foldPdfKimi('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="panel paper" keywords="knuckle,crease,finger,patterns,matching,forensic,templates,prosecuting,accurate,deformed">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease_CVPR_2025_paper.html" target="_blank" title="182/388"><span class="index notranslate">#182</span></a>
                <a id="title-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="title-link" href="/venue/Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" target="_blank">Towards Explainable and Unprecedented Accuracy in Matching Challenging Finger Crease Patterns</a>
                <a id="pdf-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu Zhou" target="_blank">Zhenyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengdong Dong" target="_blank">Chengdong Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ajay Kumar" target="_blank">Ajay Kumar</a>
            </p>
            <p id="summary-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="summary">The primary obstacle in realizing the full potential of finger crease biometrics is the accurate identification of deformed knuckle patterns, often resulting from completely contactless imaging. Current methods struggle significantly with this task, yet accurate matching is crucial for applications ranging from forensic investigations, such as child abuse cases, to surveillance and mobile security. To address this challenge, our study introduces the largest publicly available dataset of deformed knuckle patterns, comprising 805,768 images from 351 subjects. We also propose a novel framework to accurately match knuckle patterns, even under severe pose deformations, by recovering interpretable knuckle crease keypoint feature templates. These templates can dynamically uncover graph structure and feature similarity among the matched correspondences. Our experiments, using the most challenging protocols, illustrate significantly outperforming results for matching such knuckle images. For the first time, we present and evaluate a theoretical model to estimate the uniqueness of 2D finger knuckle patterns, providing a more interpretable and accurate measure of distinctiveness, which is invaluable for forensic examiners in prosecuting suspects.</p>
            <p id="subjects-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="panel paper" keywords="task,multi,tas,videos,action,segmentation,egocentric,activities,foreground,single">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos_CVPR_2025_paper.html" target="_blank" title="183/388"><span class="index notranslate">#183</span></a>
                <a id="title-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="title-link" href="/venue/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" target="_blank">Understanding Multi-Task Activities from Single-Task Videos</a>
                <a id="pdf-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Shen" target="_blank">Yuhan Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ehsan Elhamifar" target="_blank">Ehsan Elhamifar</a>
            </p>
            <p id="summary-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="summary">We introduce and develop a framework for Multi-Task Temporal Action Segmentation (MT-TAS), a novel paradigm that addresses the challenges of interleaved actions when performing multiple tasks simultaneously. Traditional action segmentation models, trained on single-task videos, struggle to handle task switches and complex scenes inherent in multi-task scenarios. To overcome these challenges, our MT-TAS approach synthesizes multi-task video data from single-task sources using our Multi-task Sequence Blending and Segment Boundary Learning modules. Additionally, we propose to dynamically isolate foreground and background elements within video frames, addressing the intricacies of object layouts in multi-task scenarios and enabling a new two-stage temporal action segmentation framework with Foreground-Aware Action Refinement. Also, we introduce the Multi-task Egocentric Kitchen Activities (MEKA) dataset, containing 12 hours of egocentric multi-task videos, to rigorously benchmark MT-TAS models. Extensive experiments demonstrate that our framework effectively bridges the gap between single-task training and multi-task testing, advancing temporal action segmentation with state-of-the-art performance in complex environments.</p>
            <p id="subjects-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" onclick="foldPdfKimi('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="panel paper" keywords="inr,inrs,equivariance,diverse,weights,equigen,equivariant,shot,implicit,functionally">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Few-shot_Implicit_Function_Generation_via_Equivariance_CVPR_2025_paper.html" target="_blank" title="184/388"><span class="index notranslate">#184</span></a>
                <a id="title-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="title-link" href="/venue/Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" target="_blank">Few-shot Implicit Function Generation via Equivariance</a>
                <a id="pdf-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Few-shot_Implicit_Function_Generation_via_Equivariance_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Suizhi Huang" target="_blank">Suizhi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyi Yang" target="_blank">Xingyi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongtao Lu" target="_blank">Hongtao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchao Wang" target="_blank">Xinchao Wang</a>
            </p>
            <p id="summary-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="summary">Implicit Neural Representations (INRs) have emerged as a powerful framework for representing continuous signals. However, generating diverse INR weights remains challenging due to limited training data. We introduce Few-shot Implicit Function Generation, a new problem setup that aims to generate diverse yet functionally consistent INR weights from only a few examples. This is challenging because even for the same signal, the optimal INRs can vary significantly depending on their initializations. To tackle this, we propose EquiGen, a framework that can generate new INRs from limited data. The core idea is that functionally similar networks can be transformed into one another through weight permutations, forming an equivariance group. By projecting these weights into an equivariant latent space, we enable diverse generation within these groups, even with few examples. EquiGen implements this through an equivariant encoder trained via contrastive learning and smooth augmentation, an equivariance-guided diffusion process, and controlled perturbations in the equivariant subspace. Experiments on 2D image and 3D shape INR datasets demonstrate that our approach effectively generates diverse INR weights while preserving their functional properties in few-shot scenarios.</p>
            <p id="subjects-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" onclick="foldPdfKimi('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="panel paper" keywords="reference,animation,viewpoint,human,selection,poses,pose,zoom,character,correlated">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection_CVPR_2025_paper.html" target="_blank" title="185/388"><span class="index notranslate">#185</span></a>
                <a id="title-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="title-link" href="/venue/Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" target="_blank">Free-viewpoint Human Animation with Pose-correlated Reference Selection</a>
                <a id="pdf-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fa-Ting Hong" target="_blank">Fa-Ting Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhan Xu" target="_blank">Zhan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyang Liu" target="_blank">Haiyang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qinjie Lin" target="_blank">Qinjie Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luchuan Song" target="_blank">Luchuan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixin Shu" target="_blank">Zhixin Shu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Zhou" target="_blank">Yang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Duygu Ceylan" target="_blank">Duygu Ceylan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Xu" target="_blank">Dan Xu</a>
            </p>
            <p id="summary-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="summary">Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in/zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints.</p>
            <p id="subjects-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" onclick="foldPdfKimi('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="panel paper" keywords="drivegpt4,loop,autonomous,driving,longest6,capabilities,vehicle,expert,llm,harnessing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.html" target="_blank" title="186/388"><span class="index notranslate">#186</span></a>
                <a id="title-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="title-link" href="/venue/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" target="_blank">DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving</a>
                <a id="pdf-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenhua Xu" target="_blank">Zhenhua Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Bai" target="_blank">Yan Bai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujia Zhang" target="_blank">Yujia Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoling Li" target="_blank">Zhuoling Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Xia" target="_blank">Fei Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwan-Yee K. Wong" target="_blank">Kwan-Yee K. Wong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianqiang Wang" target="_blank">Jianqiang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengshuang Zhao" target="_blank">Hengshuang Zhao</a>
            </p>
            <p id="summary-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="summary">Multimodal large language models (MLLMs) possess the ability to comprehend visual images or videos, and show impressive reasoning ability thanks to the vast amounts of pretrained knowledge, making them highly suitable for autonomous driving applications. Unlike the previous work, DriveGPT4-V1, which focused on open-loop tasks, this study explores the capabilities of LLMs in enhancing closed-loop autonomous driving. DriveGPT4-V2 processes camera images and vehicle states as input to generate low-level control signals for end-to-end vehicle operation. A high-resolution visual tokenizer (HR-VT) is employed enabling DriveGPT4-V2 to perceive the environment with an extensive range while maintaining critical details. The model architecture has been refined to improve decision prediction and inference speed. To further enhance the performance, an additional expert LLM is trained for online imitation learning. The expert LLM, sharing a similar structure with DriveGPT4-V2, can access privileged information about surrounding objects for more robust and reliable predictions. Experimental results show that DriveGPT4-V2 significantly outperforms all baselines on the challenging CARLA Longest6 benchmark. The code and data of DriveGPT4-V2 will be publicly available.</p>
            <p id="subjects-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" onclick="foldPdfKimi('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="panel paper" keywords="resolution,volume,cell,noise,tells,noising,fluorescence,super,microscopy,spatially">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising_CVPR_2025_paper.html" target="_blank" title="187/388"><span class="index notranslate">#187</span></a>
                <a id="title-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="title-link" href="/venue/Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" target="_blank">Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence Microscopy De-noising and Super-Resolution</a>
                <a id="pdf-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zelin Li" target="_blank">Zelin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenwei Wang" target="_blank">Chenwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoke Huang" target="_blank">Zhaoke Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Ma" target="_blank">Yiming Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cunming Zhao" target="_blank">Cunming Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongying Zhao" target="_blank">Zhongying Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong Yan" target="_blank">Hong Yan</a>
            </p>
            <p id="summary-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="summary">3D fluorescence microscopy is essential for understanding fundamental life processes through long-term live-cell imaging. However, due to inherent issues in imaging principles, it faces significant challenges including spatially varying noise and anisotropic resolution, where the axial resolution lags behind the lateral resolution up to 4.5 times. Meanwhile, laser power is kept low to maintain cell viability, leading to inaccessible low-noise and high-resolution paired ground truth (GT). To tackle these limitations, a dual Cycle-consistent Diffusion is proposed to effectively mine intra-volume imaging priors within 3D cell volumes in an unsupervised manner, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-52-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-237" style="width: 1.826em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.513em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-238"><span class="mi" id="MathJax-Span-239" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-240" style="font-family: MathJax_Main;">.</span><span class="mi" id="MathJax-Span-241" style="font-family: MathJax_Math-italic; padding-left: 0.159em;">e</span><span class="mo" id="MathJax-Span-242" style="font-family: MathJax_Main;">.</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mo>.</mo><mi>e</mi><mo>.</mo></math></span></span><script type="math/tex" id="MathJax-Element-52">i.e.</script>, Volume Tells (VTCD), achieving de-noising and super-resolution (SR) simultaneously. Specifically, a spatially iso-distributed denoiser is designed to exploit the noise distribution consistency between adjacent low-noise and high-noise regions within the 3D cell volume, suppressing the spatially varying noise.Then, in light of the structural consistency of the cell volume, a cross-plane global-propagation SR module propagates high-resolution details from the XY plane into adjacent regions in the XZ and YZ planes, progressively enhancing resolution across the entire 3D cell volume.Experimental results on 10 <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-53-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-243" style="width: 1.148em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-244"><span class="mi" id="MathJax-Span-245" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-246" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-53">in</script> <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-54-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-247" style="width: 2.19em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1001.83em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-248"><span class="mi" id="MathJax-Span-249" style="font-family: MathJax_Math-italic;">v</span><span class="mi" id="MathJax-Span-250" style="font-family: MathJax_Math-italic;">i</span><span class="mi" id="MathJax-Span-251" style="font-family: MathJax_Math-italic;">v</span><span class="mi" id="MathJax-Span-252" style="font-family: MathJax_Math-italic;">o</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi><mi>i</mi><mi>v</mi><mi>o</mi></math></span></span><script type="math/tex" id="MathJax-Element-54">vivo</script> cellular dataset demonstrate high improvements in both denoising and super-resolution, with axial resolution enhanced from ~ 430 nm to ~ 90 nm.</p>
            <p id="subjects-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" onclick="foldPdfKimi('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="panel paper" keywords="codebook,hand,contact,interaction,trajectories,holoassist,poses,vqvae,tokenizing,indexer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and_CVPR_2025_paper.html" target="_blank" title="188/388"><span class="index notranslate">#188</span></a>
                <a id="title-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="title-link" href="/venue/Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" target="_blank">How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions</a>
                <a id="pdf-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aditya Prakash" target="_blank">Aditya Prakash</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Lundell" target="_blank">Benjamin Lundell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dmitry Andreychuk" target="_blank">Dmitry Andreychuk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Forsyth" target="_blank">David Forsyth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saurabh Gupta" target="_blank">Saurabh Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harpreet Sawhney" target="_blank">Harpreet Sawhney</a>
            </p>
            <p id="summary-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="summary">We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input.Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer &amp; diffusion baselines across all settings.</p>
            <p id="subjects-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" onclick="foldPdfKimi('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="panel paper" keywords="agpreid,secap,g2aps,person,aerial,viewpoints,calibrating,identification,prompts,ground">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in_CVPR_2025_paper.html" target="_blank" title="189/388"><span class="index notranslate">#189</span></a>
                <a id="title-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="title-link" href="/venue/Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" target="_blank">SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks</a>
                <a id="pdf-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shining Wang" target="_blank">Shining Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunlong Wang" target="_blank">Yunlong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruiqi Wu" target="_blank">Ruiqi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingliang Jiao" target="_blank">Bingliang Jiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Wang" target="_blank">Wenxuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Wang" target="_blank">Peng Wang</a>
            </p>
            <p id="summary-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="summary">When discussing the Aerial-Ground Person Re-identification (AGPReID) task, we face the main challenge of the significant appearance variations caused by different viewpoints, making identity matching difficult. To address this issue, previous methods attempt to reduce the differences between viewpoints by critical attributes and decoupling the viewpoints. While these methods can mitigate viewpoint differences to some extent, they still face two main issues: (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, covering 4,231 unique identities and containing 63,841 high-quality images; the latter is reconstructed from the person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a feasible and effective solution for the AGPReID task.</p>
            <p id="subjects-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" onclick="foldPdfKimi('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="panel paper" keywords="tracking,module,priors,bit,prior,object,models,gradually,generated,free">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Song_Prior-free_3D_Object_Tracking_CVPR_2025_paper.html" target="_blank" title="190/388"><span class="index notranslate">#190</span></a>
                <a id="title-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="title-link" href="/venue/Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" target="_blank">Prior-free 3D Object Tracking</a>
                <a id="pdf-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Song_Prior-free_3D_Object_Tracking_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiuqiang Song" target="_blank">Xiuqiang Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Jin" target="_blank">Li Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengxian Zhang" target="_blank">Zhengxian Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiachen Li" target="_blank">Jiachen Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Zhong" target="_blank">Fan Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guofeng Zhang" target="_blank">Guofeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xueying Qin" target="_blank">Xueying Qin</a>
            </p>
            <p id="summary-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="summary">In this paper, we introduce a novel, truly prior-free 3D object tracking method that operates without given any model or training priors. Unlike existing methods that typically require pre-defined 3D models or specific training datasets as priors, which limit their applicability, our method is free from these constraints. Our method consists of a geometry generation module and a pose optimization module. Its core idea is to enable these two modules to automatically and iteratively enhance each other, thereby gradually building all the necessary information for the tracking task. We thus call the method as Bidirectional Iterative Tracking(BIT). The geometry generation module starts without priors and gradually generates high-precision mesh models for tracking, while the pose optimization module generates additional data during object tracking to further refine the generated models. Moreover, the generated 3D models can be stored and easily reused, allowing for seamless integration into various other tracking systems, not just our methods. Experimental results demonstrate that BIT outperforms many existing methods, even those that extensively utilize prior knowledge, while BIT does not rely on such information. Additionally, the generated 3D models deliver results comparable to actual 3D models, highlighting their superior and innovative qualities.</p>
            <p id="subjects-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" onclick="foldPdfKimi('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="panel paper" keywords="levitor,trajectory,video,dragging,synthesis,depth,movements,interaction,object,control">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.html" target="_blank" title="191/388"><span class="index notranslate">#191</span></a>
                <a id="title-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="title-link" href="/venue/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" target="_blank">LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</a>
                <a id="pdf-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanlin Wang" target="_blank">Hanlin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Ouyang" target="_blank">Hao Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiuyu Wang" target="_blank">Qiuyu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wen Wang" target="_blank">Wen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ka Leong Cheng" target="_blank">Ka Leong Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qifeng Chen" target="_blank">Qifeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Shen" target="_blank">Yujun Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Limin Wang" target="_blank">Limin Wang</a>
            </p>
            <p id="summary-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="summary">The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images.</p>
            <p id="subjects-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" onclick="foldPdfKimi('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="panel paper" keywords="rigging,skeleton,humanoid,character,humanrig,dataset,automatic,animation,plummeted,meshes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.html" target="_blank" title="192/388"><span class="index notranslate">#192</span></a>
                <a id="title-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="title-link" href="/venue/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" target="_blank">HumanRig: Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset</a>
                <a id="pdf-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zedong Chu" target="_blank">Zedong Chu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Xiong" target="_blank">Feng Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meiduo Liu" target="_blank">Meiduo Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinzhi Zhang" target="_blank">Jinzhi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingqi Shao" target="_blank">Mingqi Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxu Sun" target="_blank">Zhaoxu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Di Wang" target="_blank">Di Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mu Xu" target="_blank">Mu Xu</a>
            </p>
            <p id="summary-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="summary">With the rapid evolution of 3D generation algorithms, the cost of producing 3D humanoid character models has plummeted, yet the field is impeded by the lack of a comprehensive dataset for automatic rigginga pivotal step in character animation. Addressing this gap, we present HumanRig, the first large-scale dataset specifically designed for 3D humanoid character rigging, encompassing 11,434 meticulously curated T-posed meshes adhered to a uniform skeleton topology. Capitalizing on this dataset, we introduce an innovative, data-driven automatic rigging framework, which overcomes the limitations of GNN-based methods in handling complex AI-generated meshes. Our approach integrates a Prior-Guided Skeleton Estimator (PGSE) module, which uses 2D skeleton joints to provide a preliminary 3D skeleton, and a Mesh-Skeleton Mutual Attention Network (MSMAN) that fuses skeleton features with 3D mesh features extracted by a U-shaped point transformer. This enables a coarse-to-fine 3D skeleton joint regression and a robust skinning estimation, surpassing previous methods in quality and versatility. This work not only remedies the dataset deficiency in rigging research but also propels the animation industry towards more efficient and automated character rigging pipelines.</p>
            <p id="subjects-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" onclick="foldPdfKimi('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="panel paper" keywords="hoi,reconstruction,intercap,end,transformer,graph,meshes,encoding,human,humans">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding_CVPR_2025_paper.html" target="_blank" title="193/388"><span class="index notranslate">#193</span></a>
                <a id="title-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="title-link" href="/venue/Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" target="_blank">End-to-End HOI Reconstruction Transformer with Graph-based Encoding</a>
                <a id="pdf-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenrong Wang" target="_blank">Zhenrong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Zheng" target="_blank">Qi Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sihan Ma" target="_blank">Sihan Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maosheng Ye" target="_blank">Maosheng Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yibing Zhan" target="_blank">Yibing Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongjiang Li" target="_blank">Dongjiang Li</a>
            </p>
            <p id="summary-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="summary">Human-object interaction (HOI) reconstruction has garnered significant attention due to its diverse applications and the success of capturing human meshes. Existing HOI reconstruction methods often rely on explicitly modeling interactions between humans and objects. However, such a way leads to a natural conflict between 3D mesh reconstruction, which emphasizes global structure, and fine-grained contact reconstruction, which focuses on local details. To address the limitations of explicit modeling, we propose the End-to-End HOI Reconstruction Transformer with Graph-based Encoding (HOI-TG). It implicitly learns the interaction between humans and objects by leveraging self-attention mechanisms. Within the transformer architecture, we devise graph residual blocks to aggregate the topology among vertices of different spatial structures. This dual focus effectively balances global and local representations. Without bells and whistles, HOI-TG achieves state-of-the-art performance on BEHAVE and InterCap datasets. Particularly on the challenging InterCap dataset, our method improves the reconstruction results for human and object meshes by 8.9% and 8.6%, respectively.</p>
            <p id="subjects-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" onclick="foldPdfKimi('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="panel paper" keywords="video,videoscene,scenes,leap,diffusion,views,distilling,generate,step,sparse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in_CVPR_2025_paper.html" target="_blank" title="194/388"><span class="index notranslate">#194</span></a>
                <a id="title-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="title-link" href="/venue/Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" target="_blank">VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step</a>
                <a id="pdf-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanyang Wang" target="_blank">Hanyang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangfu Liu" target="_blank">Fangfu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiawei Chi" target="_blank">Jiawei Chi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yueqi Duan" target="_blank">Yueqi Duan</a>
            </p>
            <p id="summary-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="summary">Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications.</p>
            <p id="subjects-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" onclick="foldPdfKimi('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="panel paper" keywords="goku,video,flow,generation,rectified,curation,image,geneval,vbench,joint">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Goku_Flow_Based_Video_Generative_Foundation_Models_CVPR_2025_paper.html" target="_blank" title="195/388"><span class="index notranslate">#195</span></a>
                <a id="title-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="title-link" href="/venue/Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" target="_blank">Goku: Flow Based Video Generative Foundation Models</a>
                <a id="pdf-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Goku_Flow_Based_Video_Generative_Foundation_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shoufa Chen" target="_blank">Shoufa Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chongjian Ge" target="_blank">Chongjian Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqi Zhang" target="_blank">Yuqi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yida Zhang" target="_blank">Yida Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengda Zhu" target="_blank">Fengda Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Yang" target="_blank">Hao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxiang Hao" target="_blank">Hongxiang Hao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Wu" target="_blank">Hui Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhichao Lai" target="_blank">Zhichao Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifei Hu" target="_blank">Yifei Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ting-Che Lin" target="_blank">Ting-Che Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shilong Zhang" target="_blank">Shilong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fu Li" target="_blank">Fu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuan Li" target="_blank">Chuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Wang" target="_blank">Xing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanghua Peng" target="_blank">Yanghua Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peize Sun" target="_blank">Peize Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Luo" target="_blank">Ping Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Jiang" target="_blank">Yi Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zehuan Yuan" target="_blank">Zehuan Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingyue Peng" target="_blank">Bingyue Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobing Liu" target="_blank">Xiaobing Liu</a>
            </p>
            <p id="summary-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="summary">This paper presents our latest advancements, *Goku*, a new family of joint image-and-video generation models based on rectified flow Transformers to achieve industry-grade performance. We present the foundational elements required for high-quality visual generation, including data curation, model design, flow formulation, etc. Key contributions inclued a meticulous data filtering pipeline that ensures high-quality, fine-grained image and video data curation; and the pioneering use of rectified flow for enhanced interaction among video and image tokens. Goku models achieve superior performance in both qualitative and quantitative assessments. Notably, \ours achieves top scores on major benchmarks: 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, alongside 82.7 on VBench for text-to-video tasks. We hope this report offers valuable insights into joint image-and-video generation models for the research community.</p>
            <p id="subjects-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="panel paper" keywords="psam,point,p2r,counting,p2p,pseudo,counter,crowd,pedestrians,training">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting_CVPR_2025_paper.html" target="_blank" title="196/388"><span class="index notranslate">#196</span></a>
                <a id="title-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="title-link" href="/venue/Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" target="_blank">Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting</a>
                <a id="pdf-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Lin" target="_blank">Wei Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyang Zhao" target="_blank">Chenyang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antoni B. Chan" target="_blank">Antoni B. Chan</a>
            </p>
            <p id="summary-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="summary">Point detection has been developed to locate pedestrians in crowded scenes by training a counter through a point-to-point (P2P) supervision scheme. Despite its excellent localization and counting performance, training a point-based counter still faces challenges concerning annotation labor: hundreds to thousands of points are required to annotate a single sample containing a dense crowd. In this paper, we integrate point-based methods into a semi-supervised counting framework based on pseudo-labeling, enabling the training of a counter with only a few annotated samples supplemented by a large volume of pseudo-labeled data. However, during implementation, the training process encounters issues as the confidence for pseudo-labels fails to propagate to background pixels via the P2P. To tackle this challenge, we devise a point-specific activation map (PSAM) to visually interpret the phenomena occurring during the ill-posed training. Observations from the PSAM suggest that the feature map is excessively activated by the loss for unlabeled data, causing the decoder to misinterpret these over-activations as pedestrians. To mitigate this issue, we propose a point-to-region (P2R) matching scheme to substitute P2P, which segments out local regions rather than detects a point corresponding to a pedestrian for supervision. Consequently, pixels in the local region can share the same confidence for the corresponding pseudo points. Experimental results in both semi-supervised counting and unsupervised domain adaptation highlight the advantages of our method, illustrating P2R can resolves issues identified in PSAM.</p>
            <p id="subjects-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" onclick="foldPdfKimi('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="panel paper" keywords="adacm,video,memory,modality,videos,tasks,visual,understanding,reduction,videollama">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_CVPR_2025_paper.html" target="_blank" title="197/388"><span class="index notranslate">#197</span></a>
                <a id="title-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="title-link" href="/venue/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" target="_blank">AdaCM^2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction</a>
                <a id="pdf-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanbin Man" target="_blank">Yuanbin Man</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Huang" target="_blank">Ying Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengming Zhang" target="_blank">Chengming Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingzhe Li" target="_blank">Bingzhe Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Niu" target="_blank">Wei Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miao Yin" target="_blank">Miao Yin</a>
            </p>
            <p id="summary-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="summary">The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-55-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-253" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-254"><span class="msubsup" id="MathJax-Span-255"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-256"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mn" id="MathJax-Span-257" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-55">^2</script>, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-56-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-258" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-259"><span class="msubsup" id="MathJax-Span-260"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-261"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mn" id="MathJax-Span-262" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-56">^2</script> achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5\% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65\%.</p>
            <p id="subjects-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" onclick="foldPdfKimi('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="panel paper" keywords="robotwin,arm,world,dual,twins,real,robotic,generative,digital,evaluation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.html" target="_blank" title="198/388"><span class="index notranslate">#198</span></a>
                <a id="title-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="title-link" href="/venue/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" target="_blank">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</a>
                <a id="pdf-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Mu" target="_blank">Yao Mu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianxing Chen" target="_blank">Tianxing Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zanxin Chen" target="_blank">Zanxin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shijia Peng" target="_blank">Shijia Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqian Lan" target="_blank">Zhiqian Lan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Gao" target="_blank">Zeyu Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixuan Liang" target="_blank">Zhixuan Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiaojun Yu" target="_blank">Qiaojun Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yude Zou" target="_blank">Yude Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingkun Xu" target="_blank">Mingkun Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lunkai Lin" target="_blank">Lunkai Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiqiang Xie" target="_blank">Zhiqiang Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingyu Ding" target="_blank">Mingyu Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Luo" target="_blank">Ping Luo</a>
            </p>
            <p id="summary-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="summary">In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open-source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples improve the success rate of over 70\% for single-arm tasks and over 40\% for dual-arm tasks compared to models trained solely on real-world data. This significant improvement demonstrates RoboTwin's potential to enhance the development and evaluation of dual-arm robotic manipulation systems.</p>
            <p id="subjects-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" onclick="foldPdfKimi('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="panel paper" keywords="instruction,manipulation,editing,watching,dataset,frames,video,subject,camera,difficult">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move_CVPR_2025_paper.html" target="_blank" title="199/388"><span class="index notranslate">#199</span></a>
                <a id="title-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="title-link" href="/venue/Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" target="_blank">Instruction-based Image Manipulation by Watching How Things Move</a>
                <a id="pdf-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mingdeng Cao" target="_blank">Mingdeng Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuaner Zhang" target="_blank">Xuaner Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinqiang Zheng" target="_blank">Yinqiang Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Xia" target="_blank">Zhihao Xia</a>
            </p>
            <p id="summary-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="summary">This paper introduces a novel dataset construction pipeline that samples pairs of frames from videos and uses multimodal large language models (MLLMs) to generate editing instructions for training instruction-based image manipulation models. Video frames inherently preserve the identity of subjects and scenes, ensuring consistent content preservation during editing. Additionally, video data captures diverse, natural dynamicssuch as non-rigid subject motion and complex camera movementsthat are difficult to model otherwise, making it an ideal source for scalable dataset construction. Using this approach, we create a new dataset to train InstructMove, a model capable of instruction-based complex manipulations that are difficult to achieve with synthetically generated datasets. Our model demonstrates state-of-the-art performance in tasks such as adjusting subject poses, rearranging elements, and altering camera perspectives.</p>
            <p id="subjects-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" onclick="foldPdfKimi('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="panel paper" keywords="rewardbench,genrms,vision,tasks,reward,genrm,language,models,generative,multimodal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models_CVPR_2025_paper.html" target="_blank" title="200/388"><span class="index notranslate">#200</span></a>
                <a id="title-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="title-link" href="/venue/Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" target="_blank">VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models</a>
                <a id="pdf-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Li" target="_blank">Lei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuancheng Wei" target="_blank">Yuancheng Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihui Xie" target="_blank">Zhihui Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuqing Yang" target="_blank">Xuqing Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Song" target="_blank">Yifan Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peiyi Wang" target="_blank">Peiyi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenxin An" target="_blank">Chenxin An</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Liu" target="_blank">Tianyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sujian Li" target="_blank">Sujian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bill Yuchen Lin" target="_blank">Bill Yuchen Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingpeng Kong" target="_blank">Lingpeng Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Liu" target="_blank">Qi Liu</a>
            </p>
            <p id="summary-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="summary">Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated preference labels from traditional VL tasks, which can introduce biases and often fail to effectively challenge state-of-the-art models.To address these limitations, we introduce VL-RewardBench, a comprehensive benchmark spanning general multimodal queries, visual hallucination detection, and complex reasoning tasks.Through our AI-assisted annotation pipeline combining sample selection with human verification, we curate 1,250 high-quality examples specifically designed to probe model limitations.Comprehensive evaluation across 16 leading large vision-language models, demonstrates VL-RewardBench's effectiveness as a challenging testbed, where even GPT-4o achieves only 65.4\% accuracy, and state-of-the-art open-source models such as Qwen2-VL-72B, struggle to surpass random-guessing. Importantly, performance on VL-RewardBench strongly correlates (Pearson's r <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-57-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-263" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.461em, 1000.68em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-264"><span class="mo" id="MathJax-Span-265" style="font-family: MathJax_Main;">&gt;</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 0.816em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo></math></span></span><script type="math/tex" id="MathJax-Element-57">></script> 0.9) with MMMU-Pro accuracy using Best-of-N sampling with VL-GenRMs.Analysis experiments uncover three critical insights for improving VL-GenRMs: (i) models predominantly fail at basic visual perception tasks rather than reasoning tasks; (ii) inference-time scaling benefits vary dramatically by model capacity; and (iii) training VL-GenRMs to learn to judge substantially boosts judgment capability (+14.3\% accuracy for a 7B VL-GenRM).We believe VL-RewardBench along with the experimental insights will become a valuable resource for advancing VL-GenRMs.</p>
            <p id="subjects-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" onclick="foldPdfKimi('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="panel paper" keywords="nerfprior,prior,sdf,radiance,view,color,consistency,clues,signed,nerf">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor_CVPR_2025_paper.html" target="_blank" title="201/388"><span class="index notranslate">#201</span></a>
                <a id="title-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="title-link" href="/venue/Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" target="_blank">NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction</a>
                <a id="pdf-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenyuan Zhang" target="_blank">Wenyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emily Yue-ting Jia" target="_blank">Emily Yue-ting Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junsheng Zhou" target="_blank">Junsheng Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baorui Ma" target="_blank">Baorui Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kanle Shi" target="_blank">Kanle Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Shen Liu" target="_blank">Yu-Shen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhizhong Han" target="_blank">Zhizhong Han</a>
            </p>
            <p id="summary-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="summary">Recently, it has shown that priors are vital for neural implicit functions to reconstruct high-quality surfaces from multi-view RGB images. However, current priors require large-scale pre-training, and merely provide geometric clues without considering the importance of color. In this paper, we present NeRFPrior, which adopts a neural radiance field as a prior to learn signed distance fields using volume rendering for surface reconstruction. Our NeRF prior can provide both geometric and color clues, and also get trained fast under the same scene without additional data. Based on the NeRF prior, we are enabled to learn a signed distance function (SDF) by explicitly imposing a multi-view consistency constraint on each ray intersection for surface inference. Specifically, at each ray intersection, we use the density in the prior as a coarse geometry estimation, while using the color near the surface as a clue to check its visibility from another view angle. For the textureless areas where the multi-view consistency constraint does not work well, we further introduce a depth consistency loss with confidence weights to infer the SDF. Our experimental results outperform the state-of-the-art methods under the widely used benchmarks. The source code will be publicly available.</p>
            <p id="subjects-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="panel paper" keywords="deblurring,motion,rotational,mdt,rsas,blur,decomposition,translational,mdm,transformer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition_CVPR_2025_paper.html" target="_blank" title="202/388"><span class="index notranslate">#202</span></a>
                <a id="title-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="title-link" href="/venue/Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" target="_blank">A Polarization-Aided Transformer for Image Deblurring via Motion Vector Decomposition</a>
                <a id="pdf-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Duosheng Chen" target="_blank">Duosheng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shihao Zhou" target="_blank">Shihao Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinshan Pan" target="_blank">Jinshan Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinglei Shi" target="_blank">Jinglei Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lishen Qu" target="_blank">Lishen Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jufeng Yang" target="_blank">Jufeng Yang</a>
            </p>
            <p id="summary-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="summary">Effectively leveraging motion information is crucial for the image deblurring task. Existing methods typically build deep-learning models to restore a clean image by estimating blur patterns over the entire movement. This suggests that the blur caused by rotational motion components is processed together with the translational one. Exploring the movement without separation leads to limited performance for complex motion deblurring, especially rotational motion. In this paper, we propose Motion Decomposition Transformer (MDT), a transformer-based architecture augmented with polarized modules for deblurring via motion vector decomposition. MDT consists of a Motion Decomposition Module (MDM) for extracting hybrid rotation and translation features, and a Radial Stripe Attention Solver (RSAS) for sharp image reconstruction with enhanced rotational information. Specifically, the MDM uses a deformable Cartesian convolutional branch to capture translational motion, complemented by a polar-system branch to capture rotational motion. The RSAS employs radial stripe windows and angular relative positional encoding in the polar system to enhance rotational information. This design preserves translational details while keeping computational costs lower than dual-coordinate design. Experimental results on 6 image deblurring datasets show that MDT outperforms state-of-the-art methods, particularly in handling blur caused by complex motions with significant rotational components.</p>
            <p id="subjects-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" onclick="foldPdfKimi('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="panel paper" keywords="differentiable,abstraction,marching,shape,dsf,contact,primitives,shapes,accuracy,support">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions_CVPR_2025_paper.html" target="_blank" title="203/388"><span class="index notranslate">#203</span></a>
                <a id="title-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="title-link" href="/venue/Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" target="_blank">Shape Abstraction via Marching Differentiable Support Functions</a>
                <a id="pdf-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sunkyung Park" target="_blank">Sunkyung Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeongmin Lee" target="_blank">Jeongmin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongjun Lee" target="_blank">Dongjun Lee</a>
            </p>
            <p id="summary-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="summary">Shape abstraction, simplifying shape representation into a set of primitives, is a fundamental topic in computer vision. The choice of primitives shapes the structure of world understanding, yet achieving both high abstraction accuracy and versatility remains challenging. In this paper, we introduce a novel framework for shape abstraction utilizing a differentiable support function (DSF), which offers unique advantages in representing a wide range of convex shapes with fewer parameters, providing smooth surface approximation and enabling differentiable contact features (gap, point, normal) essential for downstream applications involving contact-related problems. To tackle the associated optimization and combinatorial challenges, we introduce two techniques: differentiable shape parameterization and hyperplane-based marching to enhance accuracy and reduce DSF requirements. We validate our method through experiments demonstrating superior accuracy and efficiency, and showcase its applicability in tasks requiring differentiable contact information.</p>
            <p id="subjects-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" onclick="foldPdfKimi('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="panel paper" keywords="wonderworld,scene,scenes,generation,interactive,depth,views,a6000,generating,interactively">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image_CVPR_2025_paper.html" target="_blank" title="204/388"><span class="index notranslate">#204</span></a>
                <a id="title-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="title-link" href="/venue/Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" target="_blank">WonderWorld: Interactive 3D Scene Generation from a Single Image</a>
                <a id="pdf-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hong-Xing Yu" target="_blank">Hong-Xing Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyi Duan" target="_blank">Haoyi Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Herrmann" target="_blank">Charles Herrmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=William T. Freeman" target="_blank">William T. Freeman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>
            </p>
            <p id="summary-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="summary">We present WonderWorld, a novel framework for interactive 3D scene generation that enables users to interactively specify scene contents and layout and see the created scenes in low latency. The major challenge lies in achieving fast generation of 3D scenes. Existing scene generation approaches fall short of speed as they often require (1) progressively generating many views and depth maps, and (2) time-consuming optimization of the scene representations. Our approach does not need multiple views, and it leverages a geometry-based initialization that significantly reduces optimization time. Another challenge is generating coherent geometry that allows all scenes to be connected. We introduce the guided depth diffusion that allows partial conditioning of depth estimation. WonderWorld generates connected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU, enabling real-time user interaction and exploration. We will release full code and software for reproducibility.</p>
            <p id="subjects-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" onclick="foldPdfKimi('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="panel paper" keywords="ugc,quality,finevq,video,grained,fine,finevd,videos,vqa,assessment">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment_CVPR_2025_paper.html" target="_blank" title="205/388"><span class="index notranslate">#205</span></a>
                <a id="title-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="title-link" href="/venue/Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" target="_blank">FineVQ: Fine-Grained User Generated Content Video Quality Assessment</a>
                <a id="pdf-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huiyu Duan" target="_blank">Huiyu Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Hu" target="_blank">Qiang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiarui Wang" target="_blank">Jiarui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liu Yang" target="_blank">Liu Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zitong Xu" target="_blank">Zitong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Liu" target="_blank">Lu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiongkuo Min" target="_blank">Xiongkuo Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chunlei Cai" target="_blank">Chunlei Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianxiao Ye" target="_blank">Tianxiao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyun Zhang" target="_blank">Xiaoyun Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zhai" target="_blank">Guangtao Zhai</a>
            </p>
            <p id="summary-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="summary">The rapid growth of user-generated content (UGC) videos has produced an urgent need for effective video quality assessment (VQA) algorithms to monitor video quality and guide optimization and recommendation procedures. However, current VQA models generally only give an overall rating for a UGC video, which lacks fine-grained labels for serving video processing and recommendation applications. To address the challenges and promote the development of UGC videos, we establish the first large-scale Fine-grained Video quality assessment Database, termed FineVD, which comprises 6104 UGC videos with fine-grained quality scores and descriptions across multiple dimensions. Based on this database, we propose a Fine-grained Video Quality assessment (FineVQ) model to learn the fine-grained quality of UGC videos, with the capabilities of quality rating, quality scoring, and quality attribution. Extensive experimental results demonstrate that our proposed FineVQ can produce fine-grained video-quality results and achieve state-of-the-art performance on FineVD and other commonly used UGC-VQA datasets. Both FineVD and FineVQ will be released upon the publication.</p>
            <p id="subjects-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" onclick="foldPdfKimi('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="panel paper" keywords="portrait,hyperlora,lora,adapter,synthesis,shot,adaptive,generation,plug,personalized">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis_CVPR_2025_paper.html" target="_blank" title="206/388"><span class="index notranslate">#206</span></a>
                <a id="title-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="title-link" href="/venue/Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" target="_blank">HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis</a>
                <a id="pdf-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mengtian Li" target="_blank">Mengtian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinshu Chen" target="_blank">Jinshu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanquan Feng" target="_blank">Wanquan Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingchuan Li" target="_blank">Bingchuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Dai" target="_blank">Fei Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Songtao Zhao" target="_blank">Songtao Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qian He" target="_blank">Qian He</a>
            </p>
            <p id="summary-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="summary">Personalized portrait synthesis, essential in domains like social entertainment, has recently made significant progress. Person-wise fine-tuning based methods, such as LoRA and DreamBooth, can produce photorealistic outputs but need training on individual samples, consuming time and resources and posing an unstable risk. Adapter based techniques such as IP-Adapter freeze the foundational model parameters and employ a plug-in architecture to enable zero-shot inference, but they often exhibit a lack of naturalness and authenticity, which are not to be overlooked in portrait synthesis tasks. In this paper, we introduce a parameter-efficient adaptive generation method, namely HyperLoRA, that uses an adaptive plug-in network to generate LoRA weights, merging the superior performance of LoRA with the zero-shot capability of adapter scheme. Through our carefully designed network structure and training strategy, we achieve zero-shot personalized portrait generation (supporting both single and multiple image inputs) with high photorealism, fidelity, and editability.</p>
            <p id="subjects-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" onclick="foldPdfKimi('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="panel paper" keywords="parametric,cameras,motion,camera,catadioptric,structure,pinhole,fisheye,opponents,self">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model_CVPR_2025_paper.html" target="_blank" title="207/388"><span class="index notranslate">#207</span></a>
                <a id="title-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="title-link" href="/venue/Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" target="_blank">Structure-from-Motion with a Non-Parametric Camera Model</a>
                <a id="pdf-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yihan Wang" target="_blank">Yihan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfei Pan" target="_blank">Linfei Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viktor Larsson" target="_blank">Viktor Larsson</a>
            </p>
            <p id="summary-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="summary">In this paper, we present a new Structure-from-Motion pipeline that uses a non-parametric camera projection model. The model is self-calibrated during the reconstruction process and can fit a wide variety of cameras, ranging from simple low-distortion pinhole cameras to more extreme optical systems such as fisheye or catadioptric cameras. The key component in our framework is an adaptive calibration procedure that can estimate partial calibrations, only modeling regions of the image where sufficient constraints are available. In experiments, we show that our method achieves comparable accuracy in scenarios where traditional Structure-from-Motion pipelines excel and it outperforms its parametric opponents in cases where they are unable to self-calibrate their parametric models.</p>
            <p id="subjects-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="panel paper" keywords="depth,foundation,estimation,matching,temporal,ch3depth,consistency,flow,flexible,efficiency">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.html" target="_blank" title="208/388"><span class="index notranslate">#208</span></a>
                <a id="title-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="title-link" href="/venue/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" target="_blank">CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching</a>
                <a id="pdf-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaqi Li" target="_blank">Jiaqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiran Wang" target="_blank">Yiran Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinghong Zheng" target="_blank">Jinghong Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junrui Zhang" target="_blank">Junrui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liao Shen" target="_blank">Liao Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianqi Liu" target="_blank">Tianqi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiguo Cao" target="_blank">Zhiguo Cao</a>
            </p>
            <p id="summary-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="summary">Depth estimation is a fundamental task in 3D vision. An ideal depth estimation model is expected to embrace meticulous detail, temporal consistency, and high efficiency. Although existing foundation models can perform well in certain specific aspects, most of them fall short of fulfilling all the above requirements simultaneously. In this paper, we present CH<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-58-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-266" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-267"><span class="msubsup" id="MathJax-Span-268"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-269"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -1.977em; left: 0em;"><span class="mn" id="MathJax-Span-270" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi></mi><mn>3</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-58">_3</script>Depth, an efficient and flexible model for depth estimation with flow matching to address this challenge. Specifically, 1) we reframe the optimization objective of flow matching as a scale-variable velocity field to improve accuracy. 2) To enhance efficiency, we propose non-uniform sampling to achieve better prediction with fewer sampling steps. 3) We design the Latent Temporal Stabilizer (LTS) to enhance temporal consistency by aggregating latent codes of adjacent frames, enabling our method to be lightweight and compatible for video depth estimation. CH<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-59-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-271" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-272"><span class="msubsup" id="MathJax-Span-273"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-274"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -1.977em; left: 0em;"><span class="mn" id="MathJax-Span-275" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi></mi><mn>3</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-59">_3</script>Depth achieves state-of-the-art performance in zero-shot evaluations across multiple image and video datasets, excelling in prediction accuracy, efficiency, and temporal consistency, highlighting its potential as the next foundation model for depth estimation.</p>
            <p id="subjects-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" onclick="foldPdfKimi('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="panel paper" keywords="foveated,human,visual,sampling,vision,seeing,resolution,mdetr,blip2,vilt">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models_CVPR_2025_paper.html" target="_blank" title="209/388"><span class="index notranslate">#209</span></a>
                <a id="title-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="title-link" href="/venue/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" target="_blank">Seeing More with Less: Human-like Representations in Vision Models</a>
                <a id="pdf-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrey Gizdov" target="_blank">Andrey Gizdov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shimon Ullman" target="_blank">Shimon Ullman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Harari" target="_blank">Daniel Harari</a>
            </p>
            <p id="summary-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="summary">Large multimodal models (LMMs) typically process visual inputs with uniform resolution across the entire field of view, leading to inefficiencies when non-critical image regions are processed as precisely as key areas. Inspired by the human visual system's foveated approach, we apply a biologically inspired method to leading architectures such as MDETR, BLIP2, InstructBLIP, LLaVA, and ViLT, and evaluate their performance with variable resolution inputs. Results show that foveated sampling boosts accuracy in visual tasks like question answering and object detection under tight pixel budgets, improving performance by up to 2.7% on the GQA dataset, 2.1% on SEED-Bench, and 2.0% on VQAv2 compared to uniform sampling. Furthermore, our research indicates that indiscriminate resolution increases yield diminishing returns, with models achieving up to 80% of their full capability using just 3% of the pixels, even on complex tasks. Foveated sampling also prompts more human-like processing within models, such as neuronal selectivity and globally-acting self-attention in vision transformers. This paper provides a foundational analysis of foveated sampling's impact on existing models, suggesting that more efficient architectural adaptations, mimicking human visual processing, are a promising research venue for the community.</p>
            <p id="subjects-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" onclick="foldPdfKimi('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="panel paper" keywords="cir,stage,reflective,reasoning,osrcir,retrieve,chain,retrieval,image,reference">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.html" target="_blank" title="210/388"><span class="index notranslate">#210</span></a>
                <a id="title-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="title-link" href="/venue/Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" target="_blank">Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval</a>
                <a id="pdf-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanmin Tang" target="_blank">Yuanmin Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jue Zhang" target="_blank">Jue Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoting Qin" target="_blank">Xiaoting Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Yu" target="_blank">Jing Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaopeng Gou" target="_blank">Gaopeng Gou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gang Xiong" target="_blank">Gang Xiong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingwei Lin" target="_blank">Qingwei Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saravan Rajmohan" target="_blank">Saravan Rajmohan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongmei Zhang" target="_blank">Dongmei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Wu" target="_blank">Qi Wu</a>
            </p>
            <p id="summary-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="summary">Composed Image Retrieval (CIR) aims to retrieve target images that closely resemble a reference image while integrating user-specified textual modifications, thereby capturing user intent more precisely. This dual-modality approach is especially valuable in internet search and e-commerce, facilitating tasks like scene image search with object manipulation and product recommendations with attribute changes. Existing training-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process: they first generate a caption for the reference image and then use Large Language Models for reasoning to obtain a target description. However, these methods suffer from missing critical visual details and limited reasoning capabilities, leading to suboptimal retrieval performance. To address these challenges, we propose a novel, training-free one-stage method, One-Stage Reflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs Multimodal Large Language Models to retain essential visual information in a single-stage reasoning process, eliminating the information loss seen in two-stage methods. Our Reflective Chain-of-Thought framework further improves interpretative accuracy by aligning manipulation intent with contextual cues from reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over existing training-free methods across multiple tasks, setting new state-of-the-art results in ZS-CIR and enhancing its utility in vision-language applications. Our code is available at https://anonymous.4open.science/r/osrcir24/.</p>
            <p id="subjects-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" onclick="foldPdfKimi('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="panel paper" keywords="3dtopia,assets,primitive,quality,generative,pbr,primx,diffusion,high,asset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html" target="_blank" title="211/388"><span class="index notranslate">#211</span></a>
                <a id="title-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" target="_blank">3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion</a>
                <a id="pdf-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxi Chen" target="_blank">Zhaoxi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxiang Tang" target="_blank">Jiaxiang Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhao Dong" target="_blank">Yuhao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziang Cao" target="_blank">Ziang Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangzhou Hong" target="_blank">Fangzhou Hong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yushi Lan" target="_blank">Yushi Lan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tengfei Wang" target="_blank">Tengfei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haozhe Xie" target="_blank">Haozhe Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Wu" target="_blank">Tong Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunsuke Saito" target="_blank">Shunsuke Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Pan" target="_blank">Liang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dahua Lin" target="_blank">Dahua Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Liu" target="_blank">Ziwei Liu</a>
            </p>
            <p id="summary-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="summary">The increasing demand for high-quality 3D assets across various industries necessitates efficient and automated 3D content creation. Despite recent advancements in 3D generative models, existing methods still face challenges with optimization speed, geometric fidelity, and the lack of assets for physically based rendering (PBR). In this paper, we introduce 3DTopia-XL, a scalable native 3D generative model designed to overcome these limitations. 3DTopia-XL leverages a novel primitive-based 3D representation, PrimX, which encodes detailed shape, albedo, and material field into a compact tensorial format, facilitating the modeling of high-resolution geometry with PBR assets. On top of the novel representation, we propose a generative framework based on Diffusion Transformer (DiT), which comprises 1) Primitive Patch Compression, 2) and Latent Primitive Diffusion. 3DTopia-XL learns to generate high-quality 3D assets from textual or visual inputs. Extensive qualitative and quantitative experiments are conducted to demonstrate that 3DTopia-XL significantly outperforms existing methods in generating high-quality 3D assets with fine-grained textures and materials, efficiently bridging the quality gap between generative models and real-world applications.</p>
            <p id="subjects-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="panel paper" keywords="badgr,floor,layouts,plan,diffusion,adjustment,reconstruction,bundle,conditioned,camera">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor_CVPR_2025_paper.html" target="_blank" title="212/388"><span class="index notranslate">#212</span></a>
                <a id="title-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="title-link" href="/venue/Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" target="_blank">BADGR: Bundle Adjustment Diffusion Conditioned by Gradients for Wide-Baseline Floor Plan Reconstruction</a>
                <a id="pdf-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuguang Li" target="_blank">Yuguang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ivaylo Boyadzhiev" target="_blank">Ivaylo Boyadzhiev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Liu" target="_blank">Zixuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linda Shapiro" target="_blank">Linda Shapiro</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Colburn" target="_blank">Alex Colburn</a>
            </p>
            <p id="summary-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="summary">Reconstructing precise camera poses and floor plan layouts from a set of wide-baseline RGB panoramas is a difficult and unsolved problem. We present BADGR, a novel diffusion model which performs both reconstruction and bundle adjustment (BA) optimization tasks, to refine camera poses and layouts from a given coarse state using 1D floor boundary information from dozens of images of varying input densities. Unlike a guided diffusion model, BADGR is conditioned on dense per-feature outputs from a single-step Levenberg-Marquardt (LM) optimizer and is trained to predict camera and wall positions while minimizing reprojection errors for view-consistency. The objective of layout generation from denoising diffusion process complements BA optimization by providing additional learned layout-structural constraints on top of the co-visible features across images. These constraints help BADGR to make plausible guesses on spatial relations which help constrain pose graph, such as wall adjacency, collinearity, and learn to mitigate errors from dense boundary observations with global contexts. BADGR trains exclusively on 2D floor plans, simplifying data acquisition, enabling robust augmentation, and supporting variety of input densities. Our experiments and analysis validate our method, which significantly outperforms the state-of-the-art pose and floor plan layouts reconstruction with different input densities.</p>
            <p id="subjects-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" onclick="foldPdfKimi('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="panel paper" keywords="grasping,dexterous,dexgrasp,anything,universal,poses,awareness,15k,robotic,objects">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness_CVPR_2025_paper.html" target="_blank" title="213/388"><span class="index notranslate">#213</span></a>
                <a id="title-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="title-link" href="/venue/Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" target="_blank">DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness</a>
                <a id="pdf-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Zhong" target="_blank">Yiming Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Jiang" target="_blank">Qi Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Yu" target="_blank">Jingyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuexin Ma" target="_blank">Yuexin Ma</a>
            </p>
            <p id="summary-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="summary">A dexterous hand capable of grasping any object is essential for the development of general-purpose embodied intelligent robots. However, due to the high degree of freedom in dexterous hands and the vast diversity of objects, generating high-quality, usable grasping poses in a robust manner is a significant challenge. In this paper, we introduce DexGrasp Anything, a method that effectively integrates physical constraints into both the training and sampling phases of a diffusion-based generative model, achieving state-of-the-art performance across nearly all open datasets. Additionally, we present a new dexterous grasping dataset containing over 3.4 million diverse grasping poses for more than 15k different objects, demonstrating its potential to advance universal dexterous grasping. The code of our method and our dataset will be publicly released soon.</p>
            <p id="subjects-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" onclick="foldPdfKimi('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="panel paper" keywords="aqc,qucoop,problems,binary,qubo,composite,quadratic,parametrised,annealers,solving">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems_CVPR_2025_paper.html" target="_blank" title="214/388"><span class="index notranslate">#214</span></a>
                <a id="title-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="title-link" href="/venue/Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" target="_blank">QuCOOP: A Versatile Framework for Solving Composite and Binary-Parametrised Problems on Quantum Annealers</a>
                <a id="pdf-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Natacha Kuete Meli" target="_blank">Natacha Kuete Meli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vladislav Golyanik" target="_blank">Vladislav Golyanik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marcel Seelbach Benkner" target="_blank">Marcel Seelbach Benkner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Moeller" target="_blank">Michael Moeller</a>
            </p>
            <p id="summary-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="summary">There is growing interest in solving computer vision problems such as mesh or point set alignment using Adiabatic Quantum Computing (AQC). Unfortunately, modern experimental AQC devices such as D-Wave only support Quadratic Unconstrained Binary Optimization (QUBO) problems, which severely limits their applicability. This paper proposes a new way to overcome this limitation and introduces QuCOOP, an optimization framework extending the scope of AQC to composite and binary-parameterized, possibly non-quadratic problems. The key idea of QuCOOP is to iteratively approximate the original objective function by a sequel of local (intermediate) QUBO forms, whose binary parameters can be sampled on AQC devices. We experiment with quadratic assignment problems, shape matching and point set registration without knowing the correspondences in advance. Our approach achieves state-of-the-art results across multiple instances of tested problems.</p>
            <p id="subjects-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" onclick="foldPdfKimi('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="panel paper" keywords="latexblend,customized,textual,generation,concepts,concept,latent,scaling,bank,blending">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending_CVPR_2025_paper.html" target="_blank" title="215/388"><span class="index notranslate">#215</span></a>
                <a id="title-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="title-link" href="/venue/Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" target="_blank">LaTexBlend: Scaling Multi-concept Customized Generation with Latent Textual Blending</a>
                <a id="pdf-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Jin" target="_blank">Jian Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenbo Yu" target="_blank">Zhenbo Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Shen" target="_blank">Yang Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyong Fu" target="_blank">Zhenyong Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>
            </p>
            <p id="summary-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="summary">Customized text-to-image generation renders user-specified concepts into novel contexts based on textual prompts. Scaling the number of concepts in customized generation meets a broader demand for user creation, whereas existing methods face challenges with generation quality and computational efficiency. In this paper, we propose LaTexBlend, a novel framework for effectively and efficiently scaling multi-concept customized generation. The core idea of LaTexBlend is to represent single concepts and blend multiple concepts within a Latent Textual space, which is positioned after the text encoder and a linear projection. LaTexBlend customizes each concept individually, storing them in a concept bank with a compact representation of latent textual features that captures sufficient concept information to ensure high fidelity. At inference, concepts from the bank can be freely and seamlessly combined in the latent textual space, offering two key merits for multi-concept generation: 1) excellent scalability, and 2) significant reduction of denoising deviation, preserving coherent layouts. Extensive experiments demonstrate that LaTexBlend can flexibly integrate multiple customized concepts with harmonious structures and high subject fidelity, substantially outperforming baselines in both generation quality and computational efficiency. Our code will be publicly available.</p>
            <p id="subjects-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" onclick="foldPdfKimi('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="panel paper" keywords="lmms,dyfo,visual,focus,search,grained,dynamic,fine,understanding,irrelevant">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs_CVPR_2025_paper.html" target="_blank" title="216/388"><span class="index notranslate">#216</span></a>
                <a id="title-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="title-link" href="/venue/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" target="_blank">DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding</a>
                <a id="pdf-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF">15</sup>]</a>
                <a id="copy-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Geng Li" target="_blank">Geng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinglin Xu" target="_blank">Jinglin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunzhen Zhao" target="_blank">Yunzhen Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxin Peng" target="_blank">Yuxin Peng</a>
            </p>
            <p id="summary-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="summary">Humans can effortlessly locate desired objects in cluttered environments, relying on a cognitive mechanism known as visual search to efficiently filter out irrelevant information and focus on task-related regions. Inspired by this process, we propose DyFo (Dynamic Focus), a training-free dynamic focusing visual search method that enhances fine-grained visual understanding in large multimodal models (LMMs). Unlike existing approaches which require additional modules or data modifications, DyFo leverages a bidirectional interaction between LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to simulate human-like focus adjustments. This enables LMMs to focus on key visual regions while filtering out irrelevant content without the need for vocabulary expansion or specialized localization modules. Experimental results demonstrate that DyFo significantly improves fine-grained visual understanding and reduces hallucination issues in LMMs, achieving superior performance across both fixed and variable resolution models.</p>
            <p id="subjects-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" onclick="foldPdfKimi('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="panel paper" keywords="climbing,climbingcap,hmr,motion,motions,ascendmotion,dataset,recovery,challenging,global">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World_CVPR_2025_paper.html" target="_blank" title="217/388"><span class="index notranslate">#217</span></a>
                <a id="title-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="title-link" href="/venue/Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" target="_blank">ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</a>
                <a id="pdf-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ming Yan" target="_blank">Ming Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xincheng Lin" target="_blank">Xincheng Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhua Luo" target="_blank">Yuhua Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuqi Fan" target="_blank">Shuqi Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yudi Dai" target="_blank">Yudi Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixin Zhong" target="_blank">Qixin Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lincai Zhong" target="_blank">Lincai Zhong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuexin Ma" target="_blank">Yuexin Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lan Xu" target="_blank">Lan Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenglu Wen" target="_blank">Chenglu Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siqi Shen" target="_blank">Siqi Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Wang" target="_blank">Cheng Wang</a>
            </p>
            <p id="summary-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="summary">Human Motion Recovery (HMR) research mainly focuses on ground-based motions such as running. The study on capturing climbing motion, an off-ground motion, is sparse. This is partly due to the limited availability of climbing motion datasets, especially large-scale and challenging 3D labeled datasets. To address the insufficiency of climbing motion datasets, we collect AscendMotion, a large-scale well-annotated, and challenging climbing motion dataset. It consists of 412k RGB, LiDAR frames, and IMU measurements, which includes the challenging climbing motions of 22 professional climbing coaches across 12 different rocks. Capturing the climbing motions is challenging as it requires precise recovery of not only the complex pose but also the global position of climbers. Although multiple global HMR methods have been proposed, they cannot faithfully capture climbing motions. To address the limitations of HMR methods for climbing, we propose ClimbingCap, a motion recovery method that reconstructs continuous 3D human climbing motion in a global coordinate system. One key insight is to use the RGB and the LiDAR modalities to separately reconstruct motions in camera coordinates and global coordinates and optimize them jointly. We demonstrate the quality of the AscendMotion dataset and present promising results from ClimbingCap. The AscendMotion dataset and the source code of ClimbingCap will be released publicly to the research community.</p>
            <p id="subjects-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" onclick="foldPdfKimi('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="panel paper" keywords="bev,bevdiffuser,denoising,plug,play,truth,guidance,representations,object,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance_CVPR_2025_paper.html" target="_blank" title="218/388"><span class="index notranslate">#218</span></a>
                <a id="title-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="title-link" href="/venue/Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" target="_blank">BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance</a>
                <a id="pdf-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Ye" target="_blank">Xin Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Burhaneddin Yaman" target="_blank">Burhaneddin Yaman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sheng Cheng" target="_blank">Sheng Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Tao" target="_blank">Feng Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Abhirup Mallik" target="_blank">Abhirup Mallik</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liu Ren" target="_blank">Liu Ren</a>
            </p>
            <p id="summary-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="summary">Bird's-eye-view (BEV) representations play a crucial role in autonomous driving tasks. Despite recent advancements in BEV generation, inherent noise, stemming from sensor limitations and the learning process, remains largely unaddressed, resulting in suboptimal BEV representations that adversely impact the performance of downstream tasks. To address this, we propose BEVDiffuser, a novel diffusion model that effectively denoises BEV feature maps using the ground-truth object layout as guidance. BEVDiffuser can be operated in a plug-and-play manner during training time to enhance existing BEV models without requiring any architectural modifications. Extensive experiments on the challenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoising and generation capabilities, which enable significant enhancement to existing BEV models, as evidenced by notable improvements of 12.3\% in mAP and 10.1\% in NDS achieved for 3D object detection without introducing additional computational complexity. Moreover, substantial improvements in long-tail object detection and under challenging weather and lighting conditions further validate BEVDiffuser's effectiveness in denoising and enhancing BEV representations.</p>
            <p id="subjects-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" onclick="foldPdfKimi('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="panel paper" keywords="par,idim,pedestrian,prompt,prompts,attribute,bimodal,visual,semantic,linguistic">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition_CVPR_2025_paper.html" target="_blank" title="219/388"><span class="index notranslate">#219</span></a>
                <a id="title-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="title-link" href="/venue/Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" target="_blank">Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition</a>
                <a id="pdf-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junyi Wu" target="_blank">Junyi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Huang" target="_blank">Yan Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Gao" target="_blank">Min Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhen Niu" target="_blank">Yuzhen Niu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhong Chen" target="_blank">Yuzhong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiang Wu" target="_blank">Qiang Wu</a>
            </p>
            <p id="summary-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="summary">Pedestrian attribute recognition (PAR) seeks to predict multiple semantic attributes associated with a specific pedestrian. There are two types of approaches for PAR: unimodal framework and bimodal framework. The former one is to seek a robust visual feature. However, the lack of exploiting semantic feature of linguistic modality is the main concern. The latter one adopts utilizes prompt learning techniques to integrate linguistic data. However, static prompt templates and simple bimodal concatenation cannot to capture the extensive intra-class attribute variability and support active modalities collaboration. In this paper, we propose an Enhanced Visual-Semantic Interaction with Tailored Prompts (EVSITP) framework for PAR. We present an Image-Conditional Dual-Prompt Initialization Module (IDIM) to adaptively generate context-sensitive prompts from visual inputs. Subsequently, a Prompt Enhanced and Regularization Module (PERM) is proposed to strengthen linguistic information from IDIM. We further design a Bimodal Mutual Interaction Module (BMIM) to ensure bidirectional modalities communication. In addition, existing PAR datasets are collected over a short period in limited scenarios, which do not align with real-world scenarios. Therefore, we annotate a long-term person re-identification dataset to create a new PAR dataset, Celeb-PAR. Experiments on several challenging PAR datasets show that our method outperforms state-of-the-art approaches.</p>
            <p id="subjects-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" onclick="foldPdfKimi('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="panel paper" keywords="uwb,umotion,imus,human,wideband,pose,ukf,motion,body,inertial">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units_CVPR_2025_paper.html" target="_blank" title="220/388"><span class="index notranslate">#220</span></a>
                <a id="title-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="title-link" href="/venue/Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" target="_blank">UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units</a>
                <a id="pdf-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Huakun Liu" target="_blank">Huakun Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hiroki Ota" target="_blank">Hiroki Ota</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Wei" target="_blank">Xin Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yutaro Hirao" target="_blank">Yutaro Hirao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Monica Perusquia-Hernandez" target="_blank">Monica Perusquia-Hernandez</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hideaki Uchiyama" target="_blank">Hideaki Uchiyama</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kiyoshi Kiyokawa" target="_blank">Kiyoshi Kiyokawa</a>
            </p>
            <p id="summary-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="summary">Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy. The code will be available for research purposes.</p>
            <p id="subjects-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" onclick="foldPdfKimi('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="panel paper" keywords="salf,bottleneck,cbm,spatially,concept,aware,tell,decisions,interpretable,backbone">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware_CVPR_2025_paper.html" target="_blank" title="221/388"><span class="index notranslate">#221</span></a>
                <a id="title-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="title-link" href="/venue/Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" target="_blank">Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models</a>
                <a id="pdf-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Itay Benou" target="_blank">Itay Benou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tammy Riklin Raviv" target="_blank">Tammy Riklin Raviv</a>
            </p>
            <p id="summary-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="summary">Modern deep neural networks have now reached human-level performance across a variety of tasks. However, unlike humans they lack the ability to explain their decisions by showing where and telling what concepts guided them. In this work, we present a unified framework for transforming any vision neural network into a spatially and conceptually interpretable model. We introduce a spatially-aware concept bottleneck layer that projects black-box features of pre-trained backbone models into interpretable concept maps, without requiring human labels. By training a classification layer over this bottleneck, we obtain a self-explaining model that articulates which concepts most influenced its prediction, along with heatmaps that ground them in the input image. Accordingly, we name this method Spatially-Aware and Label-Free Concept Bottleneck Model (SALF-CBM). Our results show that the proposed SALF-CBM: (1) Outperforms non-spatial CBM methods, as well as the original backbone, on a variety of classification tasks; (2) Produces high-quality spatial explanations, outperforming widely used heatmap-based methods on a zero-shot segmentation task; (3) Facilitates model exploration and debugging, enabling users to query specific image regions and refine the model's decisions by locally editing its concept maps.</p>
            <p id="subjects-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" onclick="foldPdfKimi('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="panel paper" keywords="t2i,generation,mobile,model,quality,snapgen,smaller,taming,geneval,parameters">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient_CVPR_2025_paper.html" target="_blank" title="222/388"><span class="index notranslate">#222</span></a>
                <a id="title-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="title-link" href="/venue/Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" target="_blank">SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training</a>
                <a id="pdf-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jierun Chen" target="_blank">Jierun Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongting Hu" target="_blank">Dongting Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xijie Huang" target="_blank">Xijie Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huseyin Coskun" target="_blank">Huseyin Coskun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Arpit Sahni" target="_blank">Arpit Sahni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Aarush Gupta" target="_blank">Aarush Gupta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anujraaj Goyal" target="_blank">Anujraaj Goyal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dishani Lahiri" target="_blank">Dishani Lahiri</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rajesh Singh" target="_blank">Rajesh Singh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yerlan Idelbayev" target="_blank">Yerlan Idelbayev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junli Cao" target="_blank">Junli Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanyu Li" target="_blank">Yanyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kwang-Ting Cheng" target="_blank">Kwang-Ting Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=S.-H. Gary Chan" target="_blank">S.-H. Gary Chan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingming Gong" target="_blank">Mingming Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sergey Tulyakov" target="_blank">Sergey Tulyakov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anil Kag" target="_blank">Anil Kag</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwu Xu" target="_blank">Yanwu Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Ren" target="_blank">Jian Ren</a>
            </p>
            <p id="summary-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="summary">Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and high-quality images on mobile platforms. We propose several techniques to achieve this goal. First, we systematically examine the design choices of the network architecture to reduce model parameters and latency, while ensuring high-quality generation. Second, to further improve generation quality, we employ cross-architecture knowledge distillation from a much larger model, using a multi-level approach to guide the training of our model from scratch. Third, we enable a few-step generation by integrating adversarial guidance with knowledge distillation. Our model, for the first time, demonstrates the generation of 1024x1024 px images on a mobile device in 1.2 to 2.3 seconds. On ImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for 256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our model with merely 379M parameters surpasses large-scale models with billions of parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14x smaller than IF-XL).</p>
            <p id="subjects-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" onclick="foldPdfKimi('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="panel paper" keywords="editable,tetgs,photorealistic,avatar,splatting,tetrahedron,editing,gaussian,avatars,constrained">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting_CVPR_2025_paper.html" target="_blank" title="223/388"><span class="index notranslate">#223</span></a>
                <a id="title-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="title-link" href="/venue/Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" target="_blank">Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</a>
                <a id="pdf-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hanxi Liu" target="_blank">Hanxi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifang Men" target="_blank">Yifang Men</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhouhui Lian" target="_blank">Zhouhui Lian</a>
            </p>
            <p id="summary-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="summary">Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.</p>
            <p id="subjects-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="panel paper" keywords="style,semantic,scsa,attention,stylized,region,sparse,continuous,arbitrary,transfer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style_CVPR_2025_paper.html" target="_blank" title="224/388"><span class="index notranslate">#224</span></a>
                <a id="title-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="title-link" href="/venue/Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" target="_blank">SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer</a>
                <a id="pdf-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chunnan Shang" target="_blank">Chunnan Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhizhong Wang" target="_blank">Zhizhong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongwei Wang" target="_blank">Hongwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangming Meng" target="_blank">Xiangming Meng</a>
            </p>
            <p id="summary-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="summary">Attention-based arbitrary style transfer methods, including CNN-based, Transformer-based, and Diffusion-based, have flourished and produced high-quality stylized images. However, they perform poorly on the content and style images with the same semantics, i.e., the style of the corresponding semantic region of the generated stylized image is inconsistent with that of the style image. We argue that the root cause lies in their failure to consider the relationship between local regions and semantic regions. To address this issue, we propose a plug-and-play semantic continuous-sparse attention, dubbed SCSA, for arbitrary semantic style transfereach query point considers certain key points in the corresponding semantic region. Specifically, semantic continuous attention ensures each query point fully attends to all the continuous key points in the same semantic region that reflect the overall style characteristics of that region; Semantic sparse attention allows each query point to focus on the most similar sparse key point in the same semantic region that exhibits the specific stylistic texture of that region. By combining the two modules, the resulting SCSA aligns the overall style of the corresponding semantic regions while transferring the vivid textures of these regions. Qualitative and quantitative results prove that SCSA enables attention-based arbitrary style transfer methods to produce high-quality semantic stylized images. The codes of this work will be made publicly available.</p>
            <p id="subjects-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" onclick="foldPdfKimi('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="panel paper" keywords="ucod,camouflaged,pseudo,dpl,decoder,label,labels,objects,apm,dba">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html" target="_blank" title="225/388"><span class="index notranslate">#225</span></a>
                <a id="title-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="title-link" href="/venue/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" target="_blank">UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning</a>
                <a id="pdf-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weiqi Yan" target="_blank">Weiqi Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lvhai Chen" target="_blank">Lvhai Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaijia Kou" target="_blank">Huaijia Kou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengchuan Zhang" target="_blank">Shengchuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Zhang" target="_blank">Yan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liujuan Cao" target="_blank">Liujuan Cao</a>
            </p>
            <p id="summary-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="summary">Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it doesn't need to rely on extensive pixel-level labels. Existing UCOD methods typically generate pseudo-labels using fixed strategies and train <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-60-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-276" style="width: 2.659em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.14em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-277"><span class="mn" id="MathJax-Span-278" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-279" style="font-family: MathJax_Main; padding-left: 0.211em;"></span><span class="mn" id="MathJax-Span-280" style="font-family: MathJax_Main; padding-left: 0.211em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo></mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-60">1 \times 1</script> convolutional layers as a simple decoder, leading to low performance compared to fully-supervised methods. We emphasize two drawbacks in these approaches: 1). The model is prone to fitting incorrect knowledge due to the pseudo-label containing substantial noise. 2). The simple decoder fails to capture and learn the semantic features of camouflaged objects, especially for small-sized objects, due to the low-resolution pseudo-labels and severe confusion between foreground and background pixels. To this end, we propose a UCOD method with a teacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL, which contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial (DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines pseudo-labels generated by fixed strategies and the teacher model to prevent the model from overfitting incorrect knowledge while preserving the ability for self-correction; the DBA decoder takes adversarial learning of different segmentation objectives, guides the model to overcome the foreground-background confusion of camouflaged objects, and the Look-Twice mechanism mimics the human tendency to zoom in on camouflaged objects and performs secondary refinement on small-sized objects. Extensive experiments show that our method demonstrates outstanding performance, even surpassing some existing fully supervised methods. Our code will be released soon.</p>
            <p id="subjects-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" onclick="foldPdfKimi('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="panel paper" keywords="rgbn,gaussians,voxel,images,generation,gaussian,reconstruction,representation,ambiguity,single">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian_CVPR_2025_paper.html" target="_blank" title="226/388"><span class="index notranslate">#226</span></a>
                <a id="title-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="title-link" href="/venue/Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" target="_blank">High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model</a>
                <a id="pdf-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiyang Shen" target="_blank">Yiyang Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhou" target="_blank">Kun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Wang" target="_blank">He Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Yang" target="_blank">Yin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianjia Shao" target="_blank">Tianjia Shao</a>
            </p>
            <p id="summary-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="summary">Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.</p>
            <p id="subjects-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" onclick="foldPdfKimi('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="panel paper" keywords="tracking,evolving,multimodal,mambavlt,vision,language,space,mamba,reference,references">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking_CVPR_2025_paper.html" target="_blank" title="227/388"><span class="index notranslate">#227</span></a>
                <a id="title-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="title-link" href="/venue/Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" target="_blank">MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking</a>
                <a id="pdf-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinqi Liu" target="_blank">Xinqi Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Zhou" target="_blank">Li Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zikun Zhou" target="_blank">Zikun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianqiu Chen" target="_blank">Jianqiu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenyu He" target="_blank">Zhenyu He</a>
            </p>
            <p id="summary-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="summary">The vision-language tracking task aims to perform object tracking based on various modality references. Existing Transformer-based vision-language tracking methods have made remarkable progress by leveraging the global modeling ability of self-attention. However, current approaches still face challenges in effectively exploiting the temporal information and dynamically updating reference features during tracking. Recently, the State Space Model (SSM), known as Mamba, has shown astonishing ability in efficient long-sequence modeling. Particularly, its state space evolving process demonstrates promising capabilities in memorizing multimodal temporal information with linear complexity. Witnessing its success, we propose a Mamba-based vision-language tracking model to exploit its state space evolving ability in temporal space for robust multimodal tracking, dubbed MambaVLT. In particular, our approach mainly integrates a time-evolving hybrid state space block and a selective locality enhancement block, to capture contextual information for multimodal modeling and adaptive reference feature update. Besides, we introduce a modality-selection module that dynamically adjusts the weighting between visual and language references, mitigating potential ambiguities from either reference type. Extensive experimental results show that our method performs favorably against state-of-the-art trackers across diverse benchmarks.</p>
            <p id="subjects-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" onclick="foldPdfKimi('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="panel paper" keywords="unirestore,restoration,tir,pir,image,task,diffusion,prior,perceptual,visual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion_CVPR_2025_paper.html" target="_blank" title="228/388"><span class="index notranslate">#228</span></a>
                <a id="title-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="title-link" href="/venue/Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" target="_blank">UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior</a>
                <a id="pdf-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=I-Hsiang Chen" target="_blank">I-Hsiang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Ting Chen" target="_blank">Wei-Ting Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Wei Liu" target="_blank">Yu-Wei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan-Chun Chiang" target="_blank">Yuan-Chun Chiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sy-Yen Kuo" target="_blank">Sy-Yen Kuo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ming-Hsuan Yang" target="_blank">Ming-Hsuan Yang</a>
            </p>
            <p id="summary-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="summary">Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestores adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios.</p>
            <p id="subjects-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" onclick="foldPdfKimi('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="panel paper" keywords="matcha,matching,anything,across,tasks,correspondence,unified,correspondences,establishing,geometric">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xue_MATCHA_Towards_Matching_Anything_CVPR_2025_paper.html" target="_blank" title="229/388"><span class="index notranslate">#229</span></a>
                <a id="title-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="title-link" href="/venue/Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" target="_blank">MATCHA: Towards Matching Anything</a>
                <a id="pdf-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xue_MATCHA_Towards_Matching_Anything_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Xue" target="_blank">Fei Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sven Elflein" target="_blank">Sven Elflein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laura Leal-Taix" target="_blank">Laura Leal-Taix</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qunjie Zhou" target="_blank">Qunjie Zhou</a>
            </p>
            <p id="summary-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="summary">Establishing correspondences across images is a fundamental challenge in computer vision, underpinning tasks like Structure-from-Motion, image editing, and point tracking. Traditional methods are often specialized for specific correspondence types-geometric, semantic, or temporal  whereas humans naturally identify alignments across these domains. Inspired by this flexibility, we propose MATCHA, a unified feature model designed to rule them all, establishing robust correspondences across diverse matching tasks. Building on insights that diffusion model features can encode multiple correspondence types, MATCHA augments this capacity by dynamically fusing high-level semantic and low-level geometric features through an attention-based module, creating expressive, versatile, and robust features. Additionally, MATCHA integrates object-level features from DINOv2 to further boost generalization, enabling a single feature capable of matching anything. Extensive experiments validate that MATCHA consistently surpasses state-of-the-art methods across geometric, semantic, and temporal tasks, setting a new foundation for a unified approach for the fundamental correspondence problem in computer vision. To the best of our knowledge, MATCHA is the first approach that is able to effectively tackle diverse matching tasks with a single unified feature.</p>
            <p id="subjects-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" onclick="foldPdfKimi('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="panel paper" keywords="event,tracking,tap,etap,motion,contexts,addressing,feature,method,handle">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hamann_ETAP_Event-based_Tracking_of_Any_Point_CVPR_2025_paper.html" target="_blank" title="230/388"><span class="index notranslate">#230</span></a>
                <a id="title-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="title-link" href="/venue/Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" target="_blank">ETAP: Event-based Tracking of Any Point</a>
                <a id="pdf-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hamann_ETAP_Event-based_Tracking_of_Any_Point_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Friedhelm Hamann" target="_blank">Friedhelm Hamann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Gehrig" target="_blank">Daniel Gehrig</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Filbert Febryanto" target="_blank">Filbert Febryanto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kostas Daniilidis" target="_blank">Kostas Daniilidis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guillermo Gallego" target="_blank">Guillermo Gallego</a>
            </p>
            <p id="summary-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="summary">Tracking any point (TAP) recently shifted the motion estimation paradigm from focusing on individual salient points with local templates to tracking arbitrary points with global image contexts. However, while research has mostly focused on driving the accuracy of models in nominal settings, addressing scenarios with difficult lighting conditions and high-speed motions remains out of reach due to the limitations of the sensor. This work addresses this challenge with the first event camera-based TAP method. It leverages the high temporal resolution and high dynamic range of event cameras for robust high-speed tracking, and the global contexts in TAP methods to handle asynchronous and sparse event measurements. We further extend the TAP framework to handle event feature variations induced by motion - thereby addressing an open challenge in purely event-based tracking - with a novel feature alignment loss which ensures the learning of motion-robust features. Our method is trained with data from a new data generation pipeline and systematically ablated across all design decisions. Our method shows strong cross-dataset generalization and performs 135% better on the average Jaccard metric than the baselines. Moreover, on an established feature tracking benchmark, it achieves a 19% improvement over the previous best event-only method and even surpasses the previous best events-and-frames method by 3.7%.</p>
            <p id="subjects-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" onclick="foldPdfKimi('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="panel paper" keywords="stereo,omnidirectional,depth,helvipad,estimation,dataset,decently,world,40k,diverse">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation_CVPR_2025_paper.html" target="_blank" title="231/388"><span class="index notranslate">#231</span></a>
                <a id="title-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="title-link" href="/venue/Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" target="_blank">HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation</a>
                <a id="pdf-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mehdi Zayene" target="_blank">Mehdi Zayene</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jannik Endres" target="_blank">Jannik Endres</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albias Havolli" target="_blank">Albias Havolli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Charles Corbire" target="_blank">Charles Corbire</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Salim Cherkaoui" target="_blank">Salim Cherkaoui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Kontouli" target="_blank">Alexandre Kontouli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandre Alahi" target="_blank">Alexandre Alahi</a>
            </p>
            <p id="summary-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="summary">Despite considerable progress in stereo depth estimation, omnidirectional imaging remains underexplored, mainly due to the lack of appropriate data. We introduce Helvipad, a real-world dataset for omnidirectional stereo depth estimation, consisting of 40K frames from video sequences across diverse environments, including crowded indoor and outdoor scenes with diverse lighting conditions. Collected using two 360 cameras in a top-bottom setup and a LiDAR sensor, the dataset includes accurate depth and disparity labels by projecting 3D point clouds onto equirectangular images. Additionally, we provide an augmented training set with a significantly increased label density by using depth completion. We benchmark leading stereo depth estimation models for both standard and omnidirectional images. Results show that while recent stereo methods perform decently, a significant challenge persists in accurately estimating depth in omnidirectional imaging. To address this, we introduce necessary adaptations to stereo models, achieving improved performance.</p>
            <p id="subjects-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" onclick="foldPdfKimi('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="panel paper" keywords="cameras,shutter,rolling,pose,absolute,projection,camera,smartphone,relative,projecting">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hahn_Order-One_Rolling_Shutter_Cameras_CVPR_2025_paper.html" target="_blank" title="232/388"><span class="index notranslate">#232</span></a>
                <a id="title-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="title-link" href="/venue/Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" target="_blank">Order-One Rolling Shutter Cameras</a>
                <a id="pdf-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hahn_Order-One_Rolling_Shutter_Cameras_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marvin Anas Hahn" target="_blank">Marvin Anas Hahn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kathln Kohn" target="_blank">Kathln Kohn</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Orlando Marigliano" target="_blank">Orlando Marigliano</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tomas Pajdla" target="_blank">Tomas Pajdla</a>
            </p>
            <p id="summary-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="summary">Rolling shutter (RS) cameras dominate consumer and smartphone markets. Several methods for computing the absolute pose of RS cameras have appeared in the last 20 years, but the relative pose problem has not been fully solved yet. We provide a unified theory for the important class of order-one rolling shutter (RS<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-61-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-281" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-282"><span class="msubsup" id="MathJax-Span-283"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-284"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -1.977em; left: 0em;"><span class="mn" id="MathJax-Span-285" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi></mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-61">_1</script>) cameras. These cameras generalize the perspective projection to RS cameras, projecting a generic space point to exactly one image point via a rational map. We introduce a new back-projection RS camera model, characterize RS<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-62-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-286" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-287"><span class="msubsup" id="MathJax-Span-288"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-289"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -1.977em; left: 0em;"><span class="mn" id="MathJax-Span-290" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi></mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-62">_1</script> cameras, construct explicit parameterizations of such cameras, and determine the image of a space line. We classify all minimal problems for solving the relative camera pose problem with linear RS<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-63-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-291" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1000.42em, 2.451em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-292"><span class="msubsup" id="MathJax-Span-293"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-294"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -1.977em; left: 0em;"><span class="mn" id="MathJax-Span-295" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi></mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-63">_1</script> cameras and discover new practical cases. Finally, we show how the theory can be used to explain RS models previously used for absolute pose computation.</p>
            <p id="subjects-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" onclick="foldPdfKimi('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="panel paper" keywords="phd,hallucination,vhe,mllms,ccs,chatgpt,visual,prompted,hitem,images">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset_CVPR_2025_paper.html" target="_blank" title="233/388"><span class="index notranslate">#233</span></a>
                <a id="title-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="title-link" href="/venue/Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" target="_blank">PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset</a>
                <a id="pdf-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiazhen Liu" target="_blank">Jiazhen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Fu" target="_blank">Yuhan Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruobing Xie" target="_blank">Ruobing Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runquan Xie" target="_blank">Runquan Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingwu Sun" target="_blank">Xingwu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fengzong Lian" target="_blank">Fengzong Lian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanhui Kang" target="_blank">Zhanhui Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xirong Li" target="_blank">Xirong Li</a>
            </p>
            <p id="summary-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="summary">Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e., task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with inaccurate context (PhD-iac) or with incorrect context (PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, inaccurate / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs' performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs.</p>
            <p id="subjects-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" onclick="foldPdfKimi('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="panel paper" keywords="turbulence,lpd,distortion,mitigation,mambatm,selective,receptive,degradation,methods,spatial">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video_CVPR_2025_paper.html" target="_blank" title="234/388"><span class="index notranslate">#234</span></a>
                <a id="title-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" target="_blank">Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation</a>
                <a id="pdf-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingguang Zhang" target="_blank">Xingguang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicholas Chimitt" target="_blank">Nicholas Chimitt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xijun Wang" target="_blank">Xijun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Yuan" target="_blank">Yu Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stanley H. Chan" target="_blank">Stanley H. Chan</a>
            </p>
            <p id="summary-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="summary">Atmospheric turbulence is a major source of image degradation in long-range imaging systems. Although numerous deep learning-based turbulence mitigation (TM) methods have been proposed, many are slow, memory-hungry, and do not generalize well. In the spatial domain, methods based on convolutional operators have a limited receptive field, so they cannot handle a large spatial dependency required by turbulence. In the temporal domain, methods relying on self-attention can, in theory, leverage the lucky effects of turbulence, but their quadratic complexity makes it difficult to scale to many frames. Traditional recurrent aggregation methods face parallelization challenges.In this paper, we present a new TM method based on two concepts: (1) A turbulence mitigation network based on the Selective State Space Model (MambaTM). MambaTM provides a global receptive field in each layer across spatial and temporal dimensions while maintaining linear computational complexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state space model. Unlike classical Zernike-based representations of phase distortion, the new LPD map uniquely captures the actual effects of turbulence, significantly improving the models capability to estimate degradation by reducing the ill-posedness. Our proposed method exceeds current state-of-the-art networks on various synthetic and real-world TM benchmarks with significantly faster inference speed. The code will be publicly available.</p>
            <p id="subjects-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="panel paper" keywords="vit,eomt,segmentation,vits,secretly,architectural,decoder,repurposes,image,vision">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html" target="_blank" title="235/388"><span class="index notranslate">#235</span></a>
                <a id="title-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="title-link" href="/venue/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" target="_blank">Your ViT is Secretly an Image Segmentation Model</a>
                <a id="pdf-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tommie Kerssies" target="_blank">Tommie Kerssies</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Niccol Cavagnero" target="_blank">Niccol Cavagnero</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Hermans" target="_blank">Alexander Hermans</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Narges Norouzi" target="_blank">Narges Norouzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giuseppe Averta" target="_blank">Giuseppe Averta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bastian Leibe" target="_blank">Bastian Leibe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gijs Dubbelman" target="_blank">Gijs Dubbelman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daan de Geus" target="_blank">Daan de Geus</a>
            </p>
            <p id="summary-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="summary">Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. Currently, to apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that leverages them to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Leveraging these findings, we introduce the Encoder-only Mask Transformer, which repurposes the plain ViT architecture to conduct image segmentation. Using large models and strong pre-training, EoMT obtains a segmentation performance similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-64-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-296" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-297"><span class="mo" id="MathJax-Span-298" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-64">\times</script> faster using ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation performance and inference speed, suggesting that compute resources are better allocated to scaling the ViT itself rather than adding architectural complexity. Code will be released upon acceptance.</p>
            <p id="subjects-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" onclick="foldPdfKimi('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="panel paper" keywords="heads,grounding,lvlms,visual,text,localization,attention,vision,image,innately">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.html" target="_blank" title="236/388"><span class="index notranslate">#236</span></a>
                <a id="title-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="title-link" href="/venue/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" target="_blank">Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding</a>
                <a id="pdf-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seil Kang" target="_blank">Seil Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinyeong Kim" target="_blank">Jinyeong Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhyeok Kim" target="_blank">Junhyeok Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seong Jae Hwang" target="_blank">Seong Jae Hwang</a>
            </p>
            <p id="summary-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="summary">Visual grounding seeks to localize the image region corresponding to a free-form text description. Recently, the strong multimodal capabilities of Large Vision-Language Models (LVLMs) have driven substantial improvements in visual grounding, though they inevitably require fine-tuning and additional model components to explicitly generate bounding boxes or segmentation masks. However, we discover that a few attention heads in frozen LVLMs demonstrate strong visual grounding capabilities. We refer to these heads, which consistently capture object locations related to text semantics, as localization heads. Using localization heads, we introduce a straightforward and effective training-free visual grounding framework that utilizes text-to-image attention maps from localization heads to identify the target objects. Surprisingly, only three out of thousands of attention heads are sufficient to achieve competitive localization performance compared to existing LVLM-based visual grounding methods that require fine-tuning. Our findings suggest that LVLMs can innately ground objects based on a deep comprehension of the text-image relationship, as they implicitly focus on relevant image regions to generate informative text outputs. All the source codes will be made available to the public.</p>
            <p id="subjects-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" onclick="foldPdfKimi('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="panel paper" keywords="abnormality,textbf,photos,human,visual,humancalibrator,body,abnormalities,abnormal,quite">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and_CVPR_2025_paper.html" target="_blank" title="237/388"><span class="index notranslate">#237</span></a>
                <a id="title-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="title-link" href="/venue/Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" target="_blank">Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body</a>
                <a id="pdf-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zeqing Wang" target="_blank">Zeqing Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingyang Ma" target="_blank">Qingyang Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Wan" target="_blank">Wentao Wan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haojie Li" target="_blank">Haojie Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keze Wang" target="_blank">Keze Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonghong Tian" target="_blank">Yonghong Tian</a>
            </p>
            <p id="summary-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="summary">Recent improvements in visual synthesis have significantly enhanced the depiction of generated human photos, which are pivotal due to their wide applicability and demand. Nonetheless, the existing text-to-image or text-to-video models often generate low-quality human photos that might differ considerably from real-world body structures, referred to as ``abnormal human bodies''. Such abnormalities, typically deemed unacceptable, pose considerable challenges in the detection and repair of them within human photos. These challenges require precise abnormality recognition capabilities, which entail pinpointing both the location and the abnormality type. Intuitively, Visual Language Models (VLMs) that have obtained remarkable performance on various visual tasks are quite suitable for this task. However, their performance on abnormality detection in human photos is quite poor.Hence, it is quite important to highlight this task for the research community. In this paper, we first introduce a simple yet challenging task, i.e., \textbf{F}ine-grained \textbf{H}uman-body \textbf{A}bnormality \textbf{D}etection \textbf{(FHAD)}, and construct two high-quality datasets for evaluation. Then, we propose a meticulous framework, named HumanCalibrator, which identifies and repairs abnormalities in human body structures while preserving the other content. Experiments indicate that our HumanCalibrator achieves high accuracy in abnormality detection and accomplishes an increase in visual comparisons while preserving the other visual content.</p>
            <p id="subjects-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="panel paper" keywords="tissue,layered,wavefront,shaping,layers,transmission,matrices,multi,understand,fit">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Levin_Understanding_Multi-layered_Transmission_Matrices_CVPR_2025_paper.html" target="_blank" title="238/388"><span class="index notranslate">#238</span></a>
                <a id="title-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="title-link" href="/venue/Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" target="_blank">Understanding Multi-layered Transmission Matrices</a>
                <a id="pdf-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Levin_Understanding_Multi-layered_Transmission_Matrices_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Anat Levin" target="_blank">Anat Levin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marina Alterman" target="_blank">Marina Alterman</a>
            </p>
            <p id="summary-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="summary">Transmission matrices, mapping the propagation of light from one end of the tissue to the other, form an important mathematical tool in the analysis of tissue scattering and the design of wavefront shaping systems. To understand the relationship between their content and the volumetric structure of the tissue, we wish to fit them with multi-slice models, composed of a set of planar aberrations spaced throughout the volume. The number of layers used in such a model would largely affect the amount of information compression and the ease in which we can use such layered models in a wavefront-shaping system. This work offers a theoretical study of such multi-layered models. We attempt to understand how many layers are required for a good fit, and how does the approximation degrade when a smaller number of such layers is used. We show analytically that transmission matrices can be well fitted with very sparse layers. This leads to optimistic predictions on our ability to use them to design future wavefront shaping systems which can correct tissue aberration over a wide field-of-view.</p>
            <p id="subjects-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" onclick="foldPdfKimi('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="panel paper" keywords="event,latency,periodic,asynchronous,low,branch,stream,aggregation,accumulation,combining">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for_CVPR_2025_paper.html" target="_blank" title="239/388"><span class="index notranslate">#239</span></a>
                <a id="title-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="title-link" href="/venue/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" target="_blank">Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision</a>
                <a id="pdf-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Manon Dampfhoffer" target="_blank">Manon Dampfhoffer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Mesquida" target="_blank">Thomas Mesquida</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Damien Joubert" target="_blank">Damien Joubert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Dalgaty" target="_blank">Thomas Dalgaty</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pascal Vivet" target="_blank">Pascal Vivet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christoph Posch" target="_blank">Christoph Posch</a>
            </p>
            <p id="summary-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="summary">Event-based cameras asynchronously detect changes in light intensity with high temporal resolution, making them a promising alternative to RGB camera for low-latency and low-power optical flow estimation. However, state-of-the-art convolutional neural network methods create frames from the event stream, therefore losing the opportunity to exploit events for both sparse computations and low-latency prediction. On the other hand, asynchronous event graph methods could leverage both, but at the cost of avoiding any form of time accumulation, which limits the prediction accuracy. In this paper, we propose to break this accuracy-latency trade-off with a novel architecture combining an asynchronous accumulation-free event branch and a periodic aggregation branch. The periodic branch performs feature aggregations on the event graphs of past data to extract global context information, which improves accuracy without introducing any latency.The solution could predict optical flow per event with a latency of tens of microseconds on asynchronous hardware, which represents a gain of three orders of magnitude with respect to state-of-the-art frame-based methods, with 48x less operations per second. We show that the solution can detect rapid motion changes faster than a periodic output. This work proposes, for the first time, an effective solution for ultra low-latency and low-power optical flow prediction from event cameras.</p>
            <p id="subjects-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" onclick="foldPdfKimi('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="panel paper" keywords="emotion,dubbing,emodubber,pronunciation,lip,movie,fuec,sync,prosody,acoustics">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing__CVPR_2025_paper.html" target="_blank" title="240/388"><span class="index notranslate">#240</span></a>
                <a id="title-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="title-link" href="/venue/Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" target="_blank">EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing</a>
                <a id="pdf-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing__CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gaoxiang Cong" target="_blank">Gaoxiang Cong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiadong Pan" target="_blank">Jiadong Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Li" target="_blank">Liang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuankai Qi" target="_blank">Yuankai Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxin Peng" target="_blank">Yuxin Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anton van den Hengel" target="_blank">Anton van den Hengel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingming Huang" target="_blank">Qingming Huang</a>
            </p>
            <p id="summary-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="summary">Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pronunciation; (2) They lack the capacity to express user-defined emotions. To address these problems, we propose EmoDubber, an emotion-controllable dubbing architecture that allows users to specify emotion type and emotional intensity while satisfying high-quality lip sync and pronunciation. Specifically, we first design Lip-related Prosody Aligning (LPA), which focuses on learning the inherent consistency between lip motion and prosody variation by duration level contrastive learning to incorporate reasonable alignment. Then, we design Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences by efficient conformer to improve speech intelligibility. Next, the speaker identity adapting module aims to decode acoustics prior and inject the speaker style embedding. After that, the proposed Flow-based User Emotion Controlling (FUEC) is used to synthesize waveform by flow matching prediction network conditioned on acoustics prior. In this process, the FUEC determines the gradient direction and guidance scale based on the user's emotion instructions by the positive and negative guidance mechanism, which focuses on amplifying the desired emotion while suppressing others. Extensive experimental results on three benchmark datasets demonstrate favorable performance compared to several state-of-the-art methods.</p>
            <p id="subjects-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" onclick="foldPdfKimi('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="panel paper" keywords="egocentric,pressure,hand,contact,touch,egopressure,dataset,poses,interaction,pose">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in_CVPR_2025_paper.html" target="_blank" title="241/388"><span class="index notranslate">#241</span></a>
                <a id="title-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="title-link" href="/venue/Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" target="_blank">EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision</a>
                <a id="pdf-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiming Zhao" target="_blank">Yiming Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taein Kwon" target="_blank">Taein Kwon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Streli" target="_blank">Paul Streli</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Holz" target="_blank">Christian Holz</a>
            </p>
            <p id="summary-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="summary">Estimating touch contact and pressure in egocentric vision is a central task for downstream applications in Augmented Reality, Virtual Reality, as well as many robotic applications, because it provides precise physical insights into hand-object interaction and object manipulation. However, existing contact pressure datasets lack egocentric views and hand poses, which are essential for accurate estimation during in-situ operation, both for AR/VR interaction and robotic manipulation.In this paper, we introduce a novel dataset of touch contact and pressure interaction from an egocentric perspective, complemented with hand pose meshes and fine-grained pressure intensities for each contact. The hand poses in our dataset are optimized using our proposed multi-view sequence-based method that processes footage from our capture rig of 8 accurately calibrated RGBD cameras. comprises 5.0 hours of touch contact and pressure interaction from 21 participants captured by a moving egocentric camera and 7 stationary Kinect cameras, which provided RGB images and depth maps at 30 Hz. In addition, we provide baselines for estimating pressure with different modalities, which will enable future developments and benchmarking on the dataset. Overall, we demonstrate that pressure and hand poses are complementary, which supports our intention to better facilitate the physical understanding of hand-object interactions in AR/VR and robotics research</p>
            <p id="subjects-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" onclick="foldPdfKimi('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="panel paper" keywords="correspondence,pseudolabels,supervised,self,view,cycle,consistency,prior,egoexo4d,siammae">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency_CVPR_2025_paper.html" target="_blank" title="242/388"><span class="index notranslate">#242</span></a>
                <a id="title-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="title-link" href="/venue/Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" target="_blank">Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency</a>
                <a id="pdf-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Baade" target="_blank">Alan Baade</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changan Chen" target="_blank">Changan Chen</a>
            </p>
            <p id="summary-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="summary">Learning self-supervised visual correspondence is a long-studied task fundamental to visual understanding and human perception. However, existing correspondence methods largely focus on small image transformations, such as object tracking in high-framerate videos or learning pixel-to-pixel mappings between images with high view overlap. This severely limits their application in dynamic multi-view settings such as robot imitation learning or augmented reality. In this work, we introduce Predictive Cycle Consistency for learning object correspondence between extremely disjoint views of a scene without paired segmentation data. Our technique bootstraps object correspondence pseudolabels from raw image segmentations using conditional grayscale colorization and a cycle-consistency refinement prior. We then train deep ViTs on these pseudolabels, which we use to generate higher-quality pseudolabels and iteratively train better correspondence models. We demonstrate the performance of our method under both extreme in-the-wild camera view changes and across large temporal gaps in video. Our approach beats all prior supervised and prior SoTA self-supervised correspondence models on the EgoExo4D correspondence benchmark (+6.7 IoU Exo Query) and the prior SoTA self-supervised methods SiamMAE and DINO V1&amp;V2 on the DAVIS-2017 and LVOS datasets across large frame gaps.</p>
            <p id="subjects-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" onclick="foldPdfKimi('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="panel paper" keywords="distinctad,movie,distinctive,ads,narratives,clips,audio,redundancy,contextual,generation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts_CVPR_2025_paper.html" target="_blank" title="243/388"><span class="index notranslate">#243</span></a>
                <a id="title-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="title-link" href="/venue/Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" target="_blank">DistinctAD: Distinctive Audio Description Generation in Contexts</a>
                <a id="pdf-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Fang" target="_blank">Bo Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhao Wu" target="_blank">Wenhao Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiangqiang Wu" target="_blank">Qiangqiang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxin Song" target="_blank">Yuxin Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antoni B. Chan" target="_blank">Antoni B. Chan</a>
            </p>
            <p id="summary-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="summary">Audio Descriptions (ADs) aim to provide a narration of a movie in text form, describing non-dialogue-related narratives, such as characters, actions, or scene establishment. Automatic generation of ADs remains challenging due to: i) the domain gap between movie-AD data and existing data used to train vision-language models, and ii) the issue of contextual redundancy arising from highly similar neighboring visual clips in a long movie. In this work, we propose **DistinctAD**, a novel two-stage framework for generating ADs that emphasize distinctiveness to produce better narratives. To address the domain gap, we introduce a CLIP-AD adaptation strategy that does not require additional AD corpora, enabling more effective alignment between movie and AD modalities at both global and fine-grained levels. In Stage-II, DistinctAD incorporates two key innovations: (i) a Contextual Expectation-Maximization Attention (EMA) module that reduces redundancy by extracting common bases from consecutive video clips, and (ii) an explicit distinctive word prediction loss that filters out repeated words in the context, ensuring the prediction of unique terms specific to the current AD. Comprehensive evaluations on MAD-Eval, CMD-AD, and TV-AD benchmarks demonstrate the superiority of DistinctAD, with the model consistently outperforming baselines, particularly in Recall@k/N, highlighting its effectiveness in producing high-quality, distinctive ADs.</p>
            <p id="subjects-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" onclick="foldPdfKimi('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="panel paper" keywords="anysat,geoplex,resolutions,modalities,earth,multimodal,datasets,jepa,observation,scales">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and_CVPR_2025_paper.html" target="_blank" title="244/388"><span class="index notranslate">#244</span></a>
                <a id="title-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="title-link" href="/venue/Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" target="_blank">AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</a>
                <a id="pdf-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guillaume Astruc" target="_blank">Guillaume Astruc</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicolas Gonthier" target="_blank">Nicolas Gonthier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clment Mallet" target="_blank">Clment Mallet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Loic Landrieu" target="_blank">Loic Landrieu</a>
            </p>
            <p id="summary-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="summary">Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-65-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-299" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-300"><span class="mn" id="MathJax-Span-301" style="font-family: MathJax_Main;">5</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn></math></span></span><script type="math/tex" id="MathJax-Element-65">5</script> multimodal datasets with varying characteristics and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-66-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-302" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-303"><span class="mn" id="MathJax-Span-304" style="font-family: MathJax_Main;">11</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>11</mn></math></span></span><script type="math/tex" id="MathJax-Element-66">11</script> distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned, we achieve better or near state-of-the-art results on the datasets of GeoPlex and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-67-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-305" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-306"><span class="mn" id="MathJax-Span-307" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-67">3</script> additional ones for <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-68-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-308" style="width: 0.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-309"><span class="mn" id="MathJax-Span-310" style="font-family: MathJax_Main;">4</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4</mn></math></span></span><script type="math/tex" id="MathJax-Element-68">4</script> environment monitoring tasks: land cover mapping, crop type classification, change detection, and forest analysis. We will release all codes, models, and data.</p>
            <p id="subjects-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" onclick="foldPdfKimi('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="panel paper" keywords="end,reasoning,navigation,agent,agents,visual,trained,realistic,numepisodes,learned">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical_CVPR_2025_paper.html" target="_blank" title="245/388"><span class="index notranslate">#245</span></a>
                <a id="title-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="title-link" href="/venue/Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" target="_blank">Reasoning in Visual Navigation of End-to-end Trained Agents: A Dynamical Systems Approach</a>
                <a id="pdf-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Steeven Janny" target="_blank">Steeven Janny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Herv Poirier" target="_blank">Herv Poirier</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonid Antsfeld" target="_blank">Leonid Antsfeld</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guillaume Bono" target="_blank">Guillaume Bono</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gianluca Monaci" target="_blank">Gianluca Monaci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boris Chidlovskii" target="_blank">Boris Chidlovskii</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francesco Giuliari" target="_blank">Francesco Giuliari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessio Del Bue" target="_blank">Alessio Del Bue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Wolf" target="_blank">Christian Wolf</a>
            </p>
            <p id="summary-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="summary">Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but evaluations and benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at https://visual-navigation-reasoning.github.io</p>
            <p id="subjects-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" onclick="foldPdfKimi('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="panel paper" keywords="hand,hamr,dyn,camera,monocular,motion,dynamic,interacting,videos,cameras">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera_CVPR_2025_paper.html" target="_blank" title="246/388"><span class="index notranslate">#246</span></a>
                <a id="title-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="title-link" href="/venue/Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" target="_blank">Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera</a>
                <a id="pdf-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengdi Yu" target="_blank">Zhengdi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stefanos Zafeiriou" target="_blank">Stefanos Zafeiriou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tolga Birdal" target="_blank">Tolga Birdal</a>
            </p>
            <p id="summary-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="summary">We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our \name~consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods.</p>
            <p id="subjects-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" onclick="foldPdfKimi('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="panel paper" keywords="degraded,lpir,diff,license,restoration,plate,world,severely,images,real">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate_CVPR_2025_paper.html" target="_blank" title="247/388"><span class="index notranslate">#247</span></a>
                <a id="title-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="title-link" href="/venue/Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" target="_blank">LP-Diff: Towards Improved Restoration of Real-World Degraded License Plate</a>
                <a id="pdf-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyan Gong" target="_blank">Haoyan Gong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenrong Zhang" target="_blank">Zhenrong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzheng Feng" target="_blank">Yuzheng Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anh Nguyen" target="_blank">Anh Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongbin Liu" target="_blank">Hongbin Liu</a>
            </p>
            <p id="summary-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="summary">License plate (LP) recognition is crucial in intelligent traffic management systems. However, factors such as long distances and poor camera quality often lead to severe degradation of captured LP images, posing challenges to accurate recognition. The design of License Plate Image Restoration (LPIR) methods frequently relies on synthetic degraded data, which limits their effectiveness on real-world severely degraded LP images. To address this issue, we introduce the first paired LPIR dataset collected in real-world scenarios, named MDLP, including 10,245 pairs of multi-frame severely degraded LP images and their corresponding clear images. To better restore severely degraded LP, we propose a novel Diffusion-based network, called LP-Diff, to tackle real-world LPIR tasks. Our approach incorporates (1) an Inter-frame Cross Attention Module to fuse temporal information across multiple frames, (2) a Texture Enhancement Module to restore texture information in degraded images, and (3) a Dual-Pathway Fusion Module to select effective features from both channel and spatial dimensions. Extensive experiments demonstrate the reliability of our dataset for model training and evaluation. Our proposed LP-Diff consistently outperforms other state-of-the-art image restoration methods on real-world LPIR tasks. Our dataset and code will be released after the paper is accepted to facilitate reproducibility and future research.</p>
            <p id="subjects-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" onclick="foldPdfKimi('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="panel paper" keywords="mar,auto,regressive,cascaded,generation,progressive,masked,unordered,latent,resolution">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation_CVPR_2025_paper.html" target="_blank" title="248/388"><span class="index notranslate">#248</span></a>
                <a id="title-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="title-link" href="/venue/Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" target="_blank">MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation</a>
                <a id="pdf-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinnan Chen" target="_blank">Jinnan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingting Zhu" target="_blank">Lingting Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyu Hu" target="_blank">Zeyu Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengju Qian" target="_blank">Shengju Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yugang Chen" target="_blank">Yugang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Wang" target="_blank">Xin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gim Hee Lee" target="_blank">Gim Hee Lee</a>
            </p>
            <p id="summary-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="summary">Recent advances in auto-regressive transformers have revolutionized generative modeling across domains, from language processing to visual generation, demonstrating remarkable capabilities. However, applying these advances to 3D generation presents three key challenges: the unordered nature of 3D data conflicts with sequential prediction paradigms, conventional vector quantization approaches incur substantial compression loss when applied to 3D meshes, and the lack of efficient scaling strategies for higher resolution. To address these limitations, we introduce MAR-3D, which integrates a pyramid variational autoencoder with a cascaded masked auto-regressive transformer (Cascaded MAR) for progressive latent token denoising. Our architecture employs random masking during training and auto-regressive denoising in random order during inference, naturally accommodating the unordered property of 3D latent tokens. Additionally, we propose a cascaded training strategy with condition augmentation that enables efficient up-scaling the latent token resolution. Extensive experiments demonstrate that MAR-3D not only achieves superior performance and generalization capabilities compared to existing methods but also exhibits enhanced scaling properties over joint distribution modeling approaches like diffusion transformers in 3D generation.</p>
            <p id="subjects-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="panel paper" keywords="voxel,grounding,pruning,tgp,text,cba,visual,completion,sparse,architecture">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding_CVPR_2025_paper.html" target="_blank" title="249/388"><span class="index notranslate">#249</span></a>
                <a id="title-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="title-link" href="/venue/Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" target="_blank">Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding</a>
                <a id="pdf-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenxuan Guo" target="_blank">Wenxuan Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiuwei Xu" target="_blank">Xiuwei Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Wang" target="_blank">Ziwei Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianjiang Feng" target="_blank">Jianjiang Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Zhou" target="_blank">Jie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiwen Lu" target="_blank">Jiwen Lu</a>
            </p>
            <p id="summary-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="summary">In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-69-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1.13&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-311" style="width: 3.076em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.5em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-312"><span class="mo" id="MathJax-Span-313" style="font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-314" style="font-family: MathJax_Main;">1.13</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>+</mo><mn>1.13</mn></math></span></span><script type="math/tex" id="MathJax-Element-69">+1.13</script> lead of Acc@0.5 on ScanRefer, and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-70-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;2.6&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-315" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.03em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-316"><span class="mo" id="MathJax-Span-317" style="font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-318" style="font-family: MathJax_Main;">2.6</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>+</mo><mn>2.6</mn></math></span></span><script type="math/tex" id="MathJax-Element-70">+2.6</script> and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-71-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;3.2&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-319" style="width: 2.503em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.03em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-320"><span class="mo" id="MathJax-Span-321" style="font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-322" style="font-family: MathJax_Main;">3.2</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>+</mo><mn>3.2</mn></math></span></span><script type="math/tex" id="MathJax-Element-71">+3.2</script> leads on NR3D and SR3D respectively. The Code will be released soon.</p>
            <p id="subjects-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" onclick="foldPdfKimi('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="panel paper" keywords="editing,hatie,aligned,human,benchmark,guided,evaluation,text,image,aspects">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing_CVPR_2025_paper.html" target="_blank" title="250/388"><span class="index notranslate">#250</span></a>
                <a id="title-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="title-link" href="/venue/Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" target="_blank">Towards Scalable Human-aligned Benchmark for Text-guided Image Editing</a>
                <a id="pdf-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Suho Ryu" target="_blank">Suho Ryu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kihyun Kim" target="_blank">Kihyun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eugene Baek" target="_blank">Eugene Baek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dongsoo Shin" target="_blank">Dongsoo Shin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joonseok Lee" target="_blank">Joonseok Lee</a>
            </p>
            <p id="summary-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="summary">A variety of text-guided image editing models have been proposed recently. However, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. To address this, we introduce a novel Human-Aligned benchmark for Text-guided Image Editing (HATIE). Providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. Also, HATIE provides a fully-automated and omnidirectional evaluation pipeline. Particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. We empirically verify that the evaluation of HATIE is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.</p>
            <p id="subjects-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" onclick="foldPdfKimi('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="panel paper" keywords="densification,gaussians,densify,reconstruction,feed,generalizable,forward,generative,fidelity,strategy">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D_CVPR_2025_paper.html" target="_blank" title="251/388"><span class="index notranslate">#251</span></a>
                <a id="title-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="title-link" href="/venue/Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" target="_blank">Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction</a>
                <a id="pdf-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seungtae Nam" target="_blank">Seungtae Nam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyu Sun" target="_blank">Xiangyu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gyeongjin Kang" target="_blank">Gyeongjin Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Younggeun Lee" target="_blank">Younggeun Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungjun Oh" target="_blank">Seungjun Oh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eunbyung Park" target="_blank">Eunbyung Park</a>
            </p>
            <p id="summary-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="summary">Generalized feed-forward Gaussian models have shown remarkable progress in sparse-view 3D reconstruction, leveraging prior knowledge learned from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of generated Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be extended and applied to the feed-forward models, it may not be ideally suited for generalized settings. In this paper, we present Generative Densification, an efficient and generalizable densification strategy that can selectively generate fine Gaussians for high-fidelity 3D reconstruction. Unlike the 3D-GS densification strategy, we densify the feature representations from the feed-forward models rather than the raw Gaussians, making use of the prior knowledge embedded in the features for enhanced generalization. Experimental results demonstrate the effectiveness of our approach, achieving the state-of-the-art rendering quality in both object-level and scene-level reconstruction, with noticeable improvements in representing fine details.</p>
            <p id="subjects-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" onclick="foldPdfKimi('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="panel paper" keywords="skeleton,protogcn,ntu,details,actions,action,prototypes,similar,motion,rgb">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective_CVPR_2025_paper.html" target="_blank" title="252/388"><span class="index notranslate">#252</span></a>
                <a id="title-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="title-link" href="/venue/Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" target="_blank">Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition</a>
                <a id="pdf-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongda Liu" target="_blank">Hongda Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunfan Liu" target="_blank">Yunfan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Ren" target="_blank">Min Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Wang" target="_blank">Hao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunlong Wang" target="_blank">Yunlong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhenan Sun" target="_blank">Zhenan Sun</a>
            </p>
            <p id="summary-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="summary">In skeleton-based action recognition, a key challenge is distinguishing between actions with similar trajectories of joints due to the lack of image-level details in skeletal representations. Recognizing that the differentiation of similar actions relies on subtle motion details in specific body parts, we direct our approach to focus on the fine-grained motion of local skeleton components. To this end, we introduce ProtoGCN, a Graph Convolutional Network (GCN)-based model that breaks down the dynamics of entire skeleton sequences into a combination of learnable prototypes representing core motion patterns of action units. By contrasting the reconstruction of prototypes, ProtoGCN can effectively identify and enhance the discriminative representation of similar actions. Without bells and whistles, ProtoGCN achieves state-of-the-art performance on multiple benchmark datasets, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, and FineGYM, which demonstrates the effectiveness of the proposed method. The source code is enclosed in the supplementary material and will be released upon acceptance.</p>
            <p id="subjects-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="panel paper" keywords="reid,annotators,styles,mllms,human,person,ham,textual,descriptions,captions">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification_CVPR_2025_paper.html" target="_blank" title="253/388"><span class="index notranslate">#253</span></a>
                <a id="title-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="title-link" href="/venue/Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" target="_blank">Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification</a>
                <a id="pdf-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayu Jiang" target="_blank">Jiayu Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Changxing Ding" target="_blank">Changxing Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wentao Tan" target="_blank">Wentao Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhong Wang" target="_blank">Junhong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Tao" target="_blank">Jin Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangmin Xu" target="_blank">Xiangmin Xu</a>
            </p>
            <p id="summary-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="summary">Text-to-image person re-identification (ReID) aims to retrieve the images of an interested person based on textual descriptions. One main challenge for this task is the high cost in manually annotating large-scale databases, which affects the generalization ability of ReID models. Recent works handle this problem by leveraging Multi-modal Large Language Models (MLLMs) to describe pedestrian images automatically. However, the captions produced by MLLMs lack diversity in description styles. To address this issue, we propose a Human Annotator Modeling (HAM) approach to enable MLLMs to mimic the description styles of thousands of human annotators. Specifically, we first extract style features from human textual descriptions and perform clustering on them. This allows us to group textual descriptions with similar styles into the same cluster. Then, we employ a prompt to represent each of these clusters and apply prompt learning to mimic the description styles of different human annotators. Furthermore, we define a style feature space and perform uniform sampling in this space to obtain more diverse clustering prototypes, which further enriches the diversity of the MLLM-generated captions. Finally, we adopt HAM to automatically annotate a massive-scale database for text-to-image ReID. Extensive experiments on this database demonstrate that it significantly improves the generalization ability of ReID models. Code of this paper will be released.</p>
            <p id="subjects-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" onclick="foldPdfKimi('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="panel paper" keywords="videoqg,grounding,cra,modal,video,cross,question,spurious,intervention,causal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding_CVPR_2025_paper.html" target="_blank" title="254/388"><span class="index notranslate">#254</span></a>
                <a id="title-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="title-link" href="/venue/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" target="_blank">Cross-modal Causal Relation Alignment for Video Question Grounding</a>
                <a id="pdf-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weixing Chen" target="_blank">Weixing Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Liu" target="_blank">Yang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Binglin Chen" target="_blank">Binglin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiandong Su" target="_blank">Jiandong Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongsen Zheng" target="_blank">Yongsen Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Lin" target="_blank">Liang Lin</a>
            </p>
            <p id="summary-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="summary">Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, although large models possess extensive prior knowledge and can demonstrate strong performance in a zero-shot setting, issues such as spurious correlations persist, making their application to specific downstream tasks challenging. In this work, we propose a novel causality-ware VideoQG framework named Cross-modal Causality Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Attention Grounding (GSAG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter. ii) Cross-modal Alignment (CA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features. iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes will be available.</p>
            <p id="subjects-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="panel paper" keywords="inference,diffusion,nfe,scaling,models,computation,conditioned,time,compute,llms">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models_CVPR_2025_paper.html" target="_blank" title="255/388"><span class="index notranslate">#255</span></a>
                <a id="title-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="title-link" href="/venue/Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" target="_blank">Scaling Inference Time Compute for Diffusion Models</a>
                <a id="pdf-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Nanye Ma" target="_blank">Nanye Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangyuan Tong" target="_blank">Shangyuan Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haolin Jia" target="_blank">Haolin Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hexiang Hu" target="_blank">Hexiang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Chuan Su" target="_blank">Yu-Chuan Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingda Zhang" target="_blank">Mingda Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuan Yang" target="_blank">Xuan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yandong Li" target="_blank">Yandong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tommi Jaakkola" target="_blank">Tommi Jaakkola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuhui Jia" target="_blank">Xuhui Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Saining Xie" target="_blank">Saining Xie</a>
            </p>
            <p id="summary-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="summary">Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in large language models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of function evaluations (NFE), although the performance gains typically flatten after a few dozen steps. In this work, we present a framework on the inference-time scaling for diffusion models, that enables diffusion models to further benefit from the increased computation beyond the NFE plateau. Specifically, we consider a search problem aimed at identifying better noises during the sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.</p>
            <p id="subjects-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" onclick="foldPdfKimi('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="panel paper" keywords="evenhancer,video,stvsr,generalizability,spatiotemporal,videos,event,continuous,resolution,super">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video_CVPR_2025_paper.html" target="_blank" title="256/388"><span class="index notranslate">#256</span></a>
                <a id="title-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="title-link" href="/venue/Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" target="_blank">EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events</a>
                <a id="pdf-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shuoyan Wei" target="_blank">Shuoyan Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Li" target="_blank">Feng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengeng Tang" target="_blank">Shengeng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Zhao" target="_blank">Yao Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huihui Bai" target="_blank">Huihui Bai</a>
            </p>
            <p id="summary-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="summary">Continuous space-time video super-resolution (C-STVSR) endeavors to upscale videos simultaneously at arbitrary spatial and temporal scales, which has recently garnered increasing interest. However, prevailing methods struggle to yield satisfactory videos at out-of-distribution spatial and temporal scales. On the other hand, event streams characterized by high temporal resolution and high dynamic range, exhibit compelling promise in vision tasks. This paper presents EvEnhancer, an innovative approach that marries the unique advantages of event streams to elevate effectiveness, efficiency, and generalizability for C-STVSR. Our approach hinges on two pivotal components: 1) Event-adapted synthesis capitalizes on the spatiotemporal correlations between frames and events to discern and learn long-term motion trajectories, enabling the adaptive interpolation and fusion of informative spatiotemporal features; 2) Local implicit video transformer integrates local implicit video neural function with cross-scale spatiotemporal attention to learn continuous video representations utilized to generate plausible videos at arbitrary resolutions and frame rates. Experiments show that EvEnhancer achieves superiority on synthetic and real-world datasets and preferable generalizability on out-of-distribution scales against state-of-the-art methods.</p>
            <p id="subjects-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" onclick="foldPdfKimi('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="panel paper" keywords="prompt,nlprompt,mae,noisy,robustness,vision,promptmae,promptot,learning,noise">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models_CVPR_2025_paper.html" target="_blank" title="257/388"><span class="index notranslate">#257</span></a>
                <a id="title-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" target="_blank">NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</a>
                <a id="pdf-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF">6</sup>]</a>
                <a id="rel-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bikang Pan" target="_blank">Bikang Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qun Li" target="_blank">Qun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoying Tang" target="_blank">Xiaoying Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Huang" target="_blank">Wei Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Fang" target="_blank">Zhen Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Liu" target="_blank">Feng Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingya Wang" target="_blank">Jingya Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyi Yu" target="_blank">Jingyi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Shi" target="_blank">Ye Shi</a>
            </p>
            <p id="summary-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="summary">The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text encoder representations in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representation and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements.</p>
            <p id="subjects-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="panel paper" keywords="egocentric,hand,hawor,camera,motion,reconstruction,world,hands,videos,trajectory">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos_CVPR_2025_paper.html" target="_blank" title="258/388"><span class="index notranslate">#258</span></a>
                <a id="title-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="title-link" href="/venue/Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" target="_blank">HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos</a>
                <a id="pdf-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jinglei Zhang" target="_blank">Jinglei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiankang Deng" target="_blank">Jiankang Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chao Ma" target="_blank">Chao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rolandos Alexandros Potamias" target="_blank">Rolandos Alexandros Potamias</a>
            </p>
            <p id="summary-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="summary">Despite the advent in 3D hand pose estimation, current methods predominantly focus on single-image 3D hand reconstruction in the camera frame, overlooking the world-space motion of the hands. Such limitation prohibits their direct use in egocentric video settings, where hands and camera are continuously in motion. In this work, we propose HaWoR, a high-fidelity method for hand motion reconstruction in world coordinates from egocentric videos. We propose to decouple the task by reconstructing the hand motion in the camera space and estimating the camera trajectory in the world coordinate system. To achieve precise camera trajectory estimation, we propose an adaptive egocentric SLAM framework that addresses the shortcomings of traditional SLAM methods, providing robust performance under challenging camera dynamics. To ensure robust hand motion trajectories, even when the hands move out of view frustum, we devise a novel motion infiller network that effectively completes the missing frames of the sequence. Through extensive quantitative and qualitative evaluations, we demonstrate that HaWoR achieves state-of-the-art performance on both hand motion reconstruction and world-frame camera trajectory estimation under different egocentric benchmark datasets. Code and models will be made publicly available for research purposes.</p>
            <p id="subjects-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="panel paper" keywords="distance,eikonal,hotspot,function,signed,loss,optimization,implicit,oversmoothing,remedies">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition_CVPR_2025_paper.html" target="_blank" title="259/388"><span class="index notranslate">#259</span></a>
                <a id="title-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="title-link" href="/venue/Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" target="_blank">HotSpot: Signed Distance Function Optimization with an Asymptotically Sufficient Condition</a>
                <a id="pdf-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zimo Wang" target="_blank">Zimo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Wang" target="_blank">Cheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiki Yoshino" target="_blank">Taiki Yoshino</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sirui Tao" target="_blank">Sirui Tao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyang Fu" target="_blank">Ziyang Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tzu-Mao Li" target="_blank">Tzu-Mao Li</a>
            </p>
            <p id="summary-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="summary">We propose a method, HotSpot, for optimizing neural signed distance functions, based on a relation between the solution of a screened Poisson equation and the distance function.Existing losses such as the eikonal loss cannot guarantee the recovered implicit function to be a distance function, even when the implicit function satisfies the eikonal equation almost everywhere.Furthermore, the eikonal loss suffers from stability issues in optimization and the remedies that introduce area or divergence minimization can lead to oversmoothing.We address these challenges by designing a loss function that when minimized can converge to the true distance function, is stable, and naturally penalize large surface area.We provide theoretical analysis and experiments on both challenging 2D and 3D datasets and show that our method provide better surface reconstruction and more accurate distance approximation.</p>
            <p id="subjects-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" onclick="foldPdfKimi('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="panel paper" keywords="bwformer,corners,lidar,airborne,wireframe,height,building,reconstruction,cloud,transformer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with_CVPR_2025_paper.html" target="_blank" title="260/388"><span class="index notranslate">#260</span></a>
                <a id="title-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="title-link" href="/venue/Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" target="_blank">BWFormer: Building Wireframe Reconstruction from Airborne LiDAR Point Cloud with Transformer</a>
                <a id="pdf-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuzhou Liu" target="_blank">Yuzhou Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lingjie Zhu" target="_blank">Lingjie Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanqiao Ye" target="_blank">Hanqiao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shangfeng Huang" target="_blank">Shangfeng Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Gao" target="_blank">Xiang Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianwei Zheng" target="_blank">Xianwei Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhan Shen" target="_blank">Shuhan Shen</a>
            </p>
            <p id="summary-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="summary">In this paper, we present BWFormer, a novel Transformer-based model for building wireframe reconstruction from airborne LiDAR point cloud. The problem is solved in a ground-up manner by detecting the building corners in 2D, lifting and connecting them in 3D space afterwards with additional data augmentation.Due to the 2.5D characteristic of the airborne LiDAR point cloud, we simplify the problem by projecting the points on the ground plane to produce a 2D height map. With the height map, a heat map is first predicted with pixel-wise corner likelihood to predict the possible 2D corners.Then, 3D corners are predicted by a Transformer-based network with extra height embedding initialization.This 2D-to-3D corner detection strategy reduces the search space significantly.To recover the topological connections among the corners, edges are finally predicted from geometrical and visual cues in the height map with the proposed edge attention mechanism, which extracts holistic features and preserves local details simultaneously.In addition, due to the limited datasets in the field and the irregularity of the point clouds, a conditional latent diffusion model for LiDAR scanning simulation is utilized for data augmentation.BWFormer surpasses other state-of-the-art methods, especially in reconstruction completeness. We commit to release all our codes and pre-trained models.</p>
            <p id="subjects-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" onclick="foldPdfKimi('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="panel paper" keywords="ccin,neutralization,compositional,conflict,conflicts,conflicting,cir,neutralizes,sleeve,instruction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval_CVPR_2025_paper.html" target="_blank" title="261/388"><span class="index notranslate">#261</span></a>
                <a id="title-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="title-link" href="/venue/Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" target="_blank">CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval</a>
                <a id="pdf-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Likai Tian" target="_blank">Likai Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Zhao" target="_blank">Jian Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zechao Hu" target="_blank">Zechao Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengwei Yang" target="_blank">Zhengwei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Li" target="_blank">Hao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Jin" target="_blank">Lei Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Wang" target="_blank">Zheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuelong Li" target="_blank">Xuelong Li</a>
            </p>
            <p id="summary-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="summary">Composed Image Retrieval (CIR) is a multi-modal task that seeks to retrieve target images by harmonizing a reference image with a modified instruction. The main challenge in CIR lies in compositional conflicts between the reference image (e.g., blue, long sleeve) and the modified instruction (e.g., grey, short sleeve). Previous works attempt to mitigate such conflicts through feature-level manipulation, commonly employing learnable masks to obscure conflicting features within the reference image. However, the inherent complexity of feature spaces poses significant challenges in precise conflict neutralization, thereby leading to uncontrollable results. To this end, this paper proposes the Compositional Conflict Identification and Neutralization (CCIN) framework, which sequentially identifies and neutralizes compositional conflicts for effective CIR. Specifically, CCIN comprises two core modules: 1) Compositional Conflict Identification module, which utilizes LLM-based analysis to identify specific conflicting attributes, and 2) Compositional Conflict Neutralization module, which first generates a kept instruction to preserve non-conflicting attributes, then neutralizes conflicts under collaborative guidance of both the kept and modified instructions. Furthermore, an orthogonal parameter regularization loss is introduced to emphasize the distinction between target and conflicting features. Extensive experiments demonstrate the superiority of CCIN over the state-of-the-arts.</p>
            <p id="subjects-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" onclick="foldPdfKimi('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="panel paper" keywords="slat,versatile,generation,asset,latents,structured,decoding,500k,textural,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation_CVPR_2025_paper.html" target="_blank" title="262/388"><span class="index notranslate">#262</span></a>
                <a id="title-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="title-link" href="/venue/Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" target="_blank">Structured 3D Latents for Scalable and Versatile 3D Generation</a>
                <a id="pdf-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jianfeng Xiang" target="_blank">Jianfeng Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zelong Lv" target="_blank">Zelong Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sicheng Xu" target="_blank">Sicheng Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Deng" target="_blank">Yu Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruicheng Wang" target="_blank">Ruicheng Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Zhang" target="_blank">Bowen Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Chen" target="_blank">Dong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Tong" target="_blank">Xin Tong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaolong Yang" target="_blank">Jiaolong Yang</a>
            </p>
            <p id="summary-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="summary">We introduce a novel 3D generation method for versatile and high-quality 3D asset creation.The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding.We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.</p>
            <p id="subjects-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="panel paper" keywords="human,motion,centric,action,fvd,representation,acc,supervised,world,scenarios">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis_CVPR_2025_paper.html" target="_blank" title="263/388"><span class="index notranslate">#263</span></a>
                <a id="title-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="title-link" href="/venue/Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" target="_blank">H-MoRe: Learning Human-centric Motion Representation for Action Analysis</a>
                <a id="pdf-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhanbo Huang" target="_blank">Zhanbo Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoming Liu" target="_blank">Xiaoming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Kong" target="_blank">Yu Kong</a>
            </p>
            <p id="summary-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="summary">In this paper, we propose H-MoRe, a novel pipeline for learning precise human-centric motion representation. Our approach dynamically preserves relevant human motion while filtering out background movement. Notably, unlike previous methods relying on fully supervised learning from synthetic data, H-MoRe learns directly from real-world scenarios in a self-supervised manner, incorporating both human pose and body shape information. Inspired by kinematics, H-MoRe represents absolute and relative movements of each body point in a matrix format that captures nuanced motion details, termed world-local flows. H-MoRe offers refined insights into human motion, which can be integrated seamlessly into various action-related applications. Experimental results demonstrate that H-MoRe brings substantial improvements across various downstream tasks, including gait recognition(CL@R1: +16.01%), action recognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally, H-MoRe exhibits high inference efficiency (34 fps), making it suitable for most real-time scenarios. Models and code will be released upon publication.</p>
            <p id="subjects-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" onclick="foldPdfKimi('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="panel paper" keywords="foundation,vision,human,openclip,contrast,models,dinov2,characteristics,dino,computer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of_CVPR_2025_paper.html" target="_blank" title="264/388"><span class="index notranslate">#264</span></a>
                <a id="title-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="title-link" href="/venue/Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" target="_blank">Do Computer Vision Foundation Models Learn the Low-level Characteristics of the Human Visual System?</a>
                <a id="pdf-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yancheng Cai" target="_blank">Yancheng Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fei Yin" target="_blank">Fei Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dounia Hammou" target="_blank">Dounia Hammou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rafal Mantiuk" target="_blank">Rafal Mantiuk</a>
            </p>
            <p id="summary-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="summary">Computer vision foundation models, such as DINO or OpenCLIP, are trained in a self-supervised manner on large image datasets. Analogously, substantial evidence suggests that the human visual system (HVS) is influenced by the statistical distribution of colors and patterns in the natural world, characteristics also present in the training data of foundation models. The question we address in this paper is whether foundation models trained on natural images mimic some of the low-level characteristics of the human visual system, such as contrast detection, contrast masking, and contrast constancy. Specifically, we designed a protocol comprising nine test types to evaluate the image encoders of 45 foundation and generative models. Our results indicate that some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some of the characteristics of human vision, but other models show little resemblance. Foundation models tend to show smaller sensitivity to low contrast and rather irregular responses to contrast across frequencies. The foundation models show the best agreement with human data in terms of contrast masking. Our findings suggest that human vision and computer vision may take both similar and different paths when learning to interpret images of the real world. Overall, while differences remain, foundation models trained on vision tasks start to align with low-level human vision, with DINOv2 showing the closest resemblance.</p>
            <p id="subjects-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" onclick="foldPdfKimi('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="panel paper" keywords="mamba,sod,samba,salient,rgb,vsod,scanning,receptive,saliency,unified">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection_CVPR_2025_paper.html" target="_blank" title="265/388"><span class="index notranslate">#265</span></a>
                <a id="title-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="title-link" href="/venue/He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" target="_blank">Samba: A Unified Mamba-based Framework for General Salient Object Detection</a>
                <a id="pdf-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao He" target="_blank">Jiahao He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keren Fu" target="_blank">Keren Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohong Liu" target="_blank">Xiaohong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qijun Zhao" target="_blank">Qijun Zhao</a>
            </p>
            <p id="summary-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="summary">Existing salient object detection (SOD) models primarily resort to convolutional neural networks (CNNs) and Transformers. However, the limited receptive fields of CNNs and quadratic computational complexity of transformers both constrain the performance of current models on discovering attention-grabbing objects. The emerging state space model, namely Mamba, has demonstrated its potential to balance global receptive fields and computational complexity. Therefore, we propose a novel unified framework based on the pure Mamba architecture, dubbed saliency Mamba (Samba), to flexibly handle general SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), and RGB-D VSOD. Specifically, we rethink Mamba's scanning strategy from the perspective of SOD, and identify the importance of maintaining spatial continuity of salient patches within scanning sequences. Based on this, we propose a saliency-guided Mamba block (SGMB), incorporating a spatial neighboring scanning (SNS) algorithm to preserve spatial continuity of salient patches. Additionally, we propose a context-aware upsampling (CAU) method to promote hierarchical feature alignment and aggregations by modeling contextual dependencies. Experimental results show that our Samba outperforms existing methods across five SOD tasks on 21 datasets with lower computational cost, confirming the superiority of introducing Mamba to the SOD areas. Our code will be made publicly available.</p>
            <p id="subjects-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" onclick="foldPdfKimi('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="panel paper" keywords="stereo,concept,cems,erasure,stage,erasing,t2id,adversarial,robust,erased">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from_CVPR_2025_paper.html" target="_blank" title="266/388"><span class="index notranslate">#266</span></a>
                <a id="title-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="title-link" href="/venue/Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" target="_blank">STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models</a>
                <a id="pdf-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Koushik Srivatsan" target="_blank">Koushik Srivatsan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fahad Shamshad" target="_blank">Fahad Shamshad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muzammal Naseer" target="_blank">Muzammal Naseer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vishal M. Patel" target="_blank">Vishal M. Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Karthik Nandakumar" target="_blank">Karthik Nandakumar</a>
            </p>
            <p id="summary-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="summary">The rapid proliferation of large-scale text-to-image diffusion (T2ID) models has raised serious concerns about their potential misuse in generating harmful content. Although numerous methods have been proposed for erasing undesired concepts from T2ID models, they often provide a false sense of security, because concept-erased models (CEMs) can be easily deceived through adversarial attacks to generate the erased concept. Though some robust concept erasure methods based on adversarial training have emerged recently, they compromise on utility (generation quality for benign concepts) to achieve robustness and/or remain vulnerable to advanced embedding-space attacks. These limitations stem from the failure of robust CEMs to search for blind spots in the embedding space thoroughly. To bridge this gap, we propose STEREO, a novel two-stage framework that employs adversarial training as a first step rather than the only step for robust concept erasure. In the first stage, STEREO employs adversarial training as a vulnerability identification mechanism to search thoroughly enough. In the second robustly erase once stage, STEREO introduces an anchor-concept-based compositional objective to robustly erase the target concept at one go while attempting to minimize the degradation on model utility. We benchmark STEREO against 7 state-of-the-art concept erasure methods, demonstrating its enhanced robustness against whitebox, black-box, and advanced embedding-space attacks and its ability to preserve utility to a large extent.</p>
            <p id="subjects-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" onclick="foldPdfKimi('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="panel paper" keywords="modal,damm,nps,tme,multi,uni,uafm,mmfm,divergence,diffusion">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction_CVPR_2025_paper.html" target="_blank" title="267/388"><span class="index notranslate">#267</span></a>
                <a id="title-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="title-link" href="/venue/Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" target="_blank">DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction</a>
                <a id="pdf-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junjie Zhou" target="_blank">Junjie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shouju Wang" target="_blank">Shouju Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxia Tang" target="_blank">Yuxia Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Zhu" target="_blank">Qi Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daoqiang Zhang" target="_blank">Daoqiang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Shao" target="_blank">Wei Shao</a>
            </p>
            <p id="summary-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="summary">The prediction of nanoparticles (NPs) distribution is crucial for the diagnosis and treatment of tumors. Recent studies indicate that the heterogeneity of tumor microenvironment (TME) highly affects the distribution of NPs across tumors. Hence, it has become a research hotspot to generate the NPs distribution by the aid of multi-modal TME components. However, the distribution divergence among multi-modal TME components may cause side effects i.e., the best uni-modal model may outperform the joint generative model. To address the above issues, we propose a \Divergence-Aware Multi-Modal Diffusion model (i.e., DAMM-Diffusion) to adaptively generate the prediction results from uni-modal and multi-modal branches in a unified network. In detail, the uni-modal branch is composed of the U-Net architecture while the multi-modal branch extends it by introducing two novel fusion modules i.e., Multi-Modal Fusion Module (MMFM) and Uncertainty-Aware Fusion Module (UAFM). Specifically, the MMFM is proposed to fuse features from multiple modalities, while the UAFM module is introduced to learn the uncertainty map for cross-attention computation. Following the individual prediction results from each branch, the Divergence-Aware Multi-Modal Predictor (DAMMP) module is proposed to assess the consistency of multi-modal data with the uncertainty map, which determines whether the final prediction results come from multi-modal or uni-modal predictions. We predict the NPs distribution given the TME components of tumor vessels and cell nuclei, and the experimental results show that DAMM-Diffusion can generate the distribution of NPs with higher accuracy than the comparing methods. Additional results on the multi-modal brain image synthesis task further validate the effectiveness of the proposed method.</p>
            <p id="subjects-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" onclick="foldPdfKimi('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="panel paper" keywords="raygs,rendering,rasterized,splatting,hardware,quality,shaders,ray,gaussian,rasterization">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting_CVPR_2025_paper.html" target="_blank" title="268/388"><span class="index notranslate">#268</span></a>
                <a id="title-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="title-link" href="/venue/Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" target="_blank">Hardware-Rasterized Ray-Based Gaussian Splatting</a>
                <a id="pdf-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Samuel Rota Bul" target="_blank">Samuel Rota Bul</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nemanja Bartolovic" target="_blank">Nemanja Bartolovic</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Porzi" target="_blank">Lorenzo Porzi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peter Kontschieder" target="_blank">Peter Kontschieder</a>
            </p>
            <p id="summary-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="summary">We present a novel, hardware rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), obtaining both fast and high-quality results for novel view synthesis. Our work contains a mathematically rigorous and geometrically intuitive derivation about how to efficiently estimate all relevant quantities for rendering RayGS models, structured with respect to standard hardware rasterization shaders. Our solution is the first enabling rendering RayGS models at sufficiently high frame rates to support quality-sensitive applications like Virtual and Mixed Reality. Our second contribution enables alias-free rendering for RayGS, by addressing mip-related issues arising when rendering diverging scales during training and testing. We demonstrate significant performance gains, across different benchmark scenes, while retaining state-of-the-art appearance quality of RayGS.</p>
            <p id="subjects-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" onclick="foldPdfKimi('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="panel paper" keywords="iqa,mpd,machine,preferences,human,preference,machines,assessment,quality,subjective">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Image_Quality_Assessment_From_Human_to_Machine_Preference_CVPR_2025_paper.html" target="_blank" title="269/388"><span class="index notranslate">#269</span></a>
                <a id="title-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="title-link" href="/venue/Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" target="_blank">Image Quality Assessment: From Human to Machine Preference</a>
                <a id="pdf-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Image_Quality_Assessment_From_Human_to_Machine_Preference_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chunyi Li" target="_blank">Chunyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Tian" target="_blank">Yuan Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyue Ling" target="_blank">Xiaoyue Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zicheng Zhang" target="_blank">Zicheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haodong Duan" target="_blank">Haodong Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoning Wu" target="_blank">Haoning Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziheng Jia" target="_blank">Ziheng Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohong Liu" target="_blank">Xiaohong Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiongkuo Min" target="_blank">Xiongkuo Min</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guo Lu" target="_blank">Guo Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weisi Lin" target="_blank">Weisi Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guangtao Zhai" target="_blank">Guangtao Zhai</a>
            </p>
            <p id="summary-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="summary">Image Quality Assessment (IQA) based on human subjective preferences has undergone extensive research in the past decades. However, with the development of communication protocols, the visual data consumption volume of machines has gradually surpassed that of humans. For machines, the preference depends on downstream tasks such as segmentation and detection, rather than visual appeal. Considering the huge gap between human and machine visual systems, this paper proposes the topic: **Image Quality Assessment for Machine Vision** for the first time. Specifically, we (1) defined the subjective preferences of machines, including downstream tasks, test models, and evaluation metrics; (2) established the Machine Preference Database (MPD), which contains 2.25M fine-grained annotations and 30k reference/distorted image pair instances; (3) verified the performance of mainstream IQA algorithms on MPD. Experiments show that current IQA metrics are human-centric and cannot accurately characterize machine preferences. We sincerely hope that MPD can promote the evolution of IQA from human to machine preferences.</p>
            <p id="subjects-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" onclick="foldPdfKimi('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="panel paper" keywords="depth,monocular,mde,estimation,affine,relative,priors,classic,solvers,uncalibrated">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors_CVPR_2025_paper.html" target="_blank" title="270/388"><span class="index notranslate">#270</span></a>
                <a id="title-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="title-link" href="/venue/Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" target="_blank">Relative Pose Estimation through Affine Corrections of Monocular Depth Priors</a>
                <a id="pdf-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Yu" target="_blank">Yifan Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Liu" target="_blank">Shaohui Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rmi Pautrat" target="_blank">Rmi Pautrat</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Viktor Larsson" target="_blank">Viktor Larsson</a>
            </p>
            <p id="summary-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="summary">Monocular depth estimation (MDE) models have undergone significant advancements over recent years. Many MDE models aim to predict affine-invariant relative depth from monocular images, while recent developments in large-scale training and vision foundation models enable reasonable estimation of metric (absolute) depth. However, effectively leveraging these predictions for geometric vision tasks, in particular relative pose estimation, remains relatively under explored. While depths provide rich constraints for cross-view image alignment, the intrinsic noise and ambiguity from the monocular depth priors present practical challenges to improving upon classic keypoint-based solutions. In this paper, we develop three solvers for relative pose estimation that explicitly account for independent affine (scale and shift) ambiguities, covering both calibrated and uncalibrated conditions. We further propose a hybrid estimation pipeline that combines our proposed solvers with classic point-based solvers and epipolar constraints. We find that the affine correction modeling is beneficial to not only the relative depth priors but also, surprisingly, the "metric" ones. Results across multiple datasets demonstrate large improvements of our approach over classic keypoint-based baselines and PnP-based solutions, under both calibrated and uncalibrated setups. We also show that our method improves consistently with different feature matchers and MDE models, and can further benefit from very recent advances on both modules.</p>
            <p id="subjects-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" onclick="foldPdfKimi('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="panel paper" keywords="peft,metapeft,tuning,fine,hyperparameters,images,module,adaptformer,tail,layer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning_CVPR_2025_paper.html" target="_blank" title="271/388"><span class="index notranslate">#271</span></a>
                <a id="title-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="title-link" href="/venue/Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" target="_blank">Meta-Learning Hyperparameters for Parameter Efficient Fine-Tuning</a>
                <a id="pdf-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zichen Tian" target="_blank">Zichen Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaoyao Liu" target="_blank">Yaoyao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianru Sun" target="_blank">Qianru Sun</a>
            </p>
            <p id="summary-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="summary">Training large foundation models of remote-sensing (RS) images is almost impossible due to the limited and long-tailed data problems. Fine-tuning natural image pre-trained models on RS images is a straightforward solution. To reduce computational costs and improve performance on tail classes, existing methods apply parameter-efficient fine-tuning (PEFT) techniques, such as LoRA and AdaptFormer. However, we observe that fixed hyperparameters -- such as intra-layer positions, layer depth, and scaling factors, can considerably hinder PEFT performance, as fine-tuning on RS images proves highly sensitive to these settings. To address this, we propose MetaPEFT, a method incorporating adaptive scalers that dynamically adjust module influence during fine-tuning. MetaPEFT dynamically adjusts three key factors of PEFT on RS images: module insertion, layer selection, and module-wise learning rates, which collectively control the influence of PEFT modules across the network. We conduct extensive experiments on three transfer-learning scenarios and five datasets. The results show that MetaPEFT achieves state-of-the-art performance in cross-spectral adaptation, requiring only a small amount of trainable parameters and improving tail-class accuracy significantly. Our code is available in the supplementary materials for review.</p>
            <p id="subjects-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" onclick="foldPdfKimi('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="panel paper" keywords="sits,perceptive,crop,clues,weakly,supervised,segmentation,satellite,mapping,exact">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.html" target="_blank" title="272/388"><span class="index notranslate">#272</span></a>
                <a id="title-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="title-link" href="/venue/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" target="_blank">Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation</a>
                <a id="pdf-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Zhu" target="_blank">Hao Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yan Zhu" target="_blank">Yan Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayu Xiao" target="_blank">Jiayu Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianxiang Xiao" target="_blank">Tianxiang Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yike Ma" target="_blank">Yike Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yucheng Zhang" target="_blank">Yucheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Dai" target="_blank">Feng Dai</a>
            </p>
            <p id="summary-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="summary">Automated crop mapping through Satellite Image Time Series (SITS) has emerged as a crucial avenue for agricultural monitoring and management. However, due to the low resolution and unclear parcel boundaries, annotating pixel-level masks is exceptionally complex and time-consuming in SITS. This paper embraces the weakly supervised paradigm (i.e., only image-level categories available) to liberate the crop mapping task from the exhaustive annotation burden. The unique characteristics of SITS give rise to several challenges in weakly supervised learning: (1) noise perturbation from spatially neighboring regions, and (2) erroneous semantic bias from anomalous temporal periods. To address the above difficulties, we propose a novel method, termed exploring space-time perceptive clues (Exact). First, we introduce a set of spatial clues to explicitly capture the representative patterns of different crops from the most class-relative regions. Besides, we leverage the temporal-to-class interaction of the model to emphasize the contributions of pivotal clips, thereby enhancing the model perception for crop regions. Build upon the space-time perceptive clues, we derive the clue-based CAMs to effectively supervise the SITS segmentation network. Our method demonstrates impressive performance on various SITS benchmarks. Remarkably, the segmentation network trained on Exact-generated masks achieves 95% of its fully supervised performance, showing the bright promise of weakly supervised paradigm in crop mapping scenario. All code will be publicly available in the future.</p>
            <p id="subjects-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" onclick="foldPdfKimi('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="panel paper" keywords="prompt,tuning,calibrating,tpt,orthogonality,textual,test,calibration,vision,vlms">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.html" target="_blank" title="273/388"><span class="index notranslate">#273</span></a>
                <a id="title-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="title-link" href="/venue/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" target="_blank">O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models</a>
                <a id="pdf-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ashshak Sharifdeen" target="_blank">Ashshak Sharifdeen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Akhtar Munir" target="_blank">Muhammad Akhtar Munir</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sanoojan Baliah" target="_blank">Sanoojan Baliah</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Salman Khan" target="_blank">Salman Khan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Muhammad Haris Khan" target="_blank">Muhammad Haris Khan</a>
            </p>
            <p id="summary-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="summary">Test-time prompt tuning for vision-language models (VLMs) are getting attention due to their ability to learn with unlabeled data without fine-tuning. Although test-time prompt tuning methods for VLMs can boost accuracy, the resulting models tend to demonstrate poor calibration, which casts doubts on the reliability and trustworthiness of these models. Notably, more attention needs to be devoted to calibrating the test-time prompt tuning in vision-language models. To this end, we propose a new approach, called O-TPT that introduces orthogonality constraints on the textual features corresponding to the learnable prompts for calibrating test-time prompt tuning in VLMsTowards introducing orthogonality constraints, we make the following contributions. First, we uncover new insights behind the suboptimal calibration performance of existing methods relying on textual feature dispersion. Second, we show that imposing a simple orthogonalization of textual features is a more effective approach towards obtaining textual dispersion.We conduct extensive experiments on various datasets with different backbones and baselines. Results indicate that our method consistently outperforms the state-of-the-art in significantly reducing the overall average calibration error. Also, our method surpasses the zero-shot calibration performance on fine-grained classification tasks. Our code will be made public upon acceptance.</p>
            <p id="subjects-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" onclick="foldPdfKimi('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="panel paper" keywords="qpd,disparity,dpnet,sensors,directional,estimation,views,illumination,sub,module">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images_CVPR_2025_paper.html" target="_blank" title="274/388"><span class="index notranslate">#274</span></a>
                <a id="title-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="title-link" href="/venue/Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" target="_blank">All-directional Disparity Estimation for Real-world QPD Images</a>
                <a id="pdf-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongtao Yu" target="_blank">Hongtao Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Song" target="_blank">Shaohui Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lihu Sun" target="_blank">Lihu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenkai Su" target="_blank">Wenkai Su</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaodong Yang" target="_blank">Xiaodong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengming Liu" target="_blank">Chengming Liu</a>
            </p>
            <p id="summary-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="summary">Quad Photodiode (QPD) sensors represent an evolution by providing four sub-views, whereas dual-pixel (DP) sensors are limited to two sub-views. In addition to enhancing auto-focus performance, QPD sensors also enable disparity estimation in horizontal and vertical directions. However, the characteristics of QPD sensors, including uneven illumination across sub-views and the narrow baseline, render algorithm design difficult. Furthermore, effectively utilizing the two-directional disparity of QPD sensors remains a challenge. The scarcity of QPD disparity datasets also limits the development of learning-based methods. In this work, we address these challenges by first proposing a DPNet for DP disparity estimation. Specifically, we design an illumination-invariant module to reduce the impact of illumination, followed by a coarse-to-fine module to estimate sub-pixel disparity. Building upon the DPNet, we further propose a QuadNet, which integrates the two-directional disparity via an edge-aware fusion module. To facilitate the evaluation of our approaches, we propose the first QPD disparity dataset QPD2K, comprising 2,100 real-world QPD images and corresponding disparity maps. Experiments demonstrate that our approaches achieve state-of-the-art performance in DP and QPD disparity estimation.</p>
            <p id="subjects-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" onclick="foldPdfKimi('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="panel paper" keywords="scene,cuboid,casagpt,cuboids,arrange,scenes,arrangement,dataset,assembly,intersections">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design_CVPR_2025_paper.html" target="_blank" title="275/388"><span class="index notranslate">#275</span></a>
                <a id="title-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="title-link" href="/venue/Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" target="_blank">CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design</a>
                <a id="pdf-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Weitao Feng" target="_blank">Weitao Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Zhou" target="_blank">Hang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Liao" target="_blank">Jing Liao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Cheng" target="_blank">Li Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbo Zhou" target="_blank">Wenbo Zhou</a>
            </p>
            <p id="summary-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="summary">We present a novel approach for indoor scene synthesis, which learns to arrange decomposed cuboid primitives to represent 3D objects within a scene. Unlike conventional methods that use bounding boxes to determine the placement and scale of 3D objects, our approach leverages cuboids as a straightforward yet highly effective alternative for modeling objects. This allows for compact scene generation while minimizing object intersections. Our approach, coined CASAGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive model to sequentially arrange cuboids, producing physically plausible scenes. By applying rejection sampling during the fine-tuning stage to filter out scenes with object collisions, our model further reduces intersections and enhances scene quality. Additionally, we introduce a refined dataset, 3DFRONT-NC, which eliminates significant noise presented in the original dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our dataset demonstrate that our approach consistently outperforms the state-of-the-art methods, enhancing the realism of generated scenes, and providing a promising direction for 3D scene synthesis.</p>
            <p id="subjects-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" onclick="foldPdfKimi('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="panel paper" keywords="depth,video,monocular,align3r,dust3r,estimation,poses,camera,dynamic,consistent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos_CVPR_2025_paper.html" target="_blank" title="276/388"><span class="index notranslate">#276</span></a>
                <a id="title-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="title-link" href="/venue/Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" target="_blank">Align3R: Aligned Monocular Depth Estimation for Dynamic Videos</a>
                <a id="pdf-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Lu" target="_blank">Jiahao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Huang" target="_blank">Tianyu Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Li" target="_blank">Peng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyang Dou" target="_blank">Zhiyang Dou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Lin" target="_blank">Cheng Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiming Cui" target="_blank">Zhiming Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Dong" target="_blank">Zhen Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sai-Kit Yeung" target="_blank">Sai-Kit Yeung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenping Wang" target="_blank">Wenping Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Liu" target="_blank">Yuan Liu</a>
            </p>
            <p id="summary-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="summary">Recent developments in monocular depth estimation methods enable high-quality depth estimation of single-view images but fail to estimate consistent video depth across different frames. Recent works address this problem by applying a video diffusion model to generate video depth conditioned on the input video, which is training-expensive and can only produce scale-invariant depth values without camera poses. In this paper, we propose a novel video-depth estimation method called Align3R to estimate temporal consistent depth maps for a dynamic video. Our key idea is to utilize the recent DUSt3R model to align estimated monocular depth maps of different timesteps. First, we fine-tune the DUSt3R model with additional estimated monocular depth as inputs for the dynamic scenes. Then, we apply optimization to reconstruct both depth maps and camera poses. Extensive experiments demonstrate that Align3R estimates consistent video depth and camera poses for a monocular video with superior performance than baseline methods.</p>
            <p id="subjects-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" onclick="foldPdfKimi('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="panel paper" keywords="alignins,updates,backdoor,alignment,malicious,defense,direction,attacks,inspection,iid">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection_CVPR_2025_paper.html" target="_blank" title="277/388"><span class="index notranslate">#277</span></a>
                <a id="title-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="title-link" href="/venue/Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" target="_blank">Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection</a>
                <a id="pdf-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahao Xu" target="_blank">Jiahao Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zikai Zhang" target="_blank">Zikai Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rui Hu" target="_blank">Rui Hu</a>
            </p>
            <p id="summary-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="summary">The distributed nature of training makes Federated Learning (FL) vulnerable to backdoor attacks, where malicious model updates aim to compromise the global models performance on specific tasks. Existing defense methods show limited efficacy as they overlook the inconsistency between benign and malicious model updates regarding both general and fine-grained directions. To fill this gap, we introduce AlignIns, a novel defense method designed to safeguard FL systems against backdoor attacks. AlignIns looks into the direction of each model update through a direction alignment inspection process. Specifically, it examines the alignment of model updates with the overall update direction and analyzes the distribution of the signs of their significant parameters, comparing them with the principle sign across all model updates. Model updates that exhibit an unusual degree of alignment are considered malicious and thus be filtered out. We provide the theoretical analysis of the robustness of AlignIns and its propagation error in FL. Our empirical results on both independent and identically distributed (IID) and non-IID datasets demonstrate that AlignIns achieves higher robustness compared to the state-of-the-art defense methods. Code is available at \url{https://anonymous.4open.science/r/AlignIns}.</p>
            <p id="subjects-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" onclick="foldPdfKimi('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="panel paper" keywords="mil,akd,slide,pmp,wsi,continual,attention,learning,forgetting,instance">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide_CVPR_2025_paper.html" target="_blank" title="278/388"><span class="index notranslate">#278</span></a>
                <a id="title-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="title-link" href="/venue/Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" target="_blank">Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging</a>
                <a id="pdf-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xianrui Li" target="_blank">Xianrui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufei Cui" target="_blank">Yufei Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Li" target="_blank">Jun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antoni B. Chan" target="_blank">Antoni B. Chan</a>
            </p>
            <p id="summary-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="summary">Advances in medical imaging and deep learning have propelled progress in whole slide image (WSI) analysis, with multiple instance learning (MIL) showing promise for efficient and accurate diagnostics. However, conventional MIL models often lack adaptability to evolving datasets, as they rely on static training that cannot incorporate new information without extensive retraining. Applying continual learning (CL) to MIL models is a possible solution, but often sees limited improvements. In this paper, we analyze CL in the context of attention MIL models and find that the model forgetting is mainly concentrated in the attention layers of the MIL model. Using the results of this analysis we propose two components for improving CL on MIL:Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates catastrophic forgetting by focusing on retaining attention layer knowledge between learning sessions, while PMP reduces the memory footprint by selectively storing only the most informative patches, or ''pseudo-bags'' from WSIs. Experimental evaluations demonstrate that our method significantly improves both accuracy and memory efficiency on diverse WSI datasets, outperforming current state-of-the-art CL methods. This work provides a foundation for CL in large-scale, weakly annotated clinical datasets, paving the way for more adaptable and resilient diagnostic models.</p>
            <p id="subjects-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" onclick="foldPdfKimi('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="panel paper" keywords="sfm,light3r,feed,forward,learnable,global,runtime,motion,module,reconstruction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion_CVPR_2025_paper.html" target="_blank" title="279/388"><span class="index notranslate">#279</span></a>
                <a id="title-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="title-link" href="/venue/Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" target="_blank">Light3R-SfM: Towards Feed-forward Structure-from-Motion</a>
                <a id="pdf-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sven Elflein" target="_blank">Sven Elflein</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qunjie Zhou" target="_blank">Qunjie Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Laura Leal-Taix" target="_blank">Laura Leal-Taix</a>
            </p>
            <p id="summary-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="summary">We present Light3R-SfM, a feed-forward, end-to-end learnable framework for efficient large-scale Structure-from-Motion (SfM) from unconstrained image collections. Unlike existing SfM solutions that rely on costly matching and global optimization to achieve accurate 3D reconstructions, Light3R-SfM addresses this limitation through a novel latent global alignment module. This module replaces traditional global optimization with a learnable attention mechanism, effectively capturing multi-view constraints across images for robust and precise camera pose estimation. Light3R-SfM constructs a sparse scene graph via retrieval-score-guided shortest path tree to dramatically reduce memory usage and computational overhead compared to the naive approach. Extensive experiments demonstrate that Light3R-SfM achieves competitive accuracy while significantly reducing runtime, making it ideal for 3D reconstruction tasks in real-world applications with a runtime constraint. This work pioneers a data-driven, feed-forward SfM approach, paving the way toward scalable, accurate, and efficient 3D reconstruction in the wild.</p>
            <p id="subjects-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" onclick="foldPdfKimi('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="panel paper" keywords="arm,appearance,texture,relightable,reconstruction,h100,generation,realistic,material,geometry">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation_CVPR_2025_paper.html" target="_blank" title="280/388"><span class="index notranslate">#280</span></a>
                <a id="title-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="title-link" href="/venue/Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" target="_blank">ARM: Appearance Reconstruction Model for Relightable 3D Generation</a>
                <a id="pdf-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Feng" target="_blank">Xiang Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chang Yu" target="_blank">Chang Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zoubin Bi" target="_blank">Zoubin Bi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yintong Shang" target="_blank">Yintong Shang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Gao" target="_blank">Feng Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongzhi Wu" target="_blank">Hongzhi Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhou" target="_blank">Kun Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenfanfu Jiang" target="_blank">Chenfanfu Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Yang" target="_blank">Yin Yang</a>
            </p>
            <p id="summary-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="summary">Recent image-to-3D reconstruction models have greatly advanced geometry generation, but they still struggle to faithfully generate realistic appearance. To address this, we introduce ARM, a novel method that reconstructs high-quality 3D meshes and realistic appearance from sparse-view images. The core of ARM lies in decoupling geometry from appearance, processing appearance within the UV texture space. Unlike previous methods, ARM improves texture quality by explicitly back-projecting measurements onto the texture map and processing them in a UV space module with a global receptive field. To resolve ambiguities between material and illumination in input images, ARM introduces a material prior that encodes semantic appearance information, enhancing the robustness of appearance decomposition. Trained on just 8 H100 GPUs, ARM outperforms existing methods both quantitatively and qualitatively.</p>
            <p id="subjects-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="panel paper" keywords="uav,nonuniformity,nuc,detection,infrared,target,unicd,union,irbfd,friendly">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target_CVPR_2025_paper.html" target="_blank" title="281/388"><span class="index notranslate">#281</span></a>
                <a id="title-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="title-link" href="/venue/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" target="_blank">Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection</a>
                <a id="pdf-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Houzhang Fang" target="_blank">Houzhang Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaolin Wang" target="_blank">Xiaolin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zengyang Li" target="_blank">Zengyang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lu Wang" target="_blank">Lu Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qingshan Li" target="_blank">Qingshan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Chang" target="_blank">Yi Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luxin Yan" target="_blank">Luxin Yan</a>
            </p>
            <p id="summary-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="summary">Infrared unmanned aerial vehicle (UAV) images captured using thermal detectors are often affected by temperature-dependent low-frequency nonuniformity, which significantly reduces the contrast of the images. Detecting UAV targets under nonuniform conditions is crucial in UAV surveillance applications. Existing methods typically treat infrared nonuniformity correction (NUC) as a preprocessing step for detection, which leads to suboptimal performance. Balancing the two tasks while enhancing detection-beneficial information remains challenging. In this paper, we present a detection-friendly union framework, termed UniCD, that simultaneously addresses both infrared NUC and UAV target detection tasks in an end-to-end manner. We first model NUC as a small number of parameter estimation problem jointly driven by priors and data to generate detection-conducive images. Then, we incorporate a new auxiliary loss with target mask supervision into the backbone of the infrared UAV target detection network to strengthen target features while suppressing the background. To better balance correction and detection, we introduce a detection-guided self-supervised loss to reduce feature discrepancies between the two tasks, thereby enhancing detection robustness to varying nonuniformity levels. Additionally, we construct a new benchmark composed of 50,000 infrared images in various nonuniformity types, multi-scale UAV targets and rich backgrounds with target annotations, called IRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust union framework for NUC and UAV target detection while achieving real-time processing capabilities. Dataset can be available at https://github.com/anonymous2025submit/UniCD.</p>
            <p id="subjects-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" onclick="foldPdfKimi('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="panel paper" keywords="unipose,pose,poses,human,unified,comprehension,editing,across,tasks,framework">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation_CVPR_2025_paper.html" target="_blank" title="282/388"><span class="index notranslate">#282</span></a>
                <a id="title-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="title-link" href="/venue/Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" target="_blank">UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing</a>
                <a id="pdf-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yiheng Li" target="_blank">Yiheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruibing Hou" target="_blank">Ruibing Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong Chang" target="_blank">Hong Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiguang Shan" target="_blank">Shiguang Shan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xilin Chen" target="_blank">Xilin Chen</a>
            </p>
            <p id="summary-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="summary">Human pose plays a crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only a single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, a framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically, we apply a pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within a unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with a mixture of visual encoders, among them a pose-specific visual encoder. Benefiting from a unified learning strategy, UniPose effectively transfers knowledge across different pose-relevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building a general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPose's competitive and even superior performance across various pose-relevant tasks.</p>
            <p id="subjects-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="panel paper" keywords="bimanual,gigahands,activities,hand,annotated,84k,annotations,text,14k,massive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities_CVPR_2025_paper.html" target="_blank" title="283/388"><span class="index notranslate">#283</span></a>
                <a id="title-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="title-link" href="/venue/Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" target="_blank">GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities</a>
                <a id="pdf-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rao Fu" target="_blank">Rao Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dingxi Zhang" target="_blank">Dingxi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex Jiang" target="_blank">Alex Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanjia Fu" target="_blank">Wanjia Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Austin Funk" target="_blank">Austin Funk</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel Ritchie" target="_blank">Daniel Ritchie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Srinath Sridhar" target="_blank">Srinath Sridhar</a>
            </p>
            <p id="summary-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="summary">Understanding bimanual human hand activities is a critical problem in AI and robotics. We cannot build large models of bimanual activities because existing datasets lack the scale, coverage of diverse hand activities, and detailed annotations. We introduce GigaHands, a massive annotated dataset capturing 34 hours of bimanual hand activities from 56 subjects and 417 objects, totaling 14k motion clips derived from 183 million frames paired with 84k text annotations. Our markerless capture setup and data acquisition protocol enable fully automatic 3D hand and object estimation while minimizing the effort required for text annotation. The scale and diversity of GigaHands enable broad applications, including text-driven action synthesis, hand motion captioning, and dynamic radiance field reconstruction.</p>
            <p id="subjects-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" onclick="foldPdfKimi('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="panel paper" keywords="freepca,consistency,video,frames,quality,appearance,long,videos,principal,information">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long_CVPR_2025_paper.html" target="_blank" title="284/388"><span class="index notranslate">#284</span></a>
                <a id="title-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="title-link" href="/venue/Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" target="_blank">FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis</a>
                <a id="pdf-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangtong Tan" target="_blank">Jiangtong Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hu Yu" target="_blank">Hu Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Huang" target="_blank">Jie Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jie Xiao" target="_blank">Jie Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Zhao" target="_blank">Feng Zhao</a>
            </p>
            <p id="summary-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="summary">Long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. It necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. Existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to inconsistency and poor quality. In this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying Principal Component Analysis (PCA), allowing for refined complementary integration of global consistency and local quality. With this insight, we propose FreePCA, a training-free long video generation paradigm based on PCA that simultaneously achieves high consistency and quality. Concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. Critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. Experiments demonstrate that FreePCA can be applied to various video diffusion models without requiring training, leading to substantial improvements.</p>
            <p id="subjects-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" onclick="foldPdfKimi('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="panel paper" keywords="effidec3d,decoder,params,swinunetr,flops,swinunetrv2,optimized,medical,segmentation,reduction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical_CVPR_2025_paper.html" target="_blank" title="285/388"><span class="index notranslate">#285</span></a>
                <a id="title-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="title-link" href="/venue/Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" target="_blank">EffiDec3D: An Optimized Decoder for High-Performance and Efficient 3D Medical Image Segmentation</a>
                <a id="pdf-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Md Mostafijur Rahman" target="_blank">Md Mostafijur Rahman</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Radu Marculescu" target="_blank">Radu Marculescu</a>
            </p>
            <p id="summary-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="summary">Recent 3D deep networks such as SwinUNETR, SwinUNETRv2, and 3D UX-Net have shown promising performance by leveraging self-attention and large-kernel convolutions to capture the volumetric context. However, their substantial computational requirements limit their use in real-time and resource-constrained environments. The high #FLOPs and #Params in these networks stem largely fromcomplex decoder designs with high-resolution layers and excessive channel counts. In this paper, we propose EffiDec3D, an optimized 3D decoder that employs a channel reduction strategy across all decoder stages, which sets the number of channels to the minimum needed for accurate feature representation. Additionally, EffiDec3D removes the high-resolution layers when their contribution to segmentation quality is minimal. Our optimized EffiDec3D decoder achieves a 96.4% reduction in #Params and a 93.0% reduction in #FLOPs compared to the decoder of original 3D UX-Net. Similarly, for SwinUNETR and SwinUNETRv2 (which share an identical decoder), we observe reductions of 94.9% in #Params and 86.2% in #FLOPs. Our extensive experiments on 12 different medical imaging tasks confirm that EffiDec3D not only significantly reduces the computational demands, but also maintains a performance level comparable to original models, thus establishing a new standard for efficient 3D medical image segmentation. We will make the source code public upon paper acceptance.</p>
            <p id="subjects-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" onclick="foldPdfKimi('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="panel paper" keywords="manipulation,shot,image,exemplar,autoregressive,context,unleashing,learning,notable,visual">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation_CVPR_2025_paper.html" target="_blank" title="286/388"><span class="index notranslate">#286</span></a>
                <a id="title-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="title-link" href="/venue/Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" target="_blank">Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation</a>
                <a id="pdf-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bolin Lai" target="_blank">Bolin Lai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Juefei-Xu" target="_blank">Felix Juefei-Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miao Liu" target="_blank">Miao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoliang Dai" target="_blank">Xiaoliang Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nikhil Mehta" target="_blank">Nikhil Mehta</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenguang Zhu" target="_blank">Chenguang Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zeyi Huang" target="_blank">Zeyi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James M. Rehg" target="_blank">James M. Rehg</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangmin Lee" target="_blank">Sangmin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ning Zhang" target="_blank">Ning Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Xiao" target="_blank">Tong Xiao</a>
            </p>
            <p id="summary-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="summary">Text-guided image manipulation has experienced notable advancement in recent years. In order to mitigate linguistic ambiguity, few-shot learning with visual examples has been applied for instructions that are underrepresented in the training set, or difficult to describe purely in language. However, learning from visual prompts requires strong reasoning capability, which diffusion models are struggling with. To address this issue, we introduce a novel multi-modal autoregressive model, dubbed InstaManip, that can instantly learn a new image manipulation operation from textual and visual guidance via in-context learning, and apply it to new query images. Specifically, we propose an innovative group self-attention mechanism to break down the in-context learning process into two separate stages -- learning and applying, which simplifies the complex problem into two easier tasks. We also introduce a relation regularization method to further disentangle image transformation features from irrelevant contents in exemplar images. Extensive experiments suggest that our method surpasses previous few-shot image manipulation models by a notable margin (&gt;=19% in human evaluation). We also find our model can be further boosted by increasing the number or diversity of exemplar images.</p>
            <p id="subjects-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" onclick="foldPdfKimi('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="panel paper" keywords="graphics,branch,vector,3dgs,view,viewing,visibility,optimization,dependent,progressive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility_CVPR_2025_paper.html" target="_blank" title="287/388"><span class="index notranslate">#287</span></a>
                <a id="title-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="title-link" href="/venue/Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" target="_blank">Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility</a>
                <a id="pdf-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yidi Li" target="_blank">Yidi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Xiao" target="_blank">Jun Xiao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengda Lu" target="_blank">Zhengda Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yiqun Wang" target="_blank">Yiqun Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haiyong Jiang" target="_blank">Haiyong Jiang</a>
            </p>
            <p id="summary-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="summary">This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages. We also improve the view-dependent occlusions by devising a visibility-awareness rendering module. Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling.</p>
            <p id="subjects-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" onclick="foldPdfKimi('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="panel paper" keywords="meteorological,era5,observations,satellite,textbf,sgd,states,gridsat,resolution,downscaling">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at_CVPR_2025_paper.html" target="_blank" title="288/388"><span class="index notranslate">#288</span></a>
                <a id="title-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="title-link" href="/venue/Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" target="_blank">Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution</a>
                <a id="pdf-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Siwei Tu" target="_blank">Siwei Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ben Fei" target="_blank">Ben Fei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weidong Yang" target="_blank">Weidong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fenghua Ling" target="_blank">Fenghua Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Chen" target="_blank">Hao Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zili Liu" target="_blank">Zili Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Chen" target="_blank">Kun Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Fan" target="_blank">Hang Fan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wanli Ouyang" target="_blank">Wanli Ouyang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Bai" target="_blank">Lei Bai</a>
            </p>
            <p id="summary-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="summary">Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-72-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;S&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-323" style="width: 0.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-324"><span class="texatom" id="MathJax-Span-325"><span class="mrow" id="MathJax-Span-326"><span class="mtext" id="MathJax-Span-327" style="font-family: MathJax_Main-bold;">S</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">S</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-72">\textbf{S}</script>atellite-observations <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-73-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;G&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-328" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-329"><span class="texatom" id="MathJax-Span-330"><span class="mrow" id="MathJax-Span-331"><span class="mtext" id="MathJax-Span-332" style="font-family: MathJax_Main-bold;">G</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">G</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-73">\textbf{G}</script>uided <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-74-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;D&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-333" style="width: 1.096em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1000.84em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-334"><span class="texatom" id="MathJax-Span-335"><span class="mrow" id="MathJax-Span-336"><span class="mtext" id="MathJax-Span-337" style="font-family: MathJax_Main-bold;">D</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">D</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-74">\textbf{D}</script>iffusion Model (<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-75-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;SGD&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-338" style="width: 2.971em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.357em, 1002.4em, 2.346em, -999.997em); top: -2.185em; left: 0em;"><span class="mrow" id="MathJax-Span-339"><span class="texatom" id="MathJax-Span-340"><span class="mrow" id="MathJax-Span-341"><span class="mtext" id="MathJax-Span-342" style="font-family: MathJax_Main-bold;">SGD</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">SGD</mtext></mrow></math></span></span><script type="math/tex" id="MathJax-Element-75">\textbf{SGD}</script>), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km.</p>
            <p id="subjects-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" onclick="foldPdfKimi('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="panel paper" keywords="vfms,vlms,dgss,mamba,mfuser,vision,strengths,miou,foundation,segmentation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision_CVPR_2025_paper.html" target="_blank" title="289/388"><span class="index notranslate">#289</span></a>
                <a id="title-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" target="_blank">Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</a>
                <a id="pdf-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Zhang" target="_blank">Xin Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Robby T. Tan" target="_blank">Robby T. Tan</a>
            </p>
            <p id="summary-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="summary">Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in token length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.19 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code will be released upon acceptance.</p>
            <p id="subjects-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="panel paper" keywords="face,pixel,groundingface,grounding,understanding,grained,multimodal,fine,textbf,grounded">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language_CVPR_2025_paper.html" target="_blank" title="290/388"><span class="index notranslate">#290</span></a>
                <a id="title-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="title-link" href="/venue/Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" target="_blank">GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model</a>
                <a id="pdf-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yue Han" target="_blank">Yue Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiangning Zhang" target="_blank">Jiangning Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junwei Zhu" target="_blank">Junwei Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Runze Hou" target="_blank">Runze Hou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaozhong Ji" target="_blank">Xiaozhong Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chuming Lin" target="_blank">Chuming Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaobin Hu" target="_blank">Xiaobin Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhucun Xue" target="_blank">Zhucun Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Liu" target="_blank">Yong Liu</a>
            </p>
            <p id="summary-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="summary">Multimodal Language Learning Models (MLLMs) have shown remarkable performance in image understanding, generation, and editing, with recent advancements achieving pixel-level grounding with reasoning. However, these models for common objects struggle with fine-grained face understanding. In this work, we introduce the \textbf{\textit{FacePlayGround-240K}} dataset, the first pioneering large-scale, pixel-grounded face caption and question-answer (QA) dataset, meticulously curated for alignment pretraining and instruction-tuning. We present the \textbf{\textit{GroundingFace}} framework, specifically designed to enhance fine-grained face understanding. This framework significantly augments the capabilities of existing grounding models in face part segmentation, face attribute comprehension, while preserving general scene understanding. Comprehensive experiments validate that our approach surpasses current state-of-the-art models in pixel-grounded face captioning/QA and various downstream tasks, including face captioning, referring segmentation, and zero-shot face attribute recognition.</p>
            <p id="subjects-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" onclick="foldPdfKimi('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="panel paper" keywords="denoising,diffusion,shortest,path,shortdf,optimizing,ddim,model,efficiency,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model_CVPR_2025_paper.html" target="_blank" title="291/388"><span class="index notranslate">#291</span></a>
                <a id="title-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="title-link" href="/venue/Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" target="_blank">Optimizing for the Shortest Path in Denoising Diffusion Model</a>
                <a id="pdf-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Chen" target="_blank">Ping Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingpeng Zhang" target="_blank">Xingpeng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoxiang Liu" target="_blank">Zhaoxiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huan Hu" target="_blank">Huan Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Liu" target="_blank">Xiang Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kai Wang" target="_blank">Kai Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Wang" target="_blank">Min Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanlin Qian" target="_blank">Yanlin Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiguo Lian" target="_blank">Shiguo Lian</a>
            </p>
            <p id="summary-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="summary">In this research, we propose a novel denoising diffusion model based on shortest-path modeling that optimizes residual propagation to enhance both denoising efficiency and quality. Drawing on Denoising Diffusion Implicit Models (DDIM) and insights from graph theory, our model, termed the Shortest Path Diffusion Model (ShortDF), treats the denoising process as a shortest-path problem aimed at minimizing reconstruction error. By optimizing the initial residuals, we improve the efficiency of the reverse diffusion process and the quality of the generated samples. Extensive experiments on multiple standard benchmarks demonstrate that ShortDF significantly reduces diffusion time (or steps) while enhancing the visual fidelity of generated samples compared to prior methods. This work, we suppose, paves the way for interactive diffusion-based applications and establishes a foundation for rapid data generation.</p>
            <p id="subjects-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="panel paper" keywords="speech,video,silent,quality,representations,hierarchical,voices,modality,face,synthesis">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech_CVPR_2025_paper.html" target="_blank" title="292/388"><span class="index notranslate">#292</span></a>
                <a id="title-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="title-link" href="/venue/Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" target="_blank">From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech</a>
                <a id="pdf-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ji-Hoon Kim" target="_blank">Ji-Hoon Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jeongsoo Choi" target="_blank">Jeongsoo Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaehun Kim" target="_blank">Jaehun Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chaeyoung Jung" target="_blank">Chaeyoung Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joon Son Chung" target="_blank">Joon Son Chung</a>
            </p>
            <p id="summary-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="summary">The objective of this study is to generate high-quality speech from silent talking face videos, a task also known as video-to-speech synthesis.A significant challenge in video-to-speech synthesis lies in the substantial modality gap between silent video and multi-faceted speech. In this paper, we propose a novel video-to-speech system that effectively bridges this modality gap, significantly enhancing the quality of synthesized speech.This is achieved by learning of hierarchical representations from video to speech.Specifically, we gradually transform silent video into acoustic feature spaces through three sequential stages--content, timbre, and prosody modeling.In each stage, we align visual factors---lip movements, face identity, and facial expressions---with corresponding acoustic counterparts to ensure the seamless transformation.Additionally, to generate realistic and coherent speech from the visual representations, we employ a flow matching model that estimates direct trajectories from a simple prior distribution to the target speech distribution.Extensive experiments demonstrate that our method achieves exceptional generation quality comparable to real utterances, outperforming existing methods by a significant margin.</p>
            <p id="subjects-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" onclick="foldPdfKimi('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="panel paper" keywords="splat,splatting,language,registration,gaussians,scene,embeddings,vocabulary,clip,referring">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language_CVPR_2025_paper.html" target="_blank" title="293/388"><span class="index notranslate">#293</span></a>
                <a id="title-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="title-link" href="/venue/Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" target="_blank">Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration</a>
                <a id="pdf-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kim Jun-Seong" target="_blank">Kim Jun-Seong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=GeonU Kim" target="_blank">GeonU Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kim Yu-Ji" target="_blank">Kim Yu-Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Chiang Frank Wang" target="_blank">Yu-Chiang Frank Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jaesung Choe" target="_blank">Jaesung Choe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tae-Hyun Oh" target="_blank">Tae-Hyun Oh</a>
            </p>
            <p id="summary-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="summary">We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray. Moreover, we integrate Product Quantization (PQ) trained on general large scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks. Code will be publicly available if accepted.</p>
            <p id="subjects-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" onclick="foldPdfKimi('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="panel paper" keywords="vlms,adaptation,test,tta,realistic,transductive,vision,classes,deployment,scenarios">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.html" target="_blank" title="294/388"><span class="index notranslate">#294</span></a>
                <a id="title-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" target="_blank">Realistic Test-Time Adaptation of Vision-Language Models</a>
                <a id="pdf-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Maxime Zanella" target="_blank">Maxime Zanella</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Clment Fuchs" target="_blank">Clment Fuchs</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christophe De Vleeschouwer" target="_blank">Christophe De Vleeschouwer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ismail Ben Ayed" target="_blank">Ismail Ben Ayed</a>
            </p>
            <p id="summary-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="summary">The zero-shot capabilities of Vision-Language Models (VLMs) have been widely leveraged to improve predictive performance. However, previous works on transductive or test-time adaptation (TTA) often make strong assumptions about the data distribution, such as the presence of all classes. Our work challenges these favorable deployment scenarios, and introduces a more realistic evaluation framework, including: (i) a variable number of effective classes for adaptation within a single batch, and (ii) non-i.i.d. batches of test samples in online adaptation settings. We provide comprehensive evaluations, comparisons, and ablation studies that demonstrate how current transductive or TTA methods for VLMs systematically compromise the models initial zero-shot robustness across various realistic scenarios, favoring performance gains under advantageous assumptions about the test samples' distributions. Furthermore, we introduce Stat<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-76-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-343" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.253em, 1000.78em, 2.346em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-344"><span class="texatom" id="MathJax-Span-345"><span class="mrow" id="MathJax-Span-346"><span class="mi" id="MathJax-Span-347" style="font-family: MathJax_Caligraphic;">A<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-76">{\cal A}</script>, a versatile method that could handle a wide range of deployment scenarios, including those with a variable number of effective classes at test time. Our approach incorporates a novel regularization term designed specifically for VLMs, which acts as a statistical anchor preserving the initial text-encoder knowledge, particularly in low-data regimes. Code will be made available.</p>
            <p id="subjects-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="panel paper" keywords="negative,reneg,embeddings,embedding,text,reward,learned,guidance,zeroscope,sd1">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance_CVPR_2025_paper.html" target="_blank" title="295/388"><span class="index notranslate">#295</span></a>
                <a id="title-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="title-link" href="/venue/Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" target="_blank">ReNeg: Learning Negative Embedding with Reward Guidance</a>
                <a id="pdf-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaomin Li" target="_blank">Xiaomin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yixuan Liu" target="_blank">Yixuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Takashi Isobe" target="_blank">Takashi Isobe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Jia" target="_blank">Xu Jia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qinpeng Cui" target="_blank">Qinpeng Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Zhou" target="_blank">Dong Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Li" target="_blank">Dong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=You He" target="_blank">You He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huchuan Lu" target="_blank">Huchuan Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongdao Wang" target="_blank">Zhongdao Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Emad Barsoum" target="_blank">Emad Barsoum</a>
            </p>
            <p id="summary-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="summary">In text-to-image (T2I) generation applications, negative embeddings have proven to be a simple yet effective approach for enhancing generation quality. Typically, these negative embeddings are derived from user-defined negative prompts, which, while being functional, are not necessarily optimal.In this paper, we introduce ReNeg, an end-to-end method designed to learn improved Negative embeddings guided by a Reward model. We employ a reward feedback learning framework and integrate classifier-free guidance (CFG) into the training process, which was previously utilized only during inference, thus enabling the effective learning of negative embeddings.We also propose two strategies for learning both global and per-sample negative embeddings. Extensive experiments show that the learned negative embedding significantly outperforms null-text and handcrafted counterparts, achieving substantial improvements in human preference alignment. Additionally, the negative embedding learned within the same text embedding space exhibits strong generalization capabilities.For example, using the same CLIP text encoder, the negative embedding learned on SD1.5 can be seamlessly transferred to text-to-image or even text-to-video models such as ControlNet, ZeroScope, and VideCrafter2, resulting in consistent performance improvements across the board. Code and learned negative embeddings will be released.</p>
            <p id="subjects-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" onclick="foldPdfKimi('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="panel paper" keywords="casp,compression,multimodal,quantization,lmms,bit,sparsity,attention,aqlm,technique">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity_CVPR_2025_paper.html" target="_blank" title="296/388"><span class="index notranslate">#296</span></a>
                <a id="title-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="title-link" href="/venue/Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" target="_blank">CASP: Compression of Large Multimodal Models Based on Attention Sparsity</a>
                <a id="pdf-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Mohsen Gholami" target="_blank">Mohsen Gholami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mohammad Akbari" target="_blank">Mohammad Akbari</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Cannons" target="_blank">Kevin Cannons</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Zhang" target="_blank">Yong Zhang</a>
            </p>
            <p id="summary-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="summary">In this work, we propose an extreme compression technique for Large Multimodal Models (LMMs). While previous studies have explored quantization as an efficient post-training compression method for Large Language Models (LLMs), low-bit compression for multimodal models remains under-explored. The redundant nature of inputs in multimodal models results in a highly sparse attention matrix. We theoretically and experimentally demonstrate that the attention matrix's sparsity bounds the compression error of the Query and Key weight matrices. Based on this, we introduce CASP, a model compression technique for LMMs. Our approach performs a data-aware low-rank decomposition on the Query and Key weight matrix, followed by quantization across all layers based on an optimal bit allocation process. CASP is compatible with any quantization technique and enhances state-of-the-art 2-bit quantization methods (AQLM and QuIP\#) by an average of 21% on image- and video-language benchmarks. The code is provided in the supplementary materials.</p>
            <p id="subjects-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" onclick="foldPdfKimi('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="panel paper" keywords="robot,joint,pose,robopepp,angle,encoder,estimation,occlusions,joints,keypoint">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding_CVPR_2025_paper.html" target="_blank" title="297/388"><span class="index notranslate">#297</span></a>
                <a id="title-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="title-link" href="/venue/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" target="_blank">RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training</a>
                <a id="pdf-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Raktim Gautam Goswami" target="_blank">Raktim Gautam Goswami</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Prashanth Krishnamurthy" target="_blank">Prashanth Krishnamurthy</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yann LeCun" target="_blank">Yann LeCun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Farshad Khorrami" target="_blank">Farshad Khorrami</a>
            </p>
            <p id="summary-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="summary">Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robots physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robots physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robots joints and pre-train an encoder-predictor model to infer the joints embeddings from surrounding unmasked regions, enhancing the encoders understanding of the robots physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.</p>
            <p id="subjects-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" onclick="foldPdfKimi('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="panel paper" keywords="video,motion,ema,mllm,gop,motionbench,otion,aware,fficient,compressed">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Efficient_Motion-Aware_Video_MLLM_CVPR_2025_paper.html" target="_blank" title="298/388"><span class="index notranslate">#298</span></a>
                <a id="title-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="title-link" href="/venue/Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" target="_blank">Efficient Motion-Aware Video MLLM</a>
                <a id="pdf-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_Efficient_Motion-Aware_Video_MLLM_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zijia Zhao" target="_blank">Zijia Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuqi Huo" target="_blank">Yuqi Huo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tongtian Yue" target="_blank">Tongtian Yue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longteng Guo" target="_blank">Longteng Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyu Lu" target="_blank">Haoyu Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bingning Wang" target="_blank">Bingning Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weipeng Chen" target="_blank">Weipeng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jing Liu" target="_blank">Jing Liu</a>
            </p>
            <p id="summary-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="summary">Most current video MLLMs rely on uniform frame sampling and image-level encoders, resulting in inefficient data processing and limited motion awareness. To address these challenges, we introduce **EMA**, an **E**fficient **M**otion-**A**ware video MLLM that utilizes compressed video structures as inputs. We propose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and motion information within a GOP unit in the compressed video stream, generating compact, informative visual tokens. By integrating fewer but denser RGB frames with more but sparser motion vectors in this native slow-fast input architecture, our approach reduces redundancy and enhances motion representation. Additionally, we introduce MotionBench, a benchmark for evaluating motion understanding across four motion types: linear, curved, rotational, and contact-based. Experimental results show that EMA achieves state-of-the-art performance on both MotionBench and popular video question answering benchmarks, while reducing inference costs. Moreover, EMA demonstrates strong scalability, as evidenced by its competitive performance on long video understanding benchmarks.</p>
            <p id="subjects-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" onclick="foldPdfKimi('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="panel paper" keywords="diffcam,saliency,xai,feature,dnns,maps,decision,importance,reference,capturing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences_CVPR_2025_paper.html" target="_blank" title="299/388"><span class="index notranslate">#299</span></a>
                <a id="title-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="title-link" href="/venue/Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" target="_blank">DiffCAM: Data-Driven Saliency Maps by Capturing Feature Differences</a>
                <a id="pdf-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingjian Li" target="_blank">Xingjian Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiming Zhao" target="_blank">Qiming Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Neelesh Bisht" target="_blank">Neelesh Bisht</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mostofa Rafid Uddin" target="_blank">Mostofa Rafid Uddin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Yu Kim" target="_blank">Jin Yu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bryan Zhang" target="_blank">Bryan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Min Xu" target="_blank">Min Xu</a>
            </p>
            <p id="summary-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="summary">In recent years, the interpretability of Deep Neural Networks (DNNs) has garnered significant attention, particularly due to their widespread deployment in critical domains like healthcare, finance, and autonomous systems. To address the challenge of understanding how DNNs make decisions, Explainable AI (XAI) methods, such as saliency maps, have been developed to provide insights into the inner workings of these models. This paper introduces DiffCAM, a novel XAI method designed to overcome limitations in existing Class Activation Map (CAM)-based techniques, which often rely on decision boundary gradients to estimate feature importance. DiffCAM differentiates itself by considering the actual data distribution of the reference class, identifying feature importance based on how a target example differs from reference examples. This approach captures the most discriminative features without relying on decision boundaries or prediction results, making DiffCAM applicable to a broader range of models, including foundation models. Through extensive experiments, we demonstrate the superior performance and flexibility of DiffCAM in providing meaningful explanations across diverse datasets and scenarios.</p>
            <p id="subjects-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" onclick="foldPdfKimi('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="panel paper" keywords="defect,defectfill,inpainting,inspection,realistic,images,generation,diffusion,visual,mvtec">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual_CVPR_2025_paper.html" target="_blank" title="300/388"><span class="index notranslate">#300</span></a>
                <a id="title-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="title-link" href="/venue/Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" target="_blank">DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection</a>
                <a id="pdf-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jaewoo Song" target="_blank">Jaewoo Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daemin Park" target="_blank">Daemin Park</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kanghyun Baek" target="_blank">Kanghyun Baek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangyub Lee" target="_blank">Sangyub Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jooyoung Choi" target="_blank">Jooyoung Choi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eunji Kim" target="_blank">Eunji Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungroh Yoon" target="_blank">Sungroh Yoon</a>
            </p>
            <p id="summary-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="summary">Developing effective visual inspection models remains challenging due to the scarcity of defect data, especially in new or low-defect-rate manufacturing processes. While recent approaches have attempted to generate defect images using image generation models, producing highly realistic defects remains difficult. In this paper, we propose DefectFill, a novel method for realistic defect generation that requires only a few reference defect images. DefectFill leverages a fine-tuned inpainting diffusion model, optimized with our custom loss functions that incorporate defect, object, and cross-attention terms. This approach enables the inpainting diffusion model to precisely capture detailed, localized defect features and seamlessly blend them into defect-free objects. Additionally, we introduce the Low-Fidelity Selection method to further enhance the quality of the generated defect samples. Experiments demonstrate that DefectFill can generate high-quality defect images, and visual inspection models trained on these images achieve state-of-the-art performance on the MVTec AD dataset.</p>
            <p id="subjects-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" onclick="foldPdfKimi('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="panel paper" keywords="exposure,ultrafusion,fusion,exposed,hdr,dynamic,stops,tone,lighting,high">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion_CVPR_2025_paper.html" target="_blank" title="301/388"><span class="index notranslate">#301</span></a>
                <a id="title-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="title-link" href="/venue/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" target="_blank">UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion</a>
                <a id="pdf-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zixuan Chen" target="_blank">Zixuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujin Wang" target="_blank">Yujin Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Cai" target="_blank">Xin Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan You" target="_blank">Zhiyuan You</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheming Lu" target="_blank">Zheming Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Zhang" target="_blank">Fan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shi Guo" target="_blank">Shi Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianfan Xue" target="_blank">Tianfan Xue</a>
            </p>
            <p id="summary-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="summary">Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion technique, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference, normally 3-4 stops. When applying to very high dynamic scenes where a large exposure difference is required, this approach often fails due to incorrect alignment or inconsistent lighting between inputs, or tone mapping artifacts. In this work, we propose UltraFusion, the first exposure fusion technique that can merge input with 9 stops differences. The key idea is that we model the exposure fusion as a guided inpainting problem, where the under-exposed image is used as a guidance to fill the missing information of over-exposed highlight in the over-exposed region. Using under-exposed image as a soft guidance, instead of a hard constrain, our model is robust to potential alignment issue or lighting variations. Moreover, utilizing the image prior of the generative model, our model also generates natural tone mapping, even for very high-dynamic range scene. Our approach outperforms HDR-Transformer on latest HDR benchmarks. Moreover, to test its performance in ultra high dynamic range scene, we capture a new real-world exposure fusion benchmark, UltraFusion dataset, with exposure difference up to 9 stops, and experiments show that \model~can generate beautiful and high-quality fusion results under various scenarios.</p>
            <p id="subjects-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" onclick="foldPdfKimi('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="panel paper" keywords="ncfd,minmax,characteristic,distillation,distributional,discrepancy,neural,matching,synthetic,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective_CVPR_2025_paper.html" target="_blank" title="302/388"><span class="index notranslate">#302</span></a>
                <a id="title-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="title-link" href="/venue/Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" target="_blank">Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</a>
                <a id="pdf-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shaobo Wang" target="_blank">Shaobo Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yicun Yang" target="_blank">Yicun Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Liu" target="_blank">Zhiyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenghao Sun" target="_blank">Chenghao Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuming Hu" target="_blank">Xuming Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Conghui He" target="_blank">Conghui He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linfeng Zhang" target="_blank">Linfeng Zhang</a>
            </p>
            <p id="summary-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="summary">Dataset distillation has emerged as a powerful approach for reducing data requirements in deep learning. Among various methods, distribution matching-based approaches stand out for their balance of computational efficiency and strong performance. However, existing distance metrics used in distribution matching often fail to accurately capture distributional differences, leading to unreliable measures of discrepancy. In this paper, we reformulate dataset distillation as a minmax optimization problem and introduce Neural Characteristic Function Discrepancy (NCFD), a comprehensive and theoretically grounded metric for measuring distributional differences. NCFD leverages the Characteristic Function (CF) to encapsulate full distributional information, employing a neural network to optimize the sampling strategy for the CF's frequency arguments, thereby maximizing the discrepancy to enhance distance estimation. Simultaneously, we minimize the difference between real and synthetic data under this optimized NCFD measure. Our approach, termed Neural Characteristic Function Matching (NCFM), inherently aligns the phase and amplitude of neural features in the complex plane for both real and synthetic data, achieving a balance between realism and diversity in synthetic samples. Experiments demonstrate that our method achieves significant performance gains over state-of-the-art methods on both low- and high-resolution datasets. Notably, we achieve a 20.5\% accuracy boost on ImageSquawk. Our method also reduces GPU memory usage by over 300<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-77-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-348" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-349"><span class="mo" id="MathJax-Span-350" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-77">\times</script> and achieves 20<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-78-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-351" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-352"><span class="mo" id="MathJax-Span-353" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-78">\times</script> faster processing speeds compared to state-of-the-art methods. To the best of our knowledge, this is the first work to achieve lossless compression of CIFAR-100 on a single NVIDIA 2080 Ti GPU using only 2.3 GB of memory. *Code is provided in the supplementary material.*</p>
            <p id="subjects-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="panel paper" keywords="gaussian,mlps,avatar,human,interpolation,avatars,appearance,basis,distributed,fidelity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially_CVPR_2025_paper.html" target="_blank" title="303/388"><span class="index notranslate">#303</span></a>
                <a id="title-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="title-link" href="/venue/Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" target="_blank">Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs</a>
                <a id="pdf-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Youyi Zhan" target="_blank">Youyi Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianjia Shao" target="_blank">Tianjia Shao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yin Yang" target="_blank">Yin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kun Zhou" target="_blank">Kun Zhou</a>
            </p>
            <p id="summary-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="summary">Many works have succeeded in reconstructing Gaussian human avatars from multi-view videos. However, they either struggle to capture pose-dependent appearance details with a single MLP, or rely on a computationally intensive neural network to reconstruct high-fidelity appearance but with rendering performance degraded to non-real-time. We propose a novel Gaussian human avatar representation that can reconstruct high-fidelity pose-dependence appearance with details and meanwhile can be rendered in real time. Our Gaussian avatar is empowered by spatially distributed MLPs which are explicitly located on different positions on human body. The parameters stored in each Gaussian are obtained by interpolating from the outputs of its nearby MLPs based on their distances. To avoid undesired smooth Gaussian property changing during interpolation, for each Gaussian we define a set of Gaussian offset basis, and a linear combination of basis represents the Gaussian property offsets relative to the neutral properties. Then we propose to let the MLPs output a set of coefficients corresponding to the basis. In this way, although Gaussian coefficients are derived from interpolation and change smoothly, the Gaussian offset basis is learned freely without constraints. The smoothly varying coefficients combined with freely learned basis can still produce distinctly different Gaussian property offsets, allowing the ability to learn high-frequency spatial signals. We further use control points to constrain the Gaussians distributed on a surface layer rather than allowing them to be irregularly distributed inside the body, to help the human avatar generalize better when animated under novel poses. Compared to the state-of-the-art method, our method achieves better appearance quality with finer details while the rendering speed is significantly faster under novel views and novel poses.</p>
            <p id="subjects-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" onclick="foldPdfKimi('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="panel paper" keywords="question,avqa,experts,tiger,temporal,audio,answering,gaussian,frames,consecutive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering_CVPR_2025_paper.html" target="_blank" title="304/388"><span class="index notranslate">#304</span></a>
                <a id="title-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="title-link" href="/venue/Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" target="_blank">Question-Aware Gaussian Experts for Audio-Visual Question Answering</a>
                <a id="pdf-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongyeob Kim" target="_blank">Hongyeob Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Inyoung Jung" target="_blank">Inyoung Jung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dayoon Suh" target="_blank">Dayoon Suh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youjia Zhang" target="_blank">Youjia Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sangmin Lee" target="_blank">Sangmin Lee</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sungeun Hong" target="_blank">Sungeun Hong</a>
            </p>
            <p id="summary-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="summary">Audio-Visual Question Answering (AVQA) requires not only question-based multimodal reasoning but also precise temporal grounding to capture subtle dynamics for accurate prediction. However, existing methods mainly use question information implicitly, limiting focus on question-specific details. Furthermore, most studies rely on uniform frame sampling, which can miss key question-relevant frames. Although recent Top-K frame selection methods aim to address this, their discrete nature still overlooks fine-grained temporal details. This paper proposes QA-TIGER, a novel framework that explicitly incorporates question information and models continuous temporal dynamics. Our key idea is to use Gaussian-based modeling to adaptively focus on both consecutive and non-consecutive frames based on the question, while explicitly injecting question information and applying progressive refinement. We leverage a Mixture of Experts (MoE) to flexibly implement multiple Gaussian models, activating temporal experts specifically tailored to the question. Extensive experiments on multiple AVQA benchmarks show that QA-TIGER consistently achieves state-of-the-art performance.</p>
            <p id="subjects-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" onclick="foldPdfKimi('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="panel paper" keywords="style,mamba,samam,transfer,receptive,aware,ssm,ssms,global,stylized">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer_CVPR_2025_paper.html" target="_blank" title="305/388"><span class="index notranslate">#305</span></a>
                <a id="title-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="title-link" href="/venue/Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" target="_blank">SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer</a>
                <a id="pdf-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hongda Liu" target="_blank">Hongda Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Longguang Wang" target="_blank">Longguang Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ye Zhang" target="_blank">Ye Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziru Yu" target="_blank">Ziru Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulan Guo" target="_blank">Yulan Guo</a>
            </p>
            <p id="summary-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="summary">Global effective receptive field plays a crucial role for image style transfer (ST) to obtain high-quality stylized results. However, existing ST backbones (e.g., CNNs and Transformers) suffer huge computational complexity to achieve global receptive fields. Recently, the State Space Model (SSM), especially the improved variant Mamba, has shown great potential for long-range dependency modeling with linear complexity, which offers a approach to resolve the above dilemma. In this paper, we develop a Mamba-based style transfer framework, termed SaMam. Specifically, a mamba encoder is designed to efficiently extract content and style information. In addition, a style-aware mamba decoder is developed to flexibly adapt to various styles. Moreover, to address the problems of local pixel forgetting, channel redundancy and spatial discontinuity of existing SSMs, we introduce both local enhancement and zigzag scan. Qualitative and quantitative results demonstrate that our SaMam outperforms state-of-the-art methods in terms of both accuracy and efficiency.</p>
            <p id="subjects-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" onclick="foldPdfKimi('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="panel paper" keywords="medical,counterfactual,drifting,diffusion,latent,models,image,datasets,generation,issues">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis_CVPR_2025_paper.html" target="_blank" title="306/388"><span class="index notranslate">#306</span></a>
                <a id="title-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="title-link" href="/venue/Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" target="_blank">Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis</a>
                <a id="pdf-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF">8</sup>]</a>
                <a id="copy-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yousef Yeganeh" target="_blank">Yousef Yeganeh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Azade Farshad" target="_blank">Azade Farshad</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ioannis Charisiadis" target="_blank">Ioannis Charisiadis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marta Hasny" target="_blank">Marta Hasny</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Martin Hartenberger" target="_blank">Martin Hartenberger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bjrn Ommer" target="_blank">Bjrn Ommer</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nassir Navab" target="_blank">Nassir Navab</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ehsan Adeli" target="_blank">Ehsan Adeli</a>
            </p>
            <p id="summary-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="summary">Scaling by training on large datasets has been shown to enhance the quality and fidelity of image generation and manipulation with diffusion models; however, such large datasets are not always accessible in medical imaging due to cost and privacy issues, which contradicts one of the main applications of such models to produce synthetic samples where real data is scarce. Also, finetuning on pre-trained general models has been a challenge due to the distribution shift between the medical domain and the pre-trained models. Here, we propose Latent Drift (LD) for diffusion models that can be adopted for any fine-tuning method to mitigate the issues faced by the distribution shift or employed in inference time as a condition. Latent Drifting enables diffusion models to be conditioned for medical images fitted for the complex task of counterfactual image generation, which is crucial to investigate how parameters such as gender, age, and adding or removing diseases in a patient would alter the medical images. We evaluate our method on three public longitudinal benchmark datasets of brain MRI and chest X-rays for counterfactual image generation. Our results demonstrate significant performance gains in various scenarios when combined with different fine-tuning schemes. The source code of this work will be publicly released upon its acceptance.</p>
            <p id="subjects-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" onclick="foldPdfKimi('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="panel paper" keywords="crossflow,cross,matching,modal,noise,flow,mapping,distribution,modality,media">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality_CVPR_2025_paper.html" target="_blank" title="307/388"><span class="index notranslate">#307</span></a>
                <a id="title-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="title-link" href="/venue/Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" target="_blank">Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution</a>
                <a id="pdf-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qihao Liu" target="_blank">Qihao Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Yin" target="_blank">Xi Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Yuille" target="_blank">Alan Yuille</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Brown" target="_blank">Andrew Brown</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mannat Singh" target="_blank">Mannat Singh</a>
            </p>
            <p id="summary-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="summary">Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.</p>
            <p id="subjects-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" onclick="foldPdfKimi('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="panel paper" keywords="cubify,cutr,anything,rgb,handheld,indoor,commodity,object,objects,inductive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection_CVPR_2025_paper.html" target="_blank" title="308/388"><span class="index notranslate">#308</span></a>
                <a id="title-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="title-link" href="/venue/Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" target="_blank">Cubify Anything: Scaling Indoor 3D Object Detection</a>
                <a id="pdf-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Justin Lazarow" target="_blank">Justin Lazarow</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Griffiths" target="_blank">David Griffiths</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gefen Kohavi" target="_blank">Gefen Kohavi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francisco Crespo" target="_blank">Francisco Crespo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Afshin Dehghan" target="_blank">Afshin Dehghan</a>
            </p>
            <p id="summary-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="summary">We consider indoor 3D object detection with respect to a single RGB(-D) frame acquired from a commodity handheld device. We seek to significantly advance the status quo with respect to both data and modeling. First, we establish that existing datasets have significant limitations to scale, accuracy, and diversity of objects. As a result, we introduce the **Cubify-Anything 1M (CA-1M) dataset**, which exhaustively labels over 400K 3D objects on over 1K highly accurate laser-scanned scenes with near-perfect registration to over 3.5K handheld, egocentric captures. Next, we establish **Cubify Transformer (CuTR)**, a fully Transformer 3D object detection baseline which rather than operating in 3D on point or voxel-based representations, predicts 3D boxes directly from 2D features derived from RGB(-D) inputs. While this approach lacks any 3D inductive biases, we show that paired with CA-1M, CuTR outperforms point-based methods on CA-1M - accurately recalling over 62% of objects in 3D, and is significantly more capable at handling noise and uncertainty present in commodity LiDAR-derived depth maps while also providing promising RGB only performance without architecture changes. Furthermore, by pre-training on CA-1M, CuTR can outperform point-based methods on a more diverse variant of SUN RGB-D - supporting the notion that while inductive biases in 3D are useful at the smaller sizes of existing datasets, they fail to scale to the data-rich regime of CA-1M. Overall, this dataset and baseline model provide strong evidence that we are moving towards models which can effectively **Cubify Anything**.</p>
            <p id="subjects-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" onclick="foldPdfKimi('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="panel paper" keywords="mosca,scaffolds,videos,dynamic,motion,casual,casually,gaussian,encodedby,scaffold">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion_CVPR_2025_paper.html" target="_blank" title="309/388"><span class="index notranslate">#309</span></a>
                <a id="title-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="title-link" href="/venue/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" target="_blank">MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds</a>
                <a id="pdf-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui Lei" target="_blank">Jiahui Lei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yijia Weng" target="_blank">Yijia Weng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adam W. Harley" target="_blank">Adam W. Harley</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas Guibas" target="_blank">Leonidas Guibas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kostas Daniilidis" target="_blank">Kostas Daniilidis</a>
            </p>
            <p id="summary-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="summary">We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models and lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions/deformations. The scene geometry and appearance are then disentangled from the deformation field and are encodedby globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera focal length and poses can be solved using bundle adjustment without the need of any other pose estimation tools.Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks and its effectiveness on real videos.</p>
            <p id="subjects-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" onclick="foldPdfKimi('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="panel paper" keywords="inceventgs,camera,splatting,event,representation,motion,scene,gaussian,tracker,rgb">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera_CVPR_2025_paper.html" target="_blank" title="310/388"><span class="index notranslate">#310</span></a>
                <a id="title-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="title-link" href="/venue/Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" target="_blank">IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera</a>
                <a id="pdf-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Huang" target="_blank">Jian Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengrui Dong" target="_blank">Chengrui Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xuanhua Chen" target="_blank">Xuanhua Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peidong Liu" target="_blank">Peidong Liu</a>
            </p>
            <p id="summary-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="summary">Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for novel view synthesis have achieved remarkable progress with frame-based camera (e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel type of bio-inspired visual sensor, \ie event camera, has demonstrated advantages in high temporal resolution, high dynamic range, low power consumption and low latency, which make it being favored for many robotic applications. In this work, we present IncEventGS, an incremental 3D Gaussian Splatting reconstruction algorithm with a single event camera, without the assumption of known camera poses. To recover the 3D scene representation incrementally, we exploit the tracking and mapping paradigm of conventional SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker first estimates an initial camera motion based on prior reconstructed 3D-GS scene representation. The mapper then jointly refines both the 3D scene representation and camera motion based on the previously estimated motion trajectory from the tracker. The experimental results demonstrate that IncEventGS delivers superior performance compared to prior NeRF-based methods and other related baselines, even we do not have the ground-truth camera poses. Furthermore, our method can also deliver better performance compared to state-of-the-art event visual odometry methods in terms of camera motion estimation.</p>
            <p id="subjects-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" onclick="foldPdfKimi('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="panel paper" keywords="video,fiction,person,future,interaction,exo4d,interact,ungrounded,cabinet,fridge">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video_CVPR_2025_paper.html" target="_blank" title="311/388"><span class="index notranslate">#311</span></a>
                <a id="title-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="title-link" href="/venue/Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" target="_blank">FIction: 4D Future Interaction Prediction from Video</a>
                <a id="pdf-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kumar Ashutosh" target="_blank">Kumar Ashutosh</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Georgios Pavlakos" target="_blank">Georgios Pavlakos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kristen Grauman" target="_blank">Kristen Grauman</a>
            </p>
            <p id="summary-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="summary">Anticipating how a person will interact with objects in an environment is essential for activity understanding, but existing methods are limited to the 2D space of video framescapturing physically ungrounded predictions of what and ignoring the where and how. We introduce 4D future interaction prediction from videos. Given an input video of a human activity, the goal is to predict what objects at what 3D locations the person will interact with in the next time period (e.g., cabinet, fridge), and how they will execute that interaction (e.g., poses for bending, reaching, pulling). We propose a novel model FICTION that fuses the past video observation of the persons actions and their environment to predict both the where and how of future interactions. Through comprehensive experiments on a variety of activities and real-world environments in Ego-Exo4D, we show that our proposed approach outperforms prior autoregressive and (lifted) 2D video models substantially, with more than 30% relative gains.</p>
            <p id="subjects-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" onclick="foldPdfKimi('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="panel paper" keywords="3dod,event,object,detection,cameras,camera,latency,bandwidth,dsec,pushing">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with_CVPR_2025_paper.html" target="_blank" title="312/388"><span class="index notranslate">#312</span></a>
                <a id="title-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="title-link" href="/venue/Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" target="_blank">Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras</a>
                <a id="pdf-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hoonhee Cho" target="_blank">Hoonhee Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jae-Young Kang" target="_blank">Jae-Young Kang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Youngho Kim" target="_blank">Youngho Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kuk-Jin Yoon" target="_blank">Kuk-Jin Yoon</a>
            </p>
            <p id="summary-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="summary">Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multi-modal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., LiDAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC-3DOD, which includes ground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark for event-based 3D detectors. Our code and dataset will be publicly available.</p>
            <p id="subjects-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" onclick="foldPdfKimi('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="panel paper" keywords="dpo,images,textbf,winning,human,image,preference,generating,realism,uman">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization_CVPR_2025_paper.html" target="_blank" title="313/388"><span class="index notranslate">#313</span></a>
                <a id="title-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="title-link" href="/venue/Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" target="_blank">Boost Your Human Image Generation Model via Direct Preference Optimization</a>
                <a id="pdf-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sanghyeon Na" target="_blank">Sanghyeon Na</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yonggyu Kim" target="_blank">Yonggyu Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hyunjoon Lee" target="_blank">Hyunjoon Lee</a>
            </p>
            <p id="summary-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="summary">Human image generation is a key focus in image synthesis due to its broad applications. However, generating high-quality human images remains challenging because even slight inaccuracies in anatomy, pose, or fine details can compromise visual realism. To address these challenges, we explore Direct Preference Optimization (DPO), a method that trains models to generate images similar to preferred (winning) images while diverging from non-preferred (losing) ones. Conventional DPO approaches typically employ generated images as winning images, which may limit the model's ability to achieve high levels of realism. To overcome this limitation, we propose an enhanced DPO approach that incorporates high-quality real images as winning images, encouraging the model to produce outputs that resemble those real images rather than generated ones. Specifically, our approach, \textbf{HG-DPO} (\textbf{H}uman image \textbf{G}eneration through \textbf{DPO}), employs a novel curriculum learning framework that allows the model to gradually improve toward generating realistic human images, making the training more feasible than attempting the improvement all at once. Furthermore, we demonstrate that HG-DPO effectively adapts to personalized text-to-image tasks, generating high-quality, identity-specific images, which highlights the practical value of our approach.</p>
            <p id="subjects-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" onclick="foldPdfKimi('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="panel paper" keywords="client,layer,vlm,meta,peft,vision,updating,3ocus,federated,importance">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.html" target="_blank" title="314/388"><span class="index notranslate">#314</span></a>
                <a id="title-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="title-link" href="/venue/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" target="_blank">F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics</a>
                <a id="pdf-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Pramit Saha" target="_blank">Pramit Saha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Felix Wagner" target="_blank">Felix Wagner</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Divyanshu Mishra" target="_blank">Divyanshu Mishra</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Can Peng" target="_blank">Can Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Anshul Thakur" target="_blank">Anshul Thakur</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David A. Clifton" target="_blank">David A. Clifton</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Konstantinos Kamnitsas" target="_blank">Konstantinos Kamnitsas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=J. Alison Noble" target="_blank">J. Alison Noble</a>
            </p>
            <p id="summary-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="summary">Effective training of large Vision-Language Models (VLMs) on resource-constrained client devices in Federated Learning (FL) requires the usage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we demonstrate the impact of two factors \textit{viz.}, client-specific layer importance score that selects the most important VLM layers for fine-tuning and inter-client layer diversity score that encourages diverse layer selection across clients for optimal VLM layer selection. We first theoretically motivate and leverage the principal eigenvalue magnitude of layerwise Neural Tangent Kernels and show its effectiveness as client-specific layer importance score. Next, we propose a novel layer updating strategy dubbed \textbf{F<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-79-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-354" style="width: 0.523em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.148em, 1000.42em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-355"><span class="msubsup" id="MathJax-Span-356"><span style="display: inline-block; position: relative; width: 0.419em; height: 0px;"><span style="position: absolute; clip: rect(3.857em, 1000em, 4.169em, -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-357"></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -2.497em; left: 0em;"><span class="mn" id="MathJax-Span-358" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-79">^3</script>OCUS} that jointly optimizes the layer importance and diversity factors by employing a data-free, multi-objective, meta-heuristic optimization on the server. We explore 5 different meta-heuristic algorithms and compare their effectiveness for selecting model layers and adapter layers towards PEFT-FL. Furthermore, we release a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9 modality-specific clients and utilize it to train and evaluate our method. Overall, we conduct more than 10,000 client-level experiments on 6 Vision-Language FL task settings involving 58 medical image datasets and 4 different VLM architectures of varying sizes to demonstrate the effectiveness of the proposed method.</p>
            <p id="subjects-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" onclick="foldPdfKimi('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="panel paper" keywords="div,egocentric,videos,dynamic,understanding,video,scene,environment,image,feature">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric_CVPR_2025_paper.html" target="_blank" title="315/388"><span class="index notranslate">#315</span></a>
                <a id="title-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="title-link" href="/venue/Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" target="_blank">DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos</a>
                <a id="pdf-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF">7</sup>]</a>
                <a id="rel-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Mur-Labadia" target="_blank">Lorenzo Mur-Labadia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Josechu Guerrero" target="_blank">Josechu Guerrero</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruben Martinez-Cantin" target="_blank">Ruben Martinez-Cantin</a>
            </p>
            <p id="summary-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="summary">Environment understanding in egocentric videos is an important step for applications like robotics, augmented reality and assistive technologies. These videos are characterized by dynamic interactions and a strong dependence on the wearers engagement with the environment. Traditional approaches often focus on isolated clips or fail to integrate rich semantic and geometric information, limiting scene comprehension. We introduce Dynamic Image-Video Feature Fields (DIV-FF), a framework that decomposes the egocentric scene into persistent, dynamic, and actor-based components while integrating both image and video-language features. Our model enables detailed segmentation, captures affordances, understands the surroundings and maintains consistent understanding over time. DIV-FF outperforms state-of-the-art methods, particularly in dynamically evolving scenarios, demonstrating its potential to advance long-term, spatio-temporal scene understanding.</p>
            <p id="subjects-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" onclick="foldPdfKimi('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="panel paper" keywords="vad,rgb,modality,modal,cues,modalities,poly,anomaly,backbones,inductor">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video_CVPR_2025_paper.html" target="_blank" title="316/388"><span class="index notranslate">#316</span></a>
                <a id="title-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="title-link" href="/venue/Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" target="_blank">Just Dance with pi! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</a>
                <a id="pdf-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Snehashis Majhi" target="_blank">Snehashis Majhi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Giacomo D'Amicantonio" target="_blank">Giacomo D'Amicantonio</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Antitza Dantcheva" target="_blank">Antitza Dantcheva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quan Kong" target="_blank">Quan Kong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lorenzo Garattoni" target="_blank">Lorenzo Garattoni</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gianpiero Francesca" target="_blank">Gianpiero Francesca</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Egor Bondarev" target="_blank">Egor Bondarev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francois Bremond" target="_blank">Francois Bremond</a>
            </p>
            <p id="summary-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="summary">Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we introduce the Poly-modal Induced framework for VAD: PI-VAD (or <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-80-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C0;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-359" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-360"><span class="mi" id="MathJax-Span-361" style="font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-80">\pi</script>-VAD), a novel approach that augments RGB representations by five additional modalities. Specifically, the modalities include sensitivity to fine-grained motion (Pose), three dimensional scene and entity representation (Depth), surrounding objects (Panoptic masks), global motion (optical flow), as well as language cues (VLM). Each modality represents an axis of a polygon, streamlined to add salient cues to RGB. <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-81-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C0;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-362" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-363"><span class="mi" id="MathJax-Span-364" style="font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-81">\pi</script>-VAD includes two plug-in modules, namely Pseudo-modality Generation module and Cross Modal Induction module, which generate modality-specific prototypical representation and, thereby, induce multi-modal information into RGB cues. These modules operate by performing anomaly-aware auxiliary tasks and necessitate five modality backbones -- only during training. Notably, <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-82-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C0;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-365" style="width: 0.732em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.565em, 1000.58em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-366"><span class="mi" id="MathJax-Span-367" style="font-family: MathJax_Math-italic;"><span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi></mi></math></span></span><script type="math/tex" id="MathJax-Element-82">\pi</script>-VAD achieves state-of-the-art accuracy on three prominent VAD datasets encompassing real-world scenarios, without requiring the computational overhead of five modality backbones at inference.</p>
            <p id="subjects-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" onclick="foldPdfKimi('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="panel paper" keywords="alien,latency,motion,task,prediction,implicit,neural,human,arbitrary,network">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary_CVPR_2025_paper.html" target="_blank" title="317/388"><span class="index notranslate">#317</span></a>
                <a id="title-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="title-link" href="/venue/Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" target="_blank">ALIEN: Implicit Neural Representations for Human Motion Prediction under Arbitrary Latency</a>
                <a id="pdf-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Wei" target="_blank">Dong Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoning Sun" target="_blank">Xiaoning Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xizhan Gao" target="_blank">Xizhan Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengxiang Hu" target="_blank">Shengxiang Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaijiang Sun" target="_blank">Huaijiang Sun</a>
            </p>
            <p id="summary-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="summary">We investigate a new task in human motion prediction, which aims to forecast future body poses from historically observed sequences while accounting for arbitrary latency. This differs from existing works that assume an ideal scenario where future motions can be ``instantaneously'' predicted, thereby neglecting time delays caused by network transmission and algorithm execution. Addressing this task requires tackling two key challenges: The length of latency period can vary significantly across samples; the prediction model must be efficient. In this paper, we propose ALIEN, which treats the motion as a continuous function parameterized by a neural network, enabling predictions under any latency condition. By incorporating Mamba-like linear attention as a hyper-network and designing subsequent low-rank modulation, ALIEN efficiently learns a set of implicit neural representation weights from the observed motion to encode instance-specific information. Additionally, our model integrates the primary motion prediction task with an extra-designed variable-delay pose reconstruction task in a unified multi-task learning framework, enhancing its ability to capture richer motion patterns. Extensive experiments demonstrate that our approach outperforms state-of-the-art baselines adapted for our new task, while maintaining competitive performance in traditional prediction setting.</p>
            <p id="subjects-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" onclick="foldPdfKimi('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="panel paper" keywords="depth,seurat,trajectories,tapvid,stereopsis,ambiguities,relative,infers,spatial,temporal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Seurat_From_Moving_Points_to_Depth_CVPR_2025_paper.html" target="_blank" title="318/388"><span class="index notranslate">#318</span></a>
                <a id="title-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="title-link" href="/venue/Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" target="_blank">Seurat: From Moving Points to Depth</a>
                <a id="pdf-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cho_Seurat_From_Moving_Points_to_Depth_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Seokju Cho" target="_blank">Seokju Cho</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiahui Huang" target="_blank">Jiahui Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seungryong Kim" target="_blank">Seungryong Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joon-Young Lee" target="_blank">Joon-Young Lee</a>
            </p>
            <p id="summary-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="summary">Accurate depth estimation from monocular videos remains challenging due to ambiguities inherent in single-view geometry, as crucial depth cues like stereopsis are absent. However, humans often perceive relative depth intuitively by observing variations in the size and spacing of objects as they move. Inspired by this, we propose a novel method that infers relative depth by examining the spatial relationships and temporal evolution of a set of tracked 2D trajectories. Specifically, we use off-the-shelf point tracking models to capture 2D trajectories. Then, our approach employs spatial and temporal transformers to process these trajectories and directly infer depth changes over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust zero-shot performance, generalizing effectively from synthetic to real-world datasets. Results indicate that our approach achieves temporally smooth, high-accuracy depth predictions across diverse domains.</p>
            <p id="subjects-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" onclick="foldPdfKimi('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="panel paper" keywords="nexusgs,3dgs,depth,epipolar,view,splatting,point,sparse,synthesis,flow">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D_CVPR_2025_paper.html" target="_blank" title="319/388"><span class="index notranslate">#319</span></a>
                <a id="title-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="title-link" href="/venue/Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" target="_blank">NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting</a>
                <a id="pdf-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yulong Zheng" target="_blank">Yulong Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zicheng Jiang" target="_blank">Zicheng Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengfeng He" target="_blank">Shengfeng He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yandu Sun" target="_blank">Yandu Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junyu Dong" target="_blank">Junyu Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Huaidong Zhang" target="_blank">Huaidong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Du" target="_blank">Yong Du</a>
            </p>
            <p id="summary-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="summary">Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably advanced photo-realistic novel view synthesis using images from densely spaced camera viewpoints. However, these methods struggle in few-shot scenarios due to limited supervision. In this paper, we present NexusGS, a 3DGS-based approach that enhances novel view synthesis from sparse-view images by directly embedding depth information into point clouds, without relying on complex manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our method introduces a novel point cloud densification strategy that initializes 3DGS with a dense point cloud, reducing randomness in point placement while preventing over-smoothing and overfitting. Specifically, NexusGS comprises three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses to compute accurate depth maps, while mitigating the inaccuracies often associated with optical flow. By incorporating epipolar depth priors, NexusGS ensures reliable dense point cloud coverage and supports stable 3DGS training under sparse-view conditions. Experiments demonstrate that NexusGS significantly enhances depth accuracy and rendering quality, surpassing state-of-the-art methods by a considerable margin. Furthermore, we validate the superiority of our generated point clouds by substantially boosting the performance of competing methods.</p>
            <p id="subjects-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" onclick="foldPdfKimi('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="panel paper" keywords="manual,checkmanual,appliance,manipulation,manuals,appliances,benchmark,electrical,based,bread">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation_CVPR_2025_paper.html" target="_blank" title="320/388"><span class="index notranslate">#320</span></a>
                <a id="title-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="title-link" href="/venue/Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" target="_blank">CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation</a>
                <a id="pdf-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxing Long" target="_blank">Yuxing Long</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiyao Zhang" target="_blank">Jiyao Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingjie Pan" target="_blank">Mingjie Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianshu Wu" target="_blank">Tianshu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taewhan Kim" target="_blank">Taewhan Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hao Dong" target="_blank">Hao Dong</a>
            </p>
            <p id="summary-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="summary">Correct use of electrical appliances has significantly improved human life quality. Unlike simple tools that can be manipulated with common sense, different parts of electrical appliances have specific functions defined by manufacturers. If we want the robot to heat bread by microwave, we should enable them to review the microwaves manual first. From the manual, it can learn about component functions, interaction methods, and representative task steps about appliances. However, previous manual-related works remain limited to question-answering tasks while existing manipulation researchers ignore the manual's important role and fail to comprehend multi-page manuals. In this paper, we propose the first manual-based appliance manipulation benchmark CheckManual. Specifically, we design a large model-assisted human-revised data generation pipeline to create manuals based on CAD appliance models. With these manuals, we establish novel manual-based manipulation challenges, metrics, and simulator environments for model performance evaluation. Furthermore, we propose the first manual-based manipulation planning model ManualPlan to set up a group of baselines for the CheckManual benchmark.</p>
            <p id="subjects-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" onclick="foldPdfKimi('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="panel paper" keywords="flexidit,compute,generate,denoising,conditioned,dits,diffusion,quality,samples,less">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with_CVPR_2025_paper.html" target="_blank" title="321/388"><span class="index notranslate">#321</span></a>
                <a id="title-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="title-link" href="/venue/Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" target="_blank">FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute</a>
                <a id="pdf-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sotiris Anagnostidis" target="_blank">Sotiris Anagnostidis</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gregor Bachmann" target="_blank">Gregor Bachmann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yeongmin Kim" target="_blank">Yeongmin Kim</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jonas Kohler" target="_blank">Jonas Kohler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Markos Georgopoulos" target="_blank">Markos Georgopoulos</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Artsiom Sanakoyeu" target="_blank">Artsiom Sanakoyeu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuming Du" target="_blank">Yuming Du</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Albert Pumarola" target="_blank">Albert Pumarola</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ali Thabet" target="_blank">Ali Thabet</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Edgar Schnfeld" target="_blank">Edgar Schnfeld</a>
            </p>
            <p id="summary-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="summary">Despite their remarkable performance, modern Diffusion Transformers (DiTs) are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into flexible ones --- dubbed FlexiDiT --- allowing them to process inputs at varying compute budgets. We demonstrate how a single flexible model can generate images without any drop in quality, while reducing the required FLOPs by more than <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-83-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;40&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-368" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-369"><span class="mn" id="MathJax-Span-370" style="font-family: MathJax_Main;">40</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>40</mn></math></span></span><script type="math/tex" id="MathJax-Element-83">40</script>\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-84-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;75&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-371" style="width: 1.201em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-372"><span class="mn" id="MathJax-Span-373" style="font-family: MathJax_Main;">75</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>75</mn></math></span></span><script type="math/tex" id="MathJax-Element-84">75</script>\% less compute without compromising performance.</p>
            <p id="subjects-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" onclick="foldPdfKimi('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="panel paper" keywords="tinyfusion,transformers,pruning,diffusion,pruned,shallow,tuning,learnable,fine,dits">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow_CVPR_2025_paper.html" target="_blank" title="322/388"><span class="index notranslate">#322</span></a>
                <a id="title-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="title-link" href="/venue/Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" target="_blank">TinyFusion: Diffusion Transformers Learned Shallow</a>
                <a id="pdf-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF">11</sup>]</a>
                <a id="copy-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF">9</sup>]</a>
                <a id="rel-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gongfan Fang" target="_blank">Gongfan Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kunjun Li" target="_blank">Kunjun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyin Ma" target="_blank">Xinyin Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinchao Wang" target="_blank">Xinchao Wang</a>
            </p>
            <p id="summary-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="summary">Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-85-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-374" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.513em, 1000.63em, 2.294em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-375"><span class="mo" id="MathJax-Span-376" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-85">\times</script> speedup with an FID score of 2.86, outperforming competitors with comparable efficiency.</p>
            <p id="subjects-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" onclick="foldPdfKimi('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="panel paper" keywords="nsd,imagery,mental,decoding,fmri,dataset,brain,vision,images,activity">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods_CVPR_2025_paper.html" target="_blank" title="323/388"><span class="index notranslate">#323</span></a>
                <a id="title-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="title-link" href="/venue/Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" target="_blank">NSD-Imagery: A Benchmark Dataset for Extending fMRI Vision Decoding Methods to Mental Imagery</a>
                <a id="pdf-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Reese Kneeland" target="_blank">Reese Kneeland</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul S. Scotti" target="_blank">Paul S. Scotti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ghislain St-Yves" target="_blank">Ghislain St-Yves</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jesse Breedlove" target="_blank">Jesse Breedlove</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kendrick Kay" target="_blank">Kendrick Kay</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thomas Naselaris" target="_blank">Thomas Naselaris</a>
            </p>
            <p id="summary-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="summary">We release NSD-Imagery, a benchmark dataset of human fMRI activity paired with mental images, to complement the existing Natural Scenes Dataset (NSD), a large-scale dataset of fMRI activity paired with seen images that enabled unprecedented improvements in fMRI-to-image reconstruction efforts. Recent models trained on NSD have been evaluated only on seen image reconstruction. Using NSD-Imagery, it is possible to assess how well these models perform on mental image reconstruction. This is a challenging generalization requirement because mental images are encoded in human brain activity with relatively lower signal-to-noise and spatial resolution; however, generalization from seen to mental imagery is critical for real-world applications in medical domains and brain-computer interfaces, where the desired information is always internally generated. We provide benchmarks for a suite of recent NSD-trained open-source visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.) on NSD-Imagery, and show that the performance of decoding methods on mental images is largely decoupled from performance on vision tasks. We further demonstrate that architectural choices significantly impact cross-decoding performance: models employing simple linear decoding architectures and multimodal feature decoding generalize better to mental imagery, while complex architectures tend to overfit training data recorded exclusively from vision. Our findings indicate that mental imagery datasets are critical for the development of practical applications, and establish NSD-Imagery as a useful resource for better aligning visual decoding methods with this goal.</p>
            <p id="subjects-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" onclick="foldPdfKimi('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="panel paper" keywords="forestlpr,bev,place,forests,heights,images,lidar,attentioning,recognition,sectional">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density_CVPR_2025_paper.html" target="_blank" title="324/388"><span class="index notranslate">#324</span></a>
                <a id="title-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="title-link" href="/venue/Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" target="_blank">ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images</a>
                <a id="pdf-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yanqing Shen" target="_blank">Yanqing Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Turcan Tuna" target="_blank">Turcan Tuna</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Hutter" target="_blank">Marco Hutter</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cesar Cadena" target="_blank">Cesar Cadena</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nanning Zheng" target="_blank">Nanning Zheng</a>
            </p>
            <p id="summary-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="summary">Place recognition is essential to maintain global consistency in large-scale localization systems. While research in urban environments has progressed significantly using LiDARs or cameras, applications in natural forest-like environments remain largely underexplored.Furthermore, forests present particular challenges due to high self-similarity and substantial variations in vegetation growth over time.In this work, we propose a robust LiDAR-based place recognition method for natural forests, ForestLPR. We hypothesize that a set of cross-sectional images of the forests geometry at different heights contains the information needed to recognize revisiting a place.The cross-sectional images are represented by birds-eye view (BEV) density images of horizontal slices of the point cloud at different heights. Our approach utilizes a visual transformer as the shared backbone to produce sets of local descriptors and introduces a multi-BEV interaction module to attend to information at different heights adaptively. It is followed by an aggregation layer that produces a rotation-invariant place descriptor. We evaluated the efficacy of our method extensively on real-world data from public benchmarks as well as robotic datasets and compared it against the state-of-the-art (SOTA) methods. The results indicate that ForestLPR has consistently good performance on all evaluations and achieves an average increase of 7.38\% and 9.11\% on Recall@1 over the closest competitor on intra-sequence loop closure detection and inter-sequence re-localization, respectively, validating our hypothesis.</p>
            <p id="subjects-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" onclick="foldPdfKimi('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="panel paper" keywords="concept,tide,domain,concepts,saliency,visually,training,correction,local,maps">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.html" target="_blank" title="325/388"><span class="index notranslate">#325</span></a>
                <a id="title-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="title-link" href="/venue/Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" target="_blank">TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction</a>
                <a id="pdf-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Aishwarya Agarwal" target="_blank">Aishwarya Agarwal</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Srikrishna Karanam" target="_blank">Srikrishna Karanam</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vineet Gandhi" target="_blank">Vineet Gandhi</a>
            </p>
            <p id="summary-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="summary">We consider the problem of single-source domain generalization. Existing methods typically rely on extensive augmentations to synthetically cover diverse domains during training. However, they struggle with semantic shifts (e.g., background and viewpoint changes), as they often learn global features instead of local concepts that tend to be domain invariant. To address this gap, we propose an approach that compels models to leverage such local concepts during prediction. Given no suitable dataset with per-class concepts and localization maps exists, we first develop a novel pipeline to generate annotations by exploiting the rich features of diffusion and large-language models. Our next innovation is TIDE, a novel training scheme with a concept saliency alignment loss that ensures model focus on the right per-concept regions and a local concept contrastive loss that promotes learning domain-invariant concept representations. This not only gives a robust model but also can be visually interpreted using the predicted concept saliency maps. Given these maps at test time, our final contribution is a new correction algorithm that uses the corresponding local concept representations to iteratively refine the prediction until it aligns with prototypical concept representations that we store at the end of model training. We evaluate our approach extensively on four standard DG benchmark datasets and substantially outperform the current state-of-the-art (12% improvement on average) while also demonstrating that our predictions can be visually interpreted.</p>
            <p id="subjects-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" onclick="foldPdfKimi('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="panel paper" keywords="hsi,gpt,scene,textbf,motion,language,tokens,purpose,modal,human">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction_CVPR_2025_paper.html" target="_blank" title="326/388"><span class="index notranslate">#326</span></a>
                <a id="title-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="title-link" href="/venue/Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" target="_blank">HSI-GPT: A General-Purpose Large Scene-Motion-Language Model for Human Scene Interaction</a>
                <a id="pdf-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Wang" target="_blank">Yuan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yali Li" target="_blank">Yali Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Li" target="_blank">Xiang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shengjin Wang" target="_blank">Shengjin Wang</a>
            </p>
            <p id="summary-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="summary">While flourishing developments have been witnessed in text-to-motion generation, synthesizing physically realistic, controllable, language-conditioned Human Scene Interactions (HSI) remains a relatively underexplored landscape. Current HSI methods naively rely on conditional Variational AutoEncoder (cVAE) and diffusion models. They are typically associated with \textbf{limited modalities of control signals} and \textbf{task-specific frameworks design}, leading to inflexible adaptation across various interaction scenarios and descriptive-unfaithful motions in diverse 3D physical environments. In this paper, we propose HSI-GPT, a General-Purpose \textbf{Large Scene-Motion-Language Model} that applies ``next-token prediction'' paradigm of Large Language Models to the HSI domain. HSI-GPT not only exhibits remarkable flexibility to accommodate diverse control signals (3D scenes, textual commands, key-frame poses, as well as scene affordances), but it seamlessly supports various HSI-related tasks (\textit{e.g}., multi-modal controlled HSI generation, HSI understanding, and general motion completion in 3D scenes). First, HSI-GPT quantizes textual descriptions and human motions into discrete, LLM-interpretable tokens with multi-modal tokenizers. Inspired by multi-modal learning, we develop a recipe for aligning mixed-modality tokens into the shared embedding space of LLMs. These interaction tokens are then organized into unified instruction following prompts, allowing our HSI-GPT to fine-tune on prompt-based question-and-answer tasks. Extensive experiments and visualizations validate that our general-purpose HSI-GPT model delivers exceptional performance across multiple HSI-related tasks.</p>
            <p id="subjects-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" onclick="foldPdfKimi('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="panel paper" keywords="startpoint,style,content,stylessp,layout,leakage,stage,training,sampling,transfer">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style_CVPR_2025_paper.html" target="_blank" title="327/388"><span class="index notranslate">#327</span></a>
                <a id="title-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="title-link" href="/venue/Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" target="_blank">StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer</a>
                <a id="pdf-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ruojun Xu" target="_blank">Ruojun Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Weijie Xi" target="_blank">Weijie Xi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=XiaoDi Wang" target="_blank">XiaoDi Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongbo Mao" target="_blank">Yongbo Mao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zach Cheng" target="_blank">Zach Cheng</a>
            </p>
            <p id="summary-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="summary">Training-free diffusion-based methods have achieved remarkable success in style transfer, eliminating the need for extensive training or fine-tuning. However, due to the lack of targeted training for style information extraction and constraints on the content image layout, training-free methods often suffer from layout changes of original content and content leakage from style images. Through a series of experiments, we discovered that an effective startpoint in the sampling stage significantly enhances the style transfer process. Based on this discovery, we propose StyleSSP, which focuses on obtaining a better startpoint to address layout changes of original content and content leakage from style image. StyleSSP comprises two key components: (1) Frequency Manipulation: To improve content preservation, we reduce the low-frequency components of the DDIM latent, allowing the sampling stage to pay more attention to the layout of content images; and (2) Negative Guidance via Inversion: To mitigate the content leakage from style image, we employ negative guidance in the inversion stage to ensure that the startpoint of the sampling stage is distanced from the content of style image. Experiments show that StyleSSP surpasses previous training-free style transfer baselines, particularly in preserving original content and minimizing the content leakage from style image.</p>
            <p id="subjects-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" onclick="foldPdfKimi('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="panel paper" keywords="traffic,regulations,driving,sign,vectorized,mapdr,rules,maps,integrating,autonomous">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.html" target="_blank" title="328/388"><span class="index notranslate">#328</span></a>
                <a id="title-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="title-link" href="/venue/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" target="_blank">Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map</a>
                <a id="pdf-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinyuan Chang" target="_blank">Xinyuan Chang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maixuan Xue" target="_blank">Maixuan Xue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinran Liu" target="_blank">Xinran Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zheng Pan" target="_blank">Zheng Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Wei" target="_blank">Xing Wei</a>
            </p>
            <p id="summary-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="summary">Ensuring adherence to traffic sign regulations is essential for both human and autonomous vehicle navigation. While current online mapping solutions often prioritize the construction of the geometric and connectivity layers of HD maps, overlooking the construction of the traffic regulation layer within HD maps. Addressing this gap, we introduce MapDR, a novel dataset designed for the extraction of Driving Rules from traffic signs and their association with vectorized, locally perceived HD Maps. MapDR features over <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-86-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;000&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-377" style="width: 3.544em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.305em, 1002.87em, 2.503em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-378"><span class="mn" id="MathJax-Span-379" style="font-family: MathJax_Main;">10</span><span class="mo" id="MathJax-Span-380" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-381" style="font-family: MathJax_Main; padding-left: 0.159em;">000</span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>10</mn><mo>,</mo><mn>000</mn></math></span></span><script type="math/tex" id="MathJax-Element-86">10,000</script> annotated video clips that capture the intricate correlation between traffic sign regulations and lanes. Built upon this benchmark and the newly defined task of integrating traffic regulations into online HD maps, we provide modular and end-to-end solutions: VLE-MEE and RuleVLM, offering a strong baseline for advancing autonomous driving technology. It fills a critical gap in the integration of traffic sign rules, contributing to the development of reliable autonomous driving systems.</p>
            <p id="subjects-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" onclick="foldPdfKimi('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="panel paper" keywords="text,typos,image,retouching,words,erroneous,rendering,retouch,regenerates,typographical">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation_CVPR_2025_paper.html" target="_blank" title="329/388"><span class="index notranslate">#329</span></a>
                <a id="title-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-link" href="/venue/Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" target="_blank">Type-R: Automatically Retouching Typos for Text-to-Image Generation</a>
                <a id="pdf-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wataru Shimoda" target="_blank">Wataru Shimoda</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Naoto Inoue" target="_blank">Naoto Inoue</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daichi Haraguchi" target="_blank">Daichi Haraguchi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hayato Mitani" target="_blank">Hayato Mitani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Seiichi Uchida" target="_blank">Seiichi Uchida</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kota Yamaguchi" target="_blank">Kota Yamaguchi</a>
            </p>
            <p id="summary-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="summary">While recent text-to-image models can generate photorealistic images from text prompts that reflect detailed instructions, they still face significant challenges in accurately rendering words in the image.In this paper, we propose to retouch erroneous text renderings in the post-processing pipeline.Our approach, called Type-R, identifies typographical errors in the generated image, erases the erroneous text, regenerates text boxes for missing words, and finally corrects typos in the rendered words.Through extensive experiments, we show that Type-R, in combination with the latest text-to-image models such as Stable Diffusion or Flux, achieves the highest text rendering accuracy while maintaining image quality and also outperforms text-focused generation baselines in terms of balancing text accuracy and image quality.</p>
            <p id="subjects-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="panel paper" keywords="prototype,mup,label,vss,ccp,semantic,tokens,loss,segmentation,prototypes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html" target="_blank" title="330/388"><span class="index notranslate">#330</span></a>
                <a id="title-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" target="_blank">Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation</a>
                <a id="pdf-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Songsong Duan" target="_blank">Songsong Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xi Yang" target="_blank">Xi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nannan Wang" target="_blank">Nannan Wang</a>
            </p>
            <p id="summary-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="summary">Existing Weakly Supervised Semantic Segmentation (WSSS) relies on the CNN-based Class Activation Map (CAM) and Transformer-based self-attention map to generate class-specific masks for semantic segmentation. However, CAM and self-attention maps usually cause incomplete segmentation due to classification bias issue. To address this issue, we propose a Multi-Label Prototype Visual Spatial Search (MuP-VSS) method with a spatial query mechanism, which learns a set of learnable class token vectors as queries to search the similarity visual tokens from image patch tokens. Specifically, MuP-VSS consists of two key components: \textbf{multi-label prototype representation} and \textbf{multi-label prototype optimization}. The former designs a global embedding to learn the global tokens from the images, and then proposes a Prototype Embedding Module (PEM) to interact with patch tokens to understand the local semantic information. The latter utilizes the exclusivity and consistency principles of the multi-label prototypes to design three prototype losses to optimize them, which contain cross-class prototype (CCP) contrastive loss, cross-image prototype (CIP) contrastive loss, and patch-to-prototype (P2P) consistency loss. CCP loss models exclusivity of multi-label prototypes learned from a single image to enhance the discriminative properties of each class better. CCP loss learns the consistency of the same class-specific prototypes extracted from multiple images to enhance the semantic consistency. P2P loss is proposed to control the semantic response of the prototype to the image patches. Experimental results on Pascal VOC 2012 and MS COCO show that MuP-VSS significantly outperforms recent methods and achieves state-of-the-art performance.</p>
            <p id="subjects-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="panel paper" keywords="codebook,text,alignment,aligned,image,long,hierarchical,learning,improved,modal">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long_CVPR_2025_paper.html" target="_blank" title="331/388"><span class="index notranslate">#331</span></a>
                <a id="title-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="title-link" href="/venue/Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" target="_blank">Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text</a>
                <a id="pdf-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Guotao Liang" target="_blank">Guotao Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baoquan Zhang" target="_blank">Baoquan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Wen" target="_blank">Zhiyuan Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junteng Zhao" target="_blank">Junteng Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunming Ye" target="_blank">Yunming Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kola Ye" target="_blank">Kola Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao He" target="_blank">Yao He</a>
            </p>
            <p id="summary-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="summary">Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level.In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning.However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.</p>
            <p id="subjects-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" onclick="foldPdfKimi('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="panel paper" keywords="ood,background,vlms,shortcut,irrelevant,vlm,images,misidentifying,decoupling,regions">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection_CVPR_2025_paper.html" target="_blank" title="332/388"><span class="index notranslate">#332</span></a>
                <a id="title-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-link" href="/venue/Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" target="_blank">Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection</a>
                <a id="pdf-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuo Xu" target="_blank">Zhuo Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiang Xiang" target="_blank">Xiang Xiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Liang" target="_blank">Yifan Liang</a>
            </p>
            <p id="summary-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="summary">Vision-language models (VLMs), such as CLIP, have shown remarkable capabilities in downstream tasks. However, the coupling of semantic information between the foreground and the background in images leads to significant shortcut issues that adversely affect out-of-distribution (OOD) detection abilities. When confronted with a background OOD sample, VLMs are prone to misidentifying it as in-distribution (ID) data. In this paper, we analyze the OOD problem from the perspective of shortcuts in VLMs and propose OSPCoOp which includes background decoupling and mask-guided region regularization. We first decouple images into ID-relevant and ID-irrelevant regions and utilize the latter to generate a large number of augmented OOD background samples as pseudo-OOD supervision. We then use the masks from background decoupling to adjust the model's attention, minimizing its focus on ID-irrelevant regions. To assess the model's robustness against background interference, we introduce a new OOD evaluation dataset, ImageNet-Bg, which solely consists of background images with all ID-relevant regions removed. Our method demonstrates exceptional performance in few-shot scenarios, achieving strong results even in one-shot setting, and outperforms existing methods.</p>
            <p id="subjects-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" onclick="foldPdfKimi('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="panel paper" keywords="autofocus,event,focus,hunting,focusing,evk4,driven,speed,davis346,laplacian">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bao_One-Step_Event-Driven_High-Speed_Autofocus_CVPR_2025_paper.html" target="_blank" title="333/388"><span class="index notranslate">#333</span></a>
                <a id="title-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="title-link" href="/venue/Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" target="_blank">One-Step Event-Driven High-Speed Autofocus</a>
                <a id="pdf-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Bao_One-Step_Event-Driven_High-Speed_Autofocus_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Bao" target="_blank">Yuhan Bao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohua Gao" target="_blank">Shaohua Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenyong Li" target="_blank">Wenyong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaiwei Wang" target="_blank">Kaiwei Wang</a>
            </p>
            <p id="summary-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="summary">High-speed autofocus in extreme scenes remains a significant challenge. Traditional methods rely on repeated sampling around the focus position, resulting in ''focus hunting''. Event-driven methods have advanced focusing speed and improved performance in low-light conditions; however, current approaches still require at least one lengthy round of ''focus hunting'', involving the collection of a complete focus stack. We introduce the Event Laplacian Product (ELP) focus detection function, which combines event data with grayscale Laplacian information, redefining focus search as a detection task. This innovation enables the first one-step event-driven autofocus, cutting focusing time by up to two-thirds and reducing focusing error by 24 times on the DAVIS346 dataset and 22 times on the EVK4 dataset. Additionally, we present an autofocus pipeline tailored for event-only cameras, achieving accurate results across a range of challenging motion and lighting conditions. All datasets and code will be made publicly available.</p>
            <p id="subjects-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" onclick="foldPdfKimi('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="panel paper" keywords="quantization,fim,lowbit,hessian,ptq,fima,fisher,prevalent,vit,dplr">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix_CVPR_2025_paper.html" target="_blank" title="334/388"><span class="index notranslate">#334</span></a>
                <a id="title-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="title-link" href="/venue/Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" target="_blank">FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation</a>
                <a id="pdf-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuguanyu Wu" target="_blank">Zhuguanyu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shihe Wang" target="_blank">Shihe Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Zhang" target="_blank">Jiayi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxin Chen" target="_blank">Jiaxin Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhong Wang" target="_blank">Yunhong Wang</a>
            </p>
            <p id="summary-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="summary">Network quantization, a prevalent technique for network compression, significantly reduces computational demands and memory usage, thereby facilitating the deployment of large-parameter models onto hardware with constrained resources. Post-training quantization (PTQ) stands out as a cost-effective and promising approach due to its avoidance of the need for retraining. Unfortunately, many current PTQ methods in Vision Transformer (ViT) exhibit a notable decrease in accuracy, especially in lowbit cases. To tackle these challenges, we analyze the extensively utilized Hessian-guided quantization loss, and uncover certain limitations within the approximated pre-activation Hessian. By deducing the relationship between KL divergence and Fisher information matrix (FIM), we develop a more refined approximation for FIM. Building on this, we introduce the Diagonal Plus Low-Rank FIM (DPLR) to achieve a more nuanced quantization loss. Our extensive experiments, conducted across various ViT-based architectures on public benchmark datasets, demonstrate that our quantization loss calculation surpasses the performance of the prevalent mean squared error (MSE) and approximated pre-activation Hessian, and outperform previous work in lowbit cases. Code will be released upon acceptance.</p>
            <p id="subjects-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" onclick="foldPdfKimi('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="panel paper" keywords="editing,blendergym,graphics,tasks,foundational,automating,vlms,scaling,generation,bottlenecked">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing_CVPR_2025_paper.html" target="_blank" title="335/388"><span class="index notranslate">#335</span></a>
                <a id="title-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="title-link" href="/venue/Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" target="_blank">BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</a>
                <a id="pdf-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF"></sup>]</a>
                <a id="copy-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yunqi Gu" target="_blank">Yunqi Gu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ian Huang" target="_blank">Ian Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jihyeon Je" target="_blank">Jihyeon Je</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guandao Yang" target="_blank">Guandao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas Guibas" target="_blank">Leonidas Guibas</a>
            </p>
            <p id="summary-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="summary">3D graphics editing is a crucial component in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating the process is challenging because graphical editing requires performing different tasks, each requiring distinct skill sets. Recently, multi-modal foundation models have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and real-world editing complexity. In this work, we present BlenderGym, a benchmark designed to systematically evaluate foundational model systems for 3D graphics editing with tasks capturing the various aspects of 3D editing and fixed ground-truth for evaluation. We evaluate closed- and open-source VLMs with BlenderGym and observe that even the state-of-the-art VLMs struggle with tasks relatively easily for a novice Blender user. Enabled by BlenderGym, we study how inference scaling techniques impact graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through scaling, complementing recent insights on scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.</p>
            <p id="subjects-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" onclick="foldPdfKimi('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="panel paper" keywords="diagram,text,generation,diagramagent,agent,diagrams,visuals,structured,code,modifiability">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for_CVPR_2025_paper.html" target="_blank" title="336/388"><span class="index notranslate">#336</span></a>
                <a id="title-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="title-link" href="/venue/Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" target="_blank">From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing</a>
                <a id="pdf-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingxuan Wei" target="_blank">Jingxuan Wei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Tan" target="_blank">Cheng Tan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qi Chen" target="_blank">Qi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaowei Wu" target="_blank">Gaowei Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyuan Li" target="_blank">Siyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhangyang Gao" target="_blank">Zhangyang Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Linzhuang Sun" target="_blank">Linzhuang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bihui Yu" target="_blank">Bihui Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruifeng Guo" target="_blank">Ruifeng Guo</a>
            </p>
            <p id="summary-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="summary">We introduce the task of text-to-diagram generation, which focuses on creating structured visual representations directly from textual descriptions. Existing approaches in text-to-image and text-to-code generation lack the logical organization and flexibility needed to produce accurate, editable diagrams, often resulting in outputs that are either unstructured or difficult to modify. To address this gap, we introduce DiagramGenBenchmark, a comprehensive evaluation framework encompassing eight distinct diagram categories, including flowcharts, model architecture diagrams, and mind maps. Additionally, we present DiagramAgent, an innovative framework with four core modulesPlan Agent, Code Agent, Check Agent, and Diagram-to-Code Agentdesigned to facilitate both the generation and refinement of complex diagrams. Our extensive experiments, which combine objective metrics with human evaluations, demonstrate that DiagramAgent significantly outperforms existing baseline models in terms of accuracy, structural coherence, and modifiability. This work not only establishes a foundational benchmark for the text-to-diagram generation task but also introduces a powerful toolset to advance research and applications in this emerging area.</p>
            <p id="subjects-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" onclick="foldPdfKimi('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="panel paper" keywords="identity,frequency,ipt2v,dit,video,facial,preserving,consisid,generation,features">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition_CVPR_2025_paper.html" target="_blank" title="337/388"><span class="index notranslate">#337</span></a>
                <a id="title-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="title-link" href="/venue/Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" target="_blank">Identity-Preserving Text-to-Video Generation by Frequency Decomposition</a>
                <a id="pdf-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF">7</sup>]</a>
                <a id="copy-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shenghai Yuan" target="_blank">Shenghai Yuan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinfa Huang" target="_blank">Jinfa Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianyi He" target="_blank">Xianyi He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunyang Ge" target="_blank">Yunyang Ge</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yujun Shi" target="_blank">Yujun Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liuhan Chen" target="_blank">Liuhan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiebo Luo" target="_blank">Jiebo Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Yuan" target="_blank">Li Yuan</a>
            </p>
            <p id="summary-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="summary">Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in the literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving Diffusion Transformer (DiT)-based control scheme. To achieve these goals, we propose **ConsisID**, a tuning-free DiT-based controllable IPT2V model to keep human-**id**entity **consis**tent in the generated video. Inspired by prior findings in frequency analysis of vision/diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features (e.g., profile, proportions) and high-frequency intrinsic features (e.g., identity markers that remain unaffected by pose changes). First, from a low-frequency perspective, we introduce a global facial extractor, which encodes the reference image and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into the shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into the transformer blocks, enhancing the model's ability to preserve fine-grained features. To leverage the frequency information for identity preservation, we propose a hierarchical training strategy, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our **ConsisID** achieves excellent results in generating high-quality, identity-preserving videos, making strides towards more effective IPT2V.</p>
            <p id="subjects-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" onclick="foldPdfKimi('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="panel paper" keywords="rlaif,mllms,feedback,trustworthiness,hallucination,open,source,gpt,super,preference">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness_CVPR_2025_paper.html" target="_blank" title="338/388"><span class="index notranslate">#338</span></a>
                <a id="title-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="title-link" href="/venue/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" target="_blank">RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness</a>
                <a id="pdf-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tianyu Yu" target="_blank">Tianyu Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoye Zhang" target="_blank">Haoye Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiming Li" target="_blank">Qiming Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qixin Xu" target="_blank">Qixin Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuan Yao" target="_blank">Yuan Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Da Chen" target="_blank">Da Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoman Lu" target="_blank">Xiaoman Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ganqu Cui" target="_blank">Ganqu Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunkai Dang" target="_blank">Yunkai Dang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiwen He" target="_blank">Taiwen He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaocheng Feng" target="_blank">Xiaocheng Feng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jun Song" target="_blank">Jun Song</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bo Zheng" target="_blank">Bo Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhiyuan Liu" target="_blank">Zhiyuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tat-Seng Chua" target="_blank">Tat-Seng Chua</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maosong Sun" target="_blank">Maosong Sun</a>
            </p>
            <p id="summary-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="summary">Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models.This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs.In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling.Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\% and overall hallucination by 33.7\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.</p>
            <p id="subjects-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" onclick="foldPdfKimi('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="panel paper" keywords="spike,reconstruction,3dgs,splatting,pose,gaussian,usp,image,camera,correction">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting_CVPR_2025_paper.html" target="_blank" title="339/388"><span class="index notranslate">#339</span></a>
                <a id="title-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="title-link" href="/venue/Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" target="_blank">USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting</a>
                <a id="pdf-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kang Chen" target="_blank">Kang Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiyuan Zhang" target="_blank">Jiyuan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zecheng Hao" target="_blank">Zecheng Hao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yajing Zheng" target="_blank">Yajing Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiejun Huang" target="_blank">Tiejun Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaofei Yu" target="_blank">Zhaofei Yu</a>
            </p>
            <p id="summary-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="summary">Spike cameras, as an innovative neuromorphic camera that captures scenes with the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS). Previous spike-based 3D reconstruction approaches often employ a casecased pipeline: starting with high-quality image reconstruction from spike streams based on established spike-to-image reconstruction algorithms, then progressing to camera pose estimation and 3D reconstruction. However, this cascaded approach suffers from substantial cumulative errors, where quality limitations of initial image reconstructions negatively impact pose estimation, ultimately degrading the fidelity of the 3D reconstruction. To address these issues, we propose a synergistic optimization framework USP-Gaussian, that unifies spike-based image reconstruction, pose correction, and Gaussian splatting into an end-to-end framework. Leveraging the multi-view consistency afforded by 3DGS and the motion capture capability of the spike camera, our framework enables a joint iterative optimization that seamlessly integrates information between the spike-to-image network and 3DGS. Experiments on synthetic datasets with accurate poses demonstrate that our method surpasses previous approaches by effectively eliminating cascading errors. Moreover, we integrate pose optimization to achieve robust 3D reconstruction in real-world scenarios with inaccurate initial poses, outperforming alternative methods by effectively reducing noise and preserving fine texture details.</p>
            <p id="subjects-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" onclick="foldPdfKimi('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="panel paper" keywords="intermimic,object,policy,interactions,motion,imperfections,teachers,capture,human,imitation">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions_CVPR_2025_paper.html" target="_blank" title="340/388"><span class="index notranslate">#340</span></a>
                <a id="title-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="title-link" href="/venue/Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" target="_blank">InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions</a>
                <a id="pdf-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Sirui Xu" target="_blank">Sirui Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hung Yu Ling" target="_blank">Hung Yu Ling</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu-Xiong Wang" target="_blank">Yu-Xiong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang-Yan Gui" target="_blank">Liang-Yan Gui</a>
            </p>
            <p id="summary-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="summary">Achieving realistic simulations of humans engaging in a wide range of object interactions has long been a fundamental goal in animation. Extending physics-based motion imitation techniques to complex human-object interactions (HOIs) is particularly challenging due to the intricate coupling between human-object dynamics and the variability in object geometries and properties. Moreover, motion capture data often contain artifacts such as inaccurate contacts and insufficient hand details, which hinder the learning process. We introduce InterMimic, a framework that overcomes these challenges by enabling a single policy to robustly learn from imperfect motion capture sequences encompassing tens of hours of diverse full-body interaction skills with dynamic and varied objects. Our key insight is employing a curriculum strategy: perfecting first, then scaling up. We first train subject-specific teacher policies to mimic, retarget, and refine the motion capture data, effectively correcting imperfections. Then, we distill a student policy from these teachers; the teachers act as online experts providing direct supervision and supplying clean references. This ensures that the student policy learns from high-quality guidance despite imperfections in the original dataset. Our experiments demonstrate that InterMimic produces realistic and diverse interactions across various HOI datasets. Notably, the learned policy exhibits zero-shot generalization, allowing seamless integration with kinematic generators and transforming the entire framework from mere imitation to generative modeling tasks.</p>
            <p id="subjects-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" onclick="foldPdfKimi('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="panel paper" keywords="twin,digital,dataset,reconstruction,object,egocentric,photorealistic,dtc,creation,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin_CVPR_2025_paper.html" target="_blank" title="341/388"><span class="index notranslate">#341</span></a>
                <a id="title-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="title-link" href="/venue/Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" target="_blank">Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset</a>
                <a id="pdf-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zhao Dong" target="_blank">Zhao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ka Chen" target="_blank">Ka Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhaoyang Lv" target="_blank">Zhaoyang Lv</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong-Xing Yu" target="_blank">Hong-Xing Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunzhi Zhang" target="_blank">Yunzhi Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Zhang" target="_blank">Cheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufeng Zhu" target="_blank">Yufeng Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephen Tian" target="_blank">Stephen Tian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhengqin Li" target="_blank">Zhengqin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Geordie Moffatt" target="_blank">Geordie Moffatt</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sean Christofferson" target="_blank">Sean Christofferson</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=James Fort" target="_blank">James Fort</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaqing Pan" target="_blank">Xiaqing Pan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingfei Yan" target="_blank">Mingfei Yan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiajun Wu" target="_blank">Jiajun Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Carl Yuheng Ren" target="_blank">Carl Yuheng Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Newcombe" target="_blank">Richard Newcombe</a>
            </p>
            <p id="summary-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="summary">We introduce Digital Twin Catalog (DTC), a new large-scale photorealistic 3D object digital twin dataset. A digital twin of a 3D object is a highly detailed, virtually indistinguishable representation of a physical object, accurately capturing its shape, appearance, physical properties, and other attributes. Recent advances in neural-based 3D reconstruction and inverse rendering have significantly improved the quality of 3D object reconstruction. Despite these advancements, there remains a lack of a large-scale, digital twin quality real-world dataset and benchmark that can quantitatively assess and compare the performance of different reconstruction methods, as well as improve reconstruction quality through training or fine-tuning. Moreover, to democratize 3D digital twin creation, it is essential to integrate creation techniques with next-generation egocentric computing platforms, such as AR glasses. Currently, there is no dataset available to evaluate 3D object reconstruction using egocentric captured images. To address these gaps, the DTC dataset features 2,000 scanned digital twin-quality 3D objects, along with image sequences captured under different lighting conditions using DSLR cameras and egocentric AR glasses. This dataset establishes the first comprehensive real-world evaluation benchmark for 3D digital twin creation tasks, offering a robust foundation for comparing and improving existing reconstruction methods. We will make the full dataset and baseline evaluations open-sourced.</p>
            <p id="subjects-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" onclick="foldPdfKimi('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="panel paper" keywords="sp3d,sparsely,prompts,semantic,supervised,accurate,modal,boosting,objectors,dcpg">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic_CVPR_2025_paper.html" target="_blank" title="342/388"><span class="index notranslate">#342</span></a>
                <a id="title-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="title-link" href="/venue/Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" target="_blank">SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts</a>
                <a id="pdf-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shijia Zhao" target="_blank">Shijia Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiming Xia" target="_blank">Qiming Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xusheng Guo" target="_blank">Xusheng Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pufan Zou" target="_blank">Pufan Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maoji Zheng" target="_blank">Maoji Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai Wu" target="_blank">Hai Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenglu Wen" target="_blank">Chenglu Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Wang" target="_blank">Cheng Wang</a>
            </p>
            <p id="summary-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="summary">Recently, sparsely-supervised 3D object detection has gained great attention, achieving performance close to fully-supervised 3D objectors while requiring only a few annotated instances. Nevertheless, these methods suffer challenges when accurate labels are extremely absent. In this paper, we propose a boosting strategy, termed SP3D, explicitly utilizing the cross-modal semantic prompts generated from Large Multimodal Models (LMMs) to boost the 3D detector with robust feature discrimination capability under sparse annotation settings. Specifically, we first develop a Confident Points Semantic Transfer (CPST) module that generates accurate cross-modal semantic prompts through boundary-constrained center cluster selection. Based on these accurate semantic prompts, which we treat as seed points, we introduce a Dynamic Cluster Pseudo-label Generation (DCPG) module to yield pseudo-supervision signals from the geometry shape of multi-scale neighbor points. Additionally, we design a Distribution Shape score (DS score) that chooses high-quality supervision signals for the initial training of the 3D detector. Experiments on the KITTI dataset and Waymo Open Dataset (WOD) have validated that SP3D can enhance the performance of sparsely supervised detectors by a large margin under meager labeling conditions.Moreover, we verified SP3D in the zero-shot setting, where its performance exceeded that of the state-of-the-art methods. The code will be made publicly available.</p>
            <p id="subjects-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" onclick="foldPdfKimi('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="panel paper" keywords="3dgs,rendering,optimization,dashgaussian,splatting,primitive,resolution,complexity,gaussian,primitives">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds_CVPR_2025_paper.html" target="_blank" title="343/388"><span class="index notranslate">#343</span></a>
                <a id="title-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="title-link" href="/venue/Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" target="_blank">DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds</a>
                <a id="pdf-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Youyu Chen" target="_blank">Youyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junjun Jiang" target="_blank">Junjun Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kui Jiang" target="_blank">Kui Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Tang" target="_blank">Xiao Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhihao Li" target="_blank">Zhihao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xianming Liu" target="_blank">Xianming Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinyu Nie" target="_blank">Yinyu Nie</a>
            </p>
            <p id="summary-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="summary">3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian primitives, where the rendering resolution and the primitive number, concluded as the optimization complexity, dominate the time cost in primitive optimization. In this paper, we propose DashGaussian, a scheduling scheme over the optimization complexity of 3DGS that strips redundant complexity to accelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as progressively fitting 3DGS to higher levels of frequency components in the training views, and propose a dynamic rendering resolution scheme that largely reduces the optimization complexity based on this formulation. Besides, we argue that a specific rendering resolution should cooperate with a proper primitive number for a better balance between computing redundancy and fitting quality, where we schedule the growth of the primitives to synchronize with the rendering resolution. Extensive experiments show that our method accelerates the optimization of various 3DGS backbones by 45.7% on average while preserving the rendering quality.</p>
            <p id="subjects-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" onclick="foldPdfKimi('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="panel paper" keywords="switch,task,binarized,merging,instantiated,vectors,storage,conflicts,redundant,fine">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch_CVPR_2025_paper.html" target="_blank" title="344/388"><span class="index notranslate">#344</span></a>
                <a id="title-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="title-link" href="/venue/Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" target="_blank">Less is More: Efficient Model Merging with Binary Task Switch</a>
                <a id="pdf-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Biqing Qi" target="_blank">Biqing Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangyuan Li" target="_blank">Fangyuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhen Wang" target="_blank">Zhen Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junqi Gao" target="_blank">Junqi Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Li" target="_blank">Dong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peng Ye" target="_blank">Peng Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bowen Zhou" target="_blank">Bowen Zhou</a>
            </p>
            <p id="summary-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="summary">As an effective approach to equip models with multi-task capabilities without additional training, model merging has garnered significant attention. However, existing merging methods face challenges of redundant parameter conflicts and the excessive storage burden of fine-tuned parameters. In this work, through controlled experiments, we reveal that for fine-tuned task vectors, only those parameters with magnitudes above a certain threshold contribute positively to the task, exhibiting a pulse-like characteristic. We then attempt leveraging this pulse-like characteristic to binarize the task vectors and reduce storage overhead. Further controlled experiments show that the binarized task vectors incur almost no decrease in fine-tuning and merging performance, and even exhibit stronger performance improvements as the proportion of redundant parameters increases. Based on these insights, we propose Task Switch (T-Switch), which decomposes task vectors into three components: 1) an activation switch instantiated by a binarized mask vector, 2) a polarity switch instantiated by a binarized sign vector, and 3) a scaling knob instantiated by a scalar coefficient. By storing task vectors in a binarized form, T-Switch alleviates parameter conflicts while ensuring efficient task parameter storage. Furthermore, to enable automated switch combination in T-Switch, we further introduce Auto-Switch, which enables training-free switch combination via retrieval from a small query set. Experiments indicate that our methods achieve significant performance improvements over existing baselines, requiring only 1-3% of the storage space of full-precision parameters.</p>
            <p id="subjects-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" onclick="foldPdfKimi('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="panel paper" keywords="registration,iclm,gpdm,overlapping,cloud,prm,point,implicit,correspondence,region">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration_CVPR_2025_paper.html" target="_blank" title="345/388"><span class="index notranslate">#345</span></a>
                <a id="title-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="title-link" href="/venue/Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" target="_blank">Implicit Correspondence Learning for Image-to-Point Cloud Registration</a>
                <a id="pdf-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xinjun Li" target="_blank">Xinjun Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenfei Yang" target="_blank">Wenfei Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacheng Deng" target="_blank">Jiacheng Deng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhixin Cheng" target="_blank">Zhixin Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xu Zhou" target="_blank">Xu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianzhu Zhang" target="_blank">Tianzhu Zhang</a>
            </p>
            <p id="summary-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="summary">Image-to-point cloud registration aims to estimate the camera pose of a given image within a 3D scene point cloud. In this area, matching-based methods have achieved leading performance by first detecting the overlapping region, then matching point and pixel features learned by neural networks and finally using the PnP-RANSAC algorithm to estimate camera pose. However, achieving accurate image-to-point cloud registration remains challenging because the overlapping region detection is unreliable merely relying on point-wise classification, direct alignment of cross-modal data is difficult and indirect optimization objective leads to unstable registration results. To address these challenges, we propose a novel implicit correspondence learning method, including a Geometric Prior-guided overlapping region Detection Module (GPDM), an Implicit Correspondence Learning Module (ICLM), and a Pose Regression Module (PRM). The proposed method enjoys several merits. First, the proposed GPDM can precisely detect the overlapping region. Second, the ICLM can generate robust cross-modality correspondences. Third, the PRM can enable end-to-end optimization. Extensive experimental results on KITTI and nuScenes datasets demonstrate that the proposed model sets a new state-of-the-art performance in registration accuracy.</p>
            <p id="subjects-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" onclick="foldPdfKimi('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="panel paper" keywords="object,indoor,outdoor,sparse,supervised,prototypes,unlabeled,prototype,mining,scene">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection_CVPR_2025_paper.html" target="_blank" title="346/388"><span class="index notranslate">#346</span></a>
                <a id="title-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="title-link" href="/venue/Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" target="_blank">Learning Class Prototypes for Unified Sparse-Supervised 3D Object Detection</a>
                <a id="pdf-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Zhu" target="_blank">Yun Zhu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Le Hui" target="_blank">Le Hui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hang Yang" target="_blank">Hang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jianjun Qian" target="_blank">Jianjun Qian</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jin Xie" target="_blank">Jin Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Yang" target="_blank">Jian Yang</a>
            </p>
            <p id="summary-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="summary">Both indoor and outdoor scene perceptions are essential for embodied intelligence. However, current sparse supervised 3D object detection methods focus solely on outdoor scenes without considering indoor settings. To this end, we propose a unified sparse supervised 3D object detection method for both indoor and outdoor scenes through learning class prototypes to effectively utilize unlabeled objects. Specifically, we first propose a prototype-based object mining module that converts the unlabeled object mining into a matching problem between class prototypes and unlabeled features. By using optimal transport matching results, we assign prototype labels to high-confidence features, thereby achieving the mining of unlabeled objects. We then present a multi-label cooperative refinement module to effectively recover missed detections in sparse supervised 3D object detection through pseudo label quality control and prototype label cooperation. Experiments show that our method achieves state-of-the-art performance under the one object per scene sparse supervised setting across indoor and outdoor datasets. With only one labeled object per scene, our method achieves about 78\%, 90\%, and 96\% performance compared to the fully supervised detector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the scalability of our method.</p>
            <p id="subjects-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" onclick="foldPdfKimi('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="panel paper" keywords="distillation,allocation,synthetic,datasets,real,approximating,696,optical,transport,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation_CVPR_2025_paper.html" target="_blank" title="347/388"><span class="index notranslate">#347</span></a>
                <a id="title-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="title-link" href="/venue/Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" target="_blank">OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation</a>
                <a id="pdf-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiao Cui" target="_blank">Xiao Cui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yulei Qin" target="_blank">Yulei Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wengang Zhou" target="_blank">Wengang Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongsheng Li" target="_blank">Hongsheng Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Houqiang Li" target="_blank">Houqiang Li</a>
            </p>
            <p id="summary-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="summary">The demands for increasingly large-scale datasets pose substantial storage and computation challenges to building deep learning models.Dataset distillation methods,especially those via sample generation techniques,rise in response to condensing large original datasets into small synthetic ones while preserving critical information.Existing subset synthesis methods simply minimize the homogeneous distance where uniform contributions from all real instances are allocated to shaping each synthetic sample.We demonstrate that such equal allocation fails to consider the instance-level relationship between each real-synthetic pair and gives rise to insufficient modeling of geometric structural nuances between the distilled and original sets.In this paper,we propose a novel framework named OPTICAL to reformulate the homogeneous distance minimization into a bi-level optimization problem via matching-and-approximating.In the matching step,we leverage optimal transport matrix to dynamically allocate contributions from real instances.Subsequently,we polish the generated samples in accordance with the established allocation scheme for approximating the real ones.Such a strategy better measures intricate geometric characteristics and handles intra-class variations for high fidelity of data distillation.Extensive experiments across seven datasets and three model architectures demonstrate our method's versatility and effectiveness.Its plug-and-play characteristic makes it compatible with a wide range of distillation frameworks.Codes are available at https://anonymous.4open.science/r/CVPR2025_696.</p>
            <p id="subjects-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" onclick="foldPdfKimi('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="panel paper" keywords="video,mllms,mme,modal,mllm,multi,ever,benchmark,comprehensive,duration">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.html" target="_blank" title="348/388"><span class="index notranslate">#348</span></a>
                <a id="title-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="title-link" href="/venue/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" target="_blank">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a>
                <a id="pdf-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chaoyou Fu" target="_blank">Chaoyou Fu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhan Dai" target="_blank">Yuhan Dai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongdong Luo" target="_blank">Yongdong Luo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lei Li" target="_blank">Lei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuhuai Ren" target="_blank">Shuhuai Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Renrui Zhang" target="_blank">Renrui Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihan Wang" target="_blank">Zihan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyu Zhou" target="_blank">Chenyu Zhou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yunhang Shen" target="_blank">Yunhang Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mengdan Zhang" target="_blank">Mengdan Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Peixian Chen" target="_blank">Peixian Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanwei Li" target="_blank">Yanwei Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shaohui Lin" target="_blank">Shaohui Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sirui Zhao" target="_blank">Sirui Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ke Li" target="_blank">Ke Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tong Xu" target="_blank">Tong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiawu Zheng" target="_blank">Xiawu Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Enhong Chen" target="_blank">Enhong Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Caifeng Shan" target="_blank">Caifeng Shan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ran He" target="_blank">Ran He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xing Sun" target="_blank">Xing Sun</a>
            </p>
            <p id="summary-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="summary">In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs to process sequential visual data is still insufficiently explored, highlighting the lack of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, and reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models with an average accuracy of 75\%, compared to 71.9% for GPT-4o. The results also demonstrate that Video-MME is a universal benchmark that applies to both image and video MLLMs. Further analysis indicates that subtitle and audio information could significantly enhance video understanding. Besides, a decline in MLLM performance is observed as video duration increases for all models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data, shedding light on future MLLM development.</p>
            <p id="subjects-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" onclick="foldPdfKimi('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="panel paper" keywords="vlms,medical,expert,ift,vila,vision,healthcare,gemini,knowledge,domain">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge_CVPR_2025_paper.html" target="_blank" title="349/388"><span class="index notranslate">#349</span></a>
                <a id="title-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="title-link" href="/venue/Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" target="_blank">VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge</a>
                <a id="pdf-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF">12</sup>]</a>
                <a id="copy-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF">3</sup>]</a>
                <a id="rel-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Vishwesh Nath" target="_blank">Vishwesh Nath</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenqi Li" target="_blank">Wenqi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dong Yang" target="_blank">Dong Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andriy Myronenko" target="_blank">Andriy Myronenko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingxin Zheng" target="_blank">Mingxin Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Lu" target="_blank">Yao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhijian Liu" target="_blank">Zhijian Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxu Yin" target="_blank">Hongxu Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yee Man Law" target="_blank">Yee Man Law</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yucheng Tang" target="_blank">Yucheng Tang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pengfei Guo" target="_blank">Pengfei Guo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Can Zhao" target="_blank">Can Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyue Xu" target="_blank">Ziyue Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yufan He" target="_blank">Yufan He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephanie Harmon" target="_blank">Stephanie Harmon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Benjamin Simon" target="_blank">Benjamin Simon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Greg Heinrich" target="_blank">Greg Heinrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stephen Aylward" target="_blank">Stephen Aylward</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Edgar" target="_blank">Marc Edgar</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Michael Zephyr" target="_blank">Michael Zephyr</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pavlo Molchanov" target="_blank">Pavlo Molchanov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Baris Turkbey" target="_blank">Baris Turkbey</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Holger Roth" target="_blank">Holger Roth</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daguang Xu" target="_blank">Daguang Xu</a>
            </p>
            <p id="summary-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="summary">Generalist vision language models (VLMs) have made significant strides in computer vision, but they fall short in specialized fields like healthcare, where expert knowledge is essential. Current large multimodal models like Gemini and GPT-4o are insufficient for medical tasks due to their reliance on memorized internet knowledge rather than the nuanced expertise required in healthcare. Meanwhile, existing medical VLMs (e.g. Med-Gemini) often lack expert consultation as part of their design, and many rely on outdated, static datasets that were not created with modern, large deep learning models in mind. VLMs are usually trained in three stages: vision pre-training, vision-language pre-training, and instruction fine-tuning (IFT). IFT has been typically applied using a mixture of generic and healthcare data. In contrast, we propose that for medical VLMs, a fourth stage of specialized IFT is necessary, which focuses on medical data and includes information from domain expert models. Domain expert models developed for medical use are crucial because they are specifically trained for certain clinical tasks, e.g. to detect tumors and classify abnormalities through segmentation and classification, which learn fine-grained features of medical data<span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-87-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-382" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.409em, 1000.68em, 2.398em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-383"><span class="mo" id="MathJax-Span-384" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-87">-</script>features that are often too intricate for a VLM to capture effectively. This paper introduces a new framework, VILA-M3, for medical VLMs that utilizes domain knowledge via expert models. We argue that generic VLM architectures alone are not viable for real-world clinical applications and on-demand usage of domain-specialized expert model knowledge is critical for advancing AI in healthcare. Through our experiments, we show an improved state-of-the-art (SOTA) performance with an average improvement of <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-88-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-385" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-386"><span class="mo" id="MathJax-Span-387" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-88">\sim</script>9\% over the prior SOTA model Med-Gemini and <span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax notranslate" id="MathJax-Element-89-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-388" style="width: 0.94em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.617em, 1000.73em, 2.138em, -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-389"><span class="mo" id="MathJax-Span-390" style="font-family: MathJax_Main;"></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: 0.128em; border-left: 0px solid; width: 0px; height: 0.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo></mo></math></span></span><script type="math/tex" id="MathJax-Element-89">\sim</script>6\% over models trained on the specific tasks. Our approach emphasizes the importance of domain expertise in creating precise, reliable VLMs for medical applications.</p>
            <p id="subjects-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" onclick="foldPdfKimi('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="panel paper" keywords="wvd,xyz,video,consistent,diffusion,rgb,frames,generation,image,world">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html" target="_blank" title="350/388"><span class="index notranslate">#350</span></a>
                <a id="title-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="title-link" href="/venue/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" target="_blank">World-consistent Video Diffusion with Explicit 3D Modeling</a>
                <a id="pdf-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Qihang Zhang" target="_blank">Qihang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shuangfei Zhai" target="_blank">Shuangfei Zhai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Miguel ngel Bautista Martin" target="_blank">Miguel ngel Bautista Martin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kevin Miao" target="_blank">Kevin Miao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexander Toshev" target="_blank">Alexander Toshev</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Joshua Susskind" target="_blank">Joshua Susskind</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiatao Gu" target="_blank">Jiatao Gu</a>
            </p>
            <p id="summary-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="summary">Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation.Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.</p>
            <p id="subjects-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="panel paper" keywords="reasoning,agent,long,chain,multi,modal,language,mllms,insight,quality">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models_CVPR_2025_paper.html" target="_blank" title="351/388"><span class="index notranslate">#351</span></a>
                <a id="title-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="title-link" href="/venue/Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" target="_blank">Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</a>
                <a id="pdf-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF">10</sup>]</a>
                <a id="copy-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF">8</sup>]</a>
                <a id="rel-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuhao Dong" target="_blank">Yuhao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zuyan Liu" target="_blank">Zuyan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hai-Long Sun" target="_blank">Hai-Long Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingkang Yang" target="_blank">Jingkang Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Winston Hu" target="_blank">Winston Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongming Rao" target="_blank">Yongming Rao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziwei Liu" target="_blank">Ziwei Liu</a>
            </p>
            <p id="summary-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="summary">Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model, our method shows an average improvement of 7.5% across seven challenging multi-modal benchmarks requiring visual reasoning. We also achieve a 4.2% improvement on a stronger base MLLM, highlighting the potential to further advance state-of-the-art models. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks. We will make our data and code publicly available to promote future research in this emerging field.</p>
            <p id="subjects-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" onclick="foldPdfKimi('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="panel paper" keywords="erased,concept,latents,concepts,images,seeds,ablation,forgotten,memories,latent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Rusanovsky_Memories_of_Forgotten_Concepts_CVPR_2025_paper.html" target="_blank" title="352/388"><span class="index notranslate">#352</span></a>
                <a id="title-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="title-link" href="/venue/Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" target="_blank">Memories of Forgotten Concepts</a>
                <a id="pdf-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Rusanovsky_Memories_of_Forgotten_Concepts_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Matan Rusanovsky" target="_blank">Matan Rusanovsky</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shimon Malnick" target="_blank">Shimon Malnick</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Amir Jevnisek" target="_blank">Amir Jevnisek</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ohad Fried" target="_blank">Ohad Fried</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shai Avidan" target="_blank">Shai Avidan</a>
            </p>
            <p id="summary-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="summary">Diffusion models dominate the space of text-to-image generation, yet they may produce undesirable outputs, including explicit content or private data. To mitigate this, concept ablation techniques have been explored to limit the generation of certain concepts.In this paper, we reveal that the erased concept information persists in the model and that erased concept images can be generated using the right latent. Utilizing inversion methods, we show that there exist latent seeds capable of generating high quality images of erased concepts.Moreover, we show that these latents have likelihoods that overlap with those of images outside the erased concept.We extend this to demonstrate that for every image from the erased concept set, we can generate many seeds that generate the erased concept.Given the vast space of latents capable of generating ablated concept images, our results suggest that fully erasing concept information may be intractable, highlighting possible vulnerabilities in current concept ablation techniques.</p>
            <p id="subjects-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" onclick="foldPdfKimi('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="panel paper" keywords="garment,aipparel,garments,multimodal,sewing,editing,patterns,apparel,foundation,mirroring">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments_CVPR_2025_paper.html" target="_blank" title="353/388"><span class="index notranslate">#353</span></a>
                <a id="title-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="title-link" href="/venue/Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" target="_blank">AIpparel: A Multimodal Foundation Model for Digital Garments</a>
                <a id="pdf-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kiyohiro Nakayama" target="_blank">Kiyohiro Nakayama</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Ackermann" target="_blank">Jan Ackermann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Timur Levent Kesdogan" target="_blank">Timur Levent Kesdogan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yang Zheng" target="_blank">Yang Zheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maria Korosteleva" target="_blank">Maria Korosteleva</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Olga Sorkine-Hornung" target="_blank">Olga Sorkine-Hornung</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leonidas J. Guibas" target="_blank">Leonidas J. Guibas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guandao Yang" target="_blank">Guandao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gordon Wetzstein" target="_blank">Gordon Wetzstein</a>
            </p>
            <p id="summary-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="summary">Apparel is essential to human life, offering protection, mirroring cultural identities, and showcasing personal style. Yet, the creation of garments remains a time-consuming process, largely due to the manual work involved in designing them. To simplify this process, we introduce AIpparel, a large multimodal model for generating and editing sewing patterns. Our model fine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated large-scale dataset of over 120,000 unique garments, each with multimodal annotations including text, images, and sewing patterns. Additionally, we propose a novel tokenization scheme that concisely encodes these complex sewing patterns so that LLMs can learn to predict them efficiently. AIpparel achieves state-of-the-art performance in single-modal tasks, including text-to-garment and image-to-garment prediction, and it enables novel multimodal garment generation applications such as interactive garment editing.</p>
            <p id="subjects-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" onclick="foldPdfKimi('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="panel paper" keywords="event,fields,cameras,multiplexing,capture,light,speed,temporal,dynamic,high">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and_CVPR_2025_paper.html" target="_blank" title="354/388"><span class="index notranslate">#354</span></a>
                <a id="title-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="title-link" href="/venue/Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" target="_blank">Event Fields: Capturing Light Fields at High Speed, Resolution, and Dynamic Range</a>
                <a id="pdf-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyuan Qu" target="_blank">Ziyuan Qu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zihao Zou" target="_blank">Zihao Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vivek Boominathan" target="_blank">Vivek Boominathan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Praneeth Chakravarthula" target="_blank">Praneeth Chakravarthula</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Adithya Pediredla" target="_blank">Adithya Pediredla</a>
            </p>
            <p id="summary-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="summary">Event cameras, which feature pixels that independently respond to changes in brightness, are becoming increasingly popular in high-speed applications due to their lower latency, reduced bandwidth requirements, and enhanced dynamic range compared to traditional frame-based cameras. Numerous imaging and vision techniques have leveraged event cameras for high-speed scene understanding by capturing high-framerate, high-dynamic range videos, primarily utilizing the temporal advantages inherent to event cameras. Additionally, imaging and vision techniques have utilized the light field---a complementary dimension to temporal information---for enhanced scene understanding. In this work, we propose "Event Fields", a new approach that utilizes innovative optical designs for event cameras to capture light fields at high speed. We develop the underlying mathematical framework for Event Fields and introduce two foundational frameworks to capture them practically: spatial multiplexing to capture temporal derivatives and temporal multiplexing to capture angular derivatives. To realize these, we design two complementary optical setups---one using a kaleidoscope for spatial multiplexing and another using a galvanometer for temporal multiplexing. We evaluate the performance of both designs using a custom-built simulator and real hardware prototypes, showcasing their distinct benefits. Our event fields unlock the full advantages of typical light fieldslike post-capture refocusing and depth estimationnow supercharged for high-speed and high-dynamic range scenes. This novel light-sensing paradigm opens doors to new applications in photography, robotics, and AR/VR, and presents fresh challenges in rendering and machine learning.</p>
            <p id="subjects-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" onclick="foldPdfKimi('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="panel paper" keywords="splatflow,nmff,dynamic,splatting,gaussian,flow,motion,urban,view,self">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field_CVPR_2025_paper.html" target="_blank" title="355/388"><span class="index notranslate">#355</span></a>
                <a id="title-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="title-link" href="/venue/Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" target="_blank">SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving</a>
                <a id="pdf-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Su Sun" target="_blank">Su Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng Zhao" target="_blank">Cheng Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuoyang Sun" target="_blank">Zhuoyang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yingjie Victor Chen" target="_blank">Yingjie Victor Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mei Chen" target="_blank">Mei Chen</a>
            </p>
            <p id="summary-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="summary">Most existing Dynamic Gaussian Splatting methods for complex dynamic urban scenarios rely on accurate object-level supervision from expensive manual labeling, limiting their scalability in real-world applications. In this paper, we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting within Neural Motion Flow Fields (NMFF) to learn 4D space-time representations without requiring tracked 3D bounding boxes, enabling accurate dynamic scene reconstruction and novel view RGB/depth/flow synthesis. SplatFlow designs a unified framework to seamlessly integrate time-dependent 4D Gaussian representation within NMFF, where NMFF is a set of implicit functions to model temporal motions of both LiDAR points and Gaussians as continuous motion flow fields. Leveraging NMFF, SplatFlow effectively decomposes static background and dynamic objects, representing them with 3D and 4D Gaussian primitives, respectively.NMFF also models the status correspondences of each 4D Gaussian across time, which aggregates temporal features to enhance cross-view consistency of dynamic components. SplatFlow further improves dynamic scene identification by distilling features from 2D foundational models into 4D space-time representation. Comprehensive evaluations conducted on the Waymo Open Dataset and KITTI Dataset validate SplatFlow's state-of-the-art (SOTA) performance for both image reconstruction and novel view synthesis in dynamic urban scenarios. The code and model will be released upon the paper's acceptance.</p>
            <p id="subjects-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" onclick="foldPdfKimi('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="panel paper" keywords="pressure,mocap,human,motionpro,rgb,fusing,trajectory,humanoid,pose,exploring">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and_CVPR_2025_paper.html" target="_blank" title="356/388"><span class="index notranslate">#356</span></a>
                <a id="title-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="title-link" href="/venue/Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" target="_blank">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a>
                <a id="pdf-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Shenghao Ren" target="_blank">Shenghao Ren</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Lu" target="_blank">Yi Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Huang" target="_blank">Jiayi Huang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiayi Zhao" target="_blank">Jiayi Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Zhang" target="_blank">He Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tao Yu" target="_blank">Tao Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qiu Shen" target="_blank">Qiu Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xun Cao" target="_blank">Xun Cao</a>
            </p>
            <p id="summary-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="summary">Existing human Motion Capture (MoCap) method mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale Human Motion capture dataset with Pressure, RGB and Optical sensors (named MotionPRO), which comprises 70 volunteers performing 400 types of motion. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small-kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose. (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy for fusing pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence.</p>
            <p id="subjects-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" onclick="foldPdfKimi('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="panel paper" keywords="glossy,polarizer,captured,acquisition,polarization,images,reconstruction,polarized,rgb,rendering">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition_CVPR_2025_paper.html" target="_blank" title="357/388"><span class="index notranslate">#357</span></a>
                <a id="title-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="title-link" href="/venue/Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" target="_blank">Glossy Object Reconstruction with Cost-effective Polarized Acquisition</a>
                <a id="pdf-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bojian Wu" target="_blank">Bojian Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yifan Peng" target="_blank">Yifan Peng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruizhen Hu" target="_blank">Ruizhen Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaowei Zhou" target="_blank">Xiaowei Zhou</a>
            </p>
            <p id="summary-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="summary">The challenge of image-based 3D reconstruction for glossy objects lies in separating diffuse and specular components on glossy surfaces from captured images, a task complicated by the ambiguity in discerning lighting conditions and material properties using RGB data alone. While state-of-the-art methods rely on tailored and/or high-end equipment for data acquisition, which can be cumbersome and time-consuming, this work introduces a scalable polarization-aided approach that employs cost-effective acquisition tools. By attaching a linear polarizer to readily available RGB cameras, multi-view polarization images can be captured without the need for advance calibration or precise measurements of the polarizer angle, substantially reducing system construction costs. The proposed approach represents polarimetric BRDF, Stokes vectors, and polarization states of object surfaces as neural implicit fields. These fields, combined with the polarizer angle, are retrieved by optimizing the rendering loss of input polarized images. By leveraging fundamental physical principles for the implicit representation of polarization rendering, our method demonstrates superiority over existing techniques through experiments on public datasets and real captured images. Code and captured data will be made available upon acceptance for further evaluation.</p>
            <p id="subjects-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" onclick="foldPdfKimi('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="panel paper" keywords="scene,functional,graphs,indoor,scenefun3d,open3dsg,vocabulary,world,relationships,fungraph3d">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces_CVPR_2025_paper.html" target="_blank" title="358/388"><span class="index notranslate">#358</span></a>
                <a id="title-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="title-link" href="/venue/Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" target="_blank">Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces</a>
                <a id="pdf-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Chenyangguang Zhang" target="_blank">Chenyangguang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alexandros Delitzas" target="_blank">Alexandros Delitzas</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fangjinhua Wang" target="_blank">Fangjinhua Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ruida Zhang" target="_blank">Ruida Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangyang Ji" target="_blank">Xiangyang Ji</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Pollefeys" target="_blank">Marc Pollefeys</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Francis Engelmann" target="_blank">Francis Engelmann</a>
            </p>
            <p id="summary-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="summary">We introduce the task of predicting functional 3D scene graphs for real-world indoor environments from posed RGB-D images. Unlike traditional 3D scene graphs that focus on spatial relationships of objects, functional 3D scene graphs capture objects, interactive elements, and their functional relationships. Due to the lack of training data, we leverage foundation models, including visual language models (VLMs) and large language models (LLMs), to encode functional knowledge. We evaluate our approach on an extended SceneFun3D dataset and a newly collected dataset, FunGraph3D, both annotated with functional 3D scene graphs. Our method significantly outperforms adapted baselines, including Open3DSG and ConceptGraph, demonstrating its effectiveness in modeling complex scene functionalities. We also demonstrate downstream applications such as 3D question answering and robotic manipulation using functional 3D scene graphs.</p>
            <p id="subjects-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" onclick="foldPdfKimi('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="panel paper" keywords="vit,instruction,instructions,presel,images,datasets,selection,tuning,vision,data">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for_CVPR_2025_paper.html" target="_blank" title="359/388"><span class="index notranslate">#359</span></a>
                <a id="title-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="title-link" href="/venue/Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" target="_blank">Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning</a>
                <a id="pdf-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Bardia Safaei" target="_blank">Bardia Safaei</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Faizan Siddiqui" target="_blank">Faizan Siddiqui</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiacong Xu" target="_blank">Jiacong Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Vishal M. Patel" target="_blank">Vishal M. Patel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shao-Yuan Lo" target="_blank">Shao-Yuan Lo</a>
            </p>
            <p id="summary-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="summary">Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. Code will be made available.</p>
            <p id="subjects-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" onclick="foldPdfKimi('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="panel paper" keywords="crisp,shape,pose,self,estimation,decoder,corrector,pipeline,object,ycbv">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation_CVPR_2025_paper.html" target="_blank" title="360/388"><span class="index notranslate">#360</span></a>
                <a id="title-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="title-link" href="/venue/Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" target="_blank">CRISP: Object Pose and Shape Estimation with Test-Time Adaptation</a>
                <a id="pdf-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Jingnan Shi" target="_blank">Jingnan Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rajat Talak" target="_blank">Rajat Talak</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Harry Zhang" target="_blank">Harry Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=David Jin" target="_blank">David Jin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luca Carlone" target="_blank">Luca Carlone</a>
            </p>
            <p id="summary-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="summary">We consider the problem of estimating object pose and shape from an RGB-D image. Our first contribution is to introduce CRISP, a category-agnostic object pose and shape estimation pipeline. The pipeline implements an encoder-decoder model for shape estimation. It uses FiLM-conditioning for implicit shape reconstruction and a DPT-based network for estimating pose-normalized points for pose estimation. As a second contribution, we propose an optimization-based pose and shape corrector that can correct estimation errors caused by a domain gap. Observing that the shape decoder is well behaved in the convex hull of known shapes, we approximate the shape decoder with an active shape model, and show that this reduces the shape correction problem to a constrained linear least squares problem, which can be solved efficiently by an interior point algorithm. Third, we introduce a self-training pipeline to perform self-supervised domain adaptation of CRISP. The self-training is based on a correct-and-certify approach, which leverages the corrector to generate pseudo-labels at test time, and uses them to self-train CRISP. We demonstrate CRISP (and the self-training) on YCBV, SPE3R, and NOCS datasets. CRISP shows high performance on all the datasets. Moreover, our self-training is capable of bridging a large domain gap. Finally, CRISP also shows an ability to generalize to unseen objects.</p>
            <p id="subjects-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" onclick="foldPdfKimi('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="panel paper" keywords="egocentric,pose,motion,ego,feeds,camera,mounted,facing,frame,floor">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video_CVPR_2025_paper.html" target="_blank" title="361/388"><span class="index notranslate">#361</span></a>
                <a id="title-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="title-link" href="/venue/Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" target="_blank">FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</a>
                <a id="pdf-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Andrea Boscolo Camiletto" target="_blank">Andrea Boscolo Camiletto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jian Wang" target="_blank">Jian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Eduardo Alvarado" target="_blank">Eduardo Alvarado</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rishabh Dabral" target="_blank">Rishabh Dabral</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Thabo Beeler" target="_blank">Thabo Beeler</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marc Habermann" target="_blank">Marc Habermann</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Christian Theobalt" target="_blank">Christian Theobalt</a>
            </p>
            <p id="summary-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="summary">Egocentric motion capture with a head-mounted body-facing stereo camera is crucial for VR and AR applications but presents significant challenges such as heavy occlusions and limited annotated real-world data.Existing methods heavily rely on synthetic pretraining and struggle to generate smooth and accurate predictions in real-world settings, particularly for lower limbs.Our work addresses these limitations by introducing a lightweight VR-based data collection setup with on-board, real-time 6D pose tracking. Using this setup, we collected the most extensive real-world dataset for ego-facing ego-mounted cameras to date in size and motion variability. Effectively integrating this multimodal input -- device pose and camera feeds -- is challenging due to the differing characteristics of each data source.To address this, we propose FRAME, a simple yet effective architecture that combines device pose and camera feeds for state-of-the-art body pose prediction through geometrically sound multimodal integration and can run at 300 FPS on modern hardware.Lastly, we showcase a novel training strategy to enhance the model's generalization capabilities.Our approach exploits the problem's geometric properties, yielding high-quality motion capture free from common artifacts in prior work. Qualitative and quantitative evaluations, along with extensive comparisons, demonstrate the effectiveness of our method.We will release data, code, and CAD designs for the benefit of the research community.</p>
            <p id="subjects-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" onclick="foldPdfKimi('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="panel paper" keywords="salient,patches,dbps,hrir,sub,pains,consumption,buffer,recycling,regions">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution_CVPR_2025_paper.html" target="_blank" title="362/388"><span class="index notranslate">#362</span></a>
                <a id="title-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="title-link" href="/venue/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" target="_blank">No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition</a>
                <a id="pdf-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rong Qin" target="_blank">Rong Qin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xin Liu" target="_blank">Xin Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xingyu Liu" target="_blank">Xingyu Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaxuan Liu" target="_blank">Jiaxuan Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jinglei Shi" target="_blank">Jinglei Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Liang Lin" target="_blank">Liang Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jufeng Yang" target="_blank">Jufeng Yang</a>
            </p>
            <p id="summary-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="summary">Over the last decade, many notable methods have emerged to tackle the computational resource challenge of the high resolution image recognition (HRIR). They typically focus on identifying and aggregating a few salient regions for classification, discarding sub-salient areas for low training consumption. Nevertheless, many HRIR tasks necessitate the exploration of wider regions to model objects and contexts, which limits their performance in such scenarios. To address this issue, we present a DBPS strategy to enable training with more patches at low consumption. Specifically, in addition to a fundamental buffer that stores the embeddings of most salient patches, DBPS further employs an auxiliary buffer to recycle those sub-salient ones. To reduce the computational cost associated with gradients of sub-salient patches, these patches are primarily used in the forward pass to provide sufficient information for classification. Meanwhile, only the gradients of the salient patches are back-propagated to update the entire network. Moreover, we design a Multiple Instance Learning (MIL) architecture that leverages aggregated information from salient patches to filter out uninformative background within sub-salient patches for better accuracy. Besides, we introduce the random patch drop to accelerate training process and uncover informative regions. Experiment results demonstrate the superiority of our method in terms of both accuracy and training consumption against other advanced methods. The code is available in the supplementary materials and will be publicly available.</p>
            <p id="subjects-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" onclick="foldPdfKimi('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="panel paper" keywords="tuning,fine,fix,reward,t2i,safety,focus,generation,rewards,image">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation_CVPR_2025_paper.html" target="_blank" title="363/388"><span class="index notranslate">#363</span></a>
                <a id="title-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-link" href="/venue/Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" target="_blank">Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation</a>
                <a id="pdf-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoying Xing" target="_blank">Xiaoying Xing</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Avinab Saha" target="_blank">Avinab Saha</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junfeng He" target="_blank">Junfeng He</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Susan Hao" target="_blank">Susan Hao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Vicol" target="_blank">Paul Vicol</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Moonkyung Ryu" target="_blank">Moonkyung Ryu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gang Li" target="_blank">Gang Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sahil Singla" target="_blank">Sahil Singla</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sarah Young" target="_blank">Sarah Young</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yinxiao Li" target="_blank">Yinxiao Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Feng Yang" target="_blank">Feng Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Deepak Ramachandran" target="_blank">Deepak Ramachandran</a>
            </p>
            <p id="summary-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="summary">Text-to-image (T2I) generation has made significant advances in recent years, but challenges still remain in the generation of perceptual artifacts, misalignment with complex prompts, and safety. The prevailing approach to address these issues involves collecting human feedback on generated images, training reward models to estimate human feedback, and then fine-tuning T2I models based on the reward models to align them with human preferences. However, while existing reward fine-tuning methods can produce images with higher rewards, they may change model behavior in unexpected ways. For example, fine-tuning for one quality aspect (e.g., safety) may degrade other aspects (e.g., prompt alignment), or may lead to reward hacking (e.g., finding a way to increase rewards without having the intended effect). In this paper, we propose Focus-N-Fix, a region-aware fine-tuning method that trains models to correct only previously problematic image regions. The resulting fine-tuned model generates images with the same high-level structure as the original model but shows significant improvements in regions where the original model was deficient in safety (over-sexualization and violence), plausibility, or other criteria. Our experiments demonstrate that Focus-N-Fix improves these localized quality aspects with little or no degradation to others and typically imperceptible changes in the rest of the image.</p>
            <p id="subjects-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="panel paper" keywords="human,video,centric,dataset,openhumanvid,quality,generation,motion,scale,videos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation_CVPR_2025_paper.html" target="_blank" title="364/388"><span class="index notranslate">#364</span></a>
                <a id="title-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="title-link" href="/venue/Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" target="_blank">OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</a>
                <a id="pdf-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Hui Li" target="_blank">Hui Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mingwang Xu" target="_blank">Mingwang Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yun Zhan" target="_blank">Yun Zhan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shan Mu" target="_blank">Shan Mu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jiaye Li" target="_blank">Jiaye Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Kaihui Cheng" target="_blank">Kaihui Cheng</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yuxuan Chen" target="_blank">Yuxuan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tan Chen" target="_blank">Tan Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Mao Ye" target="_blank">Mao Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingdong Wang" target="_blank">Jingdong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Siyu Zhu" target="_blank">Siyu Zhu</a>
            </p>
            <p id="summary-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="summary">Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce \textbf{OpenHumanVid}, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio.To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs.Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos.</p>
            <p id="subjects-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" onclick="foldPdfKimi('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="panel paper" keywords="reasoning,spatial,pulsecheck457,mutimodal,lmms,spatial457,rpdr,multimodal,across,unbiased">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large_CVPR_2025_paper.html" target="_blank" title="365/388"><span class="index notranslate">#365</span></a>
                <a id="title-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="title-link" href="/venue/Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" target="_blank">Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Mutimodal Models</a>
                <a id="pdf-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xingrui Wang" target="_blank">Xingrui Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wufei Ma" target="_blank">Wufei Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tiezheng Zhang" target="_blank">Tiezheng Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Celso M de Melo" target="_blank">Celso M de Melo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jieneng Chen" target="_blank">Jieneng Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Yuille" target="_blank">Alan Yuille</a>
            </p>
            <p id="summary-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="summary">Although large multimodal models (LMMs) have demonstrated remarkable capabilities in visual scene interpretation and reasoning, their capacity for complex and precise 3-dimensional spatial reasoning remains uncertain. Existing benchmarks focus predominantly on 2D spatial understanding and lack a framework to comprehensively evaluate 6D spatial reasoning across varying complexities.To address this limitation, we present **PulseCheck457**, a scalable and unbiased synthetic dataset designed with **4** key spatial components: multi-object recognition, 2D and 3D spatial relationships, and 3D orientation. **PulseCheck457** supports a cascading evaluation structure, offering **7** question types across **5** difficulty levels that progress from basic single-object recognition to our newly proposed complex 6D spatial reasoning tasks.We evaluated various large multimodal models (LMMs) on **PulseCheck457**, observing a general decline in performance as task complexity increases, particularly in 3D reasoning and 6D spatial tasks. To quantify these challenges, we introduce the Relative Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning capabilities. Leveraging the unbiased attribute design of our dataset, we also uncover prediction biases across different attributes, with similar patterns observed in real-world image settings.</p>
            <p id="subjects-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" onclick="foldPdfKimi('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="panel paper" keywords="informed,3di,lmm,reasoning,lmms,data,spatialllm,multimodal,relationships,training">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models_CVPR_2025_paper.html" target="_blank" title="366/388"><span class="index notranslate">#366</span></a>
                <a id="title-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="title-link" href="/venue/Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" target="_blank">SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models</a>
                <a id="pdf-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wufei Ma" target="_blank">Wufei Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Luoxin Ye" target="_blank">Luoxin Ye</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Celso M de Melo" target="_blank">Celso M de Melo</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alan Yuille" target="_blank">Alan Yuille</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jieneng Chen" target="_blank">Jieneng Chen</a>
            </p>
            <p id="summary-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="summary">Humans naturally understand 3D spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. Current large multimodal models (LMMs), however, lack of this capability of 3D reasoning. This limitation stems from the scarcity of 3D training data and the bias in current model designs toward 2D data. In this paper, we systematically study the impact of 3D-informed data, architecture, and training setups, introducing 3DI-LMM, an LMM with advanced 3D spatial reasoning abilities. To address data limitations, we develop two types of 3D-informed training datasets: (1) 3D-informed probing data focused on object's 3D location and orientation, and (2) 3D-informed conversation data for complex spatial relationships. Notably, we are the first to curate VQA data that incorporates 3D orientation relationships. Furthermore, we systematically integrate these two types of training data with the architectural and training designs of LMMs, providing a roadmap for optimal design aimed at achieving superior 3D reasoning capabilities. Our 3DI-LMM advances machines toward highly capable 3D-informed reasoning, surpass GPT-4o performance by 8.7%. Our systematic empirical design and resulting findings offer valuable insights for future research in this direction.</p>
            <p id="subjects-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" onclick="foldPdfKimi('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="panel paper" keywords="inversion,cem,robustness,conditional,redundancy,entropy,raw,obfuscation,maximization,collaborative">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization_CVPR_2025_paper.html" target="_blank" title="367/388"><span class="index notranslate">#367</span></a>
                <a id="title-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="title-link" href="/venue/Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" target="_blank">Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</a>
                <a id="pdf-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Song Xia" target="_blank">Song Xia</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yu" target="_blank">Yi Yu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenhan Yang" target="_blank">Wenhan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Meiwen Ding" target="_blank">Meiwen Ding</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhuo Chen" target="_blank">Zhuo Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ling-Yu Duan" target="_blank">Ling-Yu Duan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alex C. Kot" target="_blank">Alex C. Kot</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xudong Jiang" target="_blank">Xudong Jiang</a>
            </p>
            <p id="summary-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="summary">By locally encoding raw data into intermediate features, collaborative inference enables end users to leverage powerful deep learning models without exposure of sensitive raw data to cloud servers.However, recent studies have revealed that these intermediate features may not sufficiently preserve privacy, as information can be leaked and raw data can be reconstructed via model inversion attacks (MIAs).Obfuscation-based methods, such as noise corruption, adversarial representation learning, and information filters, enhance the inversion robustness by obfuscating the task-irrelevant redundancy empirically.However, methods for quantifying such redundancy remain elusive, and the explicit mathematical relation between this redundancy and worst-case robustness against inversion has not yet been established.To address that, this work first theoretically proves that the conditional entropy of inputs given intermediate features provides a guaranteed lower bound on the reconstruction mean square error (MSE) under any MIA.Then, we derive a differentiable and solvable measure for bounding this conditional entropy based on the Gaussian mixture estimation and propose a conditional entropy maximization (CEM) algorithm to enhance the inversion robustness. Experimental results on four datasets demonstrate the effectiveness and adaptability of our proposed CEM; without compromising the feature utility and computing efficiency, integrating the proposed CEM into obfuscation-based defense mechanisms consistently boosts their inversion robustness, achieving average gains ranging from 12.9% to 48.2%.</p>
            <p id="subjects-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" onclick="foldPdfKimi('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="panel paper" keywords="video,prompt,incremental,videos,learning,prompts,modeling,class,diverse,capabilities">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning_CVPR_2025_paper.html" target="_blank" title="368/388"><span class="index notranslate">#368</span></a>
                <a id="title-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="title-link" href="/venue/Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" target="_blank">Learning Conditional Space-Time Prompt Distributions for Video Class-Incremental Learning</a>
                <a id="pdf-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohan Zou" target="_blank">Xiaohan Zou</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wenchao Ma" target="_blank">Wenchao Ma</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shu Zhao" target="_blank">Shu Zhao</a>
            </p>
            <p id="summary-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="summary">Recent advancements in prompt-based learning have significantly advanced image and video class-incremental learning. However, the prompts learned by these methods often fail to capture the diverse and informative characteristics of videos, and struggle to generalize effectively to future tasks and classes. To address these challenges, this paper proposes modeling the distribution of space-time prompts conditioned on the input video using a diffusion model. This generative approach allows the proposed model to naturally handle the diverse characteristics of videos, leading to more robust prompt learning and enhanced generalization capabilities. Additionally, we develop a mechanism that transfers the token relationship modeling capabilities of a pre-trained image transformer to spatio-temporal modeling for videos. Our approach has been thoroughly evaluated across four established benchmarks, showing remarkable improvements over existing state-of-the-art methods in video class-incremental learning.</p>
            <p id="subjects-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" onclick="foldPdfKimi('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="panel paper" keywords="peft,unifying,accuracy,methods,shot,fine,complementariness,insights,tuning,tune">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.html" target="_blank" title="369/388"><span class="index notranslate">#369</span></a>
                <a id="title-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="title-link" href="/venue/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" target="_blank">Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition</a>
                <a id="pdf-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Zheda Mai" target="_blank">Zheda Mai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ping Zhang" target="_blank">Ping Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Cheng-Hao Tu" target="_blank">Cheng-Hao Tu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hong-You Chen" target="_blank">Hong-You Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quang-Huy Nguyen" target="_blank">Quang-Huy Nguyen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Li Zhang" target="_blank">Li Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Wei-Lun Chao" target="_blank">Wei-Lun Chao</a>
            </p>
            <p id="summary-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="summary">Parameter-efficient fine-tuning (PEFT) has attracted significant attention due to the growth of pre-trained model sizes and the need to fine-tune (FT) them for superior downstream performance. Despite a surge in new PEFT methods, a systematic study to understand their performance and suitable application scenarios is lacking, leaving questions like "when to apply PEFT" and "which method to use" largely unanswered, especially in visual recognition. In this paper, we conduct a unifying empirical study of representative PEFT methods with Vision Transformers. We systematically tune their hyper-parameters to fairly compare their accuracy on downstream tasks. Our study offers a practical user guide and unveils several new insights. First, if tuned carefully, different PEFT methods achieve similar accuracy in the low-shot benchmark VTAB-1K. This includes simple approaches like FT the bias terms that were reported inferior. Second, despite similar accuracy, we find that PEFT methods make different mistakes and high-confidence predictions, likely due to their different inductive biases. Such an inconsistency (or complementariness) opens up the opportunity for ensemble methods, and we make preliminary attempts at this. Third, going beyond the commonly used low-shot tasks, we find that PEFT is also useful in many-shot regimes, achieving comparable or better accuracy than full FT while using significantly fewer parameters. Lastly, we investigate PEFT's ability to preserve a pre-trained model's robustness to distribution shifts (e.g., CLIP). Perhaps not surprisingly, PEFT approaches outperform full FT alone. However, with weight-space ensembles, full FT can better balance target distribution and distribution shift performance, suggesting a future research direction for PEFT.</p>
            <p id="subjects-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" onclick="foldPdfKimi('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="panel paper" keywords="adapter,t2i,dms,hijacking,image,jailbreak,aes,prompt,jailbreaking,attack">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and_CVPR_2025_paper.html" target="_blank" title="370/388"><span class="index notranslate">#370</span></a>
                <a id="title-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="title-link" href="/venue/Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" target="_blank">Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking</a>
                <a id="pdf-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Junxi Chen" target="_blank">Junxi Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junhao Dong" target="_blank">Junhao Dong</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaohua Xie" target="_blank">Xiaohua Xie</a>
            </p>
            <p id="summary-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="summary">Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly integrated into text-to-image diffusion models (T2I-DMs) to improve controllability. However, in this paper, we reveal that T2I-DMs equipped with the IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking attack. We demonstrate that, by uploading imperceptible image-space adversarial examples (AEs), the adversary can hijack massive benign users to jailbreak an Image Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to discredit the service provider. Worse still, the IP-Adapter's dependency on open-source image encoders reduces the knowledge required to craft AEs. Extensive experiments verify the technical feasibility of the hijacking attack. In light of the revealed threat, we investigate several existing defenses and explore combining the IP-Adapter with adversarially trained models to overcome existing defenses' limitations.</p>
            <p id="subjects-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" onclick="foldPdfKimi('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="panel paper" keywords="deepfake,silence,audio,fake,video,datasets,unsupervised,circumventing,shortcuts,videos">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning_CVPR_2025_paper.html" target="_blank" title="371/388"><span class="index notranslate">#371</span></a>
                <a id="title-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="title-link" href="/venue/Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" target="_blank">Circumventing Shortcuts in Audio-visual Deepfake Detection Datasets with Unsupervised Learning</a>
                <a id="pdf-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF">6</sup>]</a>
                <a id="copy-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Stefan Smeu" target="_blank">Stefan Smeu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dragos-Alexandru Boldisor" target="_blank">Dragos-Alexandru Boldisor</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dan Oneata" target="_blank">Dan Oneata</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elisabeta Oneata" target="_blank">Elisabeta Oneata</a>
            </p>
            <p id="summary-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="summary">Good datasets are essential for developing and benchmarking any machine learning system. Their importance is even more extreme for safety critical applications such as deepfake detection---the focus of this paper. Here we reveal that two of the most widely used audio-video deepfake datasets suffer from a previously unidentified spurious feature: the leading silence. Fake videos start with a very brief moment of silence and based on this feature alone, we can separate the real and fake samples almost perfectly. As such, previous audio-only and audio-video models exploit the presence of silence in the fake videos and consequently perform worse when the leading silence is removed. To circumvent latching on such unwanted artifact and possibly other unrevealed ones we propose a shift from supervised to unsupervised learning by training models exclusively on real data. We show that by aligning self-supervised audio-video representations we remove the risk of relying on dataset-specific biases and improve robustness in deepfake detection.</p>
            <p id="subjects-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" onclick="foldPdfKimi('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="panel paper" keywords="human,huperflow,flow,perceived,perception,benchmark,scenes,optical,motion,truth">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation_CVPR_2025_paper.html" target="_blank" title="372/388"><span class="index notranslate">#372</span></a>
                <a id="title-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="title-link" href="/venue/Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" target="_blank">HuPerFlow: A Comprehensive Benchmark for Human vs. Machine Motion Estimation Comparison</a>
                <a id="pdf-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yung-Hao Yang" target="_blank">Yung-Hao Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zitang Sun" target="_blank">Zitang Sun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Taiki Fukiage" target="_blank">Taiki Fukiage</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shin'ya Nishida" target="_blank">Shin'ya Nishida</a>
            </p>
            <p id="summary-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="summary">As AI models are increasingly integrated into applications involving human interaction, understanding the alignment between human perception and machine vision has become essential. One example is the estimation of visual motion (optical flow) in dynamic applications such as driving assistance. While there are numerous optical flow datasets and benchmarks with ground truth information, human-perceived flow in natural scenes remains underexplored. We introduce HuPerFlowa benchmark for human-perceived flow, measured at 2,400 locations across ten optical flow datasets, with \~38,400 response vectors collected through online psychophysical experiments. Our data demonstrate that human-perceived flow aligns with ground truth in spatiotemporally smooth locations while also showing systematic errors influenced by various environmental properties. Additionally, we evaluated several optical flow algorithms against human-perceived flow, uncovering both similarities and unique aspects of human perception in complex natural scenes. HuPerFlow is the first large-scale human-perceived flow benchmark for alignment between computer vision models and human perception, as well as for scientific exploration of human motion perception in natural scenes. The HuPerFlow benchmark will be available online upon acceptance.</p>
            <p id="subjects-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" onclick="foldPdfKimi('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="panel paper" keywords="hsn,tree,monitoring,grained,uavtc,fine,tcs,changes,hierarchical,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a_CVPR_2025_paper.html" target="_blank" title="373/388"><span class="index notranslate">#373</span></a>
                <a id="title-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="title-link" href="/venue/Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" target="_blank">Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection</a>
                <a id="pdf-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yante Li" target="_blank">Yante Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hanwen Qi" target="_blank">Hanwen Qi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Haoyu Chen" target="_blank">Haoyu Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xinlian Liang" target="_blank">Xinlian Liang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Guoying Zhao" target="_blank">Guoying Zhao</a>
            </p>
            <p id="summary-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="summary">In environmental protection, tree monitoring plays an essential role in maintaining and improving ecosystem health. However, precise monitoring is challenging because existing datasets fail to capture continuous fine-grained changes in trees due to low-resolution images and high acquisition costs. In this paper, we introduce UAVTC, a large-scale, long-term, high-resolution dataset collected using UAVs equipped with cameras, specifically designed to detect individual Tree Changes (TCs). UAVTC includes rich annotations and statistics based on biological knowledge, offering a fine-grained view for tree monitoring. To address environmental influences and effectively model the hierarchical diversity of physiological TCs, we propose a novel Hyperbolic Siamese Network (HSN) for TC detection, enabling compact and hierarchical representations of dynamic tree changes. Extensive experiments show that HSN can effectively capture complex hierarchical changes and provide a robust solution for fine-grained TC detection. In addition, HSN generalizes well to cross-domain face anti-spoofing task, highlighting its broader significance in AI. We believe our work, combining ecological insights and interdisciplinary expertise, will benefit the community by offering a new benchmark and innovative AI technologies. Source code and dataset will be made available.</p>
            <p id="subjects-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" onclick="foldPdfKimi('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="panel paper" keywords="ssl,nnu,medical,net,segmentation,mae,brain,mri,39k,maes">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation_CVPR_2025_paper.html" target="_blank" title="374/388"><span class="index notranslate">#374</span></a>
                <a id="title-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="title-link" href="/venue/Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" target="_blank">Revisiting MAE Pre-training for 3D Medical Image Segmentation</a>
                <a id="pdf-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF">16</sup>]</a>
                <a id="copy-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Tassilo Wald" target="_blank">Tassilo Wald</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Constantin Ulrich" target="_blank">Constantin Ulrich</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Stanislav Lukyanenko" target="_blank">Stanislav Lukyanenko</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrei Goncharov" target="_blank">Andrei Goncharov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alberto Paderno" target="_blank">Alberto Paderno</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Maximilian Miller" target="_blank">Maximilian Miller</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Leander Maerkisch" target="_blank">Leander Maerkisch</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Paul Jaeger" target="_blank">Paul Jaeger</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Klaus Maier-Hein" target="_blank">Klaus Maier-Hein</a>
            </p>
            <p id="summary-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="summary">Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the potential of vast, untapped clinical datasets, for various downstream applications that suffer from the scarcity of labeled data.While SSL has revolutionized fields like natural language processing and computer vision, its adoption in 3D medical image computing has been limited by three key pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D medical image analysis, and insufficient evaluation practices. In this paper, we address these issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes and ii) using a Residual Encoder U-Net architecture within the state-of-the-art nnU-Net framework. iii) A robust development framework, incorporating 5 development and 8 testing brain MRI segmentation datasets, allowed performance-driven design decisions to optimize the simple concept of Masked Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses previous SSL methods but also outperforms the strong nnU-Net baseline by an average of approximately 3 Dice points setting a new state-of-the-art.Our code and models are made available.</p>
            <p id="subjects-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" onclick="foldPdfKimi('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="panel paper" keywords="ps3,resolution,pre,training,mllms,vision,384x384,perception,scaling,details">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Scaling_Vision_Pre-Training_to_4K_Resolution_CVPR_2025_paper.html" target="_blank" title="375/388"><span class="index notranslate">#375</span></a>
                <a id="title-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="title-link" href="/venue/Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" target="_blank">Scaling Vision Pre-Training to 4K Resolution</a>
                <a id="pdf-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Shi_Scaling_Vision_Pre-Training_to_4K_Resolution_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Baifeng Shi" target="_blank">Baifeng Shi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyi Li" target="_blank">Boyi Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Han Cai" target="_blank">Han Cai</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Lu" target="_blank">Yao Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sifei Liu" target="_blank">Sifei Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Pavone" target="_blank">Marco Pavone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jan Kautz" target="_blank">Jan Kautz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Song Han" target="_blank">Song Han</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Trevor Darrell" target="_blank">Trevor Darrell</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Pavlo Molchanov" target="_blank">Pavlo Molchanov</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongxu Yin" target="_blank">Hongxu Yin</a>
            </p>
            <p id="summary-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="summary">High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 384x384) due to the quadratic cost of processing larger images. We introduce PS3, for Pre-training with Scale-Selective Scaling, that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of processing entire global images, PS3 is pre-trained to selectively process local regions and contrast them with local detailed captions, allowing it to learn detailed representation at high resolution with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global low-resolution image and select local high-resolution regions to process based on their saliency or relevance to a text prompt. When applied to multi-modal LLMs (MLLMs), PS3 demonstrates performance that effectively scales with the pre-training resolution and significantly improves over baselines without high-resolution pre-training. We also find current benchmarks do not require recognizing details at 4K resolution, which motivates us to propose 4KPro, a new benchmark that evaluates visual perception at 4K resolution, on which PS3 outperforms state-of-the-art MLLMs, including a 13% improvement over GPT-4o.</p>
            <p id="subjects-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" onclick="foldPdfKimi('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="panel paper" keywords="comm,interleaved,multimodal,mllms,content,quality,text,dataset,coherent,consistency">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and_CVPR_2025_paper.html" target="_blank" title="376/388"><span class="index notranslate">#376</span></a>
                <a id="title-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="title-link" href="/venue/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" target="_blank">CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation</a>
                <a id="pdf-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wei Chen" target="_blank">Wei Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Lin Li" target="_blank">Lin Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yongqi Yang" target="_blank">Yongqi Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Bin Wen" target="_blank">Bin Wen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fan Yang" target="_blank">Fan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tingting Gao" target="_blank">Tingting Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yu Wu" target="_blank">Yu Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Long Chen" target="_blank">Long Chen</a>
            </p>
            <p id="summary-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="summary">Interleaved image-text generation has emerged as a crucial multimodal task, aiming at creating sequences of interleaved visual and textual content given a query. Despite notable advancements in recent multimodal large language models (MLLMs), generating integrated image-text sequences that exhibit narrative coherence and entity and style consistency remains challenging due to poor training data quality. To address this gap, we introduce CoMM, a high-quality Coherent interleaved image-text MultiModal dataset designed to enhance the coherence, consistency, and alignment of generated multimodal content. Initially, CoMM harnesses raw data from diverse sources, focusing on instructional content and visual storytelling, establishing a foundation for coherent and consistent content. To further refine the data quality, we devise a multi-perspective filter strategy that leverages advanced pre-trained models to ensure the development of sentences, consistency of inserted images, and semantic alignment between them. Various quality evaluation metrics are designed to prove the high quality of the filtered dataset. Meanwhile, extensive few-shot experiments on various downstream tasks demonstrate CoMM's effectiveness in significantly enhancing the in-context learning capabilities of MLLMs. Moreover, we propose four new tasks to evaluate MLLMs' interleaved generation abilities, supported by a comprehensive evaluation framework. We believe CoMM opens a new avenue for advanced MLLMs with superior multimodal in-context learning and understanding ability. The dataset and codes will be released.</p>
            <p id="subjects-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" onclick="foldPdfKimi('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="panel paper" keywords="language,driving,simlingo,vision,autonomous,understanding,bench2drive,action,loop,alignment">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment_CVPR_2025_paper.html" target="_blank" title="377/388"><span class="index notranslate">#377</span></a>
                <a id="title-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="title-link" href="/venue/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" target="_blank">SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment</a>
                <a id="pdf-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Katrin Renz" target="_blank">Katrin Renz</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Long Chen" target="_blank">Long Chen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elahe Arani" target="_blank">Elahe Arani</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Oleg Sinavski" target="_blank">Oleg Sinavski</a>
            </p>
            <p id="summary-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="summary">Integrating large language models (LLMs) into autonomous driving has attracted significant attention with the hope of improving generalization and explainability. However, existing methods often focus on either driving or vision-language understanding but achieving both high driving performance and extensive language understanding remains challenging. In addition, the dominant approach to tackle vision-language understanding is using visual question answering. However, for autonomous driving, this is only useful if it is grounded in the action space. Otherwise, the models answers could be inconsistent with its behavior. Therefore, we propose a model that can handle three different tasks: (1) closed-loop driving, (2) vision-language understanding, and (3) language-action alignment. Our model SimLingo is based on a vision language model (VLM) and works using only camera, excluding expensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on the widely used CARLA simulator on the Leaderboard 2.0 and the Bench2Drive benchmarks. Additionally, we achieve strong results in a wide variety of language-related tasks while maintaining high driving performance. We will release code, data and models upon acceptance.</p>
            <p id="subjects-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" onclick="foldPdfKimi('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="panel paper" keywords="compositing,layout,multitwine,object,text,reposing,selfie,hugging,guitar,props">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control_CVPR_2025_paper.html" target="_blank" title="378/388"><span class="index notranslate">#378</span></a>
                <a id="title-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="title-link" href="/venue/Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" target="_blank">Multitwine: Multi-Object Compositing with Text and Layout Control</a>
                <a id="pdf-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Gemma Canet Tarrs" target="_blank">Gemma Canet Tarrs</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhe Lin" target="_blank">Zhe Lin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhifei Zhang" target="_blank">Zhifei Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=He Zhang" target="_blank">He Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Andrew Gilbert" target="_blank">Andrew Gilbert</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=John Collomosse" target="_blank">John Collomosse</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Soo Ye Kim" target="_blank">Soo Ye Kim</a>
            </p>
            <p id="summary-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="summary">We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from simple positional relations (e.g., next to, in front of) to complex actions requiring reposing (e.g., hugging, playing guitar). When an interaction implies additional props, like 'taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as customization, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data.</p>
            <p id="subjects-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" onclick="foldPdfKimi('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="panel paper" keywords="textbf,dfmvc,akan,kan,clustering,view,attention,attributes,fair,sensitive">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN_CVPR_2025_paper.html" target="_blank" title="379/388"><span class="index notranslate">#379</span></a>
                <a id="title-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="title-link" href="/venue/Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" target="_blank">Deep Fair Multi-View Clustering with Attention KAN</a>
                <a id="pdf-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=HaiMing Xu" target="_blank">HaiMing Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Qianqian Wang" target="_blank">Qianqian Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Boyue Wang" target="_blank">Boyue Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Quanxue Gao" target="_blank">Quanxue Gao</a>
            </p>
            <p id="summary-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="summary">Multi-view clustering, while effective in integrating information from diverse data sources, may lead to biased outcomes when sensitive attributes are involved. Despite the substantial progress in recent research, most existing methods suffer from limited interpretability and lack strong mathematical theoretical foundations. In this work, we propose a novel approach, \textbf{D}eep \textbf{F}air \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{A}ttention \textbf{K}olmogorov-\textbf{A}rnold \textbf{N}etwork (DFMVC-AKAN), designed to generate fair clustering results while maintaining robust performance. DFMVC-AKAN integrates attention mechanisms with multi-view data reconstruction to enhance both clustering accuracy and fairness. The model introduces an attention mechanism and Kolmogorov-Arnold Networks (KAN), which together address the challenges of feature fusion and the influence of sensitive attributes in multi-view data. The attention mechanism enables the model to dynamically focus on the most relevant features across different views, while KAN provides a nonlinear feature representation capable of efficiently approximating arbitrary multivariate continuous functions, thereby capturing complex relationships and latent patterns within the data. Experimental results on four datasets containing sensitive attributes demonstrate that DFMVC-AKAN significantly improves fairness and clustering performance compared to state-of-the-art methods.</p>
            <p id="subjects-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" onclick="foldPdfKimi('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="panel paper" keywords="compositional,vlms,gde,visual,compositionality,representations,language,vision,space,text">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language_CVPR_2025_paper.html" target="_blank" title="380/388"><span class="index notranslate">#380</span></a>
                <a id="title-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="title-link" href="/venue/Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" target="_blank">Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models</a>
                <a id="pdf-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF">9</sup>]</a>
                <a id="copy-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF">5</sup>]</a>
                <a id="rel-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Davide Berasi" target="_blank">Davide Berasi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Matteo Farina" target="_blank">Matteo Farina</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Massimiliano Mancini" target="_blank">Massimiliano Mancini</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elisa Ricci" target="_blank">Elisa Ricci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nicola Strisciuglio" target="_blank">Nicola Strisciuglio</a>
            </p>
            <p id="summary-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="summary">Vision-Language Models (VLMs) learn a shared feature space for text and images, enabling the comparison of inputs of different modalities. While prior works demonstrated that VLMs organize natural language representations into regular structures encoding composite meanings, it remains unclear if compositional patterns also emerge in the visual embedding space. In this work, we investigate compositionality in the image domain, where the analysis of compositional properties is challenged by noise and sparsity of visual data.We propose a framework, called Geodesically Decomposable Embeddings (GDE), that addresses these problems and approximates image representations with geometry-aware compositional structures in the latent space. We demonstrate that visual embeddings of pre-trained VLMs exhibit a compositional arrangement, and evaluate the effectiveness of this property in the tasks of compositional classification and group robustness. GDE achieves stronger performance in compositional classification compared to its counterpart method that assumes linear geometry of the latent space. Notably, it is particularly effective for group robustness, where we achieve higher results than task-specific solutions. Our results indicate that VLMs can automatically develop a human-like form of compositional reasoning in the visual domain, making their underlying processes more interpretable.</p>
            <p id="subjects-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" onclick="foldPdfKimi('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="panel paper" keywords="attribute,comca,cache,vocabulary,compositional,attributes,caching,vlms,color,open">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection_CVPR_2025_paper.html" target="_blank" title="381/388"><span class="index notranslate">#381</span></a>
                <a id="title-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="title-link" href="/venue/Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" target="_blank">Compositional Caching for Training-free Open-vocabulary Attribute Detection</a>
                <a id="pdf-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Marco Garosi" target="_blank">Marco Garosi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Alessandro Conti" target="_blank">Alessandro Conti</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Gaowen Liu" target="_blank">Gaowen Liu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Elisa Ricci" target="_blank">Elisa Ricci</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Massimiliano Mancini" target="_blank">Massimiliano Mancini</a>
            </p>
            <p id="summary-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="summary">Attribute detection is crucial for many computer vision tasks, as it enables systems to describe properties such as color, texture, and material. Current approaches often rely on labor-intensive annotation processes which are inherently limited: objects can be described at an arbitrary level of detail (e.g., color vs. color shades), leading to ambiguities when the annotators are not instructed carefully. Furthermore, they operate within a predefined set of attributes, reducing scalability and adaptability to unforeseen downstream applications. We present Compositional Caching (ComCa), a training-free method for open-vocabulary attribute detection that overcomes these constraints. ComCa requires only the list of target attributes and objects as input, using them to populate an auxiliary cache of images by leveraging web-scale databases and Large Language Models to determine attribute-object compatibility. To account for the compositional nature of attributes, cache images receive soft attribute labels. Those are aggregated at inference time based on the similarity between the input and cache images, refining the predictions of underlying Vision-Language Models (VLMs). Importantly, our approach is model-agnostic, compatible with various VLMs. Experiments on public datasets demonstrate that ComCa significantly outperforms zero-shot and cache-based baselines, competing with recent training-based methods, proving that a carefully designed training-free approach can successfully address open-vocabulary attribute detection.</p>
            <p id="subjects-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" onclick="foldPdfKimi('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="panel paper" keywords="matting,color,polarization,matte,screen,polarized,alpha,mattes,compositing,lcd">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Enomoto_Polarized_Color_Screen_Matting_CVPR_2025_paper.html" target="_blank" title="382/388"><span class="index notranslate">#382</span></a>
                <a id="title-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="title-link" href="/venue/Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" target="_blank">Polarized Color Screen Matting</a>
                <a id="pdf-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Enomoto_Polarized_Color_Screen_Matting_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF"></sup>]</a>
                <a id="rel-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Kenji Enomoto" target="_blank">Kenji Enomoto</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Scott Cohen" target="_blank">Scott Cohen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Brian Price" target="_blank">Brian Price</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=TJ Rhodes" target="_blank">TJ Rhodes</a>
            </p>
            <p id="summary-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="summary">This paper considers the long-standing problem of extracting alpha mattes from video using a known background. While various color-based or polarization-based approaches have been studied in past decades, the problem remains ill-posed because the solutions solely rely on either color or polarization. We introduce Polarized Color Screen Matting, a single-shot, per-pixel matting theory for alpha matte and foreground color recovery using both color and polarization cues. Through a theoretical analysis of our diffuse-specular polarimetric compositing equation, we derive practical closed-form matting methods with their solvability conditions. Our theory concludes that an alpha matte can be extracted without manual corrections using off-the-shelf equipment such as an LCD monitor, polarization camera, and unpolarized lights with calibrated color. Experiments on synthetic and real-world datasets verify the validity of our theory and show the capability of our matting methods on real videos with quantitative and qualitative comparisons to color-based and polarization-based matting methods.</p>
            <p id="subjects-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" onclick="foldPdfKimi('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="panel paper" keywords="ebs,aps,ekf,star,event,tracking,accurate,signal,trackers,based">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking_CVPR_2025_paper.html" target="_blank" title="383/388"><span class="index notranslate">#383</span></a>
                <a id="title-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="title-link" href="/venue/Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" target="_blank">EBS-EKF: Accurate and High Frequency Event-based Star Tracking</a>
                <a id="pdf-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Albert W. Reed" target="_blank">Albert W. Reed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Connor Hashemi" target="_blank">Connor Hashemi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Dennis Melamed" target="_blank">Dennis Melamed</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nitesh Menon" target="_blank">Nitesh Menon</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Keigo Hirakawa" target="_blank">Keigo Hirakawa</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Scott McCloskey" target="_blank">Scott McCloskey</a>
            </p>
            <p id="summary-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="summary">Event-based sensors (EBS) are a promising new technology for star tracking due to their low latency and power efficiency, but prior work has thus far been evaluated exclusively in simulation with simplified signal models. We propose a novel algorithm for event-based star tracking, grounded in an analysis of the EBS circuit and an extended Kalman filter (EKF). We quantitatively evaluate our method using real night sky data, comparing its results with those from a space-ready active-pixel sensor (APS) star tracker. We demonstrate that our method is an order-of-magnitude more accurate than existing methods due to improved signal modeling and state estimation, while providing more frequent updates and greater motion tolerance than conventional APS trackers. We provide all code and the first dataset of events synchronized with APS solutions.</p>
            <p id="subjects-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" onclick="foldPdfKimi('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="panel paper" keywords="matrix3d,photogrammetry,pose,depth,modality,synthesis,modal,dit,model,multi">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One_CVPR_2025_paper.html" target="_blank" title="384/388"><span class="index notranslate">#384</span></a>
                <a id="title-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="title-link" href="/venue/Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" target="_blank">Matrix3D: Large Photogrammetry Model All-in-One</a>
                <a id="pdf-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF">3</sup>]</a>
                <a id="copy-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF">1</sup>]</a>
                <a id="rel-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanxun Lu" target="_blank">Yuanxun Lu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jingyang Zhang" target="_blank">Jingyang Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tian Fang" target="_blank">Tian Fang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jean-Daniel Nahmias" target="_blank">Jean-Daniel Nahmias</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yanghai Tsin" target="_blank">Yanghai Tsin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Long Quan" target="_blank">Long Quan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xun Cao" target="_blank">Xun Cao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yao Yao" target="_blank">Yao Yao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shiwei Li" target="_blank">Shiwei Li</a>
            </p>
            <p id="summary-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="summary">We present Matrix3D, a unified model that performs several photogrammetry subtasks, including pose estimation, depth prediction, and novel view synthesis using just the same model. Matrix3D utilizes a multi-modal diffusion transformer (DiT) to integrate transformations across several modalities, such as images, camera parameters, and depth maps. The key to Matrix3D's large-scale multi-modal training lies in the incorporation of a mask learning strategy. This enables full-modality model training even with partially complete data, such as bi-modality data of image-pose and image-depth pairs, thus significantly increases the pool of available training data.Matrix3D demonstrates state-of-the-art performance in pose estimation and novel view synthesis tasks. Additionally, it offers fine-grained control through multi-round interactions, making it an innovative tool for 3D content creation.</p>
            <p id="subjects-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" onclick="foldPdfKimi('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="panel paper" keywords="depthcrafter,depth,videos,video,open,world,sequences,generating,long,consistent">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.html" target="_blank" title="385/388"><span class="index notranslate">#385</span></a>
                <a id="title-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="title-link" href="/venue/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" target="_blank">DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</a>
                <a id="pdf-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF">1</sup>]</a>
                <a id="copy-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Wenbo Hu" target="_blank">Wenbo Hu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiangjun Gao" target="_blank">Xiangjun Gao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyu Li" target="_blank">Xiaoyu Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Sijie Zhao" target="_blank">Sijie Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaodong Cun" target="_blank">Xiaodong Cun</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yong Zhang" target="_blank">Yong Zhang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Long Quan" target="_blank">Long Quan</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ying Shan" target="_blank">Ying Shan</a>
            </p>
            <p id="summary-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="summary">Estimating video depth in open-world scenarios is challenging due to the diversity of videos in appearance, content motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. The generalization ability to open-world videos is achieved by training the video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that can process extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.</p>
            <p id="subjects-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" onclick="foldPdfKimi('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="panel paper" keywords="skinning,avatar,fresa,personalized,avatars,canonicalization,feedforward,skinned,animation,weights">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images_CVPR_2025_paper.html" target="_blank" title="386/388"><span class="index notranslate">#386</span></a>
                <a id="title-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="title-link" href="/venue/Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" target="_blank">FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images</a>
                <a id="pdf-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF">2</sup>]</a>
                <a id="copy-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF">2</sup>]</a>
                <a id="rel-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Rong Wang" target="_blank">Rong Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Fabian Prada" target="_blank">Fabian Prada</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Ziyan Wang" target="_blank">Ziyan Wang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zhongshi Jiang" target="_blank">Zhongshi Jiang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chengxiang Yin" target="_blank">Chengxiang Yin</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Junxuan Li" target="_blank">Junxuan Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Shunsuke Saito" target="_blank">Shunsuke Saito</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Igor Santesteban" target="_blank">Igor Santesteban</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Javier Romero" target="_blank">Javier Romero</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Rohan Joshi" target="_blank">Rohan Joshi</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hongdong Li" target="_blank">Hongdong Li</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jason Saragih" target="_blank">Jason Saragih</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yaser Sheikh" target="_blank">Yaser Sheikh</a>
            </p>
            <p id="summary-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="summary">We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos.</p>
            <p id="subjects-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" onclick="foldPdfKimi('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="panel paper" keywords="sonata,self,probing,reliable,supervised,representations,point,spatial,140k,cloud">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations_CVPR_2025_paper.html" target="_blank" title="387/388"><span class="index notranslate">#387</span></a>
                <a id="title-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="title-link" href="/venue/Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" target="_blank">Sonata: Self-Supervised Learning of Reliable Point Representations</a>
                <a id="pdf-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF">5</sup>]</a>
                <a id="copy-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Xiaoyang Wu" target="_blank">Xiaoyang Wu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Daniel DeTone" target="_blank">Daniel DeTone</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Duncan Frost" target="_blank">Duncan Frost</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Tianwei Shen" target="_blank">Tianwei Shen</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Chris Xie" target="_blank">Chris Xie</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Nan Yang" target="_blank">Nan Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Jakob Engel" target="_blank">Jakob Engel</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Richard Newcombe" target="_blank">Richard Newcombe</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Hengshuang Zhao" target="_blank">Hengshuang Zhao</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Julian Straub" target="_blank">Julian Straub</a>
            </p>
            <p id="summary-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="summary">In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on representation quality through linear probing. We hypothesize that this is due to what we term the geometric shortcut, which causes representations to collapse to low-level spatial features. This challenge is unique to 3D and arises from the sparse nature of point cloud data. We address it through two key strategies: obscuring spatial information and enhancing the reliance on input features, ultimately composing a Sonata of 140k point clouds through self-distillation. Sonata is simple and intuitive, yet its learned representations are strong and reliable: zero-shot visualizations demonstrate semantic grouping, alongside strong spatial reasoning through nearest-neighbor relationships. Sonata demonstrates exceptional parameter and data efficiency, tripling linear probing accuracy (from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1% of the data compared to previous approaches. Full fine-tuning further advances SOTA across both 3D indoor and outdoor perception tasks. All code and weights will be made available.</p>
            <p id="subjects-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" onclick="foldPdfKimi('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF', this)" class="hr hr-fold">
        </div><div id="Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="panel paper" keywords="skeletons,generation,skeletal,view,skdream,controllable,skeleton,multi,mesh,dataset">
            <h2 class="title">
                <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons_CVPR_2025_paper.html" target="_blank" title="388/388"><span class="index notranslate">#388</span></a>
                <a id="title-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="title-link" href="/venue/Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" target="_blank">SKDream: Controllable Multi-view and 3D Generation with Arbitrary Skeletons</a>
                <a id="pdf-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="title-pdf notranslate" onclick="togglePdf('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF', this)" data="https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons_CVPR_2025_paper.pdf">[PDF<sup id="pdf-stars-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF">4</sup>]</a>
                <a id="copy-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="title-copy notranslate" onclick="copyToClipboard('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF')">[Copy]</a>
                <a id="kimi-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="title-kimi notranslate" onclick="toggleKimi('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF', this)">[Kimi<sup id="kimi-stars-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF">4</sup>]</a>
                <a id="rel-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="title-rel notranslate" onclick="openRelatedPapers('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF')">[REL]</a>
            </h2>
            <p id="authors-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="metainfo authors notranslate"><strong>Authors</strong>:
                <a class="author notranslate" href="https://www.google.com/search?q=Yuanyou Xu" target="_blank">Yuanyou Xu</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Zongxin Yang" target="_blank">Zongxin Yang</a>,
                <a class="author notranslate" href="https://www.google.com/search?q=Yi Yang" target="_blank">Yi Yang</a>
            </p>
            <p id="summary-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="summary">Controllable generation has achieved substantial progress in both 2D and 3D domains, yet current conditional generation methods still face limitations in describing detailed shape structures. Skeletons can effectively represent and describe object anatomy and pose. Unfortunately, past studies are often limited to human skeletons. In this work, we generalize skeletal conditioned generation to arbitrary structures. First, we design a reliable mesh skeletonization pipeline to generate a large-scale mesh-skeleton paired dataset.Based on the dataset, a multi-view and 3D generation pipeline is built. We propose to represent 3D skeletons by Coordinate Color Encoding as 2D conditional images. A Skeletal Correlation Module is designed to extract global skeletal features for condition injection. After multi-view images are generated, 3D assets can be obtained by incorporating a large reconstruction model, followed with a UV texture refinement stage. As a result, our method achieves instant generation of multi-view and 3D contents which are aligned with given skeletons. The proposed techniques largely improve the object-skeleton alignment and generation quality.Project page at https://skdream3d.github.io/. Dataset, code and models will be released in public.</p>
            <p id="subjects-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="metainfo subjects"><strong>Subject</strong>:
                <a class="subject-1" href="/venue/CVPR.2025?group=Highlight" target="_blank">CVPR.2025 - Highlight</a>
            </p>
            <div id="pdf-container-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="pdf-container" style="display:none"></div>
            <div id="kimi-container-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" class="kimi-container notranslate" style="display:none"></div>
            <hr id="fold-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" onclick="foldPdfKimi('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF', this)" class="hr hr-fold">
        </div></div>
    <div class="footer notranslate">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel notranslate" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include(<a id="logic-included" title="The logical relationship between keywords (OR/AND)" onclick="toggleOrAnd(this)">OR</a>):</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#1</span>
                    <a class="i-title" href="#Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF">TKG-DM: Training-free Chroma Key Content Generation Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#2</span>
                    <a class="i-title" href="#Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF">Context-Aware Multimodal Pretraining</a>
                    <a class="i-star" onclick="toggleAppStar('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Roth_Context-Aware_Multimodal_Pretraining@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#3</span>
                    <a class="i-title" href="#Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF">Towards RAW Object Detection in Diverse Conditions</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Towards_RAW_Object_Detection_in_Diverse_Conditions@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#4</span>
                    <a class="i-title" href="#Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF">ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#5</span>
                    <a class="i-title" href="#Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF">Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#6</span>
                    <a class="i-title" href="#Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF">Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</a>
                    <a class="i-star" onclick="toggleAppStar('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#7</span>
                    <a class="i-title" href="#Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF">Diffusion-based Realistic Listening Head Generation via Hybrid Motion Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#8</span>
                    <a class="i-title" href="#Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF">Hyperbolic Safety-Aware Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#9</span>
                    <a class="i-title" href="#Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF">XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#10</span>
                    <a class="i-title" href="#Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF">Improving Personalized Search with Regularized Low-Rank Parameter Updates</a>
                    <a class="i-star" onclick="toggleAppStar('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#11</span>
                    <a class="i-title" href="#Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF">All-Optical Nonlinear Diffractive Deep Network for Ultrafast Image Denoising</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#12</span>
                    <a class="i-title" href="#Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF">NTClick: Achieving Precise Interactive Segmentation With Noise-tolerant Clicks</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#13</span>
                    <a class="i-title" href="#Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF">Balanced Rate-Distortion Optimization in Learned Image Compression</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#14</span>
                    <a class="i-title" href="#Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF">Estimating Body and Hand Motion in an Ego-sensed World</a>
                    <a class="i-star" onclick="toggleAppStar('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#15</span>
                    <a class="i-title" href="#Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF">Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#16</span>
                    <a class="i-title" href="#Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF">COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts</a>
                    <a class="i-star" onclick="toggleAppStar('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#17</span>
                    <a class="i-title" href="#Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF">ESC: Erasing Space Concept for Knowledge Deletion</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#18</span>
                    <a class="i-title" href="#Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF">Scene-Centric Unsupervised Panoptic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#19</span>
                    <a class="i-title" href="#Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF">RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars</a>
                    <a class="i-star" onclick="toggleAppStar('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#20</span>
                    <a class="i-title" href="#Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF">MITracker: Multi-View Integration for Visual Object Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#21</span>
                    <a class="i-title" href="#Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF">You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_You_See_it_You_Got_it_Learning_3D_Creation_on@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#22</span>
                    <a class="i-title" href="#Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF">InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#23</span>
                    <a class="i-title" href="#Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF">Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#24</span>
                    <a class="i-title" href="#Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF">4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#25</span>
                    <a class="i-title" href="#Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF">ICP: Immediate Compensation Pruning for Mid-to-high Sparsity</a>
                    <a class="i-star" onclick="toggleAppStar('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            <p id="app-bar-star-Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#26</span>
                    <a class="i-title" href="#Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF">Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</a>
                    <a class="i-star" onclick="toggleAppStar('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#27</span>
                    <a class="i-title" href="#Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF">FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#28</span>
                    <a class="i-title" href="#Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF">Reference-Based 3D-Aware Image Editing with Triplanes</a>
                    <a class="i-star" onclick="toggleAppStar('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#29</span>
                    <a class="i-title" href="#Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF">3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes</a>
                    <a class="i-star" onclick="toggleAppStar('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#30</span>
                    <a class="i-title" href="#Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF">ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#31</span>
                    <a class="i-title" href="#Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF">GenVDM: Generating Vector Displacement Maps From a Single Image</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#32</span>
                    <a class="i-title" href="#Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF">CrossOver: 3D Scene Cross-Modal Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#33</span>
                    <a class="i-title" href="#Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF">MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations</a>
                    <a class="i-star" onclick="toggleAppStar('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#34</span>
                    <a class="i-title" href="#Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF">PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes</a>
                    <a class="i-star" onclick="toggleAppStar('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#35</span>
                    <a class="i-title" href="#Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF">Multimodal Autoregressive Pre-training of Large Vision Encoders</a>
                    <a class="i-star" onclick="toggleAppStar('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#36</span>
                    <a class="i-title" href="#Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF">Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#37</span>
                    <a class="i-title" href="#Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF">MLLM-as-a-Judge for Image Safety without Human Labeling</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#38</span>
                    <a class="i-title" href="#Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF">SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#39</span>
                    <a class="i-title" href="#Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF">Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#40</span>
                    <a class="i-title" href="#Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF">All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</a>
                    <a class="i-star" onclick="toggleAppStar('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#41</span>
                    <a class="i-title" href="#Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF">MUSt3R: Multi-view Network for Stereo 3D Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#42</span>
                    <a class="i-title" href="#Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF">MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views</a>
                    <a class="i-star" onclick="toggleAppStar('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#43</span>
                    <a class="i-title" href="#Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF">CADDreamer: CAD Object Generation from Single-view Images</a>
                    <a class="i-star" onclick="toggleAppStar('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#44</span>
                    <a class="i-title" href="#Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF">X-Dyna: Expressive Dynamic Human Image Animation</a>
                    <a class="i-star" onclick="toggleAppStar('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#45</span>
                    <a class="i-title" href="#Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF">Visual Representation Learning through Causal Intervention for Controllable Image Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#46</span>
                    <a class="i-title" href="#Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</a>
                    <a class="i-star" onclick="toggleAppStar('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#47</span>
                    <a class="i-title" href="#Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF">PGC: Physics-Based Gaussian Cloth from a Single Pose</a>
                    <a class="i-star" onclick="toggleAppStar('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#48</span>
                    <a class="i-title" href="#Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF">Doppelgangers and Adversarial Vulnerability</a>
                    <a class="i-star" onclick="toggleAppStar('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kamberov_Doppelgangers_and_Adversarial_Vulnerability@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#49</span>
                    <a class="i-title" href="#Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF">Structure-Aware Correspondence Learning for Relative Pose Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#50</span>
                    <a class="i-title" href="#Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF">Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#51</span>
                    <a class="i-title" href="#Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF">Active Hyperspectral Imaging Using an Event Camera</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#52</span>
                    <a class="i-title" href="#Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF">PartGen: Part-level 3D Generation and Reconstruction with Multi-view Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#53</span>
                    <a class="i-title" href="#Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF">MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</a>
                    <a class="i-star" onclick="toggleAppStar('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#54</span>
                    <a class="i-title" href="#Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF">Simulator HC: Regression-based Online Simulation of Starting Problem-Solution Pairs for Homotopy Continuation in Geometric Vision</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#55</span>
                    <a class="i-title" href="#Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF">EventPSR: Surface Normal and Reflectance Estimation from Photometric Stereo Using an Event Camera</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#56</span>
                    <a class="i-title" href="#Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF">DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#57</span>
                    <a class="i-title" href="#Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF">Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#58</span>
                    <a class="i-title" href="#Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF">The Scene Language: Representing Scenes with Programs, Words, and Embeddings</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#59</span>
                    <a class="i-title" href="#Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF">Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</a>
                    <a class="i-star" onclick="toggleAppStar('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#60</span>
                    <a class="i-title" href="#Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF">Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB</a>
                    <a class="i-star" onclick="toggleAppStar('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#61</span>
                    <a class="i-title" href="#Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF">FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#62</span>
                    <a class="i-title" href="#Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF">TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#63</span>
                    <a class="i-title" href="#Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF">Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret</a>
                    <a class="i-star" onclick="toggleAppStar('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#64</span>
                    <a class="i-title" href="#Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF">A Unified Approach to Interpreting Self-supervised Pre-training Methods for 3D Point Clouds via Interactions</a>
                    <a class="i-star" onclick="toggleAppStar('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#65</span>
                    <a class="i-title" href="#Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF">Improving Gaussian Splatting with Localized Points Management</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#66</span>
                    <a class="i-title" href="#Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF">Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#67</span>
                    <a class="i-title" href="#Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF">SkillMimic: Learning Basketball Interaction Skills from Demonstrations</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#68</span>
                    <a class="i-title" href="#Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF">UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#69</span>
                    <a class="i-title" href="#Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF">ROLL: Robust Noisy Pseudo-label Learning for Multi-View Clustering with Noisy Correspondence</a>
                    <a class="i-star" onclick="toggleAppStar('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#70</span>
                    <a class="i-title" href="#Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF">Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding</a>
                    <a class="i-star" onclick="toggleAppStar('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#71</span>
                    <a class="i-title" href="#Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF">SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#72</span>
                    <a class="i-title" href="#Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF">Towards In-the-wild 3D Plane Reconstruction from a Single Image</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#73</span>
                    <a class="i-title" href="#Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF">T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting</a>
                    <a class="i-star" onclick="toggleAppStar('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#74</span>
                    <a class="i-title" href="#Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF">TFCustom: Customized Image Generation with Time-Aware Frequency Feature Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#75</span>
                    <a class="i-title" href="#Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF">Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#76</span>
                    <a class="i-title" href="#Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF">Lifting Motion to the 3D World via 2D Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#77</span>
                    <a class="i-title" href="#Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF">Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#78</span>
                    <a class="i-title" href="#Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF">CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#79</span>
                    <a class="i-title" href="#Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF">Pippo: High-Resolution Multi-View Humans from a Single Image</a>
                    <a class="i-star" onclick="toggleAppStar('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#80</span>
                    <a class="i-title" href="#Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF">EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#81</span>
                    <a class="i-title" href="#Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF">Enduring, Efficient and Robust Trajectory Prediction Attack in Autonomous Driving via Optimization-Driven Multi-Frame Perturbation Framework</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#82</span>
                    <a class="i-title" href="#Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF">STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection</a>
                    <a class="i-star" onclick="toggleAppStar('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#83</span>
                    <a class="i-title" href="#Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF">Interpreting Object-level Foundation Models via Visual Precision Search</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#84</span>
                    <a class="i-title" href="#Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF">Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays</a>
                    <a class="i-star" onclick="toggleAppStar('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#85</span>
                    <a class="i-title" href="#Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF">Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better</a>
                    <a class="i-star" onclick="toggleAppStar('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#86</span>
                    <a class="i-title" href="#Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF">Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures</a>
                    <a class="i-star" onclick="toggleAppStar('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#87</span>
                    <a class="i-title" href="#Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF">MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#88</span>
                    <a class="i-title" href="#Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF">Can Generative Video Models Help Pose Estimation?</a>
                    <a class="i-star" onclick="toggleAppStar('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cai_Can_Generative_Video_Models_Help_Pose_Estimation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#89</span>
                    <a class="i-title" href="#Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF">Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#90</span>
                    <a class="i-title" href="#Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF">High-Fidelity Lightweight Mesh Reconstruction from Point Clouds</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#91</span>
                    <a class="i-title" href="#Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF">UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#92</span>
                    <a class="i-title" href="#Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF">DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving</a>
                    <a class="i-star" onclick="toggleAppStar('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#93</span>
                    <a class="i-title" href="#Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF">Open-Canopy: Towards Very High Resolution Forest Monitoring</a>
                    <a class="i-star" onclick="toggleAppStar('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#94</span>
                    <a class="i-title" href="#Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF">SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</a>
                    <a class="i-star" onclick="toggleAppStar('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#95</span>
                    <a class="i-title" href="#Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF">MangaNinja: Line Art Colorization with Precise Reference Following</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#96</span>
                    <a class="i-title" href="#Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF">SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#97</span>
                    <a class="i-title" href="#Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF">SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#98</span>
                    <a class="i-title" href="#Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF">Panorama Generation From NFoV Image Done Right</a>
                    <a class="i-star" onclick="toggleAppStar('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zheng_Panorama_Generation_From_NFoV_Image_Done_Right@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#99</span>
                    <a class="i-title" href="#Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF">Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy</a>
                    <a class="i-star" onclick="toggleAppStar('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#100</span>
                    <a class="i-title" href="#Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF">Material Anything: Generating Materials for Any 3D Object via Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#101</span>
                    <a class="i-title" href="#Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF">Unveiling Differences in Generative Models: A Scalable Differential Clustering Approach</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#102</span>
                    <a class="i-title" href="#Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF">Parallelized Autoregressive Visual Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Parallelized_Autoregressive_Visual_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#103</span>
                    <a class="i-title" href="#Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF">SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#104</span>
                    <a class="i-title" href="#Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF">KAC: Kolmogorov-Arnold Classifier for Continual Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#105</span>
                    <a class="i-title" href="#Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF">Rethinking Personalized Aesthetics Assessment: Employing Physique Aesthetics Assessment as An Exemplification</a>
                    <a class="i-star" onclick="toggleAppStar('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#106</span>
                    <a class="i-title" href="#Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF">Style-Editor: Text-driven Object-centric Style Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Park_Style-Editor_Text-driven_Object-centric_Style_Editing@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#107</span>
                    <a class="i-title" href="#Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF">Generative Omnimatte: Learning to Decompose Video into Layers</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#108</span>
                    <a class="i-title" href="#Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF">Gradient-Guided Annealing for Domain Generalization</a>
                    <a class="i-star" onclick="toggleAppStar('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ballas_Gradient-Guided_Annealing_for_Domain_Generalization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#109</span>
                    <a class="i-title" href="#Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF">ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping</a>
                    <a class="i-star" onclick="toggleAppStar('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#110</span>
                    <a class="i-title" href="#Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF">MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#111</span>
                    <a class="i-title" href="#Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF">Towards Autonomous Micromobility through Scalable Urban Simulation</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#112</span>
                    <a class="i-title" href="#Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF">Locally Orderless Images for Optimization in Differentiable Rendering</a>
                    <a class="i-star" onclick="toggleAppStar('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#113</span>
                    <a class="i-title" href="#Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF">Multi-modal Vision Pre-training for Medical Image Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#114</span>
                    <a class="i-title" href="#Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF">SACB-Net: Spatial-awareness Convolutions for Medical Image Registration</a>
                    <a class="i-star" onclick="toggleAppStar('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kaneko_Structure_from_Collision@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#115</span>
                    <a class="i-title" href="#Kaneko_Structure_from_Collision@CVPR2025@CVF">Structure from Collision</a>
                    <a class="i-star" onclick="toggleAppStar('Kaneko_Structure_from_Collision@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kaneko_Structure_from_Collision@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#116</span>
                    <a class="i-title" href="#Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF">Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video</a>
                    <a class="i-star" onclick="toggleAppStar('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#117</span>
                    <a class="i-title" href="#Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF">Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#118</span>
                    <a class="i-title" href="#Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF">Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation</a>
                    <a class="i-star" onclick="toggleAppStar('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#119</span>
                    <a class="i-title" href="#Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF">Augmented Deep Contexts for Spatially Embedded Video Coding</a>
                    <a class="i-star" onclick="toggleAppStar('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#120</span>
                    <a class="i-title" href="#Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF">CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering</a>
                    <a class="i-star" onclick="toggleAppStar('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#121</span>
                    <a class="i-title" href="#Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF">HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#122</span>
                    <a class="i-title" href="#Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF">MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#123</span>
                    <a class="i-title" href="#Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF">CoSER: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation</a>
                    <a class="i-star" onclick="toggleAppStar('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#124</span>
                    <a class="i-title" href="#Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF">Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models</a>
                    <a class="i-star" onclick="toggleAppStar('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#125</span>
                    <a class="i-title" href="#Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF">Olympus: A Universal Task Router for Computer Vision Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#126</span>
                    <a class="i-title" href="#Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF">Volumetrically Consistent 3D Gaussian Rasterization</a>
                    <a class="i-star" onclick="toggleAppStar('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#127</span>
                    <a class="i-title" href="#Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF">One-shot 3D Object Canonicalization based on Geometric and Semantic Consistency</a>
                    <a class="i-star" onclick="toggleAppStar('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#128</span>
                    <a class="i-title" href="#Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF">Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision</a>
                    <a class="i-star" onclick="toggleAppStar('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#129</span>
                    <a class="i-title" href="#Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF">Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes</a>
                    <a class="i-star" onclick="toggleAppStar('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#130</span>
                    <a class="i-title" href="#Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF">VEU-Bench: Towards Comprehensive Understanding of Video Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#131</span>
                    <a class="i-title" href="#Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF">Generative Modeling of Class Probability for Multi-Modal Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#132</span>
                    <a class="i-title" href="#Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF">ImViD: Immersive Volumetric Videos for Enhanced VR Engagement</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#133</span>
                    <a class="i-title" href="#Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF">SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#134</span>
                    <a class="i-title" href="#Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF">SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#135</span>
                    <a class="i-title" href="#Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF">Erase Diffusion: Empowering Object Removal Through Calibrating Diffusion Pathways</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#136</span>
                    <a class="i-title" href="#Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF">OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#137</span>
                    <a class="i-title" href="#Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF">MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps</a>
                    <a class="i-star" onclick="toggleAppStar('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#138</span>
                    <a class="i-title" href="#Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF">Unlocking Generalization Power in LiDAR Point Cloud Registration</a>
                    <a class="i-star" onclick="toggleAppStar('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#139</span>
                    <a class="i-title" href="#Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF">Good, Cheap, and Fast: Overfitted Image Compression with Wasserstein Distortion</a>
                    <a class="i-star" onclick="toggleAppStar('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#140</span>
                    <a class="i-title" href="#Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF">Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs</a>
                    <a class="i-star" onclick="toggleAppStar('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#141</span>
                    <a class="i-title" href="#Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF">Unified Reconstruction of Static and Dynamic Scenes from Events</a>
                    <a class="i-star" onclick="toggleAppStar('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#142</span>
                    <a class="i-title" href="#Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF">FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#143</span>
                    <a class="i-title" href="#Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF">WISH: Weakly Supervised Instance Segmentation using Heterogeneous Labels</a>
                    <a class="i-star" onclick="toggleAppStar('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#144</span>
                    <a class="i-title" href="#Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF">Change3D: Revisiting Change Detection and Captioning from A Video Modeling Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#145</span>
                    <a class="i-title" href="#Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF">Multirate Neural Image Compression with Adaptive Lattice Vector Quantization</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#146</span>
                    <a class="i-title" href="#Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF">Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#147</span>
                    <a class="i-title" href="#Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF">Event Ellipsometer: Event-based Mueller-Matrix Video Imaging</a>
                    <a class="i-star" onclick="toggleAppStar('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#148</span>
                    <a class="i-title" href="#Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF">Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters</a>
                    <a class="i-star" onclick="toggleAppStar('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#149</span>
                    <a class="i-title" href="#Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF">Supervising Sound Localization by In-the-wild Egomotion</a>
                    <a class="i-star" onclick="toggleAppStar('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#150</span>
                    <a class="i-title" href="#Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF">v-CLR: View-Consistent Learning for Open-World Instance Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#151</span>
                    <a class="i-title" href="#Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF">GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#152</span>
                    <a class="i-title" href="#Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF">LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#153</span>
                    <a class="i-title" href="#Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF">Assessing and Learning Alignment of Unimodal Vision and Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#154</span>
                    <a class="i-title" href="#Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF">Task-driven Image Fusion with Learnable Fusion Loss</a>
                    <a class="i-star" onclick="toggleAppStar('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#155</span>
                    <a class="i-title" href="#Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF">Distraction is All You Need for Multimodal Large Language Model Jailbreaking</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#156</span>
                    <a class="i-title" href="#Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF">Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#157</span>
                    <a class="i-title" href="#Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF">Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers</a>
                    <a class="i-star" onclick="toggleAppStar('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#158</span>
                    <a class="i-title" href="#Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF">OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints</a>
                    <a class="i-star" onclick="toggleAppStar('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#159</span>
                    <a class="i-title" href="#Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF">Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#160</span>
                    <a class="i-title" href="#Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF">Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#161</span>
                    <a class="i-title" href="#Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF">Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#162</span>
                    <a class="i-title" href="#Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF">CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#163</span>
                    <a class="i-title" href="#Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF">Functionality Understanding and Segmentation in 3D Scenes</a>
                    <a class="i-star" onclick="toggleAppStar('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#164</span>
                    <a class="i-title" href="#An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF">Cross-View Completion Models are Zero-shot Correspondence Estimators</a>
                    <a class="i-star" onclick="toggleAppStar('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#165</span>
                    <a class="i-title" href="#Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF">Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#166</span>
                    <a class="i-title" href="#Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF">Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#167</span>
                    <a class="i-title" href="#Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF">Learning to Filter Outlier Edges in Global SfM</a>
                    <a class="i-star" onclick="toggleAppStar('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#168</span>
                    <a class="i-title" href="#Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF">SpecTRe-GS: Modeling Highly Specular Surfaces with Reflected Nearby Objects by Tracing Rays in 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#169</span>
                    <a class="i-title" href="#Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF">OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities</a>
                    <a class="i-star" onclick="toggleAppStar('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#170</span>
                    <a class="i-title" href="#Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF">GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</a>
                    <a class="i-star" onclick="toggleAppStar('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#171</span>
                    <a class="i-title" href="#Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF">DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#172</span>
                    <a class="i-title" href="#Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF">Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#173</span>
                    <a class="i-title" href="#Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF">ICE: Intrinsic Concept Extraction from a Single Image via Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#174</span>
                    <a class="i-title" href="#Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF">Doppelgangers++: Improved Visual Disambiguation with Geometric 3D Features</a>
                    <a class="i-star" onclick="toggleAppStar('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#175</span>
                    <a class="i-title" href="#Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF">Universal Scene Graph Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Universal_Scene_Graph_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#176</span>
                    <a class="i-title" href="#Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF">DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#177</span>
                    <a class="i-title" href="#Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF">MonSter: Marry Monodepth to Stereo Unleashes Power</a>
                    <a class="i-star" onclick="toggleAppStar('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#178</span>
                    <a class="i-title" href="#Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF">Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#179</span>
                    <a class="i-title" href="#Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF">Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#180</span>
                    <a class="i-title" href="#Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF">InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#181</span>
                    <a class="i-title" href="#Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF">Reconstructing People, Places, and Cameras</a>
                    <a class="i-star" onclick="toggleAppStar('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Muller_Reconstructing_People_Places_and_Cameras@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#182</span>
                    <a class="i-title" href="#Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF">Towards Explainable and Unprecedented Accuracy in Matching Challenging Finger Crease Patterns</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#183</span>
                    <a class="i-title" href="#Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF">Understanding Multi-Task Activities from Single-Task Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#184</span>
                    <a class="i-title" href="#Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF">Few-shot Implicit Function Generation via Equivariance</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_Few-shot_Implicit_Function_Generation_via_Equivariance@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#185</span>
                    <a class="i-title" href="#Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF">Free-viewpoint Human Animation with Pose-correlated Reference Selection</a>
                    <a class="i-star" onclick="toggleAppStar('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#186</span>
                    <a class="i-title" href="#Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF">DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#187</span>
                    <a class="i-title" href="#Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF">Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence Microscopy De-noising and Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#188</span>
                    <a class="i-title" href="#Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF">How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions</a>
                    <a class="i-star" onclick="toggleAppStar('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#189</span>
                    <a class="i-title" href="#Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF">SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#190</span>
                    <a class="i-title" href="#Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF">Prior-free 3D Object Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Song_Prior-free_3D_Object_Tracking@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#191</span>
                    <a class="i-title" href="#Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF">LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#192</span>
                    <a class="i-title" href="#Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF">HumanRig: Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset</a>
                    <a class="i-star" onclick="toggleAppStar('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#193</span>
                    <a class="i-title" href="#Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF">End-to-End HOI Reconstruction Transformer with Graph-based Encoding</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#194</span>
                    <a class="i-title" href="#Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF">VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#195</span>
                    <a class="i-title" href="#Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF">Goku: Flow Based Video Generative Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Goku_Flow_Based_Video_Generative_Foundation_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#196</span>
                    <a class="i-title" href="#Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF">Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting</a>
                    <a class="i-star" onclick="toggleAppStar('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#197</span>
                    <a class="i-title" href="#Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF">AdaCM^2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction</a>
                    <a class="i-star" onclick="toggleAppStar('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#198</span>
                    <a class="i-title" href="#Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</a>
                    <a class="i-star" onclick="toggleAppStar('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#199</span>
                    <a class="i-title" href="#Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF">Instruction-based Image Manipulation by Watching How Things Move</a>
                    <a class="i-star" onclick="toggleAppStar('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#200</span>
                    <a class="i-title" href="#Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF">VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models</a>
                    <a class="i-star" onclick="toggleAppStar('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#201</span>
                    <a class="i-title" href="#Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF">NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#202</span>
                    <a class="i-title" href="#Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF">A Polarization-Aided Transformer for Image Deblurring via Motion Vector Decomposition</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#203</span>
                    <a class="i-title" href="#Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF">Shape Abstraction via Marching Differentiable Support Functions</a>
                    <a class="i-star" onclick="toggleAppStar('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#204</span>
                    <a class="i-title" href="#Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF">WonderWorld: Interactive 3D Scene Generation from a Single Image</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#205</span>
                    <a class="i-title" href="#Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF">FineVQ: Fine-Grained User Generated Content Video Quality Assessment</a>
                    <a class="i-star" onclick="toggleAppStar('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#206</span>
                    <a class="i-title" href="#Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF">HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#207</span>
                    <a class="i-title" href="#Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF">Structure-from-Motion with a Non-Parametric Camera Model</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#208</span>
                    <a class="i-title" href="#Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF">CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching</a>
                    <a class="i-star" onclick="toggleAppStar('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#209</span>
                    <a class="i-title" href="#Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF">Seeing More with Less: Human-like Representations in Vision Models</a>
                    <a class="i-star" onclick="toggleAppStar('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#210</span>
                    <a class="i-title" href="#Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF">Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#211</span>
                    <a class="i-title" href="#Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF">3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#212</span>
                    <a class="i-title" href="#Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF">BADGR: Bundle Adjustment Diffusion Conditioned by Gradients for Wide-Baseline Floor Plan Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#213</span>
                    <a class="i-title" href="#Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF">DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness</a>
                    <a class="i-star" onclick="toggleAppStar('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#214</span>
                    <a class="i-title" href="#Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF">QuCOOP: A Versatile Framework for Solving Composite and Binary-Parametrised Problems on Quantum Annealers</a>
                    <a class="i-star" onclick="toggleAppStar('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#215</span>
                    <a class="i-title" href="#Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF">LaTexBlend: Scaling Multi-concept Customized Generation with Latent Textual Blending</a>
                    <a class="i-star" onclick="toggleAppStar('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#216</span>
                    <a class="i-title" href="#Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF">DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding</a>
                    <a class="i-star" onclick="toggleAppStar('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#217</span>
                    <a class="i-title" href="#Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF">ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</a>
                    <a class="i-star" onclick="toggleAppStar('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#218</span>
                    <a class="i-title" href="#Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF">BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#219</span>
                    <a class="i-title" href="#Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF">Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#220</span>
                    <a class="i-title" href="#Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF">UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#221</span>
                    <a class="i-title" href="#Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF">Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models</a>
                    <a class="i-star" onclick="toggleAppStar('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#222</span>
                    <a class="i-title" href="#Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF">SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#223</span>
                    <a class="i-title" href="#Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF">Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#224</span>
                    <a class="i-title" href="#Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF">SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer</a>
                    <a class="i-star" onclick="toggleAppStar('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#225</span>
                    <a class="i-title" href="#Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF">UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#226</span>
                    <a class="i-title" href="#Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF">High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model</a>
                    <a class="i-star" onclick="toggleAppStar('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#227</span>
                    <a class="i-title" href="#Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF">MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#228</span>
                    <a class="i-title" href="#Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF">UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#229</span>
                    <a class="i-title" href="#Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF">MATCHA: Towards Matching Anything</a>
                    <a class="i-star" onclick="toggleAppStar('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xue_MATCHA_Towards_Matching_Anything@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#230</span>
                    <a class="i-title" href="#Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF">ETAP: Event-based Tracking of Any Point</a>
                    <a class="i-star" onclick="toggleAppStar('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hamann_ETAP_Event-based_Tracking_of_Any_Point@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#231</span>
                    <a class="i-title" href="#Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF">HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation</a>
                    <a class="i-star" onclick="toggleAppStar('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#232</span>
                    <a class="i-title" href="#Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF">Order-One Rolling Shutter Cameras</a>
                    <a class="i-star" onclick="toggleAppStar('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hahn_Order-One_Rolling_Shutter_Cameras@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#233</span>
                    <a class="i-title" href="#Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF">PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#234</span>
                    <a class="i-title" href="#Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF">Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#235</span>
                    <a class="i-title" href="#Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF">Your ViT is Secretly an Image Segmentation Model</a>
                    <a class="i-star" onclick="toggleAppStar('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#236</span>
                    <a class="i-title" href="#Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF">Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding</a>
                    <a class="i-star" onclick="toggleAppStar('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#237</span>
                    <a class="i-title" href="#Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF">Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#238</span>
                    <a class="i-title" href="#Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF">Understanding Multi-layered Transmission Matrices</a>
                    <a class="i-star" onclick="toggleAppStar('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Levin_Understanding_Multi-layered_Transmission_Matrices@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#239</span>
                    <a class="i-title" href="#Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF">Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision</a>
                    <a class="i-star" onclick="toggleAppStar('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#240</span>
                    <a class="i-title" href="#Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF">EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing</a>
                    <a class="i-star" onclick="toggleAppStar('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing_@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#241</span>
                    <a class="i-title" href="#Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF">EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#242</span>
                    <a class="i-title" href="#Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF">Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency</a>
                    <a class="i-star" onclick="toggleAppStar('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#243</span>
                    <a class="i-title" href="#Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF">DistinctAD: Distinctive Audio Description Generation in Contexts</a>
                    <a class="i-star" onclick="toggleAppStar('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#244</span>
                    <a class="i-title" href="#Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF">AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</a>
                    <a class="i-star" onclick="toggleAppStar('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#245</span>
                    <a class="i-title" href="#Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF">Reasoning in Visual Navigation of End-to-end Trained Agents: A Dynamical Systems Approach</a>
                    <a class="i-star" onclick="toggleAppStar('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#246</span>
                    <a class="i-title" href="#Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF">Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#247</span>
                    <a class="i-title" href="#Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF">LP-Diff: Towards Improved Restoration of Real-World Degraded License Plate</a>
                    <a class="i-star" onclick="toggleAppStar('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#248</span>
                    <a class="i-title" href="#Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF">MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#249</span>
                    <a class="i-title" href="#Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF">Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding</a>
                    <a class="i-star" onclick="toggleAppStar('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#250</span>
                    <a class="i-title" href="#Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF">Towards Scalable Human-aligned Benchmark for Text-guided Image Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#251</span>
                    <a class="i-title" href="#Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF">Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction</a>
                    <a class="i-star" onclick="toggleAppStar('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#252</span>
                    <a class="i-title" href="#Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF">Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#253</span>
                    <a class="i-title" href="#Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF">Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification</a>
                    <a class="i-star" onclick="toggleAppStar('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#254</span>
                    <a class="i-title" href="#Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF">Cross-modal Causal Relation Alignment for Video Question Grounding</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#255</span>
                    <a class="i-title" href="#Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF">Scaling Inference Time Compute for Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#256</span>
                    <a class="i-title" href="#Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF">EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events</a>
                    <a class="i-star" onclick="toggleAppStar('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#257</span>
                    <a class="i-title" href="#Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF">NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#258</span>
                    <a class="i-title" href="#Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF">HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#259</span>
                    <a class="i-title" href="#Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF">HotSpot: Signed Distance Function Optimization with an Asymptotically Sufficient Condition</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#260</span>
                    <a class="i-title" href="#Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF">BWFormer: Building Wireframe Reconstruction from Airborne LiDAR Point Cloud with Transformer</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#261</span>
                    <a class="i-title" href="#Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF">CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval</a>
                    <a class="i-star" onclick="toggleAppStar('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#262</span>
                    <a class="i-title" href="#Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF">Structured 3D Latents for Scalable and Versatile 3D Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#263</span>
                    <a class="i-title" href="#Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF">H-MoRe: Learning Human-centric Motion Representation for Action Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#264</span>
                    <a class="i-title" href="#Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF">Do Computer Vision Foundation Models Learn the Low-level Characteristics of the Human Visual System?</a>
                    <a class="i-star" onclick="toggleAppStar('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#265</span>
                    <a class="i-title" href="#He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF">Samba: A Unified Mamba-based Framework for General Salient Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#266</span>
                    <a class="i-title" href="#Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF">STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#267</span>
                    <a class="i-title" href="#Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF">DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction</a>
                    <a class="i-star" onclick="toggleAppStar('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#268</span>
                    <a class="i-title" href="#Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF">Hardware-Rasterized Ray-Based Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#269</span>
                    <a class="i-title" href="#Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF">Image Quality Assessment: From Human to Machine Preference</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Image_Quality_Assessment_From_Human_to_Machine_Preference@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#270</span>
                    <a class="i-title" href="#Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF">Relative Pose Estimation through Affine Corrections of Monocular Depth Priors</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#271</span>
                    <a class="i-title" href="#Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF">Meta-Learning Hyperparameters for Parameter Efficient Fine-Tuning</a>
                    <a class="i-star" onclick="toggleAppStar('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#272</span>
                    <a class="i-title" href="#Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF">Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#273</span>
                    <a class="i-title" href="#Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF">O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#274</span>
                    <a class="i-title" href="#Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF">All-directional Disparity Estimation for Real-world QPD Images</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#275</span>
                    <a class="i-title" href="#Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF">CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design</a>
                    <a class="i-star" onclick="toggleAppStar('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#276</span>
                    <a class="i-title" href="#Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF">Align3R: Aligned Monocular Depth Estimation for Dynamic Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#277</span>
                    <a class="i-title" href="#Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF">Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#278</span>
                    <a class="i-title" href="#Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF">Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#279</span>
                    <a class="i-title" href="#Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF">Light3R-SfM: Towards Feed-forward Structure-from-Motion</a>
                    <a class="i-star" onclick="toggleAppStar('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#280</span>
                    <a class="i-title" href="#Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF">ARM: Appearance Reconstruction Model for Relightable 3D Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#281</span>
                    <a class="i-title" href="#Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF">Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#282</span>
                    <a class="i-title" href="#Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF">UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#283</span>
                    <a class="i-title" href="#Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF">GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities</a>
                    <a class="i-star" onclick="toggleAppStar('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#284</span>
                    <a class="i-title" href="#Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF">FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#285</span>
                    <a class="i-title" href="#Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF">EffiDec3D: An Optimized Decoder for High-Performance and Efficient 3D Medical Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#286</span>
                    <a class="i-title" href="#Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF">Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation</a>
                    <a class="i-star" onclick="toggleAppStar('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#287</span>
                    <a class="i-title" href="#Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF">Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#288</span>
                    <a class="i-title" href="#Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF">Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#289</span>
                    <a class="i-title" href="#Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF">Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#290</span>
                    <a class="i-title" href="#Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF">GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model</a>
                    <a class="i-star" onclick="toggleAppStar('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#291</span>
                    <a class="i-title" href="#Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF">Optimizing for the Shortest Path in Denoising Diffusion Model</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#292</span>
                    <a class="i-title" href="#Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF">From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#293</span>
                    <a class="i-title" href="#Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF">Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration</a>
                    <a class="i-star" onclick="toggleAppStar('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#294</span>
                    <a class="i-title" href="#Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF">Realistic Test-Time Adaptation of Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#295</span>
                    <a class="i-title" href="#Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF">ReNeg: Learning Negative Embedding with Reward Guidance</a>
                    <a class="i-star" onclick="toggleAppStar('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#296</span>
                    <a class="i-title" href="#Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF">CASP: Compression of Large Multimodal Models Based on Attention Sparsity</a>
                    <a class="i-star" onclick="toggleAppStar('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#297</span>
                    <a class="i-title" href="#Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF">RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training</a>
                    <a class="i-star" onclick="toggleAppStar('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#298</span>
                    <a class="i-title" href="#Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF">Efficient Motion-Aware Video MLLM</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_Efficient_Motion-Aware_Video_MLLM@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#299</span>
                    <a class="i-title" href="#Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF">DiffCAM: Data-Driven Saliency Maps by Capturing Feature Differences</a>
                    <a class="i-star" onclick="toggleAppStar('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#300</span>
                    <a class="i-title" href="#Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF">DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection</a>
                    <a class="i-star" onclick="toggleAppStar('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#301</span>
                    <a class="i-title" href="#Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF">UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#302</span>
                    <a class="i-title" href="#Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF">Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#303</span>
                    <a class="i-title" href="#Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF">Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs</a>
                    <a class="i-star" onclick="toggleAppStar('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#304</span>
                    <a class="i-title" href="#Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF">Question-Aware Gaussian Experts for Audio-Visual Question Answering</a>
                    <a class="i-star" onclick="toggleAppStar('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#305</span>
                    <a class="i-title" href="#Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF">SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#306</span>
                    <a class="i-title" href="#Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF">Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#307</span>
                    <a class="i-title" href="#Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF">Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution</a>
                    <a class="i-star" onclick="toggleAppStar('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#308</span>
                    <a class="i-title" href="#Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF">Cubify Anything: Scaling Indoor 3D Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#309</span>
                    <a class="i-title" href="#Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF">MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds</a>
                    <a class="i-star" onclick="toggleAppStar('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#310</span>
                    <a class="i-title" href="#Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF">IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera</a>
                    <a class="i-star" onclick="toggleAppStar('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#311</span>
                    <a class="i-title" href="#Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF">FIction: 4D Future Interaction Prediction from Video</a>
                    <a class="i-star" onclick="toggleAppStar('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#312</span>
                    <a class="i-title" href="#Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF">Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras</a>
                    <a class="i-star" onclick="toggleAppStar('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#313</span>
                    <a class="i-title" href="#Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF">Boost Your Human Image Generation Model via Direct Preference Optimization</a>
                    <a class="i-star" onclick="toggleAppStar('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#314</span>
                    <a class="i-title" href="#Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF">F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics</a>
                    <a class="i-star" onclick="toggleAppStar('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#315</span>
                    <a class="i-title" href="#Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF">DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#316</span>
                    <a class="i-title" href="#Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF">Just Dance with pi! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#317</span>
                    <a class="i-title" href="#Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF">ALIEN: Implicit Neural Representations for Human Motion Prediction under Arbitrary Latency</a>
                    <a class="i-star" onclick="toggleAppStar('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#318</span>
                    <a class="i-title" href="#Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF">Seurat: From Moving Points to Depth</a>
                    <a class="i-star" onclick="toggleAppStar('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cho_Seurat_From_Moving_Points_to_Depth@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#319</span>
                    <a class="i-title" href="#Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF">NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#320</span>
                    <a class="i-title" href="#Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF">CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation</a>
                    <a class="i-star" onclick="toggleAppStar('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#321</span>
                    <a class="i-title" href="#Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF">FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute</a>
                    <a class="i-star" onclick="toggleAppStar('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#322</span>
                    <a class="i-title" href="#Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF">TinyFusion: Diffusion Transformers Learned Shallow</a>
                    <a class="i-star" onclick="toggleAppStar('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#323</span>
                    <a class="i-title" href="#Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF">NSD-Imagery: A Benchmark Dataset for Extending fMRI Vision Decoding Methods to Mental Imagery</a>
                    <a class="i-star" onclick="toggleAppStar('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#324</span>
                    <a class="i-title" href="#Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF">ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images</a>
                    <a class="i-star" onclick="toggleAppStar('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#325</span>
                    <a class="i-title" href="#Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF">TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction</a>
                    <a class="i-star" onclick="toggleAppStar('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#326</span>
                    <a class="i-title" href="#Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF">HSI-GPT: A General-Purpose Large Scene-Motion-Language Model for Human Scene Interaction</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#327</span>
                    <a class="i-title" href="#Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF">StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#328</span>
                    <a class="i-title" href="#Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF">Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map</a>
                    <a class="i-star" onclick="toggleAppStar('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#329</span>
                    <a class="i-title" href="#Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF">Type-R: Automatically Retouching Typos for Text-to-Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#330</span>
                    <a class="i-title" href="#Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF">Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#331</span>
                    <a class="i-title" href="#Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF">Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text</a>
                    <a class="i-star" onclick="toggleAppStar('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#332</span>
                    <a class="i-title" href="#Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF">Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#333</span>
                    <a class="i-title" href="#Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF">One-Step Event-Driven High-Speed Autofocus</a>
                    <a class="i-star" onclick="toggleAppStar('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Bao_One-Step_Event-Driven_High-Speed_Autofocus@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#334</span>
                    <a class="i-title" href="#Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF">FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#335</span>
                    <a class="i-title" href="#Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF">BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#336</span>
                    <a class="i-title" href="#Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF">From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing</a>
                    <a class="i-star" onclick="toggleAppStar('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#337</span>
                    <a class="i-title" href="#Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF">Identity-Preserving Text-to-Video Generation by Frequency Decomposition</a>
                    <a class="i-star" onclick="toggleAppStar('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#338</span>
                    <a class="i-title" href="#Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF">RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness</a>
                    <a class="i-star" onclick="toggleAppStar('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#339</span>
                    <a class="i-title" href="#Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF">USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#340</span>
                    <a class="i-title" href="#Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF">InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#341</span>
                    <a class="i-title" href="#Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF">Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset</a>
                    <a class="i-star" onclick="toggleAppStar('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#342</span>
                    <a class="i-title" href="#Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF">SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts</a>
                    <a class="i-star" onclick="toggleAppStar('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#343</span>
                    <a class="i-title" href="#Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF">DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#344</span>
                    <a class="i-title" href="#Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF">Less is More: Efficient Model Merging with Binary Task Switch</a>
                    <a class="i-star" onclick="toggleAppStar('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#345</span>
                    <a class="i-title" href="#Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF">Implicit Correspondence Learning for Image-to-Point Cloud Registration</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#346</span>
                    <a class="i-title" href="#Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF">Learning Class Prototypes for Unified Sparse-Supervised 3D Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#347</span>
                    <a class="i-title" href="#Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF">OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation</a>
                    <a class="i-star" onclick="toggleAppStar('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#348</span>
                    <a class="i-title" href="#Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a>
                    <a class="i-star" onclick="toggleAppStar('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#349</span>
                    <a class="i-title" href="#Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF">VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge</a>
                    <a class="i-star" onclick="toggleAppStar('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#350</span>
                    <a class="i-title" href="#Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF">World-consistent Video Diffusion with Explicit 3D Modeling</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#351</span>
                    <a class="i-title" href="#Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF">Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#352</span>
                    <a class="i-title" href="#Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF">Memories of Forgotten Concepts</a>
                    <a class="i-star" onclick="toggleAppStar('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Rusanovsky_Memories_of_Forgotten_Concepts@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#353</span>
                    <a class="i-title" href="#Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF">AIpparel: A Multimodal Foundation Model for Digital Garments</a>
                    <a class="i-star" onclick="toggleAppStar('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#354</span>
                    <a class="i-title" href="#Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF">Event Fields: Capturing Light Fields at High Speed, Resolution, and Dynamic Range</a>
                    <a class="i-star" onclick="toggleAppStar('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#355</span>
                    <a class="i-title" href="#Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF">SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving</a>
                    <a class="i-star" onclick="toggleAppStar('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#356</span>
                    <a class="i-title" href="#Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</a>
                    <a class="i-star" onclick="toggleAppStar('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#357</span>
                    <a class="i-title" href="#Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF">Glossy Object Reconstruction with Cost-effective Polarized Acquisition</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#358</span>
                    <a class="i-title" href="#Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF">Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces</a>
                    <a class="i-star" onclick="toggleAppStar('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#359</span>
                    <a class="i-title" href="#Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF">Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning</a>
                    <a class="i-star" onclick="toggleAppStar('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#360</span>
                    <a class="i-title" href="#Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF">CRISP: Object Pose and Shape Estimation with Test-Time Adaptation</a>
                    <a class="i-star" onclick="toggleAppStar('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#361</span>
                    <a class="i-title" href="#Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF">FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</a>
                    <a class="i-star" onclick="toggleAppStar('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#362</span>
                    <a class="i-title" href="#Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF">No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#363</span>
                    <a class="i-title" href="#Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF">Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#364</span>
                    <a class="i-title" href="#Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF">OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#365</span>
                    <a class="i-title" href="#Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF">Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Mutimodal Models</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#366</span>
                    <a class="i-title" href="#Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF">SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models</a>
                    <a class="i-star" onclick="toggleAppStar('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#367</span>
                    <a class="i-title" href="#Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF">Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</a>
                    <a class="i-star" onclick="toggleAppStar('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#368</span>
                    <a class="i-title" href="#Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF">Learning Conditional Space-Time Prompt Distributions for Video Class-Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#369</span>
                    <a class="i-title" href="#Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF">Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition</a>
                    <a class="i-star" onclick="toggleAppStar('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#370</span>
                    <a class="i-title" href="#Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF">Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#371</span>
                    <a class="i-title" href="#Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF">Circumventing Shortcuts in Audio-visual Deepfake Detection Datasets with Unsupervised Learning</a>
                    <a class="i-star" onclick="toggleAppStar('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#372</span>
                    <a class="i-title" href="#Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF">HuPerFlow: A Comprehensive Benchmark for Human vs. Machine Motion Estimation Comparison</a>
                    <a class="i-star" onclick="toggleAppStar('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#373</span>
                    <a class="i-title" href="#Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF">Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#374</span>
                    <a class="i-title" href="#Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF">Revisiting MAE Pre-training for 3D Medical Image Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#375</span>
                    <a class="i-title" href="#Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF">Scaling Vision Pre-Training to 4K Resolution</a>
                    <a class="i-star" onclick="toggleAppStar('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Shi_Scaling_Vision_Pre-Training_to_4K_Resolution@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#376</span>
                    <a class="i-title" href="#Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF">CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation</a>
                    <a class="i-star" onclick="toggleAppStar('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#377</span>
                    <a class="i-title" href="#Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF">SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment</a>
                    <a class="i-star" onclick="toggleAppStar('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#378</span>
                    <a class="i-title" href="#Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF">Multitwine: Multi-Object Compositing with Text and Layout Control</a>
                    <a class="i-star" onclick="toggleAppStar('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#379</span>
                    <a class="i-title" href="#Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF">Deep Fair Multi-View Clustering with Attention KAN</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#380</span>
                    <a class="i-title" href="#Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF">Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#381</span>
                    <a class="i-title" href="#Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF">Compositional Caching for Training-free Open-vocabulary Attribute Detection</a>
                    <a class="i-star" onclick="toggleAppStar('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#382</span>
                    <a class="i-title" href="#Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF">Polarized Color Screen Matting</a>
                    <a class="i-star" onclick="toggleAppStar('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Enomoto_Polarized_Color_Screen_Matting@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#383</span>
                    <a class="i-title" href="#Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF">EBS-EKF: Accurate and High Frequency Event-based Star Tracking</a>
                    <a class="i-star" onclick="toggleAppStar('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#384</span>
                    <a class="i-title" href="#Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF">Matrix3D: Large Photogrammetry Model All-in-One</a>
                    <a class="i-star" onclick="toggleAppStar('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#385</span>
                    <a class="i-title" href="#Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF">DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</a>
                    <a class="i-star" onclick="toggleAppStar('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#386</span>
                    <a class="i-title" href="#Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF">FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images</a>
                    <a class="i-star" onclick="toggleAppStar('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#387</span>
                    <a class="i-title" href="#Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF">Sonata: Self-Supervised Learning of Reliable Point Representations</a>
                    <a class="i-star" onclick="toggleAppStar('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p><p id="app-bar-star-Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF" style="display:none">
                    <span class="i-index">#388</span>
                    <a class="i-title" href="#Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF">SKDream: Controllable Multi-view and 3D Generation with Arbitrary Skeletons</a>
                    <a class="i-star" onclick="toggleAppStar('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF')" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar('Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons@CVPR2025@CVF')" style="display:none"><i class="fa fa-star-o"></i></a>
                </p></div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh"></option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Please read our <a href="https://github.com/bojone/papers.cool/blob/main/Disclaimer/README_en.md" target="_blank">Disclaimer</a> before proceeding.</p>
            <p>For more interesting features, please visit <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp('app-bar-search', this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp('app-bar-star', this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp('app-bar-config', this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp('app-bar-bug', this)"><i class="fa fa-bug"></i></a>
    </div>
    <div id="scroll-btn" style="opacity: 0;">
        <button onclick="scroll2(0)" id="totop" title="Go to top"><i class="fa fa-chevron-up"></i></button>
        <button onclick="scroll2(1)" id="tobottom" title="Go to bottom"><i class="fa fa-chevron-down"></i></button>
    </div>
    <script src="/static/mark.js/dist/mark.min.js"></script>
    <script src="/static/marked/lib/marked.umd.js?16.2.1"></script>
    <script src="/static/flatpickr/dist/flatpickr.min.js?v=4.6.13"></script>
    <script src="/static/translate/translate.js?v=3.7.0.20240810"></script>
    <script src="/static/cool.js?v=1.5.1.6"></script>
    <script type="text/x-mathjax-config;executed=true">
        var macros = {
            "argmin": "\\mathop{\\text{argmin}}",
            "argmax": "\\mathop{\\text{argmax}}"
        };
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true},
            TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}, extensions: ["AMSmath.js", "AMSsymbols.js", "extpfeil.js"], Macros: macros},
            "HTML-CSS": {noReflows: false, availableFonts: ["tex"], styles: {".MathJax_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "CommonHTML": {noReflows: false, availableFonts: ["tex"], styles: {".MJXc-display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}},
            "SVG": {styles: {".MathJax_SVG_Display": {margin: "1em 0em 0.7em;", display: "inline-block!important;"}}}
        });
        MathJax.Hub.Queue(function() {
            document.querySelectorAll('.MathJax').forEach(element => element.classList.add('notranslate'));
            document.querySelectorAll('a.title-link, p.summary').forEach(element => element.classList.remove('notranslate'));
            highlightQuery();
        });
    </script>
    <script src="/static/MathJax-2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-214H31WLDF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; min-width: 0px; max-width: none; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: MathJax_Caligraphic, sans-serif;"></div></div></body></html>