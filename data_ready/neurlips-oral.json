{
  "source_html": "html\\neurlips-oral.html",
  "paper_count": 77,
  "conference": "neurlips",
  "year": 2025,
  "status": "oral",
  "papers": [
    {
      "paper_id": "KurYdcCbjv@OpenReview",
      "index": 1,
      "title": "Generalized Linear Mode Connectivity for Transformers",
      "authors": [
        "Alexander Theus",
        "Alessandro Cabodi",
        "Sotiris Anagnostidis",
        "Antonio Orvieto",
        "Sidak Pal Singh",
        "Valentina Boeva"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "permutations",
        "transformers",
        "connectivity",
        "neuron",
        "loss",
        "barrier",
        "reparameterizations",
        "linear",
        "mode",
        "independently"
      ],
      "summary": "Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is linear mode connectivity linear mode connectivity (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KurYdcCbjv"
        ],
        "venue": [
          "/venue/KurYdcCbjv@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KurYdcCbjv"
        ],
        "detail": [
          "https://openreview.net/forum?id=KurYdcCbjv"
        ]
      },
      "scores": {
        "pdf": 110,
        "kimi": 127
      },
      "raw_excerpt": "Generalized Linear Mode Connectivity for Transformers [PDF 110 ] [Copy] [Kimi 127 ] [REL] Authors : Alexander Theus , Alessandro Cabodi , Sotiris Anagnostidis , Antonio Orvieto , Sidak Pal Singh , Valentina Boeva Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is linear mode connectivity linear mode connectivity (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "jzPQRbGkAq@OpenReview",
      "index": 2,
      "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
      "authors": [
        "Ho Yin Au",
        "Jie Chen",
        "Junkun Jiang",
        "Jingyu Xiang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "motion",
        "clips",
        "phase",
        "semantically",
        "tpdm",
        "diffusion",
        "spdm",
        "compositional",
        "transitional",
        "sequences"
      ],
      "summary": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jzPQRbGkAq"
        ],
        "venue": [
          "/venue/jzPQRbGkAq@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jzPQRbGkAq"
        ],
        "detail": [
          "https://openreview.net/forum?id=jzPQRbGkAq"
        ]
      },
      "scores": {
        "pdf": 52,
        "kimi": 51
      },
      "raw_excerpt": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation [PDF 52 ] [Copy] [Kimi 51 ] [REL] Authors : Ho Yin Au , Jie Chen , Junkun Jiang , Jingyu Xiang Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "eafIjoZAHm@OpenReview",
      "index": 3,
      "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "authors": [
        "Burouj Armgaan",
        "Eshan Jain",
        "Harsh Pandey",
        "Mahesh Chandran",
        "Sayan Ranu"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "gnnxemplar",
        "exemplars",
        "global",
        "rules",
        "gnn",
        "exemplar",
        "explanations",
        "interpretability",
        "language",
        "predictions"
      ],
      "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k k -nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eafIjoZAHm"
        ],
        "venue": [
          "/venue/eafIjoZAHm@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eafIjoZAHm"
        ],
        "detail": [
          "https://openreview.net/forum?id=eafIjoZAHm"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 30
      },
      "raw_excerpt": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability [PDF 20 ] [Copy] [Kimi 30 ] [REL] Authors : Burouj Armgaan , Eshan Jain , Harsh Pandey , Mahesh Chandran , Sayan Ranu Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k k -nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "tirl2l9oKg@OpenReview",
      "index": 4,
      "title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
      "authors": [
        "Xingliang Wang",
        "Zemin Liu",
        "Junxiao Han",
        "Shuiguang Deng"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "graph",
        "rag4gfm",
        "retrieval",
        "gfms",
        "indexing",
        "gfm",
        "knowledge",
        "faithfulness",
        "foundation",
        "updating"
      ],
      "summary": "Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. RAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. The graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference. Extensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\\footnote{Code: \\url{https://github.com/Matrixmax/RAG4GFM}.}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=tirl2l9oKg"
        ],
        "venue": [
          "/venue/tirl2l9oKg@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=tirl2l9oKg"
        ],
        "detail": [
          "https://openreview.net/forum?id=tirl2l9oKg"
        ]
      },
      "scores": {
        "pdf": 32,
        "kimi": 39
      },
      "raw_excerpt": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation [PDF 32 ] [Copy] [Kimi 39 ] [REL] Authors : Xingliang Wang , Zemin Liu , Junxiao Han , Shuiguang Deng Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. RAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. The graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference. Extensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\\footnote{Code: \\url{https://github.com/Matrixmax/RAG4GFM}.}. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "XPe55Uffd7@OpenReview",
      "index": 5,
      "title": "Agnostic Active Learning Is Always Better Than Passive Learning",
      "authors": [
        "Steve Hanneke"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "agnostic",
        "active",
        "passive",
        "learning",
        "complexity",
        "always",
        "leading",
        "term",
        "query",
        "sharply"
      ],
      "summary": "We sharply characterize the optimal first-order query complexity of agnostic active learning for all concept classes, and propose a new general active learning algorithm which achieves it. Remarkably, the optimal query complexity admits a leading term which is always strictly smaller than the sample complexity of passive supervised learning (by a factor proportional to the best-in-class error rate). This was not previously known to be possible in the agnostic setting. For comparison, in all previous general analyses, the leading term exhibits an additional factor, such as the disagreement coefficient or related complexity measure, and therefore only provides improvements over passive learning in restricted cases. The present work completely removes such factors from the leading term, implying that every every concept class benefits from active learning in the non-realizable case. The results established in this work resolve an important long-standing open question central to the past two decades of research on the theory of agnostic active learning.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XPe55Uffd7"
        ],
        "venue": [
          "/venue/XPe55Uffd7@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XPe55Uffd7"
        ],
        "detail": [
          "https://openreview.net/forum?id=XPe55Uffd7"
        ]
      },
      "scores": {
        "pdf": 32,
        "kimi": 44
      },
      "raw_excerpt": "Agnostic Active Learning Is Always Better Than Passive Learning [PDF 32 ] [Copy] [Kimi 44 ] [REL] Author : Steve Hanneke We sharply characterize the optimal first-order query complexity of agnostic active learning for all concept classes, and propose a new general active learning algorithm which achieves it. Remarkably, the optimal query complexity admits a leading term which is always strictly smaller than the sample complexity of passive supervised learning (by a factor proportional to the best-in-class error rate). This was not previously known to be possible in the agnostic setting. For comparison, in all previous general analyses, the leading term exhibits an additional factor, such as the disagreement coefficient or related complexity measure, and therefore only provides improvements over passive learning in restricted cases. The present work completely removes such factors from the leading term, implying that every every concept class benefits from active learning in the non-realizable case. The results established in this work resolve an important long-standing open question central to the past two decades of research on the theory of agnostic active learning. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "QN0E0KX2LM@OpenReview",
      "index": 6,
      "title": "Learning Linear Attention in Polynomial Time",
      "authors": [
        "Morris Yau",
        "Ekin Akyürek",
        "Jiayuan Mao",
        "Joshua B. Tenenbaum",
        "Stefanie Jegelka",
        "Jacob Andreas"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "learnability",
        "attention",
        "linear",
        "expressivity",
        "polynomial",
        "transformers",
        "head",
        "learning",
        "transformer",
        "rkhs"
      ],
      "summary": "Previous research has explored the expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the efficient learnability of Transformers from data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that learning the optimal multi head linear attention can be recast as finding the optimal kernel predictor in a suitably defined RKHS. Moving to generalization, we construct an algorithm that, given a dataset, checks in polynomial time whether the set of best fit multi head linear attention networks on this data all perform an identical computation--a powerful notion for out of distribution generalization. We empirically validate our theoretical findings on several canonical tasks: learning random linear attention networks, key--value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformer models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=QN0E0KX2LM"
        ],
        "venue": [
          "/venue/QN0E0KX2LM@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=QN0E0KX2LM"
        ],
        "detail": [
          "https://openreview.net/forum?id=QN0E0KX2LM"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 40
      },
      "raw_excerpt": "Learning Linear Attention in Polynomial Time [PDF 35 ] [Copy] [Kimi 40 ] [REL] Authors : Morris Yau , Ekin Akyürek , Jiayuan Mao , Joshua B. Tenenbaum , Stefanie Jegelka , Jacob Andreas Previous research has explored the expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the efficient learnability of Transformers from data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that learning the optimal multi head linear attention can be recast as finding the optimal kernel predictor in a suitably defined RKHS. Moving to generalization, we construct an algorithm that, given a dataset, checks in polynomial time whether the set of best fit multi head linear attention networks on this data all perform an identical computation--a powerful notion for out of distribution generalization. We empirically validate our theoretical findings on several canonical tasks: learning random linear attention networks, key--value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformer models. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "EoebmBe9fG@OpenReview",
      "index": 7,
      "title": "Optimal Mistake Bounds for Transductive Online Learning",
      "authors": [
        "Zachary Chase",
        "Steve Hanneke",
        "Shay Moran",
        "Jonathan Shafer"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "transductive",
        "mistake",
        "online",
        "littlestone",
        "bound",
        "omega",
        "log",
        "ben",
        "david",
        "1997"
      ],
      "summary": "We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class H H with Littlestone dimension d d , the transductive mistake bound is at least Ω ( d − − √ ) Ω ( d ) . This establishes an exponential improvement over previous lower bounds of Ω ( log log d ) Ω ( log ⁡ log ⁡ d ) , Ω ( log d − − − − √ ) Ω ( log ⁡ d ) , and Ω ( log d ) Ω ( log ⁡ d ) , respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every d d , there exists a class of Littlestone dimension d d with transductive mistake bound O ( d − − √ ) O ( d ) . Our upper bound also improves the previous best known upper bound of ( 2 / 3 ) ⋅ d ( 2 / 3 ) ⋅ d from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=EoebmBe9fG"
        ],
        "venue": [
          "/venue/EoebmBe9fG@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=EoebmBe9fG"
        ],
        "detail": [
          "https://openreview.net/forum?id=EoebmBe9fG"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 19
      },
      "raw_excerpt": "Optimal Mistake Bounds for Transductive Online Learning [PDF 13 ] [Copy] [Kimi 19 ] [REL] Authors : Zachary Chase , Steve Hanneke , Shay Moran , Jonathan Shafer We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class H H with Littlestone dimension d d , the transductive mistake bound is at least Ω ( d − − √ ) Ω ( d ) . This establishes an exponential improvement over previous lower bounds of Ω ( log log d ) Ω ( log ⁡ log ⁡ d ) , Ω ( log d − − − − √ ) Ω ( log ⁡ d ) , and Ω ( log d ) Ω ( log ⁡ d ) , respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every d d , there exists a class of Littlestone dimension d d with transductive mistake bound O ( d − − √ ) O ( d ) . Our upper bound also improves the previous best known upper bound of ( 2 / 3 ) ⋅ d ( 2 / 3 ) ⋅ d from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "rtG7n93Ru8@OpenReview",
      "index": 8,
      "title": "State Entropy Regularization for Robust Reinforcement Learning",
      "authors": [
        "Yonatan Ashlag",
        "Uri Koren",
        "Mirco Mutti",
        "Esther Derman",
        "Pierre-Luc Bacon",
        "Shie Mannor"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "entropy",
        "regularization",
        "robustness",
        "policy",
        "reinforcement",
        "state",
        "robust",
        "guarantees",
        "rollouts",
        "standpoint"
      ],
      "summary": "State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rtG7n93Ru8"
        ],
        "venue": [
          "/venue/rtG7n93Ru8@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rtG7n93Ru8"
        ],
        "detail": [
          "https://openreview.net/forum?id=rtG7n93Ru8"
        ]
      },
      "scores": {
        "pdf": 24,
        "kimi": 31
      },
      "raw_excerpt": "State Entropy Regularization for Robust Reinforcement Learning [PDF 24 ] [Copy] [Kimi 31 ] [REL] Authors : Yonatan Ashlag , Uri Koren , Mirco Mutti , Esther Derman , Pierre-Luc Bacon , Shie Mannor State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "kVz9uvqUna@OpenReview",
      "index": 9,
      "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
      "authors": [
        "Quentin Bertrand",
        "Anne Gagneux",
        "Mathurin Massias",
        "Rémi Emonet"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "matching",
        "flow",
        "closed",
        "stochasticity",
        "loss",
        "form",
        "generalization",
        "stochastic",
        "nature",
        "arise"
      ],
      "summary": "Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching. First, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=kVz9uvqUna"
        ],
        "venue": [
          "/venue/kVz9uvqUna@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=kVz9uvqUna"
        ],
        "detail": [
          "https://openreview.net/forum?id=kVz9uvqUna"
        ]
      },
      "scores": {
        "pdf": 24,
        "kimi": 17
      },
      "raw_excerpt": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity [PDF 24 ] [Copy] [Kimi 17 ] [REL] Authors : Quentin Bertrand , Anne Gagneux , Mathurin Massias , Rémi Emonet Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching. First, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "BSZqpqgqM0@OpenReview",
      "index": 10,
      "title": "Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training",
      "authors": [
        "Tony Bonnaire",
        "Raphaël Urfin",
        "Giulio Biroli",
        "Marc Mezard"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "memorization",
        "training",
        "tau",
        "mathrm",
        "mem",
        "gen",
        "memorize",
        "implicit",
        "regularization",
        "diffusion"
      ],
      "summary": "Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time τ g e n τ g e n at which models begin to generate high-quality samples, and a later time τ m e m τ m e m beyond which memorization emerges. Crucially, we find that τ m e m τ m e m increases linearly with the training set size n n , while τ g e n τ g e n remains constant. This creates a growing window of training times with n n where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when n n becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=BSZqpqgqM0"
        ],
        "venue": [
          "/venue/BSZqpqgqM0@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=BSZqpqgqM0"
        ],
        "detail": [
          "https://openreview.net/forum?id=BSZqpqgqM0"
        ]
      },
      "scores": {
        "pdf": 30,
        "kimi": 23
      },
      "raw_excerpt": "Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training [PDF 30 ] [Copy] [Kimi 23 ] [REL] Authors : Tony Bonnaire , Raphaël Urfin , Giulio Biroli , Marc Mezard Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time τ g e n τ g e n at which models begin to generate high-quality samples, and a later time τ m e m τ m e m beyond which memorization emerges. Crucially, we find that τ m e m τ m e m increases linearly with the training set size n n , while τ g e n τ g e n remains constant. This creates a growing window of training times with n n where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when n n becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "rMhQBlhh4c@OpenReview",
      "index": 11,
      "title": "Adjoint Schrödinger Bridge Sampler",
      "authors": [
        "Guan-Horng Liu",
        "Jaemoo Choi",
        "Yongxin Chen",
        "Benjamin Kurt Miller",
        "Ricky T. Q. Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "asbs",
        "adjoint",
        "sampler",
        "samplers",
        "bridge",
        "schrödinger",
        "diffusion",
        "boltzmann",
        "target",
        "sampling"
      ],
      "summary": "Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rMhQBlhh4c"
        ],
        "venue": [
          "/venue/rMhQBlhh4c@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rMhQBlhh4c"
        ],
        "detail": [
          "https://openreview.net/forum?id=rMhQBlhh4c"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 13
      },
      "raw_excerpt": "Adjoint Schrödinger Bridge Sampler [PDF 16 ] [Copy] [Kimi 13 ] [REL] Authors : Guan-Horng Liu , Jaemoo Choi , Yongxin Chen , Benjamin Kurt Miller , Ricky T. Q. Chen Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "RxkCwOKVKa@OpenReview",
      "index": 12,
      "title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
      "authors": [
        "Felix Chalumeau",
        "Daniel Rajaonarivonivelomanantsoa",
        "Ruan John de Kock",
        "Juan Claude Formanek",
        "Sasha Abramowitz",
        "Omayma Mahjoub",
        "Wiem Khlifi",
        "Simon Verster Du Toit",
        "Louay Ben Nessir",
        "Refiloe Shabe",
        "Arnol Manuel Fokam",
        "Siddarth Singh",
        "Ulrich Armel Mbou Sob",
        "Arnu Pretorius"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "ceiling",
        "inference",
        "breaking",
        "reinforcement",
        "strategies",
        "execution",
        "performance",
        "60k",
        "countless",
        "complex"
      ],
      "summary": "Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. We make all of our experimental data and code available.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RxkCwOKVKa"
        ],
        "venue": [
          "/venue/RxkCwOKVKa@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RxkCwOKVKa"
        ],
        "detail": [
          "https://openreview.net/forum?id=RxkCwOKVKa"
        ]
      },
      "scores": {
        "pdf": 33,
        "kimi": 32
      },
      "raw_excerpt": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies [PDF 33 ] [Copy] [Kimi 32 ] [REL] Authors : Felix Chalumeau , Daniel Rajaonarivonivelomanantsoa , Ruan John de Kock , Juan Claude Formanek , Sasha Abramowitz , Omayma Mahjoub , Wiem Khlifi , Simon Verster Du Toit , Louay Ben Nessir , Refiloe Shabe , Arnol Manuel Fokam , Siddarth Singh , Ulrich Armel Mbou Sob , Arnu Pretorius Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. We make all of our experimental data and code available. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "UVDihUz0iT@OpenReview",
      "index": 13,
      "title": "High-Dimensional Calibration from Swap Regret",
      "authors": [
        "Maxwell Fishelson",
        "Noah Golowich",
        "Mehryar Mohri",
        "Jon Schneider"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "vert",
        "epsilon",
        "calibration",
        "regret",
        "cdot",
        "forecasts",
        "rounds",
        "exp",
        "swap",
        "online"
      ],
      "summary": "We study the online calibration of multi-dimensional forecasts over an arbitrary convex set P ⊂ R d P ⊂ R d relative to an arbitrary norm ∥ ⋅ ∥ ‖ ⋅ ‖ . We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee O ( ρ T − − − √ ) O ( ρ T ) worst-case regret after T T rounds when actions are drawn from P P and losses are drawn from the dual ∥ ⋅ ∥ ∗ ‖ ⋅ ‖ ∗ unit norm ball, then it is also possible to obtain ϵ ϵ -calibrated forecasts after T = exp ( O ( ρ / ϵ 2 ) ) T = exp ⁡ ( O ( ρ / ϵ 2 ) ) rounds. When P P is the d d -dimensional simplex and ∥ ⋅ ∥ ‖ ⋅ ‖ is the ℓ 1 ℓ 1 -norm, the existence of O ( T log d − − − − − √ ) O ( T log ⁡ d ) algorithms for learning with experts implies that it is possible to obtain ϵ ϵ -calibrated forecasts after T = exp ( O ( log d / ϵ 2 ) ) = d O ( 1 / ϵ 2 ) T = exp ⁡ ( O ( log ⁡ d / ϵ 2 ) ) = d O ( 1 / ϵ 2 ) rounds, recovering a recent result of Peng 2025. Interestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate ρ ρ -- in fact, our algorithm is identical for every setting of P P and ∥ ⋅ ∥ ‖ ⋅ ‖ . Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine. The resulting algorithm is highly efficient and plays a distribution over simple averages of past observations in each round. Finally, we prove that any online calibration algorithm that guarantees ϵ T ϵ T ℓ 1 ℓ 1 -calibration error over the d d -dimensional simplex requires T ≥ exp ( p o l y ( 1 / ϵ ) ) T ≥ exp ⁡ ( p o l y ( 1 / ϵ ) ) (assuming d ≥ p o l y ( 1 / ϵ ) d ≥ p o l y ( 1 / ϵ ) ). This strengthens the corresponding d Ω ( log 1 / ϵ ) d Ω ( log ⁡ 1 / ϵ ) lower bound of Peng 2025, and shows that an exponential dependence on 1 / ϵ 1 / ϵ is necessary.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=UVDihUz0iT"
        ],
        "venue": [
          "/venue/UVDihUz0iT@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=UVDihUz0iT"
        ],
        "detail": [
          "https://openreview.net/forum?id=UVDihUz0iT"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 13
      },
      "raw_excerpt": "High-Dimensional Calibration from Swap Regret [PDF 4 ] [Copy] [Kimi 13 ] [REL] Authors : Maxwell Fishelson , Noah Golowich , Mehryar Mohri , Jon Schneider We study the online calibration of multi-dimensional forecasts over an arbitrary convex set P ⊂ R d P ⊂ R d relative to an arbitrary norm ∥ ⋅ ∥ ‖ ⋅ ‖ . We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee O ( ρ T − − − √ ) O ( ρ T ) worst-case regret after T T rounds when actions are drawn from P P and losses are drawn from the dual ∥ ⋅ ∥ ∗ ‖ ⋅ ‖ ∗ unit norm ball, then it is also possible to obtain ϵ ϵ -calibrated forecasts after T = exp ( O ( ρ / ϵ 2 ) ) T = exp ⁡ ( O ( ρ / ϵ 2 ) ) rounds. When P P is the d d -dimensional simplex and ∥ ⋅ ∥ ‖ ⋅ ‖ is the ℓ 1 ℓ 1 -norm, the existence of O ( T log d − − − − − √ ) O ( T log ⁡ d ) algorithms for learning with experts implies that it is possible to obtain ϵ ϵ -calibrated forecasts after T = exp ( O ( log d / ϵ 2 ) ) = d O ( 1 / ϵ 2 ) T = exp ⁡ ( O ( log ⁡ d / ϵ 2 ) ) = d O ( 1 / ϵ 2 ) rounds, recovering a recent result of Peng 2025. Interestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate ρ ρ -- in fact, our algorithm is identical for every setting of P P and ∥ ⋅ ∥ ‖ ⋅ ‖ . Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine. The resulting algorithm is highly efficient and plays a distribution over simple averages of past observations in each round. Finally, we prove that any online calibration algorithm that guarantees ϵ T ϵ T ℓ 1 ℓ 1 -calibration error over the d d -dimensional simplex requires T ≥ exp ( p o l y ( 1 / ϵ ) ) T ≥ exp ⁡ ( p o l y ( 1 / ϵ ) ) (assuming d ≥ p o l y ( 1 / ϵ ) d ≥ p o l y ( 1 / ϵ ) ). This strengthens the corresponding d Ω ( log 1 / ϵ ) d Ω ( log ⁡ 1 / ϵ ) lower bound of Peng 2025, and shows that an exponential dependence on 1 / ϵ 1 / ϵ is necessary. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "CH72XyZs4y@OpenReview",
      "index": 14,
      "title": "In Search of Adam’s Secret Sauce",
      "authors": [
        "Antonio Orvieto",
        "Robert M. Gower"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "adam",
        "sauce",
        "signed",
        "momentum",
        "secret",
        "insights",
        "beta",
        "underperform",
        "reformulations",
        "affords"
      ],
      "summary": "Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study — training over 1,500 language models across different data configurations and scales — comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal, β 1 = β 2 β 1 = β 2 . Beyond robust performance, this choice affords new theoretical insights, highlights the \"secret sauce\" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients—one that arises from a mean-field Gaussian variational inference perspective.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CH72XyZs4y"
        ],
        "venue": [
          "/venue/CH72XyZs4y@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CH72XyZs4y"
        ],
        "detail": [
          "https://openreview.net/forum?id=CH72XyZs4y"
        ]
      },
      "scores": {
        "pdf": 15,
        "kimi": 18
      },
      "raw_excerpt": "In Search of Adam’s Secret Sauce [PDF 15 ] [Copy] [Kimi 18 ] [REL] Authors : Antonio Orvieto , Robert M. Gower Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study — training over 1,500 language models across different data configurations and scales — comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal, β 1 = β 2 β 1 = β 2 . Beyond robust performance, this choice affords new theoretical insights, highlights the \"secret sauce\" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients—one that arises from a mean-field Gaussian variational inference perspective. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "U8BwT6Rmw4@OpenReview",
      "index": 15,
      "title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
      "authors": [
        "Siyu Chen",
        "Theodor Misiakiewicz",
        "Ilias Zadik",
        "Peiyuan Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "franz",
        "parisi",
        "statistical",
        "criterion",
        "ngca",
        "lower",
        "bounds",
        "truncation",
        "convex",
        "equivalence"
      ],
      "summary": "Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for characterizing the computational hard phases in statistical detection problems. The FP criterion, based on an annealed version of the celebrated Franz-Parisi potential from statistical physics, was shown to be equivalent to low-degree polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting two distinct approaches to understanding the computational hardness in statistical inference. In this paper, we propose a refined FP criterion that aims to better capture the geometric ``overlap\" structure of statistical models. Our main result establishes that this optimized FP criterion is equivalent to Statistical Query (SQ) lower bounds---another foundational framework in computational complexity of statistical inference. Crucially, this equivalence holds under a mild, verifiable assumption satisfied by a broad class of statistical models, including Gaussian additive models, planted sparse models, as well as non-Gaussian component analysis (NGCA), single-index (SI) models, and convex truncation detection settings. For instance, in the case of convex truncation tasks, the assumption is equivalent with the Gaussian correlation inequality (Royen, 2014) from convex geometry. In addition to the above, our equivalence not only unifies and simplifies the derivation of several known SQ lower bounds—such as for the NGCA model (Diakonikolas et al., 2017) and the SI model (Damian et al., 2024)—but also yields new SQ lower bounds of independent interest, including for the computational gaps in mixed sparse linear regression (Arpino et al., 2023) and convex truncation (De et al., 2023).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=U8BwT6Rmw4"
        ],
        "venue": [
          "/venue/U8BwT6Rmw4@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=U8BwT6Rmw4"
        ],
        "detail": [
          "https://openreview.net/forum?id=U8BwT6Rmw4"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Siyu Chen , Theodor Misiakiewicz , Ilias Zadik , Peiyuan Zhang Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for characterizing the computational hard phases in statistical detection problems. The FP criterion, based on an annealed version of the celebrated Franz-Parisi potential from statistical physics, was shown to be equivalent to low-degree polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting two distinct approaches to understanding the computational hardness in statistical inference. In this paper, we propose a refined FP criterion that aims to better capture the geometric ``overlap\" structure of statistical models. Our main result establishes that this optimized FP criterion is equivalent to Statistical Query (SQ) lower bounds---another foundational framework in computational complexity of statistical inference. Crucially, this equivalence holds under a mild, verifiable assumption satisfied by a broad class of statistical models, including Gaussian additive models, planted sparse models, as well as non-Gaussian component analysis (NGCA), single-index (SI) models, and convex truncation detection settings. For instance, in the case of convex truncation tasks, the assumption is equivalent with the Gaussian correlation inequality (Royen, 2014) from convex geometry. In addition to the above, our equivalence not only unifies and simplifies the derivation of several known SQ lower bounds—such as for the NGCA model (Diakonikolas et al., 2017) and the SI model (Damian et al., 2024)—but also yields new SQ lower bounds of independent interest, including for the computational gaps in mixed sparse linear regression (Arpino et al., 2023) and convex truncation (De et al., 2023). Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "efOq8wHH9o@OpenReview",
      "index": 16,
      "title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
      "authors": [
        "Yuxuan Zhou",
        "Heng Li",
        "Zhi-Qi Cheng",
        "Xudong Yan",
        "Yifei Dong",
        "Mario Fritz",
        "Margret Keuper"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "maxsup",
        "overconfidence",
        "logit",
        "smoothing",
        "regularization",
        "zhouyuxuanyx",
        "incorrect",
        "collapse",
        "label",
        "predictions"
      ],
      "summary": "Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS.Code and reproducibility scripts are available at https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=efOq8wHH9o"
        ],
        "venue": [
          "/venue/efOq8wHH9o@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=efOq8wHH9o"
        ],
        "detail": [
          "https://openreview.net/forum?id=efOq8wHH9o"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 15
      },
      "raw_excerpt": "MaxSup: Overcoming Representation Collapse in Label Smoothing [PDF 18 ] [Copy] [Kimi 15 ] [REL] Authors : Yuxuan Zhou , Heng Li , Zhi-Qi Cheng , Xudong Yan , Yifei Dong , Mario Fritz , Margret Keuper Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS.Code and reproducibility scripts are available at https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "IfD2MKTmWv@OpenReview",
      "index": 17,
      "title": "Memory Mosaics at scale",
      "authors": [
        "Jianyu Zhang",
        "Leon Bottou"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "mosaics",
        "memory",
        "trillion",
        "scale",
        "tokens",
        "transformers",
        "knowledge",
        "capabilities",
        "10b",
        "storage"
      ],
      "summary": "Memory Mosaics, networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets. To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (*memory mosaics v2*), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning. Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=IfD2MKTmWv"
        ],
        "venue": [
          "/venue/IfD2MKTmWv@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=IfD2MKTmWv"
        ],
        "detail": [
          "https://openreview.net/forum?id=IfD2MKTmWv"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 15
      },
      "raw_excerpt": "Memory Mosaics at scale [PDF 18 ] [Copy] [Kimi 15 ] [REL] Authors : Jianyu Zhang , Leon Bottou Memory Mosaics, networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets. To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (*memory mosaics v2*), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning. Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "jMhRbV47pS@OpenReview",
      "index": 18,
      "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
      "authors": [
        "Nicolas Zucchet",
        "Francesco D'Angelo",
        "Andrew Kyle Lampinen",
        "Stephanie C.Y. Chan"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "emergence",
        "attention",
        "sparse",
        "repetition",
        "abilities",
        "transformers",
        "understanding",
        "task",
        "benefits",
        "fascinating"
      ],
      "summary": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jMhRbV47pS"
        ],
        "venue": [
          "/venue/jMhRbV47pS@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jMhRbV47pS"
        ],
        "detail": [
          "https://openreview.net/forum?id=jMhRbV47pS"
        ]
      },
      "scores": {
        "pdf": 21,
        "kimi": 17
      },
      "raw_excerpt": "The emergence of sparse attention: impact of data distribution and benefits of repetition [PDF 21 ] [Copy] [Kimi 17 ] [REL] Authors : Nicolas Zucchet , Francesco D'Angelo , Andrew Kyle Lampinen , Stephanie C.Y. Chan Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "aLhA7AYLLR@OpenReview",
      "index": 19,
      "title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
      "authors": [
        "Linfeng Tang",
        "Yeda Wang",
        "Zhanchuan Cai",
        "Junjun Jiang",
        "Jiayi Ma"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "degradations",
        "degradation",
        "controlfusion",
        "fusion",
        "prompts",
        "composite",
        "user",
        "controllable",
        "vision",
        "network"
      ],
      "summary": "Current image fusion methods struggle with real-world composite degradations and lack the flexibility to accommodate user-specific needs. To address this, we propose ControlFusion, a controllable fusion network guided by language-vision prompts that adaptively mitigates composite degradations. On the one hand, we construct a degraded imaging model based on physical mechanisms, such as the Retinex theory and atmospheric scattering principle, to simulate composite degradations and provide a data foundation for addressing realistic degradations. On the other hand, we devise a prompt-modulated restoration and fusion network that dynamically enhances features according to degradation prompts, enabling adaptability to varying degradation levels. To support user-specific preferences in visual quality, a text encoder is incorporated to embed user-defined degradation types and levels as degradation prompts. Moreover, a spatial-frequency collaborative visual adapter is designed to autonomously perceive degradations from source images, thereby reducing complete reliance on user instructions. Extensive experiments demonstrate that ControlFusion outperforms SOTA fusion methods in fusion quality and degradation handling, particularly under real-world and compound degradations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aLhA7AYLLR"
        ],
        "venue": [
          "/venue/aLhA7AYLLR@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aLhA7AYLLR"
        ],
        "detail": [
          "https://openreview.net/forum?id=aLhA7AYLLR"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 12
      },
      "raw_excerpt": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts [PDF 20 ] [Copy] [Kimi 12 ] [REL] Authors : Linfeng Tang , Yeda Wang , Zhanchuan Cai , Junjun Jiang , Jiayi Ma Current image fusion methods struggle with real-world composite degradations and lack the flexibility to accommodate user-specific needs. To address this, we propose ControlFusion, a controllable fusion network guided by language-vision prompts that adaptively mitigates composite degradations. On the one hand, we construct a degraded imaging model based on physical mechanisms, such as the Retinex theory and atmospheric scattering principle, to simulate composite degradations and provide a data foundation for addressing realistic degradations. On the other hand, we devise a prompt-modulated restoration and fusion network that dynamically enhances features according to degradation prompts, enabling adaptability to varying degradation levels. To support user-specific preferences in visual quality, a text encoder is incorporated to embed user-defined degradation types and levels as degradation prompts. Moreover, a spatial-frequency collaborative visual adapter is designed to autonomously perceive degradations from source images, thereby reducing complete reliance on user instructions. Extensive experiments demonstrate that ControlFusion outperforms SOTA fusion methods in fusion quality and degradation handling, particularly under real-world and compound degradations. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "MrUsZfQ9pC@OpenReview",
      "index": 20,
      "title": "Identifiability of Deep Polynomial Neural Networks",
      "authors": [
        "Konstantin Usevich",
        "Ricardo Augusto Borsoi",
        "Clara Dérand",
        "Marianne Clausel"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "identifiability",
        "pnns",
        "activation",
        "widths",
        "identifiable",
        "degrees",
        "neurovarieties",
        "decoder",
        "pnn",
        "deep"
      ],
      "summary": "Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability-a key property for ensuring interpretability-remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly compared to the activation degrees. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. We also settle an open conjecture on the dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach the expected dimension.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=MrUsZfQ9pC"
        ],
        "venue": [
          "/venue/MrUsZfQ9pC@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=MrUsZfQ9pC"
        ],
        "detail": [
          "https://openreview.net/forum?id=MrUsZfQ9pC"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 8
      },
      "raw_excerpt": "Identifiability of Deep Polynomial Neural Networks [PDF 6 ] [Copy] [Kimi 8 ] [REL] Authors : Konstantin Usevich , Ricardo Augusto Borsoi , Clara Dérand , Marianne Clausel Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability-a key property for ensuring interpretability-remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly compared to the activation degrees. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. We also settle an open conjecture on the dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach the expected dimension. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "Q3qAsZAEZw@OpenReview",
      "index": 21,
      "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "authors": [
        "Jiayi Yuan",
        "Hao Li",
        "Xinheng Ding",
        "Wenya Xie",
        "Yu-Jhe Li",
        "Wentian Zhao",
        "Kun Wan",
        "Jing Shi",
        "Xia Hu",
        "Zirui Liu"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "precision",
        "reproducibility",
        "llm",
        "gpu",
        "nondeterminism",
        "differences",
        "inference",
        "floating",
        "numerical",
        "evaluation"
      ],
      "summary": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Q3qAsZAEZw"
        ],
        "venue": [
          "/venue/Q3qAsZAEZw@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Q3qAsZAEZw"
        ],
        "detail": [
          "https://openreview.net/forum?id=Q3qAsZAEZw"
        ]
      },
      "scores": {
        "pdf": 19,
        "kimi": 16
      },
      "raw_excerpt": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference [PDF 19 ] [Copy] [Kimi 16 ] [REL] Authors : Jiayi Yuan , Hao Li , Xinheng Ding , Wenya Xie , Yu-Jhe Li , Wentian Zhao , Kun Wan , Jing Shi , Xia Hu , Zirui Liu Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "4xvE6Iy77Y@OpenReview",
      "index": 22,
      "title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "authors": [
        "Ruiqi Wang",
        "Dezhong Zhao",
        "Ziqin Yuan",
        "Tianyu Shao",
        "Guohua Chen",
        "Dominic Kao",
        "Sungeun Hong",
        "Byung-Cheol Min"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "primt",
        "trajectory",
        "pbrl",
        "feedback",
        "preference",
        "multimodal",
        "foundation",
        "ambiguity",
        "credit",
        "reinforcement"
      ],
      "summary": "Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4xvE6Iy77Y"
        ],
        "venue": [
          "/venue/4xvE6Iy77Y@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4xvE6Iy77Y"
        ],
        "detail": [
          "https://openreview.net/forum?id=4xvE6Iy77Y"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 14
      },
      "raw_excerpt": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models [PDF 12 ] [Copy] [Kimi 14 ] [REL] Authors : Ruiqi Wang , Dezhong Zhao , Ziqin Yuan , Tianyu Shao , Guohua Chen , Dominic Kao , Sungeun Hong , Byung-Cheol Min Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "R73ybUciQF@OpenReview",
      "index": 23,
      "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "authors": [
        "David Chanin",
        "James Wilken-Smith",
        "Tomáš Dulka",
        "Hardik Bhatnagar",
        "Satvik Golechha",
        "Joseph Isaac Bloom"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "saes",
        "absorption",
        "features",
        "sae",
        "feature",
        "splitting",
        "sparse",
        "autoencoders",
        "monosemantic",
        "split"
      ],
      "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (“math” may split into “algebra”, “geometry”, etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get “absorbed” into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=R73ybUciQF"
        ],
        "venue": [
          "/venue/R73ybUciQF@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=R73ybUciQF"
        ],
        "detail": [
          "https://openreview.net/forum?id=R73ybUciQF"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 14
      },
      "raw_excerpt": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders [PDF 10 ] [Copy] [Kimi 14 ] [REL] Authors : David Chanin , James Wilken-Smith , Tomáš Dulka , Hardik Bhatnagar , Satvik Golechha , Joseph Isaac Bloom Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (“math” may split into “algebra”, “geometry”, etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get “absorbed” into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "B6bE2GC71a@OpenReview",
      "index": 24,
      "title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "authors": [
        "Zhenting Qi",
        "Fan Nie",
        "Alexandre Alahi",
        "James Zou",
        "Himabindu Lakkaraju",
        "Yilun Du",
        "Eric P. Xing",
        "Sham M. Kakade",
        "Hanlin Zhang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "training",
        "pre",
        "evolm",
        "continued",
        "post",
        "language",
        "lms",
        "stages",
        "domain",
        "lost"
      ],
      "summary": "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=B6bE2GC71a"
        ],
        "venue": [
          "/venue/B6bE2GC71a@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=B6bE2GC71a"
        ],
        "detail": [
          "https://openreview.net/forum?id=B6bE2GC71a"
        ]
      },
      "scores": {
        "pdf": 14,
        "kimi": 18
      },
      "raw_excerpt": "EvoLM: In Search of Lost Language Model Training Dynamics [PDF 14 ] [Copy] [Kimi 18 ] [REL] Authors : Zhenting Qi , Fan Nie , Alexandre Alahi , James Zou , Himabindu Lakkaraju , Yilun Du , Eric P. Xing , Sham M. Kakade , Hanlin Zhang Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "WhEPg4mUs6@OpenReview",
      "index": 25,
      "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
      "authors": [
        "Zhaoxian Wu",
        "Quan Xiao",
        "Tayfun Gokmen",
        "Omobayode Fagbohungbe",
        "Tianyi Chen"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "aimc",
        "response",
        "ideal",
        "analog",
        "training",
        "resistive",
        "functions",
        "hardware",
        "impact",
        "conductance"
      ],
      "summary": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WhEPg4mUs6"
        ],
        "venue": [
          "/venue/WhEPg4mUs6@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WhEPg4mUs6"
        ],
        "detail": [
          "https://openreview.net/forum?id=WhEPg4mUs6"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 11
      },
      "raw_excerpt": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions [PDF 8 ] [Copy] [Kimi 11 ] [REL] Authors : Zhaoxian Wu , Quan Xiao , Tayfun Gokmen , Omobayode Fagbohungbe , Tianyi Chen As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "zJdutIT6vT@OpenReview",
      "index": 26,
      "title": "Discovering Opinion Intervals from Conflicts in Signed Graphs",
      "authors": [
        "Peter Blohm",
        "Florian Chen",
        "Aristides Gionis",
        "Stefan Neumann"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "conflicts",
        "opinion",
        "signed",
        "intervals",
        "graphs",
        "opinions",
        "signs",
        "german",
        "interactions",
        "discovering"
      ],
      "summary": "Online social media provide a platform for people to discuss current events and exchange opinions with their peers. While interactions are predominantly positive, in recent years, there has been a lot of research to understand the conflicts in social networks and how they are based on different views and opinions. In this paper, we ask whether the conflicts in a network reveal a small and interpretable set of prevalent opinion ranges that explain the users' interactions. More precisely, we consider signed graphs, where the edge signs indicate positive and negative interactions of node pairs, and our goal is to infer opinion intervals that are consistent with the edge signs. We introduce an optimization problem that models this question, and we give strong hardness results and a polynomial-time approximation scheme by utilizing connections to interval graphs and the Correlation Clustering problem. We further provide scalable heuristics and show that in experiments they yield more expressive solutions than Correlation Clustering baselines. We also present a case study on a novel real-world dataset from the German parliament, showing that our algorithms can recover the political leaning of German parties based on co-voting behavior.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zJdutIT6vT"
        ],
        "venue": [
          "/venue/zJdutIT6vT@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zJdutIT6vT"
        ],
        "detail": [
          "https://openreview.net/forum?id=zJdutIT6vT"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 8
      },
      "raw_excerpt": "Discovering Opinion Intervals from Conflicts in Signed Graphs [PDF 7 ] [Copy] [Kimi 8 ] [REL] Authors : Peter Blohm , Florian Chen , Aristides Gionis , Stefan Neumann Online social media provide a platform for people to discuss current events and exchange opinions with their peers. While interactions are predominantly positive, in recent years, there has been a lot of research to understand the conflicts in social networks and how they are based on different views and opinions. In this paper, we ask whether the conflicts in a network reveal a small and interpretable set of prevalent opinion ranges that explain the users' interactions. More precisely, we consider signed graphs, where the edge signs indicate positive and negative interactions of node pairs, and our goal is to infer opinion intervals that are consistent with the edge signs. We introduce an optimization problem that models this question, and we give strong hardness results and a polynomial-time approximation scheme by utilizing connections to interval graphs and the Correlation Clustering problem. We further provide scalable heuristics and show that in experiments they yield more expressive solutions than Correlation Clustering baselines. We also present a case study on a novel real-world dataset from the German parliament, showing that our algorithms can recover the political leaning of German parties based on co-voting behavior. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "8P3QNSckMp@OpenReview",
      "index": 27,
      "title": "A Clean Slate for Offline Reinforcement Learning",
      "authors": [
        "Matthew Thomas Jackson",
        "Uljad Berdica",
        "Jarek Luca Liesen",
        "Shimon Whiteson",
        "Jakob Nicolaus Foerster"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "unifloral",
        "offline",
        "implementations",
        "algorithmic",
        "clean",
        "slate",
        "hyperparameter",
        "awr",
        "reinforcement",
        "evaluation"
      ],
      "summary": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches and enables development within a single, comprehensive hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=8P3QNSckMp"
        ],
        "venue": [
          "/venue/8P3QNSckMp@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=8P3QNSckMp"
        ],
        "detail": [
          "https://openreview.net/forum?id=8P3QNSckMp"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 11
      },
      "raw_excerpt": "A Clean Slate for Offline Reinforcement Learning [PDF 13 ] [Copy] [Kimi 11 ] [REL] Authors : Matthew Thomas Jackson , Uljad Berdica , Jarek Luca Liesen , Shimon Whiteson , Jakob Nicolaus Foerster Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches and enables development within a single, comprehensive hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "F0JzotXYgC@OpenReview",
      "index": 28,
      "title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
      "authors": [
        "Phuc Tran",
        "Van Vu",
        "Nisheeth K. Vishnoi"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "spectral",
        "norm",
        "perturbation",
        "rank",
        "bounds",
        "approximation",
        "differentially",
        "privacy",
        "private",
        "matrix"
      ],
      "summary": "A central challenge in machine learning is to understand how noise or measurement errors affect low-rank approximations, particularly in the spectral norm. This question is especially important in differentially private low-rank approximation, where one aims to preserve the top- p p structure of a data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius norm error or changes in reconstruction quality, but these metrics can over- or under-estimate true subspace distortion. The spectral norm, by contrast, captures worst-case directional error and provides the strongest utility guarantees. We establish new high-probability spectral-norm perturbation bounds for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem and explicitly capture interactions between a matrix A ∈ R n × n A ∈ R n × n and an arbitrary symmetric perturbation E E . Under mild eigengap and norm conditions, our bounds yield sharp estimates for ∥ ( A + E ) p − A p ∥ ‖ ( A + E ) p − A p ‖ , where A p A p is the best rank- p p approximation of A A , with improvements of up to a factor of n − − √ n . As an application, we derive improved utility guarantees for differentially private PCA, resolving an open problem in the literature. Our analysis relies on a novel contour bootstrapping method from complex analysis and extends it to a broad class of spectral functionals, including polynomials and matrix exponentials. Empirical results on real-world datasets confirm that our bounds closely track the actual spectral error under diverse perturbation regimes.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=F0JzotXYgC"
        ],
        "venue": [
          "/venue/F0JzotXYgC@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=F0JzotXYgC"
        ],
        "detail": [
          "https://openreview.net/forum?id=F0JzotXYgC"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy [PDF 5 ] [Copy] [Kimi 5 ] [REL] Authors : Phuc Tran , Van Vu , Nisheeth K. Vishnoi A central challenge in machine learning is to understand how noise or measurement errors affect low-rank approximations, particularly in the spectral norm. This question is especially important in differentially private low-rank approximation, where one aims to preserve the top- p p structure of a data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius norm error or changes in reconstruction quality, but these metrics can over- or under-estimate true subspace distortion. The spectral norm, by contrast, captures worst-case directional error and provides the strongest utility guarantees. We establish new high-probability spectral-norm perturbation bounds for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem and explicitly capture interactions between a matrix A ∈ R n × n A ∈ R n × n and an arbitrary symmetric perturbation E E . Under mild eigengap and norm conditions, our bounds yield sharp estimates for ∥ ( A + E ) p − A p ∥ ‖ ( A + E ) p − A p ‖ , where A p A p is the best rank- p p approximation of A A , with improvements of up to a factor of n − − √ n . As an application, we derive improved utility guarantees for differentially private PCA, resolving an open problem in the literature. Our analysis relies on a novel contour bootstrapping method from complex analysis and extends it to a broad class of spectral functionals, including polynomials and matrix exponentials. Empirical results on real-world datasets confirm that our bounds closely track the actual spectral error under diverse perturbation regimes. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "gxfusMqPIs@OpenReview",
      "index": 29,
      "title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
      "authors": [
        "Shogo Iwazaki"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "regret",
        "ucb",
        "gaussian",
        "upper",
        "bound",
        "bayesian",
        "process",
        "scarlett",
        "confidence",
        "optimization"
      ],
      "summary": "This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Matérn kernel with some extent of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves O ~ ( T − − √ ) O ~ ( T ) cumulative regret with high probability. Furthermore, our analysis yields O ( T ln 2 T − − − − − − √ ) O ( T ln 2 ⁡ T ) regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound of GP-UCB and the current best upper bound provided by Scarlett [2018]. The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling us to handle GP's information gain in a refined manner.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gxfusMqPIs"
        ],
        "venue": [
          "/venue/gxfusMqPIs@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gxfusMqPIs"
        ],
        "detail": [
          "https://openreview.net/forum?id=gxfusMqPIs"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 5
      },
      "raw_excerpt": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization [PDF 5 ] [Copy] [Kimi 5 ] [REL] Author : Shogo Iwazaki This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Matérn kernel with some extent of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves O ~ ( T − − √ ) O ~ ( T ) cumulative regret with high probability. Furthermore, our analysis yields O ( T ln 2 T − − − − − − √ ) O ( T ln 2 ⁡ T ) regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound of GP-UCB and the current best upper bound provided by Scarlett [2018]. The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling us to handle GP's information gain in a refined manner. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "eIDa6pd9iQ@OpenReview",
      "index": 30,
      "title": "Auto-Compressing Networks",
      "authors": [
        "Vaggelis Dorovatas",
        "Georgios Paraskevopoulos",
        "Alexandros Potamianos"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "acns",
        "auto",
        "architectural",
        "compressing",
        "residual",
        "connections",
        "compression",
        "redundancy",
        "catastrophic",
        "forgetting"
      ],
      "summary": "Deep neural networks with short residual connections have demonstrated remarkable success across domains, but increasing depth often introduces computational redundancy without corresponding improvements in representation quality. We introduce Auto-Compressing Networks (ACNs), an architectural variant where additive long feedforward connections from each layer to the output replace traditional short residual connections. By analyzing the distinct dynamics induced by this modification, we reveal a unique property we coin as *auto-compression*—the ability of a network to organically compress information during training with gradient descent, through architectural design alone. Through auto-compression, information is dynamically \"pushed\" into early layers during training, enhancing their representational quality and revealing potential redundancy in deeper ones. We theoretically show that this property emerges from layer-wise training patterns found only in ACNs, where layers are dynamically utilized during training based on task requirements. We also find that ACNs exhibit enhanced noise robustness compared to residual networks, superior performance in low-data settings, improved transfer learning capabilities, and mitigate catastrophic forgetting suggesting that they learn representations that generalize better despite using fewer parameters. Our results demonstrate up to 18\\% reduction in catastrophic forgetting and 30-80\\% architectural compression while maintaining accuracy across vision transformers, MLP-mixers, and BERT architectures. These findings establish ACNs as a practical approach to developing efficient neural architectures that automatically adapt their computational footprint to task complexity, while learning robust representations suitable for noisy real-world tasks and continual learning scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=eIDa6pd9iQ"
        ],
        "venue": [
          "/venue/eIDa6pd9iQ@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=eIDa6pd9iQ"
        ],
        "detail": [
          "https://openreview.net/forum?id=eIDa6pd9iQ"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 14
      },
      "raw_excerpt": "Auto-Compressing Networks [PDF 18 ] [Copy] [Kimi 14 ] [REL] Authors : Vaggelis Dorovatas , Georgios Paraskevopoulos , Alexandros Potamianos Deep neural networks with short residual connections have demonstrated remarkable success across domains, but increasing depth often introduces computational redundancy without corresponding improvements in representation quality. We introduce Auto-Compressing Networks (ACNs), an architectural variant where additive long feedforward connections from each layer to the output replace traditional short residual connections. By analyzing the distinct dynamics induced by this modification, we reveal a unique property we coin as *auto-compression*—the ability of a network to organically compress information during training with gradient descent, through architectural design alone. Through auto-compression, information is dynamically \"pushed\" into early layers during training, enhancing their representational quality and revealing potential redundancy in deeper ones. We theoretically show that this property emerges from layer-wise training patterns found only in ACNs, where layers are dynamically utilized during training based on task requirements. We also find that ACNs exhibit enhanced noise robustness compared to residual networks, superior performance in low-data settings, improved transfer learning capabilities, and mitigate catastrophic forgetting suggesting that they learn representations that generalize better despite using fewer parameters. Our results demonstrate up to 18\\% reduction in catastrophic forgetting and 30-80\\% architectural compression while maintaining accuracy across vision transformers, MLP-mixers, and BERT architectures. These findings establish ACNs as a practical approach to developing efficient neural architectures that automatically adapt their computational footprint to task complexity, while learning robust representations suitable for noisy real-world tasks and continual learning scenarios. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "oJ84bedrtM@OpenReview",
      "index": 31,
      "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
      "authors": [
        "Yake Wei",
        "Yu Miao",
        "Dongzhan Zhou",
        "Di Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "multimodal",
        "moka",
        "adaptation",
        "mllms",
        "unimodal",
        "qwen2",
        "tuning",
        "modal",
        "rank",
        "fine"
      ],
      "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal Low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oJ84bedrtM"
        ],
        "venue": [
          "/venue/oJ84bedrtM@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oJ84bedrtM"
        ],
        "detail": [
          "https://openreview.net/forum?id=oJ84bedrtM"
        ]
      },
      "scores": {
        "pdf": 36,
        "kimi": 22
      },
      "raw_excerpt": "MokA: Multimodal Low-Rank Adaptation for MLLMs [PDF 36 ] [Copy] [Kimi 22 ] [REL] Authors : Yake Wei , Yu Miao , Dongzhan Zhou , Di Hu In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal Low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "iydmH9boLb@OpenReview",
      "index": 32,
      "title": "Advancing Expert Specialization for Better MoE",
      "authors": [
        "Hongcan Guo",
        "Haolang Lu",
        "Guoshun Nan",
        "Bolun Chu",
        "Jialin Zhuang",
        "Yuan Yang",
        "Wenhao Che",
        "Xinye Cao",
        "Sicong Leng",
        "Qimei Cui",
        "Xudong Jiang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "moe",
        "specialization",
        "expert",
        "loss",
        "experts",
        "auxiliary",
        "encourage",
        "load",
        "balancing",
        "routing"
      ],
      "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=iydmH9boLb"
        ],
        "venue": [
          "/venue/iydmH9boLb@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=iydmH9boLb"
        ],
        "detail": [
          "https://openreview.net/forum?id=iydmH9boLb"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 14
      },
      "raw_excerpt": "Advancing Expert Specialization for Better MoE [PDF 20 ] [Copy] [Kimi 14 ] [REL] Authors : Hongcan Guo , Haolang Lu , Guoshun Nan , Bolun Chu , Jialin Zhuang , Yuan Yang , Wenhao Che , Xinye Cao , Sicong Leng , Qimei Cui , Xudong Jiang Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "gm5mkiTGOy@OpenReview",
      "index": 33,
      "title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
      "authors": [
        "Zheng-An Chen",
        "Tao Luo"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "stage",
        "initialization",
        "matrices",
        "transformer",
        "dynamics",
        "condensation",
        "training",
        "rank",
        "collapse",
        "dissects"
      ],
      "summary": "Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in \\cite{zhou2022towards} to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gm5mkiTGOy"
        ],
        "venue": [
          "/venue/gm5mkiTGOy@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gm5mkiTGOy"
        ],
        "detail": [
          "https://openreview.net/forum?id=gm5mkiTGOy"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 6
      },
      "raw_excerpt": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics [PDF 9 ] [Copy] [Kimi 6 ] [REL] Authors : Zheng-An Chen , Tao Luo Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in \\cite{zhou2022towards} to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "KnqiC0znVF@OpenReview",
      "index": 34,
      "title": "Large Language Diffusion Models",
      "authors": [
        "Shen Nie",
        "Fengqi Zhu",
        "Zebin You",
        "Xiaolu Zhang",
        "Jingyang Ou",
        "Jun Hu",
        "JUN ZHOU",
        "Yankai Lin",
        "Ji-Rong Wen",
        "Chongxuan Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "llada",
        "sft",
        "language",
        "diffusion",
        "arms",
        "gsai",
        "reversal",
        "capabilities",
        "llms",
        "llama3"
      ],
      "summary": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=KnqiC0znVF"
        ],
        "venue": [
          "/venue/KnqiC0znVF@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=KnqiC0znVF"
        ],
        "detail": [
          "https://openreview.net/forum?id=KnqiC0znVF"
        ]
      },
      "scores": {
        "pdf": 35,
        "kimi": 19
      },
      "raw_excerpt": "Large Language Diffusion Models [PDF 35 ] [Copy] [Kimi 19 ] [REL] Authors : Shen Nie , Fengqi Zhu , Zebin You , Xiaolu Zhang , Jingyang Ou , Jun Hu , JUN ZHOU , Yankai Lin , Ji-Rong Wen , Chongxuan Li The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "qYkhCah8OZ@OpenReview",
      "index": 35,
      "title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
      "authors": [
        "Wenbin An",
        "Jiahao Nie",
        "Feng Tian",
        "Haonan Lin",
        "mingxiang cai",
        "Yaqiang Wu",
        "QianYing Wang",
        "Xiaoqin Zhang",
        "Shijian Lu"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "alfar",
        "knowledge",
        "mllms",
        "logits",
        "reallocation",
        "contextual",
        "attention",
        "multimodal",
        "tokens",
        "conflicts"
      ],
      "summary": "Despite their recent progress, Multimodal Large Language Models (MLLMs) often struggle in knowledge-intensive tasks due to the limited and outdated parametric knowledge acquired during training. Multimodal Retrieval Augmented Generation addresses this issue by retrieving contextual knowledge from external databases, thereby enhancing MLLMs with expanded knowledge sources. However, existing MLLMs often fail to fully leverage the retrieved contextual knowledge for response generation. We examine representative MLLMs and identify two major causes, namely, attention bias toward different tokens and knowledge conflicts between parametric and contextual knowledge. To this end, we design Adaptive Logits Fusion and Attention Reallocation (ALFAR), a training-free and plug-and-play approach that improves MLLM responses by maximizing the utility of the retrieved knowledge. Specifically, ALFAR tackles the challenges from two perspectives. First, it alleviates attention bias by adaptively shifting attention from visual tokens to relevant context tokens according to query-context relevance. Second, it decouples and weights parametric and contextual knowledge at output logits, mitigating conflicts between the two types of knowledge. As a plug-and-play method, ALFAR achieves superior performance across diverse datasets without requiring additional training or external tools. Extensive experiments over multiple MLLMs and benchmarks show that ALFAR consistently outperforms the state-of-the-art by large margins. Our code and data are available at https://github.com/Lackel/ALFAR.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=qYkhCah8OZ"
        ],
        "venue": [
          "/venue/qYkhCah8OZ@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=qYkhCah8OZ"
        ],
        "detail": [
          "https://openreview.net/forum?id=qYkhCah8OZ"
        ]
      },
      "scores": {
        "pdf": 16,
        "kimi": 15
      },
      "raw_excerpt": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation [PDF 16 ] [Copy] [Kimi 15 ] [REL] Authors : Wenbin An , Jiahao Nie , Feng Tian , Haonan Lin , mingxiang cai , Yaqiang Wu , QianYing Wang , Xiaoqin Zhang , Shijian Lu Despite their recent progress, Multimodal Large Language Models (MLLMs) often struggle in knowledge-intensive tasks due to the limited and outdated parametric knowledge acquired during training. Multimodal Retrieval Augmented Generation addresses this issue by retrieving contextual knowledge from external databases, thereby enhancing MLLMs with expanded knowledge sources. However, existing MLLMs often fail to fully leverage the retrieved contextual knowledge for response generation. We examine representative MLLMs and identify two major causes, namely, attention bias toward different tokens and knowledge conflicts between parametric and contextual knowledge. To this end, we design Adaptive Logits Fusion and Attention Reallocation (ALFAR), a training-free and plug-and-play approach that improves MLLM responses by maximizing the utility of the retrieved knowledge. Specifically, ALFAR tackles the challenges from two perspectives. First, it alleviates attention bias by adaptively shifting attention from visual tokens to relevant context tokens according to query-context relevance. Second, it decouples and weights parametric and contextual knowledge at output logits, mitigating conflicts between the two types of knowledge. As a plug-and-play method, ALFAR achieves superior performance across diverse datasets without requiring additional training or external tools. Extensive experiments over multiple MLLMs and benchmarks show that ALFAR consistently outperforms the state-of-the-art by large margins. Our code and data are available at https://github.com/Lackel/ALFAR. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "fohuurA03P@OpenReview",
      "index": 36,
      "title": "Interactive Cross-modal Learning for Text-3D Scene Retrieval",
      "authors": [
        "Yanglin Feng",
        "Yongxiang Li",
        "Yuan Sun",
        "Yang Qin",
        "Dezhong Peng",
        "Peng Hu"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "scene",
        "interactive",
        "answerer",
        "retrieval",
        "text",
        "iat",
        "t3sr",
        "irr",
        "scenes",
        "retriever"
      ],
      "summary": "Text-3D Scene Retrieval (T3SR) aims to retrieve relevant scenes using linguistic queries. Although traditional T3SR methods have made significant progress in capturing fine-grained associations, they implicitly assume that query descriptions are information-complete. In practical deployments, however, limited by the capabilities of users and models, it is difficult or even impossible to directly obtain a perfect textual query suiting the entire scene and model, thereby leading to performance degradation. To address this issue, we propose a novel Interactive Text-3D Scene Retrieval Method (IDeal), which promotes the enhancement of the alignment between texts and 3D scenes through continuous interaction. To achieve this, we present an Interactive Retrieval Refinement framework (IRR), which employs a questioner to pose contextually relevant questions to an answerer in successive rounds that either promote detailed probing or encourage exploratory divergence within scenes. Upon the iterative responses received from the answerer, IRR adopts a retriever to perform both feature-level and semantic-level information fusion, facilitating scene-level interaction and understanding for more precise re-rankings. To bridge the domain gap between queries and interactive texts, we propose an Interaction Adaptation Tuning strategy (IAT). IAT mitigates the discriminability and diversity risks among augmented text features that approximate the interaction text domain, achieving contrastive domain adaptation for our retriever. Extensive experimental results on three datasets demonstrate the superiority of IDeal.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=fohuurA03P"
        ],
        "venue": [
          "/venue/fohuurA03P@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=fohuurA03P"
        ],
        "detail": [
          "https://openreview.net/forum?id=fohuurA03P"
        ]
      },
      "scores": {
        "pdf": 13,
        "kimi": 5
      },
      "raw_excerpt": "Interactive Cross-modal Learning for Text-3D Scene Retrieval [PDF 13 ] [Copy] [Kimi 5 ] [REL] Authors : Yanglin Feng , Yongxiang Li , Yuan Sun , Yang Qin , Dezhong Peng , Peng Hu Text-3D Scene Retrieval (T3SR) aims to retrieve relevant scenes using linguistic queries. Although traditional T3SR methods have made significant progress in capturing fine-grained associations, they implicitly assume that query descriptions are information-complete. In practical deployments, however, limited by the capabilities of users and models, it is difficult or even impossible to directly obtain a perfect textual query suiting the entire scene and model, thereby leading to performance degradation. To address this issue, we propose a novel Interactive Text-3D Scene Retrieval Method (IDeal), which promotes the enhancement of the alignment between texts and 3D scenes through continuous interaction. To achieve this, we present an Interactive Retrieval Refinement framework (IRR), which employs a questioner to pose contextually relevant questions to an answerer in successive rounds that either promote detailed probing or encourage exploratory divergence within scenes. Upon the iterative responses received from the answerer, IRR adopts a retriever to perform both feature-level and semantic-level information fusion, facilitating scene-level interaction and understanding for more precise re-rankings. To bridge the domain gap between queries and interactive texts, we propose an Interaction Adaptation Tuning strategy (IAT). IAT mitigates the discriminability and diversity risks among augmented text features that approximate the interaction text domain, achieving contrastive domain adaptation for our retriever. Extensive experimental results on three datasets demonstrate the superiority of IDeal. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "XoN10bZtR9@OpenReview",
      "index": 37,
      "title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
      "authors": [
        "Wei Wang",
        "Haifeng Xia",
        "Chao Huang",
        "Zhengming Ding",
        "Cong Wang",
        "Haojie Li",
        "Xiaochun Cao"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "jmmd",
        "hsic",
        "domain",
        "joint",
        "discrepancy",
        "rethinking",
        "graph",
        "discrimination",
        "maximum",
        "mean"
      ],
      "summary": "In domain adaption (DA), joint maximum mean discrepancy (JMMD), as a famous distribution-distance metric, aims to measure joint probability distribution difference between the source domain and target domain, while it is still not fully explored and especially hard to be applied into a subspace-learning framework as its empirical estimation involves a tensor-product operator whose partial derivative is difficult to obtain. To solve this issue, we deduce a concise JMMD based on the Representer theorem that avoids the tensor-product operator and obtains two essential findings. First, we reveal the uniformity of JMMD by proving that previous marginal, class conditional, and weighted class conditional probability distribution distances are three special cases of JMMD with different label reproducing kernels. Second, inspired by graph embedding, we observe that the similarity weights, which strengthen the intra-class compactness in the graph of Hilbert Schmidt independence criterion (HSIC), take opposite signs in the graph of JMMD, revealing why JMMD degrades the feature discrimination. This motivates us to propose a novel loss JMMD-HSIC by jointly considering JMMD and HSIC to promote discrimination of JMMD. Extensive experiments on several cross-domain datasets could demonstrate the validity of our revealed theoretical results and the effectiveness of our proposed JMMD-HSIC.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XoN10bZtR9"
        ],
        "venue": [
          "/venue/XoN10bZtR9@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XoN10bZtR9"
        ],
        "detail": [
          "https://openreview.net/forum?id=XoN10bZtR9"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 5
      },
      "raw_excerpt": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation [PDF 9 ] [Copy] [Kimi 5 ] [REL] Authors : Wei Wang , Haifeng Xia , Chao Huang , Zhengming Ding , Cong Wang , Haojie Li , Xiaochun Cao In domain adaption (DA), joint maximum mean discrepancy (JMMD), as a famous distribution-distance metric, aims to measure joint probability distribution difference between the source domain and target domain, while it is still not fully explored and especially hard to be applied into a subspace-learning framework as its empirical estimation involves a tensor-product operator whose partial derivative is difficult to obtain. To solve this issue, we deduce a concise JMMD based on the Representer theorem that avoids the tensor-product operator and obtains two essential findings. First, we reveal the uniformity of JMMD by proving that previous marginal, class conditional, and weighted class conditional probability distribution distances are three special cases of JMMD with different label reproducing kernels. Second, inspired by graph embedding, we observe that the similarity weights, which strengthen the intra-class compactness in the graph of Hilbert Schmidt independence criterion (HSIC), take opposite signs in the graph of JMMD, revealing why JMMD degrades the feature discrimination. This motivates us to propose a novel loss JMMD-HSIC by jointly considering JMMD and HSIC to promote discrimination of JMMD. Extensive experiments on several cross-domain datasets could demonstrate the validity of our revealed theoretical results and the effectiveness of our proposed JMMD-HSIC. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "OzdAnGHEPx@OpenReview",
      "index": 38,
      "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
      "authors": [
        "Zhongnan Cai",
        "Yingying Wang",
        "Hui Zheng",
        "Panwang Pan",
        "ZiXu Lin",
        "Ge Meng",
        "Chenxin Li",
        "Chunming He",
        "Jiaxin Xie",
        "Yunlong Lin",
        "Junbin Lu",
        "Yue Huang",
        "Xinghao Ding"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "pan",
        "lut",
        "look",
        "sharpening",
        "table",
        "images",
        "15k",
        "remote",
        "learnable",
        "methods"
      ],
      "summary": "Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, deep learning-based methods incur substantial computational overhead during inference, especially with large images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for large remote sensing images. Our method makes it possible to process 15K × × 15K remote sensing images on a 24GB GPU. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details, we introduce the spatial details look-up table (SDLUT). Furthermore, to adaptively aggregate channel information for generating high-resolution multispectral images, we design an adaptive output look-up table (AOLUT). Our model contains fewer than 700K parameters and processes a 9K × × 9K image in under 1 ms using one RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=OzdAnGHEPx"
        ],
        "venue": [
          "/venue/OzdAnGHEPx@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=OzdAnGHEPx"
        ],
        "detail": [
          "https://openreview.net/forum?id=OzdAnGHEPx"
        ]
      },
      "scores": {
        "pdf": 10,
        "kimi": 3
      },
      "raw_excerpt": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables [PDF 10 ] [Copy] [Kimi 3 ] [REL] Authors : Zhongnan Cai , Yingying Wang , Hui Zheng , Panwang Pan , ZiXu Lin , Ge Meng , Chenxin Li , Chunming He , Jiaxin Xie , Yunlong Lin , Junbin Lu , Yue Huang , Xinghao Ding Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, deep learning-based methods incur substantial computational overhead during inference, especially with large images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for large remote sensing images. Our method makes it possible to process 15K × × 15K remote sensing images on a 24GB GPU. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details, we introduce the spatial details look-up table (SDLUT). Furthermore, to adaptively aggregate channel information for generating high-resolution multispectral images, we design an adaptive output look-up table (AOLUT). Our model contains fewer than 700K parameters and processes a 9K × × 9K image in under 1 ms using one RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "ImpizBSKcu@OpenReview",
      "index": 39,
      "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
      "authors": [
        "Andrea Montanari",
        "Pierfrancesco Urbani"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "overfitting",
        "decoupling",
        "dynamical",
        "large",
        "inductive",
        "complexity",
        "generalization",
        "layer",
        "bias",
        "overparametrized"
      ],
      "summary": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm. We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width m m , and large number of samples per input dimension n / d n / d , the training dynamics exhibits a separation of timescales which implies: ( i ) ( i ) The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network; ( i i ) ( i i ) Inductive bias towards small complexity if the initialization has small enough complexity; ( i i i ) ( i i i ) A dynamical decoupling between feature learning and overfitting regimes; ( i v ) ( i v ) A non-monotone behavior of the test error, associated `feature unlearning' regime at large times.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ImpizBSKcu"
        ],
        "venue": [
          "/venue/ImpizBSKcu@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ImpizBSKcu"
        ],
        "detail": [
          "https://openreview.net/forum?id=ImpizBSKcu"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 4
      },
      "raw_excerpt": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks [PDF 9 ] [Copy] [Kimi 4 ] [REL] Authors : Andrea Montanari , Pierfrancesco Urbani Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm. We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width m m , and large number of samples per input dimension n / d n / d , the training dynamics exhibits a separation of timescales which implies: ( i ) ( i ) The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network; ( i i ) ( i i ) Inductive bias towards small complexity if the initialization has small enough complexity; ( i i i ) ( i i i ) A dynamical decoupling between feature learning and overfitting regimes; ( i v ) ( i v ) A non-monotone behavior of the test error, associated `feature unlearning' regime at large times. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "s0JVsx3bx1@OpenReview",
      "index": 40,
      "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "authors": [
        "Kevin Wang",
        "Ishaan Javali",
        "Michał Bortkiewicz",
        "Tomasz Trzcinski",
        "Benjamin Eysenbach"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "depth",
        "self",
        "supervised",
        "goal",
        "reaching",
        "conditioned",
        "commanded",
        "scaling",
        "layers",
        "increases"
      ],
      "summary": "Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by 2 × 2 × -- 50 × 50 × , outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=s0JVsx3bx1"
        ],
        "venue": [
          "/venue/s0JVsx3bx1@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=s0JVsx3bx1"
        ],
        "detail": [
          "https://openreview.net/forum?id=s0JVsx3bx1"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 4
      },
      "raw_excerpt": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities [PDF 11 ] [Copy] [Kimi 4 ] [REL] Authors : Kevin Wang , Ishaan Javali , Michał Bortkiewicz , Tomasz Trzcinski , Benjamin Eysenbach Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by 2 × 2 × -- 50 × 50 × , outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "XO9fhSZkBh@OpenReview",
      "index": 41,
      "title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
      "authors": [
        "Moritz Leo Grillo",
        "Christoph Hertrich",
        "Georg Loho"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "maxout",
        "networks",
        "braid",
        "hidden",
        "layers",
        "neural",
        "bound",
        "maximum",
        "numbers",
        "arrangement"
      ],
      "summary": "We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on R d R d . While the question has been resolved in special cases, the best known lower bound in general is still 2. We focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan. For such neural networks, we prove a non-constant lower bound of Ω ( log log d ) Ω ( log ⁡ log ⁡ d ) hidden layers required to exactly represent the maximum of d d numbers. Additionally, we provide a combinatorial proof that neural networks satisfying this assumption require three hidden layers to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far. Finally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XO9fhSZkBh"
        ],
        "venue": [
          "/venue/XO9fhSZkBh@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XO9fhSZkBh"
        ],
        "detail": [
          "https://openreview.net/forum?id=XO9fhSZkBh"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 3
      },
      "raw_excerpt": "Depth-Bounds for Neural Networks via the Braid Arrangement [PDF 4 ] [Copy] [Kimi 3 ] [REL] Authors : Moritz Leo Grillo , Christoph Hertrich , Georg Loho We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on R d R d . While the question has been resolved in special cases, the best known lower bound in general is still 2. We focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan. For such neural networks, we prove a non-constant lower bound of Ω ( log log d ) Ω ( log ⁡ log ⁡ d ) hidden layers required to exactly represent the maximum of d d numbers. Additionally, we provide a combinatorial proof that neural networks satisfying this assumption require three hidden layers to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far. Finally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "VYLdKb5dzO@OpenReview",
      "index": 42,
      "title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
      "authors": [
        "Milad Sefidgaran",
        "Kimia Nadjahi",
        "Abdellatif Zaidi"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "cmi",
        "generalization",
        "bounds",
        "memorize",
        "tighter",
        "memorization",
        "attias",
        "livni",
        "projection",
        "algorithm"
      ],
      "summary": "In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of O ( 1 / n − − √ ) O ( 1 / n ) , where n n is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data \"memorization\" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must \"memorize'' a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=VYLdKb5dzO"
        ],
        "venue": [
          "/venue/VYLdKb5dzO@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=VYLdKb5dzO"
        ],
        "detail": [
          "https://openreview.net/forum?id=VYLdKb5dzO"
        ]
      },
      "scores": {
        "pdf": 4,
        "kimi": 5
      },
      "raw_excerpt": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization [PDF 4 ] [Copy] [Kimi 5 ] [REL] Authors : Milad Sefidgaran , Kimia Nadjahi , Abdellatif Zaidi In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of O ( 1 / n − − √ ) O ( 1 / n ) , where n n is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data \"memorization\" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must \"memorize'' a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "sYK4yPDuT1@OpenReview",
      "index": 43,
      "title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
      "authors": [
        "Yuzheng Hu",
        "Fan Wu",
        "Haotian Ye",
        "David Forsyth",
        "James Zou",
        "Nan Jiang",
        "Jiaqi W. Ma",
        "Han Zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "online",
        "attribution",
        "iif",
        "training",
        "policy",
        "framework",
        "reinforcement",
        "snapshot",
        "updates",
        "filtering"
      ],
      "summary": "Online reinforcement learning (RL) excels in complex, safety-critical domains but suffers from sample inefficiency, training instability, and limited interpretability. Data attribution provides a principled way to trace model behavior back to training samples, yet existing methods assume fixed datasets, which is violated in online RL where each experience both updates the policy and shapes future data collection. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a *local* attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Together, these results open a new direction for making online RL more interpretable, efficient, and effective.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=sYK4yPDuT1"
        ],
        "venue": [
          "/venue/sYK4yPDuT1@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=sYK4yPDuT1"
        ],
        "detail": [
          "https://openreview.net/forum?id=sYK4yPDuT1"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 4
      },
      "raw_excerpt": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning [PDF 9 ] [Copy] [Kimi 4 ] [REL] Authors : Yuzheng Hu , Fan Wu , Haotian Ye , David Forsyth , James Zou , Nan Jiang , Jiaqi W. Ma , Han Zhao Online reinforcement learning (RL) excels in complex, safety-critical domains but suffers from sample inefficiency, training instability, and limited interpretability. Data attribution provides a principled way to trace model behavior back to training samples, yet existing methods assume fixed datasets, which is violated in online RL where each experience both updates the policy and shapes future data collection. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a *local* attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Together, these results open a new direction for making online RL more interpretable, efficient, and effective. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "cGks3s79hW@OpenReview",
      "index": 44,
      "title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
      "authors": [
        "Valentin Schmutz",
        "Ali Haydaroğlu",
        "Shuqi Wang",
        "Yixiao Feng",
        "Matteo Carandini",
        "Kenneth D. Harris"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "dimensional",
        "neuronal",
        "activity",
        "latents",
        "low",
        "latent",
        "neurons",
        "high",
        "cortex",
        "solvable"
      ],
      "summary": "Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=cGks3s79hW"
        ],
        "venue": [
          "/venue/cGks3s79hW@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=cGks3s79hW"
        ],
        "detail": [
          "https://openreview.net/forum?id=cGks3s79hW"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 3
      },
      "raw_excerpt": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model [PDF 8 ] [Copy] [Kimi 3 ] [REL] Authors : Valentin Schmutz , Ali Haydaroğlu , Shuqi Wang , Yixiao Feng , Matteo Carandini , Kenneth D. Harris Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "oGmROC4e4W@OpenReview",
      "index": 45,
      "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
      "authors": [
        "Korneel Van den Berghe",
        "Stein Stroobants",
        "Vijay Janapa Reddi",
        "Guido De Croon"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "spiking",
        "slopes",
        "surrogate",
        "snns",
        "gradients",
        "shallower",
        "scheduled",
        "training",
        "td3bc",
        "neuromorphic"
      ],
      "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained robotics by achieving orders-of-magnitude efficiency gains, while enabling native temporal processing. Spiking Neural Networks (SNNs) represent a promising algorithmic approach for these systems, yet their application to complex control tasks faces two critical challenges: (1) the non-differentiable nature of spiking neurons necessitates surrogate gradients with unclear optimization properties, and (2) the stateful dynamics of SNNs require training on sequences, which in reinforcement learning (RL) is hindered by limited sequence lengths during early training, preventing the network from bridging its warm-up period. We address these challenges by systematically analyzing surrogate gradient slope settings, showing that shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients. In supervised learning, we find no clear preference for fixed or scheduled slopes. The effect is much more pronounced in RL settings, where shallower slopes or scheduled slopes lead to a × 2.1 × 2.1 improvement in both training and final deployed performance. Next, we propose a novel training approach that leverages a privileged guiding policy to bootstrap the learning process, while still exploiting online environment interactions with the spiking policy. Combining our method with an adaptive slope schedule for a real-world drone position control task, we achieve an average return of 400 points, substantially outperforming prior techniques, including Behavioral Cloning and TD3BC, which achieve at most –200 points under the same conditions. This work advances both the theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers demonstrated in real-world robotic systems.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=oGmROC4e4W"
        ],
        "venue": [
          "/venue/oGmROC4e4W@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=oGmROC4e4W"
        ],
        "detail": [
          "https://openreview.net/forum?id=oGmROC4e4W"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 4
      },
      "raw_excerpt": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks [PDF 7 ] [Copy] [Kimi 4 ] [REL] Authors : Korneel Van den Berghe , Stein Stroobants , Vijay Janapa Reddi , Guido De Croon Neuromorphic computing systems are set to revolutionize energy-constrained robotics by achieving orders-of-magnitude efficiency gains, while enabling native temporal processing. Spiking Neural Networks (SNNs) represent a promising algorithmic approach for these systems, yet their application to complex control tasks faces two critical challenges: (1) the non-differentiable nature of spiking neurons necessitates surrogate gradients with unclear optimization properties, and (2) the stateful dynamics of SNNs require training on sequences, which in reinforcement learning (RL) is hindered by limited sequence lengths during early training, preventing the network from bridging its warm-up period. We address these challenges by systematically analyzing surrogate gradient slope settings, showing that shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients. In supervised learning, we find no clear preference for fixed or scheduled slopes. The effect is much more pronounced in RL settings, where shallower slopes or scheduled slopes lead to a × 2.1 × 2.1 improvement in both training and final deployed performance. Next, we propose a novel training approach that leverages a privileged guiding policy to bootstrap the learning process, while still exploiting online environment interactions with the spiking policy. Combining our method with an adaptive slope schedule for a real-world drone position control task, we achieve an average return of 400 points, substantially outperforming prior techniques, including Behavioral Cloning and TD3BC, which achieve at most –200 points under the same conditions. This work advances both the theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers demonstrated in real-world robotic systems. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "aUAG1WS7J2@OpenReview",
      "index": 46,
      "title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
      "authors": [
        "Zhuang Qi",
        "Ying-Peng Tang",
        "Lei Meng",
        "Han Yu",
        "Xiaoxiao Li",
        "Xiangxu Meng"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "replay",
        "class",
        "fedcbdr",
        "fcil",
        "imbalance",
        "wise",
        "tasks",
        "global",
        "task",
        "balancing"
      ],
      "summary": "Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class-wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task knowledge in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task-aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model’s overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=aUAG1WS7J2"
        ],
        "venue": [
          "/venue/aUAG1WS7J2@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=aUAG1WS7J2"
        ],
        "detail": [
          "https://openreview.net/forum?id=aUAG1WS7J2"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 4
      },
      "raw_excerpt": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning [PDF 8 ] [Copy] [Kimi 4 ] [REL] Authors : Zhuang Qi , Ying-Peng Tang , Lei Meng , Han Yu , Xiaoxiao Li , Xiangxu Meng Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class-wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task knowledge in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task-aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model’s overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "m7MD0sa8Re@OpenReview",
      "index": 47,
      "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
      "authors": [
        "Trinity Chung",
        "Yuchen Shen",
        "Nathan Kong",
        "Aran Nayebi"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "tactile",
        "ead",
        "rodent",
        "convrnn",
        "recurrent",
        "somatosensory",
        "neural",
        "categorization",
        "cortex",
        "supervised"
      ],
      "summary": "Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy. For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=m7MD0sa8Re"
        ],
        "venue": [
          "/venue/m7MD0sa8Re@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=m7MD0sa8Re"
        ],
        "detail": [
          "https://openreview.net/forum?id=m7MD0sa8Re"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Trinity Chung , Yuchen Shen , Nathan Kong , Aran Nayebi Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy. For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "Zd6VyjmN1S@OpenReview",
      "index": 48,
      "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
      "authors": [
        "Zedong Liu",
        "Shenggan Cheng",
        "Guangming Tan",
        "Yang You",
        "Dingwen Tao"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "serving",
        "multimodal",
        "elasticmm",
        "parallelism",
        "inference",
        "ttft",
        "emp",
        "mllms",
        "stages",
        "elastic"
      ],
      "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components—combined with complex inference pipelines and heterogeneous workloads—introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2 × × and achieving 3.2–4.5 × × higher throughput while meeting service-level objectives (SLOs).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Zd6VyjmN1S"
        ],
        "venue": [
          "/venue/Zd6VyjmN1S@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Zd6VyjmN1S"
        ],
        "detail": [
          "https://openreview.net/forum?id=Zd6VyjmN1S"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 7
      },
      "raw_excerpt": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism [PDF 12 ] [Copy] [Kimi 7 ] [REL] Authors : Zedong Liu , Shenggan Cheng , Guangming Tan , Yang You , Dingwen Tao Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components—combined with complex inference pipelines and heterogeneous workloads—introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2 × × and achieving 3.2–4.5 × × higher throughput while meeting service-level objectives (SLOs). Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "7AwFJzgIUW@OpenReview",
      "index": 49,
      "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
      "authors": [
        "Steffen Schotthöfer",
        "H. Lexie Yang",
        "Stefan Schnake"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "adversarial",
        "rank",
        "compression",
        "robustness",
        "attacks",
        "low",
        "networks",
        "dynamical",
        "uncompressed",
        "regularizer"
      ],
      "summary": "Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=7AwFJzgIUW"
        ],
        "venue": [
          "/venue/7AwFJzgIUW@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=7AwFJzgIUW"
        ],
        "detail": [
          "https://openreview.net/forum?id=7AwFJzgIUW"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Steffen Schotthöfer , H. Lexie Yang , Stefan Schnake Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "ZwCVFBFUFb@OpenReview",
      "index": 50,
      "title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "authors": [
        "Wei Dai",
        "Peilin Chen",
        "Chanakya Ekbote",
        "Paul Pu Liang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "qoq",
        "med",
        "clinical",
        "grpo",
        "drpo",
        "multimodal",
        "foundation",
        "domain",
        "training",
        "aware"
      ],
      "summary": "Clinical decision‑making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision‑centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time‑series signals, and text reports. QoQ-Med is trained with Domain‑aware Relative Policy Optimization (DRPO), a novel reinforcement‑learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro‑F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=ZwCVFBFUFb"
        ],
        "venue": [
          "/venue/ZwCVFBFUFb@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=ZwCVFBFUFb"
        ],
        "detail": [
          "https://openreview.net/forum?id=ZwCVFBFUFb"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 10
      },
      "raw_excerpt": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training [PDF 12 ] [Copy] [Kimi 10 ] [REL] Authors : Wei Dai , Peilin Chen , Chanakya Ekbote , Paul Pu Liang Clinical decision‑making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision‑centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time‑series signals, and text reports. QoQ-Med is trained with Domain‑aware Relative Policy Optimization (DRPO), a novel reinforcement‑learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro‑F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "1b7whO4SfY@OpenReview",
      "index": 51,
      "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
      "authors": [
        "Zihan Qiu",
        "Zekun Wang",
        "Bo Zheng",
        "Zeyu Huang",
        "Kaiyue Wen",
        "Songlin Yang",
        "Rui Men",
        "Le Yu",
        "Fei Huang",
        "Suozhi Huang",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "gating",
        "attention",
        "sdpa",
        "gated",
        "softmax",
        "sink",
        "qwen3",
        "variants",
        "huggingface",
        "linearity"
      ],
      "summary": "Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates `massive activation`, `attention sink` and enhances long-context extrapolation performance. We also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research. Furthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next).",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=1b7whO4SfY"
        ],
        "venue": [
          "/venue/1b7whO4SfY@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=1b7whO4SfY"
        ],
        "detail": [
          "https://openreview.net/forum?id=1b7whO4SfY"
        ]
      },
      "scores": {
        "pdf": 39,
        "kimi": 22
      },
      "raw_excerpt": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free [PDF 39 ] [Copy] [Kimi 22 ] [REL] Authors : Zihan Qiu , Zekun Wang , Bo Zheng , Zeyu Huang , Kaiyue Wen , Songlin Yang , Rui Men , Le Yu , Fei Huang , Suozhi Huang , Dayiheng Liu , Jingren Zhou , Junyang Lin Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates `massive activation`, `attention sink` and enhances long-context extrapolation performance. We also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research. Furthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next). Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "w1ihNiIBOc@OpenReview",
      "index": 52,
      "title": "Learning long range dependencies through time reversal symmetry breaking",
      "authors": [
        "Guillaume Pourcel",
        "Maxence Ernoult"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "rhel",
        "emph",
        "hrus",
        "hamiltonian",
        "bptt",
        "hssms",
        "ssms",
        "systems",
        "physical",
        "recurrent"
      ],
      "summary": "Deep State Space Models (SSMs) reignite physics-grounded compute paradigms, as RNNs could natively be embodied into dynamical systems. This calls for dedicated learning algorithms obeying to core physical principles, with efficient techniques to simulate these systems and guide their design. We propose \\emph{Recurrent Hamiltonian Echo Learning} (RHEL), an algorithm which provably computes loss gradients as finite differences of physical trajectories of non-dissipative, \\emph{Hamiltonian systems}. In ML terms, RHEL only requires three ``forward passes'' irrespective of model size, without explicit Jacobian computation, nor incurring any variance in the gradient estimation. Motivated by the potential to implement our algorithm in non-digital physical systems, we first introduce RHEL in \\emph{continuous time} and demonstrate its formal equivalence with the continuous adjoint state method. To facilitate the simulation of Hamiltonian systems trained by RHEL, we propose a \\emph{discrete-time} version of RHEL which is equivalent to Backpropagation Through Time (BPTT) when applied to a class of recurrent modules which we call \\emph{Hamiltonian Recurrent Units} (HRUs). This setting allows us to demonstrate the scalability of RHEL by generalizing these results to hierarchies of HRUs, which we call \\emph{Hamiltonian SSMs} (HSSMs). We apply RHEL to train HSSMs with linear and nonlinear dynamics on a variety of time-series tasks ranging from mid-range to long-range classification and regression with sequence length reaching ∼ 50 k ∼ 50 k . We show that RHEL consistently matches the performance of BPTT across all models and tasks. This work opens new doors for the design of scalable, energy-efficient physical systems endowed with self-learning capabilities for sequence modelling.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=w1ihNiIBOc"
        ],
        "venue": [
          "/venue/w1ihNiIBOc@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=w1ihNiIBOc"
        ],
        "detail": [
          "https://openreview.net/forum?id=w1ihNiIBOc"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 3
      },
      "raw_excerpt": "Learning long range dependencies through time reversal symmetry breaking [PDF 7 ] [Copy] [Kimi 3 ] [REL] Authors : Guillaume Pourcel , Maxence Ernoult Deep State Space Models (SSMs) reignite physics-grounded compute paradigms, as RNNs could natively be embodied into dynamical systems. This calls for dedicated learning algorithms obeying to core physical principles, with efficient techniques to simulate these systems and guide their design. We propose \\emph{Recurrent Hamiltonian Echo Learning} (RHEL), an algorithm which provably computes loss gradients as finite differences of physical trajectories of non-dissipative, \\emph{Hamiltonian systems}. In ML terms, RHEL only requires three ``forward passes'' irrespective of model size, without explicit Jacobian computation, nor incurring any variance in the gradient estimation. Motivated by the potential to implement our algorithm in non-digital physical systems, we first introduce RHEL in \\emph{continuous time} and demonstrate its formal equivalence with the continuous adjoint state method. To facilitate the simulation of Hamiltonian systems trained by RHEL, we propose a \\emph{discrete-time} version of RHEL which is equivalent to Backpropagation Through Time (BPTT) when applied to a class of recurrent modules which we call \\emph{Hamiltonian Recurrent Units} (HRUs). This setting allows us to demonstrate the scalability of RHEL by generalizing these results to hierarchies of HRUs, which we call \\emph{Hamiltonian SSMs} (HSSMs). We apply RHEL to train HSSMs with linear and nonlinear dynamics on a variety of time-series tasks ranging from mid-range to long-range classification and regression with sequence length reaching ∼ 50 k ∼ 50 k . We show that RHEL consistently matches the performance of BPTT across all models and tasks. This work opens new doors for the design of scalable, energy-efficient physical systems endowed with self-learning capabilities for sequence modelling. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "WJujF9An5L@OpenReview",
      "index": 53,
      "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
      "authors": [
        "Qiusheng Huang",
        "Yuan Niu",
        "Xiaohui Zhong",
        "AnboyuGuo",
        "Lei Chen",
        "dianjun zhang",
        "Xuefeng Zhang",
        "Hao Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "ocean",
        "fuxi",
        "forecasting",
        "daily",
        "resolution",
        "sub",
        "eddy",
        "depths",
        "resolving",
        "predictions"
      ],
      "summary": "Accurate, high-resolution ocean forecasting is crucial for maritime operations and environmental monitoring. While traditional numerical models are capable of producing sub-daily, eddy-resolving forecasts, they are computationally intensive and face challenges in maintaining accuracy at fine spatial and temporal scales. In contrast, recent data-driven approaches offer improved computational efficiency and emerging potential, yet typically operate at daily resolution and struggle with sub-daily predictions due to error accumulation over time. We introduce FuXi-Ocean, the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12° spatial resolution, reaching depths of up to 1500 meters. The model architecture integrates a context-aware feature extraction module with a predictive network employing stacked attention blocks. The core innovation is the Mixture-of-Time (MoT) module, which adaptively integrates predictions from multiple temporal contexts by learning variable-specific reliability , mitigating cumulative errors in sequential forecasting. Through comprehensive experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting key variables, including temperature, salinity, and currents, across multiple depths.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WJujF9An5L"
        ],
        "venue": [
          "/venue/WJujF9An5L@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WJujF9An5L"
        ],
        "detail": [
          "https://openreview.net/forum?id=WJujF9An5L"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 1
      },
      "raw_excerpt": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution [PDF 6 ] [Copy] [Kimi 1 ] [REL] Authors : Qiusheng Huang , Yuan Niu , Xiaohui Zhong , AnboyuGuo , Lei Chen , dianjun zhang , Xuefeng Zhang , Hao Li Accurate, high-resolution ocean forecasting is crucial for maritime operations and environmental monitoring. While traditional numerical models are capable of producing sub-daily, eddy-resolving forecasts, they are computationally intensive and face challenges in maintaining accuracy at fine spatial and temporal scales. In contrast, recent data-driven approaches offer improved computational efficiency and emerging potential, yet typically operate at daily resolution and struggle with sub-daily predictions due to error accumulation over time. We introduce FuXi-Ocean, the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12° spatial resolution, reaching depths of up to 1500 meters. The model architecture integrates a context-aware feature extraction module with a predictive network employing stacked attention blocks. The core innovation is the Mixture-of-Time (MoT) module, which adaptively integrates predictions from multiple temporal contexts by learning variable-specific reliability , mitigating cumulative errors in sequential forecasting. Through comprehensive experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting key variables, including temperature, salinity, and currents, across multiple depths. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "knPz7gtjPW@OpenReview",
      "index": 54,
      "title": "Superposition Yields Robust Neural Scaling",
      "authors": [
        "Yizhou Liu",
        "Ziming Liu",
        "Jeff Gore"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "superposition",
        "scaling",
        "loss",
        "neural",
        "law",
        "llms",
        "laws",
        "inversely",
        "representation",
        "chinchilla"
      ],
      "summary": "The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=knPz7gtjPW"
        ],
        "venue": [
          "/venue/knPz7gtjPW@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=knPz7gtjPW"
        ],
        "detail": [
          "https://openreview.net/forum?id=knPz7gtjPW"
        ]
      },
      "scores": {
        "pdf": 8,
        "kimi": 6
      },
      "raw_excerpt": "Superposition Yields Robust Neural Scaling [PDF 8 ] [Copy] [Kimi 6 ] [REL] Authors : Yizhou Liu , Ziming Liu , Jeff Gore The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "i5WnXNjwbR@OpenReview",
      "index": 55,
      "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
      "authors": [
        "Tom Burgert",
        "Oliver Stoll",
        "Paolo Rota",
        "Begüm Demir"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "reliance",
        "texture",
        "cnns",
        "biased",
        "suppression",
        "feature",
        "geirhos",
        "controlled",
        "inherently",
        "shape"
      ],
      "summary": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=i5WnXNjwbR"
        ],
        "supp": [
          "/venue/i5WnXNjwbR@OpenReview"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=i5WnXNjwbR"
        ],
        "venue": [
          "/venue/NeurIPS.2025?group=Oral",
          "/venue/i5WnXNjwbR@OpenReview"
        ],
        "detail": [
          "https://openreview.net/forum?id=i5WnXNjwbR"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 9
      },
      "raw_excerpt": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression [PDF 11 ] [Copy] [Kimi 9 ] [REL] Authors : Tom Burgert , Oliver Stoll , Paolo Rota , Begüm Demir The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "RF3miSqdXa@OpenReview",
      "index": 56,
      "title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
      "authors": [
        "Viet-Hoang Tran",
        "Van-Hoan Trinh",
        "Khanh Vinh Bui",
        "Tan Minh Nguyen"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "moe",
        "lmc",
        "architectures",
        "experts",
        "gating",
        "connectivity",
        "mixture",
        "loss",
        "neural",
        "expert"
      ],
      "summary": "Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes of neural networks, wherein independently trained models have been observed to be connected—up to permutation symmetries—by linear paths in parameter space along which the loss remains consistently low. This observation challenges classical views of non-convex optimization and has implications for model ensembling, generalization, and our understanding of neural loss geometry. Inspired by recent studies on LMC in standard neural networks, we systematically investigate this phenomenon within Mixture-of-Experts (MoE) architectures—a class of models known for their scalability and computational efficiency, which combine traditional neural networks—referred to as experts—through a learnable gating mechanism. We begin by conducting a comprehensive analysis of both dense and sparse gating regimes, demonstrating that the symmetries inherent to MoE architectures are fully characterized by permutations acting on both the expert components and the gating function. Building on these foundational findings, we propose a matching algorithm that enables alignment between independently trained MoEs, thereby facilitating the discovery of LMC. Finally, we empirically validate the presence of LMC using our proposed algorithm across diverse MoE configurations—including dense, sparse, and shared-expert variants—under a wide range of model settings and datasets of varying scales and modalities. Our results confirm the existence of LMC in MoE architectures and offer fundamental insights into the functional landscape and optimization dynamics of deep learning models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=RF3miSqdXa"
        ],
        "venue": [
          "/venue/RF3miSqdXa@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=RF3miSqdXa"
        ],
        "detail": [
          "https://openreview.net/forum?id=RF3miSqdXa"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 6
      },
      "raw_excerpt": "On Linear Mode Connectivity of Mixture-of-Experts Architectures [PDF 9 ] [Copy] [Kimi 6 ] [REL] Authors : Viet-Hoang Tran , Van-Hoan Trinh , Khanh Vinh Bui , Tan Minh Nguyen Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes of neural networks, wherein independently trained models have been observed to be connected—up to permutation symmetries—by linear paths in parameter space along which the loss remains consistently low. This observation challenges classical views of non-convex optimization and has implications for model ensembling, generalization, and our understanding of neural loss geometry. Inspired by recent studies on LMC in standard neural networks, we systematically investigate this phenomenon within Mixture-of-Experts (MoE) architectures—a class of models known for their scalability and computational efficiency, which combine traditional neural networks—referred to as experts—through a learnable gating mechanism. We begin by conducting a comprehensive analysis of both dense and sparse gating regimes, demonstrating that the symmetries inherent to MoE architectures are fully characterized by permutations acting on both the expert components and the gating function. Building on these foundational findings, we propose a matching algorithm that enables alignment between independently trained MoEs, thereby facilitating the discovery of LMC. Finally, we empirically validate the presence of LMC using our proposed algorithm across diverse MoE configurations—including dense, sparse, and shared-expert variants—under a wide range of model settings and datasets of varying scales and modalities. Our results confirm the existence of LMC in MoE architectures and offer fundamental insights into the functional landscape and optimization dynamics of deep learning models. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "0biUwyjKkm@OpenReview",
      "index": 57,
      "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
      "authors": [
        "Zhenhao Zhang",
        "Ye Shi",
        "Lingxiao Yang",
        "Suting Ni",
        "Qi Ye",
        "Jingya Wang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "openhoi",
        "affordance",
        "instructions",
        "language",
        "hoi",
        "multimodal",
        "open",
        "object",
        "hand",
        "synthesis"
      ],
      "summary": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., “Find a water bottle and take a sip”) into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI’s superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=0biUwyjKkm"
        ],
        "venue": [
          "/venue/0biUwyjKkm@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=0biUwyjKkm"
        ],
        "detail": [
          "https://openreview.net/forum?id=0biUwyjKkm"
        ]
      },
      "scores": {
        "pdf": 12,
        "kimi": 4
      },
      "raw_excerpt": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model [PDF 12 ] [Copy] [Kimi 4 ] [REL] Authors : Zhenhao Zhang , Ye Shi , Lingxiao Yang , Suting Ni , Qi Ye , Jingya Wang Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., “Find a water bottle and take a sip”) into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI’s superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "koEALFNBj1@OpenReview",
      "index": 58,
      "title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
      "authors": [
        "Ge Wu",
        "Shen Zhang",
        "Ruijing Shi",
        "Shanghua Gao",
        "Zhenyuan Chen",
        "Lei Wang",
        "Zhaowei Chen",
        "Hongcheng Gao",
        "Yao Tang",
        "jian Yang",
        "Ming-Ming Cheng",
        "Xiang Li"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "textbf",
        "reg",
        "sit",
        "repa",
        "denoising",
        "latents",
        "256",
        "times",
        "image",
        "martinser"
      ],
      "summary": "REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called \\textit{\\textbf{R}epresentation \\textbf{E}ntanglement for \\textbf{G}eneration} (\\textbf{REG}), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256 × × 256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving 63 × 63 × and 23 × 23 × faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ( 10 × 10 × longer). Code is available at: https://github.com/Martinser/REG.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=koEALFNBj1"
        ],
        "venue": [
          "/venue/koEALFNBj1@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=koEALFNBj1"
        ],
        "detail": [
          "https://openreview.net/forum?id=koEALFNBj1"
        ]
      },
      "scores": {
        "pdf": 20,
        "kimi": 9
      },
      "raw_excerpt": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think [PDF 20 ] [Copy] [Kimi 9 ] [REL] Authors : Ge Wu , Shen Zhang , Ruijing Shi , Shanghua Gao , Zhenyuan Chen , Lei Wang , Zhaowei Chen , Hongcheng Gao , Yao Tang , jian Yang , Ming-Ming Cheng , Xiang Li REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called \\textit{\\textbf{R}epresentation \\textbf{E}ntanglement for \\textbf{G}eneration} (\\textbf{REG}), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256 × × 256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving 63 × 63 × and 23 × 23 × faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ( 10 × 10 × longer). Code is available at: https://github.com/Martinser/REG. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "s6k9l5yX8e@OpenReview",
      "index": 59,
      "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
      "authors": [
        "Zihan Wang",
        "Seungjun Lee",
        "Gim Hee Lee"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "dynam3d",
        "navigation",
        "vln",
        "language",
        "vlm",
        "dynamic",
        "layered",
        "environments",
        "exploration",
        "vision"
      ],
      "summary": "Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=s6k9l5yX8e"
        ],
        "venue": [
          "/venue/s6k9l5yX8e@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=s6k9l5yX8e"
        ],
        "detail": [
          "https://openreview.net/forum?id=s6k9l5yX8e"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 4
      },
      "raw_excerpt": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation [PDF 11 ] [Copy] [Kimi 4 ] [REL] Authors : Zihan Wang , Seungjun Lee , Gim Hee Lee Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "NM4emKloy6@OpenReview",
      "index": 60,
      "title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
      "authors": [
        "Andrei Manolache",
        "Luiz F. O. Chamon",
        "Mathias Niepert"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "equivariance",
        "equivariant",
        "constrained",
        "symmetries",
        "optimization",
        "strictly",
        "gradually",
        "data",
        "smooths",
        "departs"
      ],
      "summary": "Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NM4emKloy6"
        ],
        "venue": [
          "/venue/NM4emKloy6@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NM4emKloy6"
        ],
        "detail": [
          "https://openreview.net/forum?id=NM4emKloy6"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 3
      },
      "raw_excerpt": "Learning (Approximately) Equivariant Networks via Constrained Optimization [PDF 9 ] [Copy] [Kimi 3 ] [REL] Authors : Andrei Manolache , Luiz F. O. Chamon , Mathias Niepert Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "jRXgRC6fu7@OpenReview",
      "index": 61,
      "title": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding",
      "authors": [
        "Yuan Zang",
        "Zitian Tang",
        "Junho Cho",
        "Jaewook Yoo",
        "Chen Sun"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "sage",
        "state",
        "objects",
        "language",
        "actions",
        "recognition",
        "action",
        "graph",
        "unified",
        "unseen"
      ],
      "summary": "Recognizing the physical states of objects and their transformations within videos is crucial for structured video understanding and enabling robust real-world applications, such as robotic manipulation. However, pretrained vision-language models often struggle to capture these nuanced dynamics and their temporal context, and specialized object state recognition frameworks may not generalize to unseen actions or objects. We introduce SAGE (State-Action Graph Embeddings), a novel framework that offers a unified model of physical state transitions by decomposing states into fine-grained, language-described visual concepts that are sharable across different objects and actions. SAGE initially leverages Large Language Models to construct a State-Action Graph, which is then multimodally refined using Vision-Language Models. Extensive experiments show that our method significantly outperforms baselines, generalizes effectively to unseen objects and actions in open-world settings. SAGE improves the prior state-of-the-art by as much as 14.6% on novel state recognition with less than 5% of its inference time.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=jRXgRC6fu7"
        ],
        "venue": [
          "/venue/jRXgRC6fu7@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=jRXgRC6fu7"
        ],
        "detail": [
          "https://openreview.net/forum?id=jRXgRC6fu7"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 3
      },
      "raw_excerpt": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding [PDF 9 ] [Copy] [Kimi 3 ] [REL] Authors : Yuan Zang , Zitian Tang , Junho Cho , Jaewook Yoo , Chen Sun Recognizing the physical states of objects and their transformations within videos is crucial for structured video understanding and enabling robust real-world applications, such as robotic manipulation. However, pretrained vision-language models often struggle to capture these nuanced dynamics and their temporal context, and specialized object state recognition frameworks may not generalize to unseen actions or objects. We introduce SAGE (State-Action Graph Embeddings), a novel framework that offers a unified model of physical state transitions by decomposing states into fine-grained, language-described visual concepts that are sharable across different objects and actions. SAGE initially leverages Large Language Models to construct a State-Action Graph, which is then multimodally refined using Vision-Language Models. Extensive experiments show that our method significantly outperforms baselines, generalizes effectively to unseen objects and actions in open-world settings. SAGE improves the prior state-of-the-art by as much as 14.6% on novel state recognition with less than 5% of its inference time. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "4OsgYD7em5@OpenReview",
      "index": 62,
      "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
      "authors": [
        "Yang Yue",
        "Zhiqi Chen",
        "Rui Lu",
        "Andrew Zhao",
        "Zhaokai Wang",
        "Shiji Song",
        "Gao Huang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "rlvr",
        "reasoning",
        "base",
        "llms",
        "textit",
        "abilities",
        "elicit",
        "genuinely",
        "models",
        "incentivize"
      ],
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \\textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\\textit{k} at large \\textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \\emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of k k (\\eg, k k =1), base models achieve higher pass@ k k score when k k is large. Moreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses. Further coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models' sampling distribution, suggesting that their reasoning abilities originate from and are \\textit{bounded} by the base model. From this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model. In contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities. Taken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=4OsgYD7em5"
        ],
        "venue": [
          "/venue/4OsgYD7em5@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=4OsgYD7em5"
        ],
        "detail": [
          "https://openreview.net/forum?id=4OsgYD7em5"
        ]
      },
      "scores": {
        "pdf": 24,
        "kimi": 24
      },
      "raw_excerpt": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? [PDF 24 ] [Copy] [Kimi 24 ] [REL] Authors : Yang Yue , Zhiqi Chen , Rui Lu , Andrew Zhao , Zhaokai Wang , Yang Yue , Shiji Song , Gao Huang Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \\textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\\textit{k} at large \\textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \\emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of k k (\\eg, k k =1), base models achieve higher pass@ k k score when k k is large. Moreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses. Further coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models' sampling distribution, suggesting that their reasoning abilities originate from and are \\textit{bounded} by the base model. From this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model. In contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities. Taken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "s6YHno8Ke3@OpenReview",
      "index": 63,
      "title": "Learning to Learn with Contrastive Meta-Objective",
      "authors": [
        "Shiguang Wu",
        "Yaqing Wang",
        "Yatao Bian",
        "Quanming Yao"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "meta",
        "conml",
        "contrastive",
        "learners",
        "learning",
        "objective",
        "training",
        "identity",
        "supervision",
        "learn"
      ],
      "summary": "Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. Different meta-learning approaches all work under/with the mini-batch episodic training framework. Such framework naturally gives the information about task identity, which can serve as additional supervision for meta-training to improve generalizability. We propose to exploit task identity as additional supervision in meta-training, inspired by the alignment and discrimination ability which is is intrinsic in human's fast learning. This is achieved by contrasting what meta-learners learn, i.e., model representations. The proposed ConML is evaluating and optimizing the contrastive meta-objective under a problem- and learner-agnostic meta-training framework. We demonstrate that ConML integrates seamlessly with existing meta-learners, as well as in-context learning models, and brings significant boost in performance with small implementation cost.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=s6YHno8Ke3"
        ],
        "venue": [
          "/venue/s6YHno8Ke3@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=s6YHno8Ke3"
        ],
        "detail": [
          "https://openreview.net/forum?id=s6YHno8Ke3"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 8
      },
      "raw_excerpt": "Learning to Learn with Contrastive Meta-Objective [PDF 7 ] [Copy] [Kimi 8 ] [REL] Authors : Shiguang Wu , Yaqing Wang , Yatao Bian , Quanming Yao Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. Different meta-learning approaches all work under/with the mini-batch episodic training framework. Such framework naturally gives the information about task identity, which can serve as additional supervision for meta-training to improve generalizability. We propose to exploit task identity as additional supervision in meta-training, inspired by the alignment and discrimination ability which is is intrinsic in human's fast learning. This is achieved by contrasting what meta-learners learn, i.e., model representations. The proposed ConML is evaluating and optimizing the contrastive meta-objective under a problem- and learner-agnostic meta-training framework. We demonstrate that ConML integrates seamlessly with existing meta-learners, as well as in-context learning models, and brings significant boost in performance with small implementation cost. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "JFygzwx8SJ@OpenReview",
      "index": 64,
      "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "authors": [
        "Jang-Hyun Kim",
        "Jinuk Kim",
        "Sangwoo Kwon",
        "Jae W. Lee",
        "Sangdoo Yun",
        "Hyun Oh Song"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "kvzip",
        "cache",
        "query",
        "eviction",
        "context",
        "agnostic",
        "pairs",
        "latency",
        "evicting",
        "qwen2"
      ],
      "summary": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \\textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3 3 - 4 × 4 × and FlashAttention decoding latency by approximately 2 × 2 × , with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\\% cache budget ratio under multi-query scenarios.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JFygzwx8SJ"
        ],
        "venue": [
          "/venue/JFygzwx8SJ@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JFygzwx8SJ"
        ],
        "detail": [
          "https://openreview.net/forum?id=JFygzwx8SJ"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 10
      },
      "raw_excerpt": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction [PDF 17 ] [Copy] [Kimi 10 ] [REL] Authors : Jang-Hyun Kim , Jinuk Kim , Sangwoo Kwon , Jae W. Lee , Sangdoo Yun , Hyun Oh Song Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \\textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3 3 - 4 × 4 × and FlashAttention decoding latency by approximately 2 × 2 × , with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\\% cache budget ratio under multi-query scenarios. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "NM8Apk61NA@OpenReview",
      "index": 65,
      "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
      "authors": [
        "Zelin Peng",
        "Zhengqin Xu",
        "Qingyang Liu",
        "Xiaokang Yang",
        "Wei Shen"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "granularity",
        "mllms",
        "hyperbolic",
        "alg",
        "matrices",
        "textual",
        "modal",
        "hyperet",
        "levels",
        "language"
      ],
      "summary": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with Möbius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=NM8Apk61NA"
        ],
        "venue": [
          "/venue/NM8Apk61NA@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=NM8Apk61NA"
        ],
        "detail": [
          "https://openreview.net/forum?id=NM8Apk61NA"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 13
      },
      "raw_excerpt": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models [PDF 17 ] [Copy] [Kimi 13 ] [REL] Authors : Zelin Peng , Zhengqin Xu , Qingyang Liu , Xiaokang Yang , Wei Shen Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with Möbius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "zwCb9cKHpd@OpenReview",
      "index": 66,
      "title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
      "authors": [
        "Mingfei Chen",
        "Zijun Cui",
        "Xiulong Liu",
        "Jinlin Xiang",
        "Caleb Zheng",
        "Jingyuan Li",
        "Eli Shlizerman"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "savvy",
        "audio",
        "spatial",
        "llms",
        "visual",
        "reasoning",
        "dynamic",
        "bench",
        "map",
        "queried"
      ],
      "summary": "3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of carefully curated question–answer pairs probing both directional and distance relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zwCb9cKHpd"
        ],
        "venue": [
          "/venue/zwCb9cKHpd@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zwCb9cKHpd"
        ],
        "detail": [
          "https://openreview.net/forum?id=zwCb9cKHpd"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 5
      },
      "raw_excerpt": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing [PDF 9 ] [Copy] [Kimi 5 ] [REL] Authors : Mingfei Chen , Zijun Cui , Xiulong Liu , Jinlin Xiang , Caleb Zheng , Jingyuan Li , Eli Shlizerman 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of carefully curated question–answer pairs probing both directional and distance relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "WCRPgBpbcA@OpenReview",
      "index": 67,
      "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
      "authors": [
        "Giuseppe Bruno",
        "Federico Pasqualotto",
        "Andrea Agazzi"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "collapses",
        "multiscale",
        "moderate",
        "regime",
        "tokens",
        "mean",
        "interaction",
        "clusters",
        "exemplifying",
        "transformers"
      ],
      "summary": "In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number N N of tokens is large and the inverse temperature parameter β β of the model scales together with N N . In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=WCRPgBpbcA"
        ],
        "venue": [
          "/venue/WCRPgBpbcA@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=WCRPgBpbcA"
        ],
        "detail": [
          "https://openreview.net/forum?id=WCRPgBpbcA"
        ]
      },
      "scores": {
        "pdf": 6,
        "kimi": 4
      },
      "raw_excerpt": "A multiscale analysis of mean-field transformers in the moderate interaction regime [PDF 6 ] [Copy] [Kimi 4 ] [REL] Authors : Giuseppe Bruno , Federico Pasqualotto , Andrea Agazzi In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number N N of tokens is large and the inverse temperature parameter β β of the model scales together with N N . In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "CaSQgef484@OpenReview",
      "index": 68,
      "title": "Exploring Diffusion Transformer Designs via Grafting",
      "authors": [
        "Keshigeyan Chandrasegaran",
        "Michael Poli",
        "Daniel Y Fu",
        "Dongjun Kim",
        "Lea M. Hadzic",
        "Manling Li",
        "Agrim Gupta",
        "Stefano Massaroli",
        "Azalia Mirhoseini",
        "Juan Carlos Niebles",
        "Stefano Ermon",
        "Li Fei-Fei"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "grafting",
        "designs",
        "dit",
        "dits",
        "attention",
        "pretrained",
        "quality",
        "fid",
        "depth",
        "diffusion"
      ],
      "summary": "Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present *grafting*, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38–2.64 vs. 2.27 for DiT-XL/2) using < 2 < 2 % pretraining compute. We then graft a text-to-image model (PixArt- Σ Σ ), achieving a 1.43 × × speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2 × × and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=CaSQgef484"
        ],
        "venue": [
          "/venue/CaSQgef484@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=CaSQgef484"
        ],
        "detail": [
          "https://openreview.net/forum?id=CaSQgef484"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 8
      },
      "raw_excerpt": "Exploring Diffusion Transformer Designs via Grafting [PDF 5 ] [Copy] [Kimi 8 ] [REL] Authors : Keshigeyan Chandrasegaran , Michael Poli , Daniel Y Fu , Dongjun Kim , Lea M. Hadzic , Manling Li , Agrim Gupta , Stefano Massaroli , Azalia Mirhoseini , Juan Carlos Niebles , Stefano Ermon , Li Fei-Fei Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present *grafting*, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38–2.64 vs. 2.27 for DiT-XL/2) using < 2 < 2 % pretraining compute. We then graft a text-to-image model (PixArt- Σ Σ ), achieving a 1.43 × × speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2 × × and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "rMdf8jhLR7@OpenReview",
      "index": 69,
      "title": "Generalized Gradient Norm Clipping & Non-Euclidean ( L 0 , L 1 ) ( L 0 , L 1 ) -Smoothness",
      "authors": [
        "Thomas Pethick",
        "Wanyun Xie",
        "Mete Erdogan",
        "Kimon Antonakopoulos",
        "Tony Silveti-Falls",
        "Volkan Cevher"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "clipping",
        "gradient",
        "smoothness",
        "scion",
        "norm",
        "euclidean",
        "descent",
        "generalized",
        "epfl",
        "clippedscion"
      ],
      "summary": "This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ( L 0 L 0 , L 1 L 1 )-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal O ( n − 1 / 4 ) O ( n − 1 / 4 ) convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=rMdf8jhLR7"
        ],
        "venue": [
          "/venue/rMdf8jhLR7@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=rMdf8jhLR7"
        ],
        "detail": [
          "https://openreview.net/forum?id=rMdf8jhLR7"
        ]
      },
      "scores": {
        "pdf": 5,
        "kimi": 10
      },
      "raw_excerpt": "Generalized Gradient Norm Clipping & Non-Euclidean ( L 0 , L 1 ) ( L 0 , L 1 ) -Smoothness [PDF 5 ] [Copy] [Kimi 10 ] [REL] Authors : Thomas Pethick , Wanyun Xie , Mete Erdogan , Kimon Antonakopoulos , Tony Silveti-Falls , Volkan Cevher This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ( L 0 L 0 , L 1 L 1 )-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal O ( n − 1 / 4 ) O ( n − 1 / 4 ) convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "Q6IyUpBmrG@OpenReview",
      "index": 70,
      "title": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion",
      "authors": [
        "Qing-Yuan Jiang",
        "Longfei Huang",
        "Yang Yang"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "multimodal",
        "disproportion",
        "classification",
        "ability",
        "modalities",
        "boosting",
        "mitigating",
        "imbalance",
        "njustkmg",
        "learning"
      ],
      "summary": "Multimodal learning (MML) is significantly constrained by modality imbalance, leading to suboptimal performance in practice. While existing approaches primarily focus on balancing the learning of different modalities to address this issue, they fundamentally overlook the inherent disproportion in model classification ability, which serves as the primary cause of this phenomenon. In this paper, we propose a novel multimodal learning approach to dynamically balance the classification ability of weak and strong modalities by incorporating the principle of boosting. Concretely, we first propose a sustained boosting algorithm in multimodal learning by simultaneously optimizing the classification and residual errors. Subsequently, we introduce an adaptive classifier assignment strategy to dynamically facilitate the classification performance of the weak modality. Furthermore, we theoretically analyze the convergence property of the cross-modal gap function, ensuring the effectiveness of the proposed boosting scheme. To this end, the classification ability of strong and weak modalities is expected to be balanced, thereby mitigating the imbalance issue. Empirical experiments on widely used datasets reveal the superiority of our method through comparison with various state-of-the-art (SOTA) multimodal learning baselines. The source code is available at https://github.com/njustkmg/NeurIPS25-AUG.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Q6IyUpBmrG"
        ],
        "venue": [
          "/venue/Q6IyUpBmrG@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Q6IyUpBmrG"
        ],
        "detail": [
          "https://openreview.net/forum?id=Q6IyUpBmrG"
        ]
      },
      "scores": {
        "pdf": 11,
        "kimi": 8
      },
      "raw_excerpt": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion [PDF 11 ] [Copy] [Kimi 8 ] [REL] Authors : Qing-Yuan Jiang , Longfei Huang , Yang Yang Multimodal learning (MML) is significantly constrained by modality imbalance, leading to suboptimal performance in practice. While existing approaches primarily focus on balancing the learning of different modalities to address this issue, they fundamentally overlook the inherent disproportion in model classification ability, which serves as the primary cause of this phenomenon. In this paper, we propose a novel multimodal learning approach to dynamically balance the classification ability of weak and strong modalities by incorporating the principle of boosting. Concretely, we first propose a sustained boosting algorithm in multimodal learning by simultaneously optimizing the classification and residual errors. Subsequently, we introduce an adaptive classifier assignment strategy to dynamically facilitate the classification performance of the weak modality. Furthermore, we theoretically analyze the convergence property of the cross-modal gap function, ensuring the effectiveness of the proposed boosting scheme. To this end, the classification ability of strong and weak modalities is expected to be balanced, thereby mitigating the imbalance issue. Empirical experiments on widely used datasets reveal the superiority of our method through comparison with various state-of-the-art (SOTA) multimodal learning baselines. The source code is available at https://github.com/njustkmg/NeurIPS25-AUG. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "XQ87Vo9GIz@OpenReview",
      "index": 71,
      "title": "TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability",
      "authors": [
        "Tonglong Wei",
        "Yan Lin",
        "Zeyu Zhou",
        "Haomin Wen",
        "Jilin Hu",
        "Shengnan Guo",
        "Youfang Lin",
        "Gao Cong",
        "Huaiyu Wan"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "transfertraj",
        "transferability",
        "region",
        "task",
        "vehicle",
        "transfer",
        "trajectory",
        "retraining",
        "tasks",
        "spatial"
      ],
      "summary": "Vehicle GPS trajectories provide valuable movement information that supports various downstream tasks and applications. A desirable trajectory learning model should be able to transfer across regions and tasks without retraining, avoiding the need to maintain multiple specialized models and subpar performance with limited training data. However, each region has its unique spatial features and contexts, which are reflected in vehicle movement patterns and are difficult to generalize. Additionally, transferring across different tasks faces technical challenges due to the varying input-output structures required for each task. Existing efforts towards transferability primarily involve learning embedding vectors for trajectories, which perform poorly in region transfer and require retraining of prediction modules for task transfer. To address these challenges, we propose TransferTraj TransferTraj , a vehicle GPS trajectory learning model that excels in both region and task transferability. For region transferability, we introduce RTTE as the main learnable module within TransferTraj. It integrates spatial, temporal, POI, and road network modalities of trajectories to effectively manage variations in spatial context distribution across regions. It also introduces a TRIE module for incorporating relative information of spatial features and a spatial context MoE module for handling movement patterns in diverse contexts. For task transferability, we propose a task-transferable input-output scheme that unifies the input-output structure of different tasks into the masking and recovery of modalities and trajectory points. This approach allows TransferTraj to be pre-trained once and transferred to different tasks without retraining. We conduct extensive experiments on three real-world vehicle trajectory datasets under various transfer settings, including task transfer, zero-shot region transfer, and few-shot region transfer. Experimental results demonstrate that TransferTraj significantly outperforms state-of-the-art baselines in different scenarios, validating its effectiveness in region and task transfer. Code is available at https://github.com/wtl52656/TransferTraj.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=XQ87Vo9GIz"
        ],
        "venue": [
          "/venue/XQ87Vo9GIz@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=XQ87Vo9GIz"
        ],
        "detail": [
          "https://openreview.net/forum?id=XQ87Vo9GIz"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 14
      },
      "raw_excerpt": "TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability [PDF 9 ] [Copy] [Kimi 14 ] [REL] Authors : Tonglong Wei , Yan Lin , Zeyu Zhou , Haomin Wen , Jilin Hu , Shengnan Guo , Youfang Lin , Gao Cong , Huaiyu Wan Vehicle GPS trajectories provide valuable movement information that supports various downstream tasks and applications. A desirable trajectory learning model should be able to transfer across regions and tasks without retraining, avoiding the need to maintain multiple specialized models and subpar performance with limited training data. However, each region has its unique spatial features and contexts, which are reflected in vehicle movement patterns and are difficult to generalize. Additionally, transferring across different tasks faces technical challenges due to the varying input-output structures required for each task. Existing efforts towards transferability primarily involve learning embedding vectors for trajectories, which perform poorly in region transfer and require retraining of prediction modules for task transfer. To address these challenges, we propose TransferTraj TransferTraj , a vehicle GPS trajectory learning model that excels in both region and task transferability. For region transferability, we introduce RTTE as the main learnable module within TransferTraj. It integrates spatial, temporal, POI, and road network modalities of trajectories to effectively manage variations in spatial context distribution across regions. It also introduces a TRIE module for incorporating relative information of spatial features and a spatial context MoE module for handling movement patterns in diverse contexts. For task transferability, we propose a task-transferable input-output scheme that unifies the input-output structure of different tasks into the masking and recovery of modalities and trajectory points. This approach allows TransferTraj to be pre-trained once and transferred to different tasks without retraining. We conduct extensive experiments on three real-world vehicle trajectory datasets under various transfer settings, including task transfer, zero-shot region transfer, and few-shot region transfer. Experimental results demonstrate that TransferTraj significantly outperforms state-of-the-art baselines in different scenarios, validating its effectiveness in region and task transfer. Code is available at https://github.com/wtl52656/TransferTraj. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "zIzZxDsNNP@OpenReview",
      "index": 72,
      "title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing",
      "authors": [
        "Yuezhou Ma",
        "Haixu Wu",
        "Hang Zhou",
        "Huikun Weng",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "physense",
        "placements",
        "sensor",
        "placement",
        "sensing",
        "physics",
        "sparse",
        "reconstruction",
        "stage",
        "thuml"
      ],
      "summary": "Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. Leveraging the reconstruction feedback, the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees. Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered. Code is available at this repository: https://github.com/thuml/PhySense.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=zIzZxDsNNP"
        ],
        "venue": [
          "/venue/zIzZxDsNNP@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=zIzZxDsNNP"
        ],
        "detail": [
          "https://openreview.net/forum?id=zIzZxDsNNP"
        ]
      },
      "scores": {
        "pdf": 9,
        "kimi": 7
      },
      "raw_excerpt": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing [PDF 9 ] [Copy] [Kimi 7 ] [REL] Authors : Yuezhou Ma , Haixu Wu , Hang Zhou , Huikun Weng , Jianmin Wang , Mingsheng Long Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. Leveraging the reconstruction feedback, the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees. Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered. Code is available at this repository: https://github.com/thuml/PhySense. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "JcEqp4aPmb@OpenReview",
      "index": 73,
      "title": "InfinityStar: Uniﬁed Spacetime AutoRegressive Modeling for Visual Generation",
      "authors": [
        "Jinlai Liu",
        "Jian Han",
        "Bin Yan",
        "Wuhui",
        "Fengda Zhu",
        "Xing Wang",
        "Yi Jiang",
        "BINGYUE PENG",
        "Zehuan Yuan"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "infinitystar",
        "video",
        "autoregressive",
        "720p",
        "uniﬁed",
        "generation",
        "vbench",
        "spacetime",
        "modeling",
        "unified"
      ],
      "summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long-duration video synthesis via straightforward temporal autoregression. Through extensive experiments, InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10 × × faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial-level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=JcEqp4aPmb"
        ],
        "venue": [
          "/venue/JcEqp4aPmb@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=JcEqp4aPmb"
        ],
        "detail": [
          "https://openreview.net/forum?id=JcEqp4aPmb"
        ]
      },
      "scores": {
        "pdf": 18,
        "kimi": 11
      },
      "raw_excerpt": "InfinityStar: Uniﬁed Spacetime AutoRegressive Modeling for Visual Generation [PDF 18 ] [Copy] [Kimi 11 ] [REL] Authors : Jinlai Liu , Jian Han , Bin Yan , Wuhui , Fengda Zhu , Xing Wang , Yi Jiang , BINGYUE PENG , Zehuan Yuan We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long-duration video synthesis via straightforward temporal autoregression. Through extensive experiments, InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10 × × faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial-level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "gL4muAFwsh@OpenReview",
      "index": 74,
      "title": "Does Stochastic Gradient really succeed for bandits?",
      "authors": [
        "Dorian Baudry",
        "Emmeran Johnson",
        "Simon Vary",
        "Ciara Pike-Burke",
        "Patrick Rebeschini"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "regret",
        "sgb",
        "bandits",
        "logarithmic",
        "gradient",
        "bandit",
        "armed",
        "succeed",
        "learning",
        "rate"
      ],
      "summary": "Recent works of Mei et al. (2023, 2024) have deepened the theoretical understanding of the *Stochastic Gradient Bandit* (SGB) policy, showing that using a constant learning rate guarantees asymptotic convergence to the optimal policy, and that sufficiently *small* learning rates can yield logarithmic regret. However, whether logarithmic regret holds beyond small learning rates remains unclear. In this work, we take a step towards characterizing the regret *regimes* of SGB as a function of its learning rate. For two--armed bandits, we identify a sharp threshold, scaling with the sub-optimality gap Δ Δ , below which SGB achieves *logarithmic* regret on all instances, and above which it can incur *polynomial* regret on some instances. This result highlights the necessity of knowing (or estimating) Δ Δ to ensure logarithmic regret with a constant learning rate. For general K K -armed bandits, we further show the learning rate must scale inversely with K K to avoid polynomial regret. We introduce novel techniques to derive regret upper bounds for SGB, laying the groundwork for future advances in the theory of gradient-based bandit algorithms.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=gL4muAFwsh"
        ],
        "venue": [
          "/venue/gL4muAFwsh@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=gL4muAFwsh"
        ],
        "detail": [
          "https://openreview.net/forum?id=gL4muAFwsh"
        ]
      },
      "scores": {
        "pdf": 7,
        "kimi": 6
      },
      "raw_excerpt": "Does Stochastic Gradient really succeed for bandits? [PDF 7 ] [Copy] [Kimi 6 ] [REL] Authors : Dorian Baudry , Emmeran Johnson , Simon Vary , Ciara Pike-Burke , Patrick Rebeschini Recent works of Mei et al. (2023, 2024) have deepened the theoretical understanding of the *Stochastic Gradient Bandit* (SGB) policy, showing that using a constant learning rate guarantees asymptotic convergence to the optimal policy, and that sufficiently *small* learning rates can yield logarithmic regret. However, whether logarithmic regret holds beyond small learning rates remains unclear. In this work, we take a step towards characterizing the regret *regimes* of SGB as a function of its learning rate. For two--armed bandits, we identify a sharp threshold, scaling with the sub-optimality gap Δ Δ , below which SGB achieves *logarithmic* regret on all instances, and above which it can incur *polynomial* regret on some instances. This result highlights the necessity of knowing (or estimating) Δ Δ to ensure logarithmic regret with a constant learning rate. For general K K -armed bandits, we further show the learning rate must scale inversely with K K to avoid polynomial regret. We introduce novel techniques to derive regret upper bounds for SGB, laying the groundwork for future advances in the theory of gradient-based bandit algorithms. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "INqBOmwIpG@OpenReview",
      "index": 75,
      "title": "Perception Encoder: The best visual embeddings are not at the output of the network",
      "authors": [
        "Daniel Bolya",
        "Po-Yao Huang",
        "Peize Sun",
        "Jang Hyun Cho",
        "Andrea Madotto",
        "Chen Wei",
        "Tengyu Ma",
        "Jiale Zhi",
        "Jathushan Rajasegaran",
        "Hanoona Abdul Rasheed",
        "Junke Wang",
        "Marco Monteiro",
        "Hu Xu",
        "Shiyu Dong",
        "Nikhila Ravi",
        "Shang-Wen Li",
        "Piotr Dollar",
        "Christoph Feichtenhofer"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "perception",
        "video",
        "embeddings",
        "tasks",
        "vision",
        "alignment",
        "encoders",
        "pretraining",
        "encoder",
        "image"
      ],
      "summary": "We introduce Perception Encoder (PE), a family of state-of-the-art vision encoders for image and video understanding. Traditionally, vision encoders have relied on a variety of pretraining objectives, each excelling at different downstream tasks. Surprisingly, after scaling a carefully tuned image pretraining recipe and refining with a robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves state-of-the-art results on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, tracking, and depth estimation. We release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=INqBOmwIpG"
        ],
        "venue": [
          "/venue/INqBOmwIpG@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=INqBOmwIpG"
        ],
        "detail": [
          "https://openreview.net/forum?id=INqBOmwIpG"
        ]
      },
      "scores": {
        "pdf": 25,
        "kimi": 13
      },
      "raw_excerpt": "Perception Encoder: The best visual embeddings are not at the output of the network [PDF 25 ] [Copy] [Kimi 13 ] [REL] Authors : Daniel Bolya , Po-Yao Huang , Peize Sun , Jang Hyun Cho , Andrea Madotto , Chen Wei , Tengyu Ma , Jiale Zhi , Jathushan Rajasegaran , Hanoona Abdul Rasheed , Junke Wang , Marco Monteiro , Hu Xu , Shiyu Dong , Nikhila Ravi , Shang-Wen Li , Piotr Dollar , Christoph Feichtenhofer We introduce Perception Encoder (PE), a family of state-of-the-art vision encoders for image and video understanding. Traditionally, vision encoders have relied on a variety of pretraining objectives, each excelling at different downstream tasks. Surprisingly, after scaling a carefully tuned image pretraining recipe and refining with a robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves state-of-the-art results on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, tracking, and depth estimation. We release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "Gq4Gay8rDB@OpenReview",
      "index": 76,
      "title": "PlayerOne: Egocentric World Simulator",
      "authors": [
        "Yuanpeng Tu",
        "Hao Luo",
        "Xi Chen",
        "Xiang Bai",
        "Fan Wang",
        "Hengshuang Zhao"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "egocentric",
        "playerone",
        "world",
        "video",
        "exocentric",
        "scene",
        "simulator",
        "movements",
        "motion",
        "coarse"
      ],
      "summary": "We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real-scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and world-consistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=Gq4Gay8rDB"
        ],
        "venue": [
          "/venue/Gq4Gay8rDB@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=Gq4Gay8rDB"
        ],
        "detail": [
          "https://openreview.net/forum?id=Gq4Gay8rDB"
        ]
      },
      "scores": {
        "pdf": 17,
        "kimi": 9
      },
      "raw_excerpt": "PlayerOne: Egocentric World Simulator [PDF 17 ] [Copy] [Kimi 9 ] [REL] Authors : Yuanpeng Tu , Hao Luo , Xi Chen , Xiang Bai , Fan Wang , Hengshuang Zhao We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real-scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and world-consistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications. Subject : NeurIPS.2025 - Oral"
    },
    {
      "paper_id": "uWj4s7rMnR@OpenReview",
      "index": 77,
      "title": "Mean Flows for One-step Generative Modeling",
      "authors": [
        "Zhengyang Geng",
        "Mingyang Deng",
        "Xingjian Bai",
        "J Zico Kolter",
        "Kaiming He"
      ],
      "subjects": [
        "NeurIPS.2025 - Oral"
      ],
      "keywords": [
        "meanflow",
        "step",
        "flow",
        "256",
        "generative",
        "instantaneous",
        "nfe",
        "modeling",
        "one",
        "narrows"
      ],
      "summary": "We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the \\textit{MeanFlow} model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256 × × 256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.",
      "session": null,
      "time": null,
      "links": {
        "link": [
          "https://openreview.net/forum?id=uWj4s7rMnR"
        ],
        "venue": [
          "/venue/uWj4s7rMnR@OpenReview",
          "/venue/NeurIPS.2025?group=Oral"
        ],
        "pdf": [
          "https://openreview.net/pdf?id=uWj4s7rMnR"
        ],
        "detail": [
          "https://openreview.net/forum?id=uWj4s7rMnR"
        ]
      },
      "scores": {
        "pdf": 27,
        "kimi": 14
      },
      "raw_excerpt": "Mean Flows for One-step Generative Modeling [PDF 27 ] [Copy] [Kimi 14 ] [REL] Authors : Zhengyang Geng , Mingyang Deng , Xingjian Bai , J Zico Kolter , Kaiming He We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the \\textit{MeanFlow} model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256 × × 256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models. Subject : NeurIPS.2025 - Oral"
    }
  ]
}